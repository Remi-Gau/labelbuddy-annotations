{"text": "Jaberi-Douraki, Majid and Taghian Dinani, Soudabeh and Millagaha Gedara, Nuwan Indika and Xu, Xuan and Richards, Emily and Maunsell, Fiona and Zad, Nader and Tell, Lisa A.\nFront Vet Sci, 2021\n\n# Title\n\nLarge-Scale Data Mining of Rapid Residue Detection Assay Data From HTML and PDF Documents: Improving Data Access and Visualization for Veterinarians\n\n# Keywords\n\nMRL and tolerance\ncommercial rapid assay test\nmachine learning\nlarge scale data mining\ntable extraction\ntable classification\nartificial intelligence\nextra-label drug use\n\n\n# Abstract\n \nExtra-label drug use in food animal medicine is authorized by the US Animal Medicinal Drug Use Clarification Act (AMDUCA), and estimated withdrawal intervals are based on published scientific pharmacokinetic data. Occasionally there is a paucity of scientific data on which to base a withdrawal interval or a large number of animals being treated, driving the need to test for drug residues. Rapid assay commercial farm-side tests are essential for monitoring drug residues in animal products to protect human health. Active ingredients, sensitivity, matrices, and species that have been evaluated for commercial rapid assay tests are typically reported on manufacturers' websites or in PDF documents that are available to consumers but may require a special access request. Additionally, this information is not always correlated with FDA-approved tolerances. Furthermore, parameter changes for these tests can be very challenging to regularly identify, especially those listed on websites or in documents that are not publicly available. Therefore, artificial intelligence plays a critical role in efficiently extracting the data and ensure current information. Extracting tables from PDF and HTML documents has been investigated both by academia and commercial tool builders. Research in text mining of such documents has become a widespread yet challenging arena in implementing natural language programming. However, techniques of extracting tables are still in their infancy and being investigated and improved by researchers. In this study, we developed and evaluated a data-mining method for automatically extracting rapid assay data from electronic documents. Our automatic electronic data extraction method includes a software package module, a developed pattern recognition tool, and a data mining engine. Assay details were provided by several commercial entities that produce these rapid drug residue assay tests. During this study, we developed a real-time conversion system and method for reflowing contents in these files for accessibility practice and research data mining. Embedded information was extracted using an AI technology for text extraction and text mining to convert to structured formats. These data were then made available to veterinarians and producers via an online interface, allowing interactive searching and also presenting the commercial test assay parameters in reference to FDA-approved tolerances. \n \n\n# Body\n \n## Introduction \n  \nDrug residue testing is an essential tool to ensure that animal products intended for human consumption are free of violative residues ( ). These tests are most commonly used to test the milk before it is added to the bulk tank but some tests can also be used to evaluate other matrices (urine, serum, eggs, and honey) to ensure that drug concentrations are below the tolerance after the withdrawal interval has been observed. Tolerance refers to the maximum acceptable level of a chemical residue present in food products from an exposed/treated animal, which is determined by the Food and Drug Administration ( ). Rapid quantitative drug detection has largely been applied to help minimize drug residue risks and maintain milk quality ( \u2013 ). To use these tests, information for different commodities including cow, swine, goat, sheep, camel, horse, and buffalo and matrices including serum, urine, milk, and honey is necessary and have been documented for approximately 100 different rapid test assays ( ,  ,  ). Contents of these documents are mostly published in semi or unstructured portable document format (PDF) files or hypertext markup language (HTML) documents which do not allow for interactive searching and easy comparison to tolerance limits. \n\nExtracting tables from PDF files and HTML documents has been investigated both by academia and commercial tool builders ( ). However, the techniques of extracting tables from PDF and HTML are still open and new techniques including dynamic and automatic statistical text mining are currently being studied by the researchers ( \u2013 ). Besides, techniques for extracting tables with HTML in Web pages ( \u2013 ) are different than identifying tables from scanned documents ( ,  ). Moreover, PDF documents have no specific table markups; text-based PDF documents need parsing procedures that are modified for processing the raw PDF format ( ,  ). On the other hand, table extraction techniques tend to be devised concerning the application context. For instance, the Web Table Corpora focuses on knowledge base construction ( ) and deals with matching table instances to concepts in DBpedia or Wikidata ( ). In particular, the text-based PDF processing work is impacted by a lack of standardized schema for PDF parsers, leading to limited applicability. \n\nThe HTML markups for tables are used for page layout, and only a subset of the \u201ctables\u201d actually contain tabular data ( ). However, it also focuses on separating table data from layout instructions and provides large-scale Web table data classified from general-purpose Web crawl data ( ). The processing of Web table can be categorized into two categories: table search and knowledge base construction, one of the most commonly used approaches which is a keyword-based technique that ranks extracted tables based on table content ( ). Next, matching tables can be applied to complete or extend a table itself ( ). Thus, these techniques use a machine learning approach to leverage the relationships amongst rows and columns. \n\nIn the current study, we developed a data-mining method to automatically extract commercial rapid assay data from electronic documents to ensure accurate data and so the data could be searched using an interactive interface. Our automatic electronic data extraction system comprises a software package module, a developed pattern recognition tool, and a data mining engine. Data was harvested from online websites or generously supplied by manufacturers for the available commercial assays. Some of the data are sporadically reported online but most were published in semi or unstructured PDF files or HTML documents. We developed a real-time conversion system and method for reflowing contents in these PDF and HTML files for accessibility practice and quantitative research data mining. Embedded information was extracted using an artificial intelligence (AI) technology for text extraction and text mining to convert to structured formats. The data were usually hidden in the main text and mostly in the form of a tabulated summary. \n\n\n## Materials and Methods \n  \nThere are two main methodological steps in this study, one is to extract the desired information from specified documents, and the second is to update the previously available information based on these new values. In the following, we show a summary of our workflow integrated by machine learning where first some preprocessing points are presented, and then these steps are provided in detail for data extraction ( ). \n  \nThe action workflow of text mining for HTML and PDF documents from preprocessing and dictionary creation to storing tables in the database. \n  \n### Real-Time Data Collection via PDF and Webpage Parsing \n  \nTwo formats of documents extracted from Web sources were virtually imported for this study: files (34 tables out of 60 relevant pdf files, there were also over 180 more PDFs either did not have tables or extracted tables did not have relevant fields) were produced and made available online by seven manufacturers in PDF formats while other tables were presented by HTML documents on Web sources. Automating the extraction of data from structured HTML tables was implemented with easy-to-use and effective web data extraction techniques. However, the contents of these reports are mostly published in unstructured PDF files which were also the main challenge in our research settings for extracting information. \n\nIn our work, the text-mining and information-retrieval models for rapid test assays were trained to curate data from manufacturer and producer manuals for drug and contaminant residues that may be found in dairy products, serum, urine, eggs, and honey ( ,  ,  ). These tests aim to detect drug or contaminant residues at or below established tolerances or maximum residue limits. A maximum residue limit (MRL) is similar to tolerance in that it is the maximum limit of a chemical residue acceptable in food products obtained from an exposed or treated animal, however, it is determined by the European Medicines Agency. \n\nIn our real-time text mining, the data collected from residue tests in veterinary medicine was 2-fold. In the first step, values or desired fields in a search query corresponding to the intended variables such as sensitivity, commodity, matrix, drug or contaminant, animal, manufacturer, or test are automatically extracted from the tables obtained from Web sources including PDFs and HTML documents. In the second step, these extracted data elements are compared to the previously obtained fields in our datasheets. If no fields match the new query, it means that we have received new data updated by a manufacturer that had not been previously reported and a new row will be added to the previous table (along with the other fields). On the other hand, if the fields other than sensitivity are the same (drug/contaminant name, test name, matrix, and test type) and a change was received in the sensitivity value, the new value will be updated and stored in the datasheet. Following data mining, tolerance information collected via the electronic Code of Federal Regulations ( ) was manually added to each corresponding drug or contaminant line in the datasheet to allow for a visual comparison of test sensitivity to applicable tolerance, which may help determine test suitability. The datasheet was then uploaded into an interactive, searchable interface online that allows veterinarians or producers to query. \n\n\n### Information Extraction Tools and Software for PDF and HTML Documents \n  \nFor PDF files,   PdfFileReader   from   PyPDF2  , a Python exclusive PDF module was used to scan PDF files (Python Software Foundation. Python Language Reference, version 3.7. Available at  ). Then   read_pdf   from the   tabula   module was imposed to detect tables on an individual page from a single PDF file. Drug names, sources, URL, and contact information were extracted based on the preferred drug list created beforehand and saved into separate excel files. \n\nFor HTML websites, the   webdriver   function from the Python module   selenium   was used to navigate to a webpage source accompanied by a specific URL address. The HTML and XML document parser,   BeautifulSoup  , from Python module BS4 was implemented to parse the source page; multimodal texts of Web page sources were extracted as   BeatifulSoup   object.   Pandas read_html   as a data analysis miner and manipulation and powerful web scraping tool for URL protocols was used to harvest data from HTML tables. \n\n\n### Regular Expression Learning for Information Extraction \n  \nTo briefly explain the high-throughput regular-expression pattern matching method, we have implemented some similarity methods from Regular Expression techniques in our Python codes that help identify patterns to match similar/missing characters or fields and create a synonym/dictionary table ( ,  ). The textual semantic similarity measures based on web intelligence of an ensemble of keywords were processed by using regular expressions when cleaning and merging information of sensitivity, drugs, or tests from different sources. \n\n\n### The Information Available in Each Field \n  \nBelow is a table of all necessary fields used for parsing webpage sources when a query is submitted to extract data from PDF or HTML documents. \n  \n\n\n## Results \n  \n### Preprocessing \n  \nInitially, we started the text mining model based on an original Microsoft Excel file including information previously obtained from text mining and manual curation of rapid assay tests of PDF files and HTML websites ( ,  ,  ). We faced multiple technical issues: (1) the drugs' names and bioassay test names in this file did not follow any specific standard, thus were not consistent for data extraction. For example, assay methods (e.g., sequential, competitive, or quantitative) were often written in the same field as the drug name, comma-separated in front of the field, or ahead of a test name. (2) Formulations of the same active ingredient with different generic or trade names created confusion in collecting data (for instance, benzylpenicillin procaine and benzathine penicillin). Similarly, some rapid diagnostic assay test names were presented differently in each document that might have referred to the same test (for instance, Charm KIS, Charm Kidney Inhibition Swab, and KIS referring to the same test). \n\nTo deal with the first problem for the data curation, we implemented a simple unsupervised learning algorithm to identify synonyms, based on the data amassed by implementing a web search engine for specific tables from multiple documents. The algorithm used pointwise mutual information (PMI) and information retrieval (IR) to measure the similarity of pairs of words by reporting the experiment types, if exist in any instance, to a separate column ( ,  ). For the second problem, the same method was used to discover synonymy in extracting semantic information which has been of high importance in information retrieval and automatic indexing. For this purpose, the most frequent names for each drug, matrix, and test were used for synonyms based on frequency in the file. Also, readings in information retrieval were stored in a Microsoft Excel spreadsheet file for the drug names and other fields including the matrix, manufacturer, or test names. Briefly, the most frequent name was reported in the first column and other names for the corresponding drug, matrix, or test were reported in the other columns in front of that (each name in one column). These two files were further cleaned if changes for drug names or other fields were observed while extracting data tables. \n\n\n### Desired Information From Structured or Unstructured Documents \n  \nBelow we review multiple cases to extract data from tables. For these cases, it is required to check if the keywords determined important in the real-time data collection via PDF and webpage parsing are clearly characterized in the extracted tables provided that any data are available. The necessary information is presented by their types and ensemble of keyword extraction from a single document using word co-occurrence statistical information in  . Here each item is supported by a regular expression to make sure each keyword is not part of another word to misclassify the keyword. For instance, urine can be found in purines or tissue as part of intertissued. Using those regular expressions, we avoid the misclassification of a keyword preceded or followed by the other prefixes, suffixes, or words. Here using the regular expression matching \u201c\\W\u201d ensures no letter or numbers will be part of a keyword and the bar \u201c|\u201d acts as an OR operator for the regular expression search algorithm. \n  \nIntended keywords to be checked in the extracted tables. \n  \nHowever, each field can be presented differently as authors may use incoherent terminologies based on their backgrounds or other international standards. To make all the field names consistent and create a synonyms table, the field names are compared to the list of names for each field, and they are replaced by a simple name as the main field name (given in  ). Some other fields such as MRL or Tolerance may also be considered in the data extraction process since these fields are available for retrieval in some documents and tables, and but not all the time. This creates repeated columns and we are required to deal with such cases since the names of repeated columns should be consistent. Here we similarly attempted to authenticate the input string of each field using regular expression matching to cover more cases in our queries. \n  \nDictionary for synonyms or corresponding names considered for each field. \n  \n Names are followed by some regular expressions to ensure correct field extractions  . \n  \n\n### Extracting Semistructured Information From the Web \n  \nAs mentioned before, data mining of information for rapid drug residue assays is an essential tool in veterinary medicine with source information ranging from tables in PDF files to HTML text which is presented on the Web. The data and information-retrieval model for rapid assays were obtained from the dairy products for groups of antibiotics including beta-lactams, tetracyclines, aminoglycosides, and sulfonamides from the website   ( ). Therefore, using our trained model based on the Python packages of   requests   and   BeautifulSoup  , all the rapid assay URL links for dairy tests are parsed and automatically examined for potentially available tables on each page. Below we presented an example of adaptable parsing of real-time data extracting. When the query pinpoints the above-said keywords in the extracted table as presented in  , the desired fields along with the source link of the table are collected and stored in an Excel file. For this particular example, the matrix is found as \u201cmilk\u201d from this link and then it is added as a new field to the stored file. In addition, we can also identify the title of each dairy test page by checking the HTML file and then use it as the \u201ctest name\u201d corresponding to each table; this requires us to create another new field as well. \n\nIn  , we found the dairy test for the \u201cSNAP NBL Test\u201d which \u201cdetects beta-lactam residues at or below the U.S. FDA-established tolerance/safe levels\u201d available for distribution in the USA and Canada. As mentioned previously, using the dictionary file in   which contains all the possible drug names in this study, the drug names are automatically compared one by one and (since drugs may have different generic or trade names) then replaced by the most frequent name. As indicated in the Preprocessing section, the same procedure is also used for consolidating the test names using another file containing the test names for the extracted tables. All of these are summarized for the tabular data in   and the corresponding extracted fields in  . \n  \nAn example of data table extraction for \u201cSNAP NBL Test\u201d ( ) and the corresponding fields cleaned in a table. \n  \n\n### Extracting Information From Unstructured PDF Files \n  \nThe main challenge is to deal with the extraction of data and information from PDF documents. Below we detail how we were able to collect data from different unstructured PDF files. \n\n#### Collecting the PDF Documents \n  \nThe first step is to locate PDF documents containing information and fields of interest for parsing. In this study, seven manufacturers of commercially available rapid residue screening assay tests were contacted to obtain PDF files containing the specifications for available tests ( ,  ). Once the PDFs were identified as potentially having useful information from these web sources to collect data, the documents are automatically stored for further analysis. It is worth mentioning that harvesting these dynamic web sources from one manufacturer (Charm Sciences, Inc.) is fast and straightforward in code implementation as these documents are put in a structured format on the Web based on the year, month, and test (as shown in  ). Using   requests   and   BeautifulSoup   packages in Python, all the PDF files with the titles containing \u201cMRK\u201d by avoiding cases sensitivity of uppercase or lowercase for each letter (e.g., \u201cmRK,\u201d \u201cMrK,\u201d \u201cmrk,\u201d etc.) from the years 2018, 2019, and 2020 were automatically collected and saved in a separate folder for further steps. In this process, we obtained 233 PDF files in total that may contain the necessary information for the rapid assay tests. \n  \n (A)   Content of the website screenshots from Charm Sciences page ( );   (B)   contents of the year 2018 in   (A)  ; and   (C)   contents in one of the months (09). \n  \n\n#### Extracting Tables From PDFs \n  \nDeep learning tabular data was then implemented to retrieve data from each PDF obtained in the previous step. First, each document is consecutively investigated for tables (page by page) using a built-in module of Python called   read_pdf  , which is a function from a Python package called   tabula  . Once a table is found in a particular PDF, the desired keywords as presented in   are searched in the document, and if any are found, the column names are then modified. Similar to the HTML documents, using the dictionary file in  , each drug is automatically linked one after another one and (since drugs may have different generic or trade names) then replaced by the most frequent name. In these PDFs, the test, the matrix, and the type are not among the column names. Therefore, the text extracting   PyPDF2  , a library built as a PDF toolkit and is a function of the Python package   PdfFileReader  , is used to read the PDF containing tables and search for the three fields. These three fields, along with sensitivity and the page source of each PDF are saved in an excel file for later comparison with the original Excel file. \n\nBelow we provided three examples of tabular data that were that parsed and analyzed for data extraction. The first typical example of tables found in multiple PDFs is demonstrated in  . Our PDF parser was able to clean and retrieve data as shown in the corresponding extracted table in   Examples such as   require the least software-intensive method as all the information in rows of these types of tables is successfully extracted. However, we have detected other cases where one or more rows of the table are missing after data collection. For instance,   shows the extracted tables of  . As can be seen, the last row is missing. To fix this issue, we needed to revise our code, search for cases similar to   and then redo the data extraction to find missing rows and then fill the extracted documents in a semi-automatic way. \n  \nAn example of data table extraction for \u201cCharm Flunixin and Beta-Lactam\u201d and the corresponding fields cleaned in a table. \n    \nAn example of data table extraction for \u201cCharm 3 SL3 Beta-Lactam\u201d and the corresponding fields cleaned in a table where the information for Penicillin G in the last row was not extracted initially from our data collection. \n  \nAnother issue faced was related to tables containing multiple titles for different drug classes. These types of tables when parsed created confusion for our text mining model and treated a table with multiple titles similar to the example shows below in  . As can be seen, another row exists in the middle of the table for a different drug class with the title. Our first assumption was each table may have only one title, and we searched for the unique title to extract information for drugs or rapid assay tests from that specific title. Since we did not emphasize much on any drug classes in this study, these rows are simply removed by checking if rows only contain strings and do not have any positive real numbers in the entire row. Other unnecessary information was also removed from tables including the headlines as shown by the blue ribbon in  . We then obtained the corresponding extracted table as given by  . It is worth pointing out that one row is also missing and needed further investigation. \n  \nAn example of data table extraction for \u201cCharm MRL Beta-Lactam\u201d and the corresponding fields cleaned in a table where the last row for Tetracycline was not extracted initially from our data collection. \n  \nAnother case that we had to deal with was related to tables that had repeated column headings or tables that were broken into two sub-tables located side by side. As an example, we can observe in   that for each field there are two headings for antimicrobial drugs, sensitivity (concentration in ppb), etc. Since in the PDF documents, such tables are always present, we had to code in a way to check the possibility of repeated columns for all the extracted tables. If any are found, the repeated columns are merged into a single column for each field. Therefore, the result of the corresponding extracted table is shown in  . \n  \nAn example of data table extraction for \u201cCharm Cowside II\u201d and the corresponding fields cleaned in a table. \n  \nLast but not least, it is worth mentioning that we also had cases where it was a combination of  ,  . We then had to check these cases and ensure to capture all the available data from the PDF files. As explained above, since such cases are available in our files, we needed to check all of these scenarios one by one for each and every single document. \n\n\n\n### Comparing the Extracted Tables With the Original Excel File \n  \nThe last step is to consolidate the extracted information from tables in PDFs and the webpage sources with the original Excel file. For this purpose, any of the rows corresponding to the same drug with the same matrix, type, and a different sensitivity value will be updated. As a result, the sensitivity field is updated since new information was found in this case. If any of the three fields (drug, matrix, or type) are different, a new row with the new information will be added to the master Excel file. Following consolidation, this information was then uploaded to a publicly available online searchable interface ( ). \n\n\n\n## Discussion \n  \nFields of natural language processing (NLP) and text mining provide tools and methodologies to retrieve relevant information ( ,  ). However, most of the current strategies are limited to articles' textual body, usually ignoring tables and other formats of information, including figures. Tables can be hard to understand, even human readers struggle to understand the information ( ,  ). Thus, the reader is required to consider a mental operation to obtain all the necessary information ( ). Tables are used for other purposes, where authors need to present a relatively large amount of multi-dimensional information in a compact manner ( ). Also, tables contain essential information needed for reproducibility of research and comparison to other studies. In addition to NLP, the text mining approach to tables performs poorly, and it is hard to understand the information that the table introduces. Information extraction from tables requires a multi-dimensional approach that will include pragmatic processing, syntactic processing and extraction, functional processing, and semantic tagging. \n\nIn our study, most of the files were produced and made available online by seven manufacturers in PDF formats while other tables were presented by HTML documents on Web sources. The latter were accurately abridged in an embodiment essential and accessible for our data collection system and provided a content management portal for interactive access to an encoded information reader system. The reconfigurable data collection process was arranged to be responsive with less challenge and configurated data were expressed in an extensible markup language since the information on web pages is typically structured and thus, extracting tables and other desired information from it is straightforward. \n\nThe main challenge in extracting information was, however, to deal with PDF documents, which are not most often compressed or labeled for a reconfigurable data collection system and semantic parsing data collection, and do not follow any specific logical structural information ( ). In this case, the relevant information is present in a table format which is typically unidentifiable after the PDF to text conversion. Due to the technical issues, any query on these documents relies upon accurate conversion of PDF documents to text, but long lists of information in tables or inadvertent run-on sentences can lead to the erroneous determination of searched fields. If possible, prospects for correcting these technical malfunctions include revising the search engine model, improving algorithms to improve regular expression matching and learning for information extraction, and if available, obtaining documents in XML/HTML/JSON format to enhance PDF-to-text conversion. \n\nIt should also be noted that out of the total 233 PDFs, only 60 tables had relevant information and fields for data extraction. Out of 60 PDF documents, we were able to extract data points from 34 files providing approximately 1,100 records from commercial rapid assay tests. The other PDFs either did not have tables, or the extracted tables did not have relevant fields, or the tables could not be extracted in the first place. So, further investigation is needed, and it should be considered for the continuation of this research. \n\nThe data extracted in the study were made available to veterinarians and producers via an online interface ( ). Previously, these types of data were variably accessible for individual tests through company websites and in package inserts, but there was no centralized resource that veterinarians could refer to for information on tests available for a particular drug residue, and in some cases, the information could only be obtained through special requests to the manufacturer. In addition, manufacturer-provided information often does not include the FDA-approved tolerance values for that drug residue, thus making it difficult to know whether the commercial rapid assay test detects residues down to or below the tolerance. If there is a paucity of scientific data on which to base a withdrawal interval after extra-label drug use, it is often necessary to test for drug residues once the estimated withdrawal interval has been observed, prior to returning a treated animal or its products to food production. In the dairy industry particularly, producers and veterinarians often test the milk of all cattle that have been treated with a drug (whether label or extra-label drug use), after observing the required withdrawal interval and before returning the cow to the lactating herd. Therefore, a single online reference source where information on species, matrix, assay sensitivity, and FDA-approved tolerances for these rapid assay tests is a valuable resource for food animal veterinarians. \n\nOverall, automatically collecting data from web pages and updating the corresponding data in the current available excel file resulted in the following advantages: (i) obtaining the sensitivity value corresponding to a specific test for a drug and a matrix without conducting the test, (ii) decreasing the errors caused by manually collecting and inserting the data, (iii) decreasing time and cost of obtaining sensitivity values since this will not be dependent on people for manually extracting the documents and data, (iv) more documents can be investigated for useful information, and (v) real-time implementation of the text mining for dynamic web sources is the most advantage of such model development. This has a real-world veterinary application as being able to automatically collect and update residue assay tests allows for access to up-to-date information that helps drive decision making regarding which tests to use, whose results that can then help determine if food products will be safe for human consumption. \n\nUnstructured data is the accessible form of data and mostly presented in the form of publications that can be generated in any application scenario. Therefore, there has been an extreme necessity to devise methods and algorithms that are capable of efficiently processing an extensive variety of text applications from electronic documents. This study has provided a data-mining method for automatically extracting rapid assay data of the different document types which are common in the text domain, with a distinct target of table extraction. However, data are not always presented in the form of tables. As part of our future work, it is worth mentioning that we are currently developing and working on a learned information extraction system to transform any text format of pharmacokinetic data into more structured data which is then mined for the use of therapeutic drugs in modern animal agriculture including recommendations for safe withdrawal intervals of drugs and chemicals in food-producing animals. The objective of the data mining project is to create a comprehensive drug database to help the mission of FARAD by improving overall animal health and promoting efficient and humane production practices. \n\n\n## Data Availability Statement \n  \nThe original contributions presented in the study are included in the article/ , further inquiries can be directed to the corresponding author/s. \n\n\n## Author Contributions \n  \nMJ-D: conceptualization, methodology, validation, writing\u2014original draft, writing\u2014review, editing, funding acquisition, and data science. ST: model development, validation, resources, writing\u2014draft, writing\u2014review, and editing. NM: conceptualization, writing\u2014review, editing, online repository, and data science. XX: conceptualization, software, writing\u2014review, editing, and data science. ER and FM: writing\u2014review, editing, and interpretation. NZ: writing\u2014review and editing. LT: conceptualization, methodology, writing\u2014review, editing, and funding acquisition. All authors contributed to the article and approved the submitted version. \n\n\n## Conflict of Interest \n  \nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n \n", "metadata": {"pmcid": 8334182, "text_md5": "813c6fa5fd4080082d6a66ba4a2fee44", "field_positions": {"authors": [0, 171], "journal": [172, 185], "publication_year": [187, 191], "title": [202, 350], "keywords": [364, 534], "abstract": [547, 2991], "body": [3000, 34249]}, "batch": 1, "pmid": 34368270, "doi": "10.3389/fvets.2021.674730", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8334182", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8334182"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8334182\">8334182</a>", "list_title": "PMC8334182  Large-Scale Data Mining of Rapid Residue Detection Assay Data From HTML and PDF Documents: Improving Data Access and Visualization for Veterinarians"}
{"text": "Xing, Yuting and Wu, Chengkun and Yang, Xi and Wang, Wei and Zhu, En and Yin, Jianping\nMolecules, 2018\n\n# Title\n\nParaBTM: A Parallel Processing Framework for Biomedical Text Mining on Supercomputers\n\n# Keywords\n\nbiomedical text mining\nbig data\nTianhe-2\nparallel computing\nload balancing\n\n\n# Abstract\n \nA prevailing way of extracting valuable information from biomedical literature is to apply text mining methods on unstructured texts. However, the massive amount of literature that needs to be analyzed poses a big data challenge to the processing efficiency of text mining. In this paper, we address this challenge by introducing parallel processing on a supercomputer. We developed paraBTM, a runnable framework that enables parallel text mining on the Tianhe-2 supercomputer. It employs a low-cost yet effective load balancing strategy to maximize the efficiency of parallel processing. We evaluated the performance of paraBTM on several datasets, utilizing three types of named entity recognition tasks as demonstration. Results show that, in most cases, the processing efficiency can be greatly improved with parallel processing, and the proposed load balancing strategy is simple and effective. In addition, our framework can be readily applied to other tasks of biomedical text mining besides NER. \n \n\n# Body\n \n## 1. Introduction \n  \nWith the rapid development of biotechnology, the amount of biomedical literature is growing exponentially. For instance, PubMed ( ), the most recognized biomedical literature database, indexes over 28 million entries for biomedical literature. Most of that information is presented in the form of unstructured texts. It is almost impossible for any domain expert to digest such a massive amount of information within a short period of time. Therefore, automated tools are essential for a systematic understanding of literature. To deal with the literature big data challenge, text mining methods are commonly applied to extract relevant knowledge from vast amounts of literature, and this has become a prominent trend in recent years [ ]. \n\nTypical tasks of biomedical text mining include named entity recognition and relation extraction. One of the most fundamental tasks of biomedical text mining is named entity recognition (NER). Its task is to recognize target entities that represents key concepts from unstructured biomedical texts, such as proteins, genes, mutations, diseases, etc. There are some existing start-of-art biomedical tools that use text mining methods to identify some specific types of entities, such as mutations [ , ], genes [ , ], and diseases [ , ]. Most of these tools can achieve satisfactory recognition performance (F score over 80%) on standard datasets. \n\nRelation extraction (RE) is a process that typically follows NER and aims to discover semantic connections between entities. Nowadays, there are a number of RE tools using different methods to identify biomedical entity interactions [ , ], such as drug\u2013gene relationships [ , , ], gene\u2013disease relationships [ , , ] and protein\u2013protein interaction [ ]. Some of them can achieve high F scores (over 80%) on several annotated datasets. \n\nNER and RE are the preliminary steps in mining information from literature. With the uncovered facts, it is possible to construct a complex knowledge graph, which can assist new knowledge discovery and hypotheses generation. In order to achieve this goal, it is necessary to process as many articles as possible. However, text mining procedures are time consuming. BioContext, for instance, an integrated text mining system for large-scale extraction and contextualization of biomolecular events, took nearly 3 months to complete a full run of the system, which analyzed 20 million MEDLINE abstracts and several hundred thousand PMC open access full-texts using 100 concurrent processes [ ]. In addition, some text mining tools, like GNormPlus [ ] require a substantial amount of memory (\u22655 GB), due to the necessity of loading a large gene dictionary and complementary data structures. Consequently, commodity servers cannot fulfil the computation and storage demands of large-scale text mining. Cloud-based solutions in Map-Reduce mode can partially fulfil computational resource demands. However, practically speaking, many text mining components were written in different languages, and they are dependent on a complex collection of third-party libraries, which prevents them from being readily transplanted into a high-level framework, like Hadoop and Spark. In addition, we dived into the details of load balancing, which cannot be readily supported by Map-Reduce. \n\nAn alternative solution to address this computational challenge is to harness the power of high performance computers. High performance computers (HPC) like Tianhe-2 [ ] represent high-end computing infrastructures that have traditionally been used to solve compute-intensive scientific and engineering problems. The system configuration of Tianhe-2 is listed in  . \n\nAlthough the software stack on Tianhe-2 is designed and optimized for compute-intensive tasks, its high-performance architecture does provide the capability and capacity of big data processing. Nonetheless, to employ Tianhe-2 for big data processing is not a trivial task, which requires expert knowledge of the system architecture and parallel programming. The programming model is MPI-based (message passing interface) [ ], which adds an extra dimension of complexity to normal programming languages like C/C++, Python, Java, etc. Most existing text mining tools are implemented without parallel processing. Therefore, it is necessary to develop an enabling framework that can support parallel text mining without the need to rewrite the original code. In this paper, we develop a parallel processing framework for text mining on the Tianhe-2 supercomputer. The framework integrates text mining tools as plugins. It unifies the input\u2013output stream, implements the parallel processing across multiple compute nodes using the MPI model, and it applies a carefully crafted load balancing strategy to improve the parallelization efficiency. Without a loss of generality, we demonstrate the effectiveness of our framework using multiple NER tools as the demonstration plugins, which can recognize genes, mutations and diseases appearing in biomedical literature. More sophisticated tools of biomedical text mining can be readily integrated into the framework. In the remaining of this paper, we will introduce how paraBTM works and evaluate its performance on Tianhe-2. \n\n\n## 2. Results and Discussion \n  \nTo verify the effectiveness of the parallel framework, we constructed a corpus named 60K, which consists of 60,000 randomly selected articles from PubMed. For NER plugins, we chose three state-of-the-art tools (GNormPlus [ ], tmVar [ ], DNorm [ ]), developed by NCBI (National Centre for Biotechnology Information). We measured the performance in terms of the total processing time and the average processing time (across all processes), and the total time includes the time of initialization and the actual processing time of different plugins. \n\nThe 60 K corpus is presented in the NXML format, which is a standard format provided by NCBI. Titles, abstracts, and full-texts from NXML files are extracted and re-written in the PubTator format. All input and output files processed by paraBTM should follow the PubTator format and the PubTator format starts with:   \n<PMID>|t|<Title of the paper> \n  \n<PMID>|a|<The rest of the paper> \n  \n\nThe output file will be appended with annotated information like named entities followed in a tab-separated way. \n\nA basic fact is that the time overhead of text mining is not proportional to the number of input articles. We verified this via a single process run over several groups of randomly selected articles. The result is depicted in  . Here, different colors represent different processing plugins. Related numbers are also listed in  . \n\nAs the number of input files increases, the time cost also increases but not linearly. For example, when the number of input articles is equal to 10, it takes about 36 min for tagging entities, and when the number of articles increases to 100, the spent time is about 3 h (180 min). This can be attributed to another important observation, that is, the total processing time is approximately proportional to the total input size (sum of file lengths as measured by number of characters), which is illustrated in   (size unit is MB, mega-bytes) and  . The workload of each plugin can be better estimated by the total length of input files, which is the basis for our load balancing strategy in the following part. \n\n shows the time spent on paraBTM processing with different numbers of parallel processes on an input dataset of 16 MBs (including 175 articles) which is composed of articles randomly selected from the 60 K corpus. Parallel processing greatly reduces the processing time and different load-balancing strategies do affect the parallel efficiency. paraBTM costs about 500 s (under the Short-Board balancing strategy) when 64 processes are employed, which is around 1/16 the processing time of 2 processes. To note, each process needs to carry out initialization for every plugin, which means you cannot reduce the total processing time any further if the initialization time cost becomes the majority part. \n\nTo profile different load balancing strategies, we summarize their effects under different parallel scales, as listed in  . In all 6 test cases, the Short-Board strategy is the best in 4 cases and 2nd best in 2 remaining cases. We employ the load balancing efficiency (LBE) to quantify the effects of different strategies. Here, LBE is defined as: \n\nHere, AET is the average execution time and MET is the maximum execution time. According to the above definition, the maxima of LBE is 1 (achieved if AET is equal to MET) and a greater LBE represents a better load balancing efficiency. \n\n shows that the Short-Board strategy exhibits the best LBE in almost all test cases. However, LBE values drop significantly when the number of parallel processes is greater than 16 in the 16M test set. The reason is that this test set contains only 175 articles, which means each process will only process two articles on average. If the input data set is big enough, the LBE will be maintained at a satisfactory level. \n\nWe also conducted an experiment on the whole 60 K corpus (61,078 articles).   shows that it took over 12 h to process 61,078 papers through three NER plugins (128 nodes under the Short-Board strategy, each node runs 5 processes). According to the results, we can see that parallelization greatly enhances the processing efficiency. To note, the speed-ups of different plugins differ as each plugin has its own characteristic computation and memory access patterns. To carry out a full-scale processing on the whole PMC-OA full-text dataset (over 1 million), it will take about 200 h if we only use 128 nodes. Fortunately, the computation capacity of Tianhe-2 is enormous, and we can reduce the total time down to several hours by harnessing the power of a few thousand nodes (over 16,000 available on Tianhe-2). We plan to carry out a full analysis on the whole PubMed dataset (the real large-scale biomedical texts) in the future. However, the cost of such a full run is currently beyond our funding support. We are currently in the application process of a bigger grant for this large-scale analysis. In our previous study, we have demonstrated that using text mining on a larger dataset does provide more comprehensive and insightful results compared with using a small dataset (say, can be handled by a few people) using thyroid cancer as a case study [ ]. \n\n\n## 3. Materials and Methods \n  \n### 3.1. Data Sources and Storage \n  \nThe biomedical literature has typical characteristics of large quantity, professional content, public resources, easy-accessibility, etc. Because of these characteristics, biomedical literature data has become one of the most noticeable data in biomedical field. For example, PubMed Central (PMC) is a free digital repository that archives publicly accessible articles. Until now, PMC has contained over 4.1 million references to full-text journal papers, covering a wide range of biomedical fields, and the literature data is stored in NXML format, from which we can extract some parts according to our interest. \n\nHowever, most of the state-of-art NER tools do not support parallel processing, and it would take an enormous amount of time if we want to process the massive set of biomedical literature. One feasible solution is to harness the computing power provided by HPC systems by implementing a parallel NER processing framework. With this framework, text mining tools can be easily integrated into the framework and developers will not need to consider the details of parallel processing. \n\nThere are different levels of parallelism in text mining tasks. First, each input article is relatively independent; secondly, multiple sentences in each of the articles can be approximately regarded as independent. However, in practice, we usually use a single file as a processing unit, the reason is that many text mining tools spend a substantial amount of time to initialize on each processing pass. In addition, the memory size also limits the number of processes that can run in parallel on each computing node. For instance, on Tianhe-2 each node is equipped with 24 cores and 64 GB of memory, and the stable memory that users can control is about 50 GB (the operating system and other necessary tools need to use about 10 GB). The memory costs of a typical TmVar and gnormplus run for NER can be up to 5 GB and 10 GB. Therefore, at most 5 GNormPlus processes and 10 TmVar processes can run on one node.   shows the implementation and deployment of a text mining system (paraBTM) in large-scale parallel environment. \n\n\n### 3.2. Parallel Processing \n  \n#### 3.2.1. MPI-Based Multi-Node Computation \n  \nThe message passing interface (MPI) is a standard model for parallel programming on HPCs. It is well established over 20 years, and has been implemented in different sorts of programming languages including C/C++ and Python. Our method can run on any supercomputer or cluster configured with MPI support. To note, different supercomputers might have different node configurations. When running on other platforms, the configuration (RAM, number of concurrent processes) might have to change accordingly. \n\nIn this work, we use MPI4PY ( ) to implement the parallel processing. MPI4PY is a well-regarded, clean, and efficient implementation of MPI for Python. Our framework can simultaneously submit many jobs to cores distributed across computing nodes in Tianhe-2. \n\n\n#### 3.2.2. Load Balancing Strategy \n  \nA typical challenge in parallel computation is load unbalance, that is, workload is unevenly distributed among nodes, making some nodes very busy for a long time and others idle [ ]. In this paper, we address this problem by designing an effective load balancing strategy. \n\nGiven a set   of files to be processed  , we initialize processes  , and the number of processes is  , the problem is to allocate each file   to an appropriate process  . \n\nA naive solution is to randomly distribute target files into nodes. We can simply distribute files to by modulo operation   P   and size represent the position of the target file in the file list and number of processes respectively, and file   finally should be sent to  . According to the formula, each file   to be processed is distributed to process   in turn. As the files are arranged in a random order, this process is actually a simulation of random distribution. This is a na\u00efve strategy and easy to implement. However, this strategy does not take into consideration the length of each file, and will very likely cause an unbalanced load distribution, which would detriment the overall parallel efficiency. For instance, if the total length of files assigned to one specific node is far larger than others, then the overall running time will be prolonged until this slowest node finishes.   shows an example of the na\u00efve random load balancing strategy. \n\nA slightly more complex load balancing method is the round-robin (RR) method. Round-robin algorithm is a term that originally comes from the field of operating systems. Here, the general idea inspires us to mix small files with large files together into one process. After sorting files by size (see  ), the system will assign files into processes in a snakelike way, making the size of files loaded in every process remains relatively balanced.   shows an example of RR algorithm. \n\nThe round-robin method also allocates the same number of files, and its serpentine way of load assignment ensures that the total size of the files in each process remains relatively balanced, since files were sorted by size in advance. However, in some circumstances, the lengths of input articles can be very biased, say, some files are extremely long while many others are short. In such cases, the RR method fails. \n\nInstead of assignments based on the number of files, we proposed our \u201cShort-Board\u201d method. Firstly, the files that need to be processed are sorted in descending order according to the length of each file, and then files that need to be processed in the file list are sequentially fetched out and dispatched to the process whose current load is the smallest.  a\u2013d shows an example of Short-Board algorithm. The pseudo code of Short-Board is shown in  . \n\n\n\n\n## 4. Conclusions \n  \nIn this paper, we present paraBTM, a parallel framework for biomedical text mining developed on the Tianhe-2 supercomputer. It supports different types of components as plugins and its usage is straightforward. The parallel efficiency is guaranteed by a carefully devised load balancing strategy. We evaluated the performance of paraBTM on both small- and large-scale datasets. Experimental results validate that paraBTM effectively improve the processing speed of biomedical named entity recognition. On large scale of datasets, ParaBTM managed to process 60178 PubMed full-text articles in about 12 h. paraBTM is open-source and available at  . \n\n \n", "metadata": {"pmcid": 6099625, "text_md5": "afde8c0b1c86e7f65cecfc89dce18f57", "field_positions": {"authors": [0, 86], "journal": [87, 96], "publication_year": [98, 102], "title": [113, 198], "keywords": [212, 287], "abstract": [300, 1308], "body": [1317, 18325]}, "batch": 1, "pmid": 29702574, "doi": "10.3390/molecules23051028", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6099625", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6099625"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6099625\">6099625</a>", "list_title": "PMC6099625  ParaBTM: A Parallel Processing Framework for Biomedical Text Mining on Supercomputers"}
{"text": "Dandala, Bharath and Joopudi, Venkata and Tsou, Ching-Huei and Liang, Jennifer J and Suryanarayanan, Parthasarathy\nJMIR Med Inform, 2020\n\n# Title\n\nExtraction of Information Related to Drug Safety Surveillance From Electronic Health Record Notes: Joint Modeling of Entities and Relations Using Knowledge-Aware Neural Attentive Models\n\n# Keywords\n\nelectronic health records\nadverse drug events\nnatural language processing\ndeep learning\ninformation extraction\nadverse drug reaction reporting systems\nnamed entity recognition\nrelation extraction\n\n\n# Abstract\n \n## Background \n  \nAn adverse drug event (ADE) is commonly defined as \u201can injury resulting from medical intervention related to a drug.\u201d Providing information related to ADEs and alerting caregivers at the point of care can reduce the risk of prescription and diagnostic errors and improve health outcomes. ADEs captured in structured data in electronic health records (EHRs) as either coded problems or allergies are often incomplete, leading to underreporting. Therefore, it is important to develop capabilities to process unstructured EHR data in the form of clinical notes, which contain a richer documentation of a patient\u2019s ADE. Several natural language processing (NLP) systems have been proposed to automatically extract information related to ADEs. However, the results from these systems showed that significant improvement is still required for the automatic extraction of ADEs from clinical notes. \n\n\n## Objective \n  \nThis study aims to improve the automatic extraction of ADEs and related information such as drugs, their attributes, and reason for administration from the clinical notes of patients. \n\n\n## Methods \n  \nThis research was conducted using discharge summaries from the Medical Information Mart for Intensive Care III (MIMIC-III) database obtained through the 2018 National NLP Clinical Challenges (n2c2) annotated with drugs, drug attributes (ie, strength, form, frequency, route, dosage, duration), ADEs, reasons, and relations between drugs and other entities. We developed a deep learning\u2013based system for extracting these drug-centric concepts and relations simultaneously using a joint method enhanced with contextualized embeddings, a position-attention mechanism, and knowledge representations. The joint method generated different sentence representations for each drug, which were then used to extract related concepts and relations simultaneously. Contextualized representations trained on the MIMIC-III database were used to capture context-sensitive meanings of words. The position-attention mechanism amplified the benefits of the joint method by generating sentence representations that capture long-distance relations. Knowledge representations were obtained from graph embeddings created using the US Food and Drug Administration Adverse Event Reporting System database to improve relation extraction, especially when contextual clues were insufficient. \n\n\n## Results \n  \nOur system achieved new state-of-the-art results on the n2c2 data set, with significant improvements in recognizing crucial drug\u2212reason (F1=0.650 versus F1=0.579) and drug\u2212ADE (F1=0.490 versus F1=0.476) relations. \n\n\n## Conclusions \n  \nThis study presents a system for extracting drug-centric concepts and relations that outperformed current state-of-the-art results and shows that contextualized embeddings, position-attention mechanisms, and knowledge graph embeddings effectively improve deep learning\u2013based concepts and relation extraction. This study demonstrates the potential for deep learning\u2013based methods to help extract real-world evidence from unstructured patient data for drug safety surveillance. \n\n \n\n# Body\n \n## Introduction \n  \n### Background \n  \nAn electronic health record (EHR) is the systematized collection of electronically stored health information of patients and the general population in a digital format [ ]. Clinical notes in EHRs summarize interactions that occur between patients and health care providers [ ]. These notes include observations, impressions, treatments, drug use, adverse drug events (ADEs), and other activities arising from each interaction between the patient and the health care system. Extracting useful information such as ADEs from these notes and alerting caregivers at the point of care has the potential to improve patient health outcomes. \n\nAn ADE is commonly defined as \u201can injury resulting from medical intervention related to a drug\u201d [ ]. ADEs are a major public health concern and one of the leading causes of morbidity and mortality [ ]. Studies have shown the substantial economic burden of these undesired effects [ , ]. Although drug safety and efficacy are tested during premarketing randomized clinical trials, these trials may not detect all ADEs because such studies are often small, short, and biased by the exclusion of patients with comorbid diseases. With the limited information available when a drug is marketed, postmarketing surveillance has become increasingly important. Spontaneous reporting systems, such as the US Food and Drug Administration Adverse Event Reporting System (FAERS) [ ], are monitoring mechanisms for postmarketing surveillance that enable both physicians and patients to report ADEs. However, previous studies [ - ] have exposed various inadequacies with such systems, including underreporting, reporting biases, and incomplete information, prompting researchers to explore additional sources to detect ADEs from real-world data. \n\nSeveral efforts have been made to extract ADEs automatically from disparate information sources, including EHRs [ - ], spontaneous reporting systems [ - ], social media [ - ], search queries on the web via search engine logs [ , ], and biology and chemistry knowledge bases [ - ]. Furthermore, the clinical natural language processing (NLP) community has organized several open challenges such as the 2010 Informatics for Integrating Biology & the Bedside/Veterans Affairs NLP Challenge [ ], Text Analysis Conference 2017 Adverse Drug Reactions Track [ ], and BioCreative V Chemical Disease Relation task [ ]. Recently, 2 such challenges, Medication and Adverse Drug Events from Electronic Health Records (MADE 1.0) [ ] and the 2018 National NLP Clinical Challenges (n2c2) Shared Task Track 2 [ ], were organized to extract   drugs  , drug attributes,   ADEs  ,   reasons   for prescribing drugs, and their relations from clinical notes. The results from these 2 challenges showed that deep learning techniques outperform traditional machine learning techniques for this task, and significant improvement is still required for   drug\u2212{ADE, reason}   relation extraction. Specifically, the organizers of these challenges hypothesized that models that can effectively incorporate the larger context to capture long-distance relations or leverage knowledge to capture implicit relations will likely improve the performance of future systems. \n\nConsidering these conclusions, we developed a joint deep learning\u2013based relation extraction system that helps in extracting long-distance relations through a position-attention mechanism and implicit relations through external knowledge from the FAERS. To the best of our knowledge, no previous research has been conducted on using the position-attention mechanism and domain-specific knowledge graph embeddings in ADE detection. \n\n\n### Relevant Literature \n  \n#### Adverse Drug Event Detection \n  \nFrom the viewpoint of NLP, effective techniques for entity and relation extraction are fundamental requirements in automatic ADE extraction. Entity and relation extraction from text has traditionally been treated as a pipeline of 2 separate subtasks: named entity recognition (NER) and relation classification. Previous studies employed traditional machine learning techniques [ - ], such as conditional random fields (CRF) [ ] for NER and support vector machines [ ] for relation classification. Several recent approaches [ - ], developed on MADE 1.0 [ ] and 2018 n2c2 Shared Task Track 2 [ ] data sets, employed deep learning techniques, such as bidirectional, long short-term memory\u2013conditional random fields (BiLSTM-CRFs) [ ], for NER and convolutional neural network (CNN) [ ] for relation classification, and showed numerous advantages resulting in better performance and less feature engineering. However, there is an inevitable error propagation issue with pipeline-based methods because of the following: \n  \nNER relying on sequence-labeling techniques suffers from lossy representation when there are overlapping annotations on entities. For example, in \u201cshe was on   furosemide   and became   hypotensive   requiring   norepinephrine  ,\u201d   hypotensive   is an   ADE   with respect to   furosemide   but a   reason   with respect to   norepinephrine  . \n  \nNER approaches usually take an input context window that may not contain the necessary information to determine the appropriate label (ie,   ADE, reason,   no label). For example, in \u201cPatient reports   nausea  . Started on   ondansetron,  \u201d the identification of   nausea   as a   reason   requires information from both sentences. \n  \nSigns or symptoms are only labeled as   ADE   or   reason   if they are related to a drug (ie, not all signs or symptoms in the clinical note are annotated). This makes the corpus less suitable to train an effective relation classification model as it misses negative candidate pairs for   drug  \u2212{  ADE, reason  } relations. \n  \nTo address the first 2 issues, we previously proposed a joint method that outperformed the pipeline method for concept and relation extraction on a similar data set (MADE 1.0) [ ]. In a separate study, Li et al [ ] proposed a joint method using multitask learning [ ] and made similar observations. To address the third issue, which was introduced with the n2c2 data set, Wei et al [ ] proposed a novel label-encoding scheme to jointly extract   ADE  ,   reason  , drug attributes, and their relations. \n\n\n#### Attention-Based Relation Extraction \n  \nThe attention mechanism allows neural networks to selectively focus on specific information [ - ]. This has proven to be effective for NLP problems with long-distance dependencies such as NER and relation extraction. Zhou et al [ ] proposed an attention-based BiLSTM network and demonstrated its effectiveness in selectively focusing on words that have decisive effects on relation classification. Next, Zhang et al [ ] extended the attention mechanism to help networks not only focus on words based on the semantic information of the sentence but also the global positions of entities within the sentence. Recently Dai et al [ ] introduced a position-attention mechanism for joint extraction of entities and overlapping relations. The position-attention mechanism builds on self-attention by focusing on both the global dependencies of the input and tokens of the target entities of interest for relation extraction. Recent research [ , ] on ADE extraction showed the benefits of self-attention mechanisms in pipeline-based methods, specifically for relation classification. However, to the best of our knowledge, no previous work has focused on using self-attention or position-attention mechanisms for joint extraction of entities and relations for ADE extraction. \n\n\n#### Knowledge-Aware Relation Extraction \n  \nSeveral approaches [ - ] in the open domain have shown that incorporating embeddings learned from knowledge bases benefit deep learning\u2013based relation classification. These embeddings are typically learned using translation-based methods such as TransE [ ], TransH [ ], and TransR [ ]; walk-based methods such as DeepWalk [ ] and node2vec [ ]; or neural network\u2013based methods such as large-scale information network embedding (LINE) [ ] and bipartite network embedding [ ]. \n\nClinical notes are typically written for medical professionals. Hence, a certain degree of medical knowledge is assumed by the authors, which is not explicitly expressed in the text. This is especially true for relations between clinical findings and drugs, where a drug could either cause (  ADE  ) or treat (  reason  ) a clinical finding. In our previous study [ ], we showed that augmenting knowledge base features such as proportional report ratio and reporting odds ratio calculated from the FAERS into deep learning models can benefit relation classification. Recently, Chen et al [ ] proposed a hybrid clinical NLP system by combining a general knowledge-based system using the Unified Medical Language System (UMLS) and BiLSTM-CRF for concept extraction and attention-BiLSTM for relation classification. However, to the best of our knowledge, no previous work has focused on using knowledge graph embeddings generated from the FAERS for joint extraction of entities and relations for ADE extraction. \n\n\n\n\n## Methods \n  \n### Data Set \n  \nThe n2c2 data set consists of 505 deidentified clinical narratives, of which 303 and 202 narratives were released as train and test data sets, respectively. Each narrative was manually annotated with drug-centric entities, including   drugs  , their attributes (  strength  ,   form  ,   frequency  ,   route  ,   dosage  , and   duration  ),   ADEs  ,   reasons  , and relations between drugs and other entities (  drug\u2212  {attributes,   ADE  ,   reason  }).   Drug\u2212  {attributes} represent 6 different types of relations:   drug\u2212  {  strength  ,   form  ,   frequency  ,   route  ,   dosage  ,   duration  }.   presents an example with annotations.   and   present the statistical overview of the annotated entities and relations. \n  \nAn illustration with annotations for entities and relations. ADE: adverse drug event; HTN: hypertension; QHS: every night at bedtime. \n    \nEntities in the data set. \n      \nRelations in the data set. \n    \n\n### Preprocessing \n  \nSentence boundary detection (SBD) and tokenization are often treated as solved problems in NLP and carried out using off-the-shelf toolkits such as Apache Natural Language Toolkit [ ], Explosion AI spaCy [ ] or the Stanford CoreNLP toolkit [ ]. However, these are still difficult and critical problems [ ] in the clinical domain because (1) sentence ends are frequently indicated by layout and not by punctuation and (2) white space is not always present to indicate token boundaries (eg,   50 mg  ). To address these issues, we incorporated domain-specific rules sensitive to low-level features such as capitalization, text-wrap properties, indentation, and punctuation into the spaCy tokenizer and SBD models. These custom rules are provided in  . \n\n\n### Representation Learning \n  \n#### Static Word Representations \n  \nWord embedding is a text vectorization technique that transforms words or subwords into vectors of real numbers. Pretrained word embeddings created using Word2Vec [ ], Glove [ ], and fastText [ ] have been broadly used to initialize deep learning architectures for NLP tasks and have shown substantial improvement over random initialization. Recent research [ ] showed that NER performance is significantly affected by the overlap between the pretrained word embedding vocabulary and the vocabulary of the target NER data set. Thus, we used Word2Vec with skip-gram to pretrain word embeddings over the Medical Information Mart for Intensive Care III (MIMIC-III) [ ] with the default parameters provided in a study by Mikolov et al [ ]. \n\n\n#### Contextualized Word Representations \n  \nA well-known limitation of word embedding methods is that they produce a single representation of all possible meanings of a word. To tackle these deficiencies, advanced approaches have attempted to model the word\u2019s context into a vector representation. Embeddings from Language Models (ELMo) [ ] is a prominent model that generates contextualized word representations by combining the internal states of different layers in a neural language model. Bidirectional Enconder Representations from Transformers (BERT) [ ] furthered this idea by training bidirectional transformers [ ] using subwords. Contextualized embeddings are particularly useful for clinical NER as entities (eg,   cold   as low temperature versus infection) have different meanings in different contexts. Recent research [ ] showed that deep learning architectures with contextualized embeddings pretrained on a large clinical corpus achieve state-of-the-art performance on several clinical NER data sets. Inspired by these, we trained contextualized representations using ELMo on MIMIC-III. Detailed explanations of ELMo and training parameters are provided in  . \n\n\n#### Knowledge Representations \n  \nTo introduce medical knowledge, we built knowledge representations on the FAERS, a database for postmarketing drug safety monitoring. Specifically, we used 2 tables from Adverse Event Open Learning through Universal Standardization (AEOLUS) [ ], a curated and standardized FAERS resource, to generate 2 separate graph embeddings. As shown in  ,   standard drug_outcome count   contains case frequencies for drug outcomes, including   ADEs  , and   standard drug indication count   contains case frequencies for drug indications (ie,   reasons)  . \n\nLet   G=(D,O,E)   be a weighted bipartite network, where   D   and   O   denote the set of   drug concept id   and   outcome concept id   in   standard drug outcome count,   and   defines the interset edges.   D   and   O   denote the   i   and   j   vertex in   D   and   O   respectively, where   i  ={1,2, \u2026 ,|  D  |} and   j  ={1,2, \u2026 ,|  O  |}. Each edge   carries a frequency   f   provided by the   drug outcome pair count   field in   standard drug outcome count  , indicating the strength between the connected vertices   D   and   O  ; if   D   and   O   are not connected,   f   is set to zero. To integrate this knowledge into our proposed architecture, we computed token-level embeddings by transforming   G   to   G\u2019   as follows: \n\nGiven a   drug concept id   (RxNorm) or   outcome concept id   (Medical Dictionary for Regulatory Activities) from AEOLUS, we mapped it to its concept unique identifiers (CUIs) in UMLS [ ] and obtained a set of tokens from all CUI variants. Let   d  ={  d , d , \u2026., d  } and   o  ={  o  o , \u2026., o  } represent all unique drug and outcome tokens obtained from mapping all   and  . Let   and   represent 2 multivalued functions that associate each element in the set of   drug concept id   and   outcome concept id   to a set of tokens. Let   G\u2019=(d,o,e)   be a weighted bipartite graph and each edge   of   G\u2019   is associated with a nonnegative weight   w   indicating the strength between the drug token   d   and the outcome token   o  . We calculated   w   as token-level co-occurrence between   d   and   o   normalized for the drug token   d  : \n\n\n\nIn   w   the numerator represents the sum of frequencies of all   drug concept id   and   outcome concept id   pairs that contain drug token   d   and outcome token   o   and the denominator represents the sum of frequencies of all pairs whose   drug concept id   contains the drug token   d  . \n\nFrom the generated bipartite weighted graph   G\u2019  =(  d,o,e  ), we used the LINE approach to generate   drug-adverse   knowledge embeddings. We used LINE because (1) relations between drugs and other concepts in the FAERS form a weighted bipartite graph with a long-tail distribution of vertex degrees and (2) it helps in embedding implicit connectivity relations between vertices of the same type. Similarly, we generated   drug-reason   knowledge embeddings from the   standard drug indication count   table. Detailed explanations of LINE and training parameters are provided in  . \n  \nExcerpts from the standard drug outcome count and standard drug indication count tables from adverse event open learning through universal standardization. \n  \n\n\n### Architecture \n  \nIn the following sections, we present our system, illustrated in  , in an incremental fashion:   joint method  ,   contextual-joint  ,   positional-joint  , and   knowledge-joint  . A detailed explanation of the deep learning architecture, BiLSTM-CRF [ ], and input embeddings used in this system is included in the  . \n  \nCanonical architecture of the proposed system. ADE: adverse drug event; BReason: beginning of reason annotation; CRF: conditional random field; ELMo: Embeddings from Language Models; KB: knowledge base; LSTM: long short-term memory; POS: part-of-speech. \n  \n#### Joint Method \n  \nWe developed a   drug recognition model   followed by 2 joint   drug-centric relation extraction models   (  drug\u2212  {attributes} and   drug\u2212  {  ADE  ,   reason  }), as explained in the following sections. \n\n\n#### Drug Recognition Model \n  \nWe modeled drug recognition as a sequence-labeling task using BiLSTM-CRF and a beginning, inside, and outside of a drug mention (BIO) tagging scheme. The input layer of the BiLSTM-CRF takes word, character, and part-of-speech embeddings. The word embeddings were obtained using Word2Vec representations generated using MIMIC-III. The character and part-of-speech embeddings were initialized randomly. We used CNNs [ ] to encode a character-level representation for a word. \n\n\n#### Drug-Centric Relation Extraction Models \n  \nTo extract entities and relations jointly, we used the encoding scheme proposed in [ ], which takes annotated sentences and produces drug-centric sequences for a specified   target-drug  . For sentences containing multiple identified drugs, 1 drug-centric sequence was generated for each   target-drug  . For example, for the sentence in  , the encoding scheme produced 2 labeled sequences: one with   lisinopril   as the   target-drug   and the other with   mirtazapine  . In each sequence, associated entities with the   target-drug   were labeled using a BIO scheme enhanced with their types. Hence, for the sequence generated with   lisinopril   as the   target-drug  , only   30 mg   and the first   QHS   were labeled using B and I tags, and other entities (eg,   15 mg  ,   PO  , and the second   QHS  ) were labeled as   O.  \n\nWe trained 2 separate models with the BiLSTM-CRF to jointly recognize (1) drug attributes and   drug\u2212  {attributes} relations and (2)   ADE, reason  , and their corresponding relations (  drug\u2212  {  ADE  ,   reason  }). Similar to the   drug recognition model  , the input layer of these models takes word, character, and part-of-speech representations, with additional positional and semantic-tag embeddings. We used the positional embedding technique introduced in [ ] to represent the positional distance from   target-drug   to each word in the input context. We used 3 different semantic tags,   target-drug, duplicate-target-drug,   and   nontarget-drugs  , to represent tokens of the current   target-drug  , other mentions of the same   target-drug  , and other drugs in the input context, respectively. \n\nTo handle intersentential relations, we provided adjacent sentences as an input context to the sentence containing the   target-drug  . We used training data to determine the optimal input context for the 2 models empirically. For the   drug\u2212  {attributes} model, we determined the optimal context as the current sentence with the   target-drug   and the sentences preceding and following it. For the   drug\u2212  {  ADE, reason  } model, the optimal context was the current sentence and the 4 sentences preceding and following it. \n  \nLabel-encoding scheme used in drug-centric relation extraction models. B: beginning; I: inside; PO: orally; QHS: every night at bedtime. \n  \n\n#### Contextual-Joint Model \n  \nWe obtained domain-specific contextualized representations for input contexts by pretraining ELMo on MIMIC-III. These contextualized representations were used to augment the representations used in the input layers of the models in the   joint method  . With the augmented input representations, we trained (1) a   drug recognition    model   and (2) 2   drug-centric relation extraction    models   (  drug\u2212  {attributes} and   drug\u2212  {  ADE, reason  }). \n\n\n#### Positional-Joint Model \n  \nAs the task involves extraction of drug-centric entities and relations, we used the position-attention mechanism to extract entities and relations jointly with respect to an entity of interest (  target-drug  ). \n\nLet   represent the hidden representations of an input sequence obtained from the BiLSTM layer of the   contextual-joint model  . Positional representations   were generated as follows: \n\n\n\n\n\n\n\nwhere   v, W , W , W   are parameters to be learned, and   s   is the score obtained through additive attention. Position-attention computes dependencies among the hidden states: (1)   h   at   target-drug   position   p  , (2)   h   at   j   token in the input sequence, and (3)   h   at current token   t  . For each token   j, s   is computed by (1) comparing   h   with   h   and (2) comparing   h   with   h   The comparison of   h   and   h   helps to encode   target-drug   (positional) information, whereas the comparison of   h   and   h   is useful for matching sentence representations against itself (self-matching) to collect contextual information.   a   is the attention weight produced by the normalization of   s   and is used in computing the positional representation   p   of the current token   t  . Finally, we concatenated this positional representation   p   with its hidden representation   h   to obtain   u :  \n\n\n\nWe trained the 2   drug-centric relation extraction    models   (  drug\u2212  {attributes} and   drug\u2212  {  ADE, reason  }) by feeding these concatenated representations to a CRF layer. During the test phase, we used the   drug recognition model   from the   contextual-joint   for predicting   drugs   and the trained   drug-centric relation extraction    models   for predicting   drug\u2212  {attributes} and   drug\u2212  {  ADE, reason  } relations. \n\n\n#### Knowledge-Joint Model \n  \nAs introduced earlier, background knowledge and hidden relations beyond the contextual and positional information play a crucial role in extracting   drug\u2212  {  ADE, reason  } relations. To address this, we propose the   knowledge-joint   model by enhancing the   positional-joint   model with knowledge embeddings created using the FAERS database. \n\nLet  ,   denote representations of the input sequence tokens obtained from the   drug-reason   and   drug-adverse   knowledge embeddings, respectively. Let   l   and   m   be the beginning and end indices of   target-drug   in the input sequence. The   target-drug    D   and   D ,   corresponding to   drug-reason   and   drug-adverse   knowledge embeddings, were computed by averaging the representations of   target-drug   tokens: \n\n\n\n\n\nThe   target-drug  \u2013centric representations   and   were obtained by computing similarities between input sequence tokens and the   target-drug  : \n\n\n\n\n\nwhere   w   and   w   represent the scalar weights corresponding to   drug-reason,   and   drug-adverse   knowledge embeddings learned during training. Finally, for a token at position   t  , we concatenated its   target-drug  \u2013centric similarities   with positional and hidden representations   u   to produce   k  : \n\n\n\nWe trained a   drug-centric relation extraction    model   (  drug\u2212  {  ADE, reason  }) by feeding these concatenated representations to a CRF layer. During the test phase, we used the   drug recognition model   from the   contextual-joint   model for predicting   drugs   and the trained   drug\u2212  {  ADE, reason  }   model   for predicting   drug\u2212ADE   and   drug\u2212reason   relations. \n\n\n\n### Evaluation Metrics and Significance Tests \n  \nWe evaluated the proposed system using the evaluation script released by the organizers of the n2c2 challenge to measure the lenient precision, recall, and F  scores, explained as follows. For NER, a predicted entity is considered as a true-positive if its span overlaps with a gold annotation and is the correct entity type. For relation extraction, a predicted relation is considered as a true-positive if both entities in the relation are true-positives and the relation type matches the gold annotation. We also report statistical significance on these results with 50,000 shuffles and a significance level set to .05 by using a test script released by the n2c2 organizers based on the approximate randomization test [ ]. \n\nIn the following sections, we present the results of our system. The experimental settings used to achieve these results are provided in  . \n\n\n\n## Results \n  \n### Named Entity Recognition \n  \n presents the results for each proposed incremental approach for NER. Compared with the   joint method  , incorporating contextualized embeddings (  contextual-joint model  ) improved the overall microaveraged F  score by 0.3 percentage points. The improvement was mainly observed in recognizing   drugs   (0.6 points), with some improvements in recognizing   strength   and   reason  . Compared with the   contextual-joint model  , the   positional-joint model   improved the overall micro-F  score by 0.2 points, with significant improvements observed in identifying   reason   (2.1 points) and   ADE   (6.8 points). Compared with the   positional-joint model  , the   knowledge-joint model   further improved the overall micro-F  score by 0.1 points, with significant improvements observed in accurately determining   reason   (1.9 points) and   ADE   (1.7 points). Note that the overall improvement between the   positional-joint   and   knowledge-joint models   is relatively small due to the biased distribution of annotations, as   ADE   and   reason   together constitute less than 10% of the entities. \n\nSignificance tests showed that the improvements in micro-F  score observed with each incremental approach are statistically significant with   P   values of .001, <.001, and <.001 for the   contextual-joint  ,   positional-joint,   and   knowledge-joint   models, respectively. As the   contextual-joint   and   positional-joint models   share the same   drug recognition model  , we ignored drug predictions when performing significance tests. Similarly, the   positional-joint   and   knowledge-joint   models share the same   drug recognition model   and   drug\u2212  {attributes}   model;   therefore, we considered only   ADE   and   reason   predictions when performing significance tests. \n  \nLenient precision, recall, and F1 score of the proposed approaches for named entity recognition. \n    \n\n### Relation Extraction \n  \n presents the results for each proposed incremental approach for relation extraction. Compared with the   joint method  , the   contextual-joint   model improved the overall micro-F1 score by 0.5 percentage points, with the majority of improvements observed in accurately recognizing   drug\u2212strength, drug\u2212frequency, drug\u2212reason,   and   drug\u2212dosage relations  . Compared with the   contextual-joint model  , the   positional-joint model   improved the F  score by 0.4 points with significant improvements observed in determining   drug\u2212ADE   (5.6 points) and   drug\u2212reason   (2.9 points) relations.   The knowledge-joint model   further improved the overall F  score by 0.1 points, with specific improvements in   drug\u2212ADE   by 3.0 points and   drug\u2212reason   by 1.7 points when compared with the   positional-joint model  . Similar to the NER significance results, significance testing for relation extraction showed that the improvements observed with each incremental approach are statistically significant with   P   values of <.001, <.001, and <.001 for the   contextual-joint  ,   positional-joint  , and   knowledge-joint   models, respectively. \n  \nLenient precision, recall, and F1 score of the proposed approaches for relation extraction. \n    \n\n\n## Discussion \n  \n### Principal Findings \n  \nContextualized representations (  contextual-joint  ) are effective in differentiating between words and abbreviations that could have multiple meanings. For example,   ensure   and   contrast   can be understood as either a   drug   (\u201cEnsure: 1 can PO three times daily\u201d and \u201ccontrast-induced nephropathy\u201d) or a verb, and terms such as   blood   could either refer to a drug (\u201ctransfused 1 unit of blood\u201d), that is, substance given to a patient, a test for the drug (\u201cblood alcohol concentration\u201d), or a natural occurring substance in the body (\u201cblood pressure\u201d). Additionally, abbreviations such as   PE   (physical examination versus pulmonary embolism) and   pcp   (primary care physician versus pneumocystis pneumonia) can have multiple expansions. In all the examples above, the   contextual-joint   correctly identifies these entities. \n\nOne prevailing challenge in ADE extraction is the presence of long-distance or intersentential relations. As shown in  , a significant portion of   drug  \u2212{  ADE, reason  } in the data set is intersentential (23% of   drug  \u2212  ADE   and 31.7% of   drug  \u2212  reason  ). These relations typically span long distances, making them more difficult to capture. To study the effectiveness of the proposed approaches over long-distance relations, we calculated the F  scores on   drug  \u2212{  ADE, reason  } with an increasing number of tokens between entities. As shown in  , we find that the positional  -joint   model performs significantly better than the   contextual-joint   model with increasing distance between entities, suggesting that the   positional-joint   can effectively model long-distance relations. \n\nIncorporating knowledge embeddings learned on the FAERS improved   drug  \u2212{  ADE, reason  } relation extraction, especially in the case of long-distance relations or when contextual clues are insufficient. As shown in  ,   the knowledge-joint   model further improved on the   positional-joint   model at all distances. The   knowledge-joint   model was also useful in cases of insufficient or ambiguous context in extracting the correct relation. For example, in the phrase \u201cWellbutrin - nausea and vomiting,\u201d the relation is indicated only by an uninformative hyphen, with no contextual clues to indicate the type of relation. Similarly, in \u201cPatient had history of depression and was on elavil previously,\u201d it is unclear whether the   history of depression   was previously treated by   drug  \u2212  reason   or caused by   drug  \u2212  ADE   of the drug   elavil  . Furthermore, the   knowledge-joint   also helped to extract correct relations when multiple drugs and candidate   ADEs   and   reasons   are discussed in a given context. For example, in \u201cUpon arrival, she was hypertensive and had a fever. She was given Tylenol  ,  \u201d based on sentence construction, 2 candidate   reasons   (  hypertensive   and   fever  ) may be associated with the   drug   Tylenol  . Knowledge is required to infer that of the two, only   fever   is related to   Tylenol  . \n  \nF1 scores of approaches with increasing distance between entities for relation extraction. ADE: adverse drug event. \n  \n\n### Error Analysis \n  \nWe investigated the most common error categories by entity and relation type and present these in  . Most of the errors in recognizing   drug  s were due to abbreviations, misspellings, generic terms, or linguistic shorthand. For   strength   and   dosage  , these entities were often mislabeled as each other\u2014both are often numeric quantities and used in similar contexts. For   duration   and   frequency  , most of the errors resulted from these entities being expressed in colloquial language. \n\nIntersentential relations remain a major category of false-negative errors for all relations despite improvements from the position-attention mechanism. For   drug\u2212  {attributes}, these errors were likely due to an insufficient number of such examples in the training data (approximately 4%). In addition to errors from intersentential relations, other important categories for false-negative   drug\u2212  {  ADE, reason  } include (1)   ADE   or   reasons   expressed in generic terms, (2)   reasons   such as procedures and activities (eg,   angioplasty/stenting  ) that occur infrequently in the training set  ,   and (3)   ADE   or   reasons   expressed as abbreviations that are nonstandard or ambiguous  .  \n\nFalse-positive errors in   drug\u2212  {  ADE  ,   reason  } mainly fall into 2 categories. In the first, one of the entities participating in the relation is negated, hypothetical, or conditional, such as when a drug is withheld to avoid an anticipated ADE (eg, contraindications). In the second, the same concept (  drug  ,   ADE  , or   reason  ) is mentioned multiple times in the same context, and the system associated the relation to one mention whereas the ground truth to the other. To add further complexity, these mentions may be synonyms, for example, \u201cthe pain medications (morphine, vicodin, codeine) worsened your mental status and made you delirious.\u201d With multiple possible   drug  \u2212  ADE   relations, some combinations were not captured in the ground truth, resulting in false-positives that may not be true errors. \n  \nError analysis on our best-performing model (knowledge-joint). \n    \n\n### Document-Level Analysis \n  \nFrom an end user perspective, the core information needed for patient care purposes is a patient-level summary of these relations, which is a unique set of extracted relations after normalization. To evaluate our system for this purpose, we measured   drug\u2212ADE   and   drug\u2212reason   F  scores by considering unique pairs of relation mentions at the document level, presented in  . We observed scores at the document level to be 1 to 2 percentage points higher than the instance level. \n  \nDocument-level analysis for drug\u2212reason and drug\u2212adverse drug event relations. \n    \n\n### Comparison With Previous Work \n  \nFor NER, the state-of-the-art system [ ] used an ensemble (committee) of 3 different methods: CRF, BiLSTM-CRF, and joint approach. They showed that the BiLSTM-CRF is the best among the single models. Thus, we compare our best model (  knowledge-joint  ) with their best-performing single model and committee approach, as shown in  . Overall,   the knowledge-joint   model outperformed the single model by 0.2 percentage points and achieved similar micro-F  to the committee approach. Notably,   the knowledge-joint   model significantly outperformed the committee approach in recognizing the crucial   ADE   (0.5 points) and   reason   (5.2 points) entities. \n\nFor relation extraction, the state-of-the-art system used the committee approach for NER, convolutional neural network \u2013 recurrent neural network (CNN-RNN) for relation classification, and postprocessing rules. Although postprocessing rules are commonly used in competitions, they often do not generalize across data sets and therefore are of limited interest in this research. As shown in  ,   the knowledge-joint model   outperformed the state-of-the-art approach, both with (0.4 points) and without rules (1.6 points). Notably,   the knowledge-joint   model achieved the best results and outperformed the state-of-the-art in recognizing the most crucial and difficult to extract relations:   drug\u2212reason   (7.1 points) and   drug\u2212ADE   (1.4 points). \n  \nThe lenient F1 scores for named entity recognition of single and state-of-the-art ensemble models compared with our best model. The lenient F1 scores for relation extraction of state-of-the-art ensemble models with and without rules, compared with our best model. \n    \n\n### Limitations and Future Work \n  \nWe acknowledge several limitations of this study. First, these results are specific to the n2c2 data set, which contains only intensive care unit (ICU) discharge summaries from a single health care organization. Ground truth generation and evaluation on a more diverse data set is needed to better understand the effectiveness of these proposed approaches. Second, we observed some annotation errors in the ground truth, likely due to the complex nature of the task. Further investigation is needed to quantify the prevalence of such errors and their impact on the results. \n\nDespite achieving state-of-the-art results, the proposed system still has room for improvement, specifically in recognizing intersentential   drug  \u2212{  ADE  ,   reason  } relations. To further improve ADE extraction, we plan to explore the following research areas: \n  \nAlthough we incorporated knowledge graph embeddings, other advanced methods that use higher-order proximity and role-preserving network embedding techniques have shown promising results in the general domain. We plan to explore methods such as Edge Label Aware Network Embedding [ ] rather than training separate graph embeddings for   drug\u2212{ADE, reason}   relations. \n  \nThe field of contextual embeddings has evolved quickly along with the release of newer language representation models trained on clinical text. We plan to explore BERT [ , ], which utilizes a transformer network to pretrain a language model for extracting better contextual word embeddings. \n  \nTo address some of the findings from the error analysis, we plan to leverage our clinical abbreviation expansion components [ ] to help resolve ambiguous mentions and also incorporate assertion recognition [ ] to capture the belief state of the physician on a concept (negated, hypothetical, conditional). \n  \nAs mentioned earlier, the proposed models performed poorly on intersentential relation extraction. To address this, we plan to explore N-ary relation extraction for cross-sentence relation extraction using graph long short-term memory networks [ ]. \n  \n\n### Conclusions \n  \nWe presented a system for extracting drug-centric concepts and relations that outperformed current state-of-the-art results. Experimental results showed that contextualized embeddings, position-attention mechanisms, and knowledge embeddings effectively improve deep learning-based concepts and relation extraction. Specifically, we showed the effectiveness of a position-attention mechanism in extracting long-distance relations and knowledge embeddings from the FAERS in recognizing relations where contextual clues are insufficient. \n\n\n \n", "metadata": {"pmcid": 7382020, "text_md5": "e6c3a083a447a1f063afaac84f9e62f0", "field_positions": {"authors": [0, 114], "journal": [115, 130], "publication_year": [132, 136], "title": [147, 332], "keywords": [346, 542], "abstract": [555, 3685], "body": [3694, 42005]}, "batch": 1, "pmid": 32459650, "doi": "10.2196/18417", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7382020", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7382020"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7382020\">7382020</a>", "list_title": "PMC7382020  Extraction of Information Related to Drug Safety Surveillance From Electronic Health Record Notes: Joint Modeling of Entities and Relations Using Knowledge-Aware Neural Attentive Models"}
{"text": "Hemati, Wahed and Mehler, Alexander\nJ Cheminform, 2019\n\n# Title\n\nLSTMVoter: chemical named entity recognition using a conglomerate of sequence labeling tools\n\n# Keywords\n\nBioCreative V.5\nCEMP\nCHEMDNER\nBioNLP\nNamed entity recognition\nDeep learning\nLSTM\nAttention mechanism\n\n\n# Abstract\n \n## Background \n  \nChemical and biomedical   named entity recognition   (NER) is an essential preprocessing task in   natural language processing  . The identification and extraction of named entities from scientific articles is also attracting increasing interest in many scientific disciplines. Locating chemical named entities in the literature is an essential step in chemical text mining pipelines for identifying chemical mentions, their properties, and relations as discussed in the literature. In this work, we describe an approach to the BioCreative V.5 challenge regarding the recognition and classification of chemical named entities. For this purpose, we transform the task of NER into a sequence labeling problem. We present a series of sequence labeling systems that we used, adapted and optimized in our experiments for solving this task. To this end, we experiment with hyperparameter optimization. Finally, we present LSTMVoter, a two-stage application of   recurrent neural network  s that integrates the optimized sequence labelers from our study into a single ensemble classifier. \n\n\n## Results \n  \nWe introduce LSTMVoter, a bidirectional   long short-term memory   (LSTM) tagger that utilizes a conditional random field layer in conjunction with attention-based feature modeling. Our approach explores information about features that is modeled by means of an attention mechanism. LSTMVoter outperforms each extractor integrated by it in a series of experiments. On the BioCreative IV chemical compound and drug name recognition (CHEMDNER) corpus, LSTMVoter achieves an F1-score of 90.04%; on the BioCreative V.5 chemical entity mention in patents corpus, it achieves an F1-score of 89.01%. \n\n\n## Availability and implementation \n  \nData and code are available at  . \n\n \n\n# Body\n \n## Introduction \n  \nIn order to advance the fields of biological, chemical and biomedical research, it is important to stay on the cutting edge of research. However, given the rapid development of the disciplines involved, this is difficult, as numerous new publications appear daily in biomedical journals. In order to avoid repetition and to contribute at least at the level of current research, researchers rely on published information to inform themselves about the latest research developments. There is therefore a growing interest in improved access to information on biological, chemical and biomedical data described in scientific articles, patents or health agency reports. In this context, improved access to chemical and drug name mentions in document repositories is of particular interest: it is these entity types that are most often searched for in the PubMed [ ] database. To achieve this goal, a fundamental preprocessing step is to automatically identify biological and chemical mentions in the underlying documents. Based on this identification, downstream NLP tasks such as the recognition of interactions between drugs and proteins, of side effects of chemical compounds and their associations with toxicological endpoints or the investigation of information on metabolic reactions can be carried out. \n\nFor these reasons, NLP initiatives have been launched in recent years to address the challenges of identifying biological, chemical and biomedical entities. One of these initiatives is the BioCreative series, which focuses on biomedical text mining. BioCreative is a \u201cChallenge Evaluation\u201d, in which the participants are given defined text mining or information extraction tasks in the biomedical and chemical field. These tasks include   GeneMention detection (GM)   [ ,  ],   Gene Normalization (GN)   [ \u2013 ],   Protein\u2013Protein Interaction (PPI)   [ ],   Chemical Compound and Drug Name Recognition (CHEMDNER)   [ ,  ] and   Chemical Disease Relation Extraction   [ ,  ] tasks. \n\nThe current   BioCreative V.5   task consists of two off-line tasks, namely   Chemical Entity Mention in Patents (CEMP)   and   Gene and Protein Related Object Recognition (GPRO)  . CEMP requires the detection of chemical named entity mentions. The task requires detecting the start and end indices corresponding to chemical entities. The GPRO task requires identifying mentions of gene and protein related objects in patent titles and abstracts [ ]. In this work, we focus on the CEMP task. The CEMP task is an abstraction of the common named entity recognition (NER) tasks, which can be reduced to a sequence labeling problem, where the sentences are represented as sequences of tokens. The task is then to tag chemical entity mentions in these sequences. The settings of the CEMP task are similar to the chemical entity mention recognition (CEM) subtask of CHEMDNER challenge in BioCreative IV [ ]. Therefore, we addressed both tasks and their underlying corpora in our experiments. Note that the current article describes an extension of previous work [ ]. \n\nThe article is organized as follows: First we describe our methodical apparatus and resources. This includes the data and corpora used in our experiments. Then, we introduce state-of-the-art tools for NER and explain how we adapted them to perform the CEMP task. Next, we present a novel tool for combining NER tools, that is, the so-called LSTMVoter. Finally, we present our results, conclude and discuss further work. \n\n\n## Materials and methods \n  \nIn this section, we first describe the datasets used in our experiments. Then, the two-stage application of LSTMVoter is introduced. \n\n### Datasets \n  \nIn our experiments, two corpora of the BioCreative Challenge were used: the CHEMDNER Corpus [ ] and the CEMP Corpus [ ]. \n\nThe CHEMDNER corpus consists of 10,000 abstracts of chemistry-related journals published in 2013. Each abstract was human annotated for chemical mentions. The mentions were assigned to one of seven different subtypes (ABBREVIATION, FAMILY, FORMULA, IDENTIFIER, MULTIPLE, SYSTEMATIC, and TRIVIAL). The BioCreative organizer divided the corpus into training (3500 abstracts), development (3500 abstracts) and test (3000 abstracts) sets. \n\nFor CEMP task, the organizers of   BioCreative V.5   provided a corpus of 30,000 patent abstracts from patents published between 2005 and 2014. These abstracts are divided into training (21,000 abstracts) and test (9000 abstracts) sets. The corpus is manually annotated with chemical mentions. For the construction of the CEMP corpus the annotation guidelines of CHEMDNER were used. Therefore, CEMP contains the same seven chemical mention subtypes as CHEMDNER. Table   shows the number of instances for both corpora for each of these subtypes.   \nNumber of instances for each subtype of CEMP and CHEMDNER corpus \n  \n\nBoth corpora were enriched with additional linguistic features. For this, multiple preprocessing steps were applied on each set including sentence splitting, tokenization, lemmatization and fine-grained morphological tagging by means of Stanford CoreNLP [ ]\u00a0and TextImager\u00a0[ ]. In addition, tokens were split on non-alphanumeric characters, as this variant brought a performance increase. Since the chemical mention detection task can be reduced to a sequence labeling problem, the corpora were converted into a sequence structure. To this end, a sequence of documents with sequences of sentences each containing a sequence of tokens was constructed and transformed according to a TSV format. Each word and its associated features are in one line separated by tabs. Sentences are separated by an empty line. For the labeling of the mentions, the IOB tagging scheme [ ] was used (I =   inside of an entity  , O =   outside of an entity  , B =   beginning of an entity  ). IOB allows the annotation of entities that span multiple tokens, where the beginning and the end of the entity is marked. This enables models to learn transition probability. LSTMVoter needs four datasets for the training process. Two pairs of training and development sets are required. Each pair is needed in one of the two stages of LSTMVoter (see section \u201cSystem description\u201d). Therefore, we divided the training set of CEMP into two series of training, development and test sets (each half of the original training set was split according to the pattern 60%/20%/20%), where the first series is used for stage one, and the second for stage two. For the CHEMDNER corpus the available training and development sets were joined and split into training and development sets according to the schema 80%/20%\u2014as before, we distinguish two such series. For evaluating our classifiers with respect to CHEMDNER, the test set provided by the organizers of the challenge was used. For the following experiments we used the corpora described as so far. \n\n\n### System description \n  \nIn this section we describe our system. Our approach implements a two-stage application of long short-term memory (LSTM) using a conglomerate of sequence labelers for the detection of chemical mentions. \n\nIn the first stage, we trained and optimized five tools for NER for tackling this task, namely   Stanford Named Entity Recognizer   [ ],   MarMoT   [ ],   CRF++   [ ],   MITIE   [ ] and   Glample   [ ]. For each of them, we optimized the corresponding hyperparameter settings. Generally speaking, hyperparameter tuning is a challenging task in machine learning. The optimal set of hyperparameters depends on the model, the dataset and the domain [ ]. Our experiments focused on optimizing the hyperparameters of each NER system independently, which led to a noticeable increase in F-score compared to the default settings. For each NER, we performed the Tree-structured Parzen Estimator (TPE) [ ] with 200 iterations. The results of the best performing model for each of these NER is listed in Table  . \n\nThe NER tools are more or less independent of each other in the sense that one can find a subset of test cases that are correctly processed by one of them, but not by another. Therefore, combining these NERs is a promising candidate for increasing performance. We started with computing combinations of these NERs by means of a simple majority vote [ ], where the target label is selected, that is assigned by the majority of classifiers. Our experiments show that a simple majority vote brings no gain in performance compared to the best performing reference systems being examined in our study (see Table  ). Thus, we developed a two-stage model, the so-called LSTMVoter, which trains a recurrent neural network (RNN) with attention mechanism to learn the best combination of the underlying sequence labeling tools from stage one.   \nArchitecture of LSTMVoter \n  \n\nIn the second stage, we combine the sequence labelers of stage one with two bidirectional   long short-term memory   (LSTM) networks with attention mechanism and a conditional random field (CRF) network to form LSTMVoter. The architecture of LSTMVoter is illustrated in Fig.  . The core of LSTMVoter is based on [ ].   \nA long short-term memory cell \n    \nA bidirectional LSTM network \n  \n\nLSTM networks are a type of RNN [ ]. RNN allow the computation of fixed-size vector representations for sequences of arbitrary length. An RNN is, so to speak, a function that reads an input sequence   of length   n   and produces an output vector  , which depends on the entire input sequence. Though, in theory, an RNN is capable of capturing long-distance dependencies in the input sequence, in practice, they may fail due to the problem of vanishing gradients [ ,  ]. On the other hand, LSTMs include a memory cell, which can maintain information in memory for long periods of time [ ,  ]. This enables finding and exploiting long range dependencies in the input sequences to cope with the problem of vanishing gradients. Figure   illustrates an LSTM memory cell, which is implemented as follows: where   is the input vector (e.g. word embedding) at time   t  .   is the hidden state vector, also called output vector, that contains information at time   t   and all time steps before   t  .   is the logistic sigmoid function [ ]. Input gate   i  , forget gate   f  , output gate   o   and cell vector   c   are of the same size as the hidden state vector   h  .  ,  ,   and   are the weight matrices for the hidden state  .  ,  ,   and   denote the weight matrices of different gates for input  . \n\nFor LSTMVoter, we apply an LSTM to sequence tagging. Additionally, as proposed by [ ], we utilize bidirectional LSTM networks. Figure   illustrates a bidirectionalLong short-term memory (Bi-LSTM) network, where the input sequence (  Treatment with haloperidol or reserpine ...  ) and the output sequence (  O, O, B-Trivial, O, B-Trivial, ...  ) are fed as a training instance to a Bi-LSTM. In Bi-LSTMs, the input sequence is presented forward and backward to two separate hidden states to capture past and future information. To efficiently make use of past features (via forward states) and future features (via backward states) for a specific time frame, the two hidden states are concatenated to form the final output. In the final output of a Bi-LSTM, all information of the complete sequence is compressed into a fixed-length hidden state vector, which may result in information loss. To overcome this information loss, an attention mechanism is introduced, which partially fixes the problem. \n\nThe method of attention mechanism has recently gained popularity in image caption generation [ ], visual question answering [ ] and language modeling tasks [ \u2013 ]. The attention mechanism plugs a context vector on top of a layer, which enables to take all cells\u2019 outputs as input to compute a probability distribution. This enables to capture global information rather then to infer based on one output vector. \n\nFor LSTMVoter, we utilized Bi-LSTM with attention mechanism to model character-level features (see Fig.  ,   Char-Bi-LSTM  ). Character-level features in chemical named entities contain rich structure information, such as prefix, suffix and n-grams. Unlike previous methods [ \u2013 ], character-level features do not have to be defined manually, rather they can be learned during training. Unlike [ ], who encodes the entire character sequence into a fixed-size vector for each word, we utilize the character-level attention mechanism introduced by [ ]. This has the advantage, that by using the attention mechanism, the model is able to dynamically decide how much information and which part of a token to use. \n\nIn addition to the character-level features, we implemented word embeddings into our model to capture dependencies between words (see Fig.  ,   Word-Embeddings  ). For this, we evaluated various methods, namely GloVe [ ], Dependency-Based embeddings [ ,  ] trained on the English Wikipedia, and word2vec [ ] trained on the English Wikipedia and a biomedical scientific literature corpus containing PubMed abstracts and full texts. In our experiments, the word2vec model trained on biomedical scientific literature gave the best results. \n\nTo utilize the results of the NERs from stage one, we encode the respective results of the NERs into one-hot vectors concatenated to a feature vector (see Fig.  ,   Stage-One-Features  ). An attention mechanism is placed on the feature vector. By creating a probability distribution through the attention mechanism, LSTMVoter learns how to weight each result of the NERs from stage one. With the attention vector it is even possible to determine for each element of a sequence how important the individual partial results from stage one were. This has the advantage that the model is no longer a black box, but can be interpreted as to how important the individual results from stage one were. \n\nAll previous elements of LSTMVoter encode word-based information. Another Bi-LSTM is used to learn relationships between these word-based information (see Fig.  ,   Bi-LSTM  ). \n\nTo deal with the independent label output problem, we utilize the output vector as elements. For this we combine the Bi-LSTM layer with a linear-chain CRF (see Fig.  ,   CRF  ). Linear-chain CRFs define the conditional probability of a state sequence to be: where   is the normalization factor that makes the probability of all state sequences sum to one;   is a feature function, and   is a learned weight associated with feature  . Feature functions measure the aspect of a state transition,  , and the entire observation sequence,   x  , centered at the current time step,   j  . Large positive values for   indicate a preference for such an event, whereas large negative values make the event unlikely. \n\nFinally, to optimize the hyperparameters, the Tree Structure Parzen estimator was used. \n\n\n\n## Results \n  \nThis section presents the results of our experiments for the chemical named entity recognition on CEMP and CHEMDNER corpus. For evaluation the BioCreative Team has specified standard evaluation statistics, namely precision (P), recall (R) and F1-score (F) [ ]. For each sequence labeling tool, the hyperparameters were optimized using Tree Structure Parzen Estimators, which led to a noticeable increase of performance. For example, in the optimization process of CRF++, the difference between the worst to the best performer is 65%. The results show the need for machine learning algorithms to perform hyperparameter optimization.   \nComparison of annotators trained and tested on CEMP and CHEMDNER corpora measured by precision (P), recall (R), f1-score (F1) \n  \nBold was intended to compare LSTMVoter to the best reference tool. Bold now shows the system with the highest F-Score, which is LSTMVoter \n  \n\nTable   shows the comparison of annotators trained on CEMP and CHEMDNER corpus. The results listed are those obtained after the hyperparameter optimization described in the methods section, which were trained, optimized and tested on the corpora described in this section. Each sequence labeling system classifies a different subset correctly. The combination of sequence labelling systems in a majority vote did not improve performance and is even below the best sequence labelling systems. In contrast, LSTMVoter increases the performance and performs best in our experiments. \n\n\n## Conclusions \n  \nIn this work, we compared a set of sequence labeling systems. We trained and optimized every sequence labeling system to detect chemical entity mention by means the TPE. We showed that optimizing hyperparameter can be crucial. One sequence labeling system in our experiments gained an improvement of more than 65 %. We showed that a naive majority vote does not bring any improvement. For this reason, we introduced and evaluated LSTMVoter, a two-stage tool for combining underlying sequence modeling tools (as given by the NER of our comparative study). LSTMVoter achieved an improvement of up to 5 % compared to the best reference systems examined in our study. This two-level classifier appears to be capable of being further developed and improved by feeding it with the output of additional sequence labelling systems. In any event, our results and those of the other participants of BioCreative V.5 Task show that the task of NER of chemical entities has not been sufficiently solved yet. For a better recognition, a larger corpus should be generated so that today\u2019s popular deep learning algorithms can work on this data. A kind of human-in-the-loop architecture for automatic annotation and intellectual rework would also be helpful at this point in order to successively increase and improve the amount of data. \n\n \n", "metadata": {"pmcid": 6689880, "text_md5": "a39d0d4c75779f9ac90f490413cd162d", "field_positions": {"authors": [0, 35], "journal": [36, 48], "publication_year": [50, 54], "title": [65, 157], "keywords": [171, 272], "abstract": [285, 2077], "body": [2086, 19742]}, "batch": 1, "pmid": 30631966, "doi": "10.1186/s13321-018-0327-2", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6689880", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6689880"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6689880\">6689880</a>", "list_title": "PMC6689880  LSTMVoter: chemical named entity recognition using a conglomerate of sequence labeling tools"}
{"text": "Baltoumas, Fotis A and Zafeiropoulou, Sofia and Karatzas, Evangelos and Paragkamian, Savvas and Thanati, Foteini and Iliopoulos, Ioannis and Eliopoulos, Aristides G and Schneider, Reinhard and Jensen, Lars Juhl and Pafilis, Evangelos and Pavlopoulos, Georgios\u00a0A\nNAR Genom Bioinform, 2021\n\n# Title\n\nOnTheFly2.0: a text-mining web application for automated biomedical entity recognition, document annotation, network and functional enrichment analysis\n\n# Keywords\n\n\n\n# Abstract\n \nExtracting and processing information from documents is of great importance as lots of experimental results and findings are stored in local files. Therefore, extracting and analyzing biomedical terms from such files in an automated way is absolutely necessary. In this article, we present OnTheFly , a web application for extracting biomedical entities from individual files such as plain texts, office documents, PDF files or images. OnTheFly  can generate informative summaries in popup windows containing knowledge related to the identified terms along with links to various databases. It uses the EXTRACT tagging service to perform named entity recognition (NER) for genes/proteins, chemical compounds, organisms, tissues, environments, diseases, phenotypes and gene ontology terms. Multiple files can be analyzed, whereas identified terms such as proteins or genes can be explored through functional enrichment analysis or be associated with diseases and PubMed entries. Finally, protein\u2013protein and protein\u2013chemical networks can be generated with the use of STRING and STITCH services. To demonstrate its capacity for knowledge discovery, we interrogated published meta-analyses of clinical biomarkers of severe COVID-19 and uncovered inflammatory and senescence pathways that impact disease pathogenesis. OnTheFly  currently supports 197 species and is available at   and  . \n \n\n# Body\n \n## INTRODUCTION \n  \nThe extraction and processing of information from literature is of paramount importance in biomedical sciences. More often than not, researchers need to cope with the task of manually sifting through large amounts of text in various forms (e.g. scientific articles in various file formats, data in spreadsheets and images) in order to obtain pertinent biological information about genes, proteins, chemical compounds, organisms and biological processes and functions. The manual approach is summarized as follows: (i) read through the article texts, (ii) detect biomedical entities of interest and (iii) query one or more databases for the relevant information. As the volume of literature and experiment-derived datasets continues to increase, this iterative procedure can become slow and heavy. For this reason, text-mining methods are often employed to aid researchers in automatically extracting and searching meaningful biological terms from texts. \n\nOne particular application of text mining that is widely used in scientific text processing is named entity recognition (NER), i.e. identifying words or phrases of interest (the so-called \u2018named entities\u2019) mentioned in plain text, and normalizing them to appropriate database/ontology identifiers ( ). In biological and biomedical sciences, these entities include gene and protein names, organisms (scientific or common names), chemical compounds and ontology terms such as biological processes, cellular components, molecular functions, diseases, phenotypes and environmental descriptors. Numerous computational tools and web services for NER have been proposed [reviewed in ( )]. Characteristic examples include tools like EXTRACT ( ), PubTator ( ), HunFlair ( ), BioTextQuest  ( ), Saber ( ), OGER++ ( ) and others. Altogether, these tools detect genes/proteins, genetic variants, diseases, chemical compounds, organisms, diseases and cell lines mentioned in documents. However, while being able to successfully identify entities is critical, NER is only one of the components for meaningful parsing and analysis of the scientific literature. The result of running NER on a set of documents will be a long list of genes and other entities, which the user then needs to navigate and make sense of. \n\nNetwork visualization is one popular way to get an overview of a large number of entities. Molecular networks can be obtained from a wide variety of different sources, including manually curated pathway databases, e.g. KEGG ( ) and Reactome ( ), and databases of interaction experiments, e.g. IntAct ( ) and BioGRID ( ). Resources like STRING ( ) and STITCH ( ) combine these with additional associations that are predicted or extracted from the biomedical literature through automatic text mining ( ). All of these resources can be queried manually by the user, or automatically through application programming interfaces (APIs), packages or plugins. Their results can be used to generate, visualize and analyze interaction networks with network viewers such as Cytoscape ( ), Gephi ( ), NORMA ( ) and others [reviewed in ( , )]. \n\nFunctional enrichment analysis is another commonly used approach, which summarizes a long list of genes by comparing their associated functional annotations against a collection of gene set annotation terms, each representing a gene ontology term, molecular pathway, protein domain, disease etc. Statistically enriched gene annotation sets are then identified by comparing their frequency against a reference background list. Widely used tools for enrichment analysis include DAVID ( ), PANTHER ( ), WebGestalt ( ), aGOtool ( ) and g:Profiler ( , ), each adopting different statistical tests and supporting different enrichment options [reviewed in ( , )]. \n\nIn this article, we present OnTheFly , a full, user-friendly pipeline which goes far beyond just applying NER and allows users to start from a collection of documents and, via a set of entities, perform network and enrichment analyses. Through a user-friendly, interactive web interface, OnTheFly  supports a plethora of different file formats for text mining and biomedical entity extraction, including text documents (in both editable and read-only formats), spreadsheets and image files. Through NER, OnTheFly  can recognize and retrieve a large variety of both biological and biomedical terms. Extracted protein and chemical entities can be combined to create datasets for various types of analyses, including functional enrichment, related literature finding, associations with diseases and protein domain reporting from protein family databases. In addition, OnTheFly  allows the generation and visualization of protein\u2013protein and protein\u2013chemical interaction networks. The capabilities of OnTheFly  are shown using a case study in which several inflammatory and senescence pathways that impact COVID-19 pathogenesis have been unraveled after analyzing six clinical articles with mentions to clinical biomarkers of severe COVID-19. \n\n\n## MATERIALS AND METHODS \n  \nThe OnTheFly  pipeline consists of four steps (Figure  ): (i) uploading of input files and conversion from their original format to HTML, (ii) identification of bioentities with EXTRACT, (iii) functional annotation on a set of selected identifiers and (iv) network analysis. A detailed description of these steps is provided in the following subsections. \n  \nFlowchart of the OnTheFly  backend pipeline for file conversion, named entity recognition and data analysis. \n  \n### File conversion pipeline \n  \nIn its current version, OnTheFly  supports annotation for PDF files, Office-formatted documents, various flat text file formats, including XML and images. In the online version, each file must have a maximum size of 10 MBs. Users can upload multiple files simultaneously and process them separately or in combination. \n\nOnTheFly  uses various tools and pipelines in its backend to convert uploaded files to HTML format prior to annotation. PDF files are converted with the use of   pdf2htmlEX  , an open-source package ( ), whereas the   LibreOffice   universal converter (  unoconv  ) is used to convert Office files, including formatted/enriched text files (MS Office .doc/.docx, OpenOffice .odt, Rich Text Format .rtf) and spreadsheets (MS Excel .xls/xlsx, OpenOffice Spreadsheet .odp), tab- and comma-delimited table files (.tsv and .csv, respectively), XML files, as well as flat text (.txt) files. Notably, in the case of spreadsheet documents, the converter is capable of handling each of the sheets. Almost all of the aforementioned file types are converted to HTML with their overall layout, text formatting, formulas and images maintained to the largest extent possible. The only exception are XML files which are rendered as plain texts, without any syntax highlighting for the XML tags. Future versions of OnTheFly will address this issue by implementing more advanced visualization options, including better support for the XML format. \n\nIn addition to the above, OnTheFly  can utilize optical character recognition (OCR) scan on images with no text encoding. Both common (.bmp, .jpg, .png, .tiff) and PostScript-compliant image file formats (.ps and .eps) are supported. Image preprocessing and file format conversion is performed with the open-source package   ImageMagick  . OCR scanning of images is performed using the   tesseract-ocr   package ( ) to produce PDF files with parseable text, which are then processed as described above. Successful OCR scanning heavily depends on the quality of the imported image, including its resolution. As a result, images containing text elements in rotated orientation or embedded in complex graphical shapes, or images with low resolution may result in poor OCR results. \n\nAfter files have been uploaded and converted, the resulting HTML version can be inspected in a viewer that has each document in a separate tab. This allows the user to identify conversion or OCR problems before continuing the analysis. \n\n\n### Document annotation using named entity recognition (NER) \n  \nOnce the files have been uploaded, users can annotate them with the help of EXTRACT tagging service ( ). EXTRACT performs dictionary-based NER using the highly efficient   tagger   software ( ) to detect words and phrases, which correspond to biomedical entities. This is performed through a dictionary-based approach, through which biological and biomedical terms, both canonical and synonyms, are assigned to database and ontology identifiers; thus producing concept-normalized results. In detail, EXTRACT is capable of identifying environment descriptive terms from environment ontology (e.g. desert and forest) ( ), organism mentions from NCBI Taxonomy ( ), tissue terms from BRENDA Tissue Ontology ( ), disease mentions from Disease Ontology ( ), phenotypes from Mammalian Phenotype Ontology ( ), biological processes, cellular components, molecular functions from Gene Ontology ( , ), small chemical molecules from PubChem ( ), non-coding RNAs from RAIN ( ) and protein-coding genes from STRING ( ). In the implementation of OnTheFly , NER can be performed for a list of 197 organisms. \n\nOnce the annotation parameters (entity types and organisms) have been set and a NER process has been completed, OnTheFly  will return the annotated document with all of the recognized terms linked and highlighted using different colors (Figure  ). On mouse-click action on a term, OnTheFly  will generate a popup window with details about the biomedical entity and links to external databases. In case of term disambiguation (e.g. when a term comes from several organisms or corresponds to more than one entity type), OnTheFly  will report all of the possible options. For a more comprehensive summary, all of the identified terms along with their database identifiers and links are collected in an interactive table and can be exported as a CSV file. The table results can be narrowed down after filtering for entity type (e.g. genes/proteins and diseases) at any stage. The annotation process is presented in Figure  . \n  \nPDF annotation using NER for article by Xiahou   et\u00a0al.  , 2017 ( ). (  A  ) The PDF abstract in its simple form. (  B  ) The annotated abstract using   Mus musculus   as an organism. (  C  ) Popup windows with information about the identified term. The term is colored according to its type and original links to external databases are provided. (  D  ) A summary table with some of the identified terms. \n  \n\n### Functional enrichment analysis \n  \nOnTheFly  uses two tools,   g:Profiler   ( , ) and   aGOtool   ( ), to provide rich functional enrichment analysis for a selected set of genes/proteins collected by one or multiple files. The user can customize parameters for the enrichment analysis and choose from a list of 197 organisms. OnTheFly  uses g:Profiler to identify enriched functional terms from Gene Ontology ( , ), pathways from KEGG ( ), Reactome ( ) and WikiPathways ( ), protein complexes from CORUM ( ), expression data from Human Protein Atlas ( ), regulatory motifs from TRANSFAC ( , ) and miRTarBase ( ), and phenotypes from the Human Phenotype Ontology ( ). The analysis results from g:Profiler are complemented by further enrichment analyses from aGOtool to also identify enriched terms from the UniProt keyword classification system, protein families and domains from Pfam ( ) and InterPro ( ), as well as human diseases from the DISEASES database ( ). g:Profiler and aGOtool test for statistically significant enrichment by using Fisher\u2019s exact test to compare the user-defined input dataset (foreground) to a background set from organism-specific genes annotated in the Ensembl database ( ) and UniProt Reference Proteomes ( ), respectively. The resulting   p  -values are corrected for multiple testing using either g:SCS (only in case of g:Profiler), Bonferroni correction or Benjamin\u2013Hochberg false discovery rate (FDR), all of which can be used as thresholds for the results. Enrichment analysis is performed using ENSEMBL IDs as input, while results can be reported as Entrez, UniProt, EMBL, ENSEMBL and RefSeq gene/protein names/identifiers, based on the user\u2019s choice. \n\nFunctional enrichment results are reported in interactive searchable tables displaying details about each functional term. One can expand each row of the table to see which of the identified genes/proteins were found to be associated with the functional term. For example, in the case of a KEGG pathway, one can see how many proteins or genes were found to be related to it and get redirected to the KEGG repository to see the actual schema of the pathway in a static form with all of the detected genes/proteins highlighted. In the case of g:Profiler, an interactive Manhattan plot is offered for a clearer overview. In this plot, functional terms are grouped along the   x  -axis and colored by their data source, whereas the   y  -axis shows the significance (  P  -value) of each term. Hovering over a data point reveals a tooltip with key information about the functional term. Finally, the most significant functional terms are shown as a bar chart, which the user can customize to show the desired number of terms. All of the aforementioned reports can be exported and saved in various file formats (CSV, XLS, PDF). An overview is shown in Figure  . \n  \nOnTheFly \u2019s functional enrichment. (  A  ) Functional enrichment input parameters. (  B  ) Summary table with the functional terms and the corresponding identified entities. Results from KEGG are shown. (  C  ) A functional enrichment overview with the use of a Manhattan plot. (  D  ) Bar plot for the distribution of enriched genes into metabolic pathways obtained from three pathway databases (KEGG, Reactome, WikiPathways), with the results of each database colored differently. The bar length is proportional to the extent of enrichment for each term, as represented by -log (  P  -value). (  E  ) Portion of a generated KEGG pathway with the genes found in documents highlighted in orange. \n  \n\n### Publication enrichment analysis \n  \nOnTheFly  uses the aGOtool to allow users to find scientific articles that mention surprisingly many of the genes/proteins identified in the uploaded input files. While conceptually similar to the functional enrichment analyses just described, publication enrichment analysis serves a very different purpose, namely to help the user identify scientific publications of relevance to the gene/protein list. The publication enrichment analysis in aGOtool is based on a text corpus of all PubMed abstracts and full-text articles from the PubMed Central Open Access subset. These have been run through the same NER   tagger   used in EXTRACT and the results are updated with new documents on a weekly basis. Consequently, all documents have been automatically annotated with the genes mentioned within them, thus turning every document into a gene set. These millions of gene sets are then used by aGOtool in the same manner as all other gene sets. \n\nWe make use of this functionality to provide publication enrichment functionality in OnTheFly  for the list of 197 organisms. The user can select up to 1000 of the genes/proteins identified in the uploaded files for analysis, which will then be submitted to aGOtool to test each document from the precomputed corpus for statistically significant enrichment, again using Fisher's exact test. The resulting   P  -values as well as Bonferroni-corrected   P  -values and Benjamini\u2013Hochberg FDR values can be used for filtering the results. Results are reported in interactive searchable tables displaying details about each literature term (scientific publication). Links are provided for publications to PubMed. In addition, users are able to rank the most significant publications using barchart plots and manually adjust the number of the reported results with the use of a slide bar. All of the aforementioned reports can be exported and saved in various file formats (CSV, XLS, PDF). \n\n\n### Network analysis \n  \nIn addition to the aforementioned enrichment options, OnTheFly  offers the capability to construct and visualize biomolecular interaction networks for a set of 197 organisms. This task is performed using the APIs of the STRING ( ) and STITCH ( ) databases for protein\u2013protein and protein\u2013chemical interactions, respectively. The users may submit their dataset obtained from the uploaded documents to retrieve interactions and visualize the results as networks with the interacting entities presented as nodes and their interactions as edges. For computational efficiency reasons, in its current version, OnTheFly  allows a maximum of 500 proteins per request for STRING and 100 proteins or small molecules per request for STITCH. \n\nSTRING and STITCH classify interactions between two entities (proteins or small molecules) as either   physical   (i.e. part of the same biomolecular complex) or   functional   (i.e. involved in the same pathway/process). To this end, OnTheFly  requires users to select whether to include the   Full   set of interactions (both physical and functional) or the   Physical subnetwork   exclusively. Users can also specify the cutoff on the   Interaction Score  . Finally, users can choose whether each edge should show the type(s) of evidence (e.g. experiments or text mining) supporting it (  Evidence   mode) or if the thickness of the edge should instead show the interaction score (  Confidence   mode). \n\nIn addition to the above, in protein\u2013chemical networks network edges can be formatted based on   Molecular Action   or   Binding Affinity  . By choosing   Molecular Action  , the edges in the network will represent the type (activation, inhibition, catalysis etc.) as well as the effect (positive, negative or unspecified) of each protein\u2013chemical interaction. By choosing   Binding Affinity  , the edge thickness will indicate the binding affinity between the proteins and bound chemicals. The resulting network is shown in a separate Network Viewer panel, preserving the characteristic STRING network layout and style. An example of such networks is shown in  . In addition, options are given to view the generated network in STRING (protein\u2013protein) or STITCH (protein\u2013chemical) for further analysis. Finally, one can export a network as an image or as a tab-delimited file compatible with external network visualization applications. \n\n\n### Implementation \n  \nOnTheFly  is a web application implemented in R, using the R/Shiny package as well as HTML, CSS and JavaScript. The Shiny and ShinyJS packages are used as mediators to establish the connection between the R and JavaScript functions. The API of the EXTRACT web service which utilizes the   tagger   text mining utility is used to perform NER. Functional enrichment analysis is performed using the g:Profiler2 ( ) package (R implementation of g:Profiler) and aGOtool. Biological networks are constructed and visualized using the STRING API, as implemented in the STRING and STITCH databases. OnTheFly  is available as a web tool and as a standalone package through a GitHub repository. The standalone version is fully functional in native Linux and other Unix-based operating systems. It can also run on Windows, by utilizing a Windows Subsystem for Linux (WSL) or other similar compatibility layers (e.g., Cygwin). The web tool is fully functional in all major web browsers (Google Chrome, Mozilla Firefox, Microsoft Edge, Tor, Apple Safari, Opera) (Table  ). \n  \nOnTheFly  versus OnTheFly \n    \n\n\n## RESULTS \n  \n### Case study \n  \nTo demonstrate the capacity of OnTheFly  for rapid extraction of biological information and knowledge discovery, we analyzed six published meta-analysis reports on clinical biomarkers of severe COVID-19 ( ) ( ). Texts in PDF format were annotated by NER, results filtered to manually remove false positives and jointly processed for functional enrichment analysis. Reassuringly, we found \u2018Respiratory failure\u2019, \u2018Pneumonia\u2019 and \u2018COVID-19\u2019 to be among the most significantly enriched diseases ( ). The GO enrichment for biological processes ( ) identified several GO terms related to inflammation, cell activation and response to stress, in line with COVID-19 being associated with exaggerated lung inflammation and systemic immune dysfunction. Similarly, the annotated text terms were found to be enriched for molecular functions that are associated with cytokine activity and cytokine receptor signaling ( ). These results were supported by the UniProt keyword analysis, which revealed \u2018Cytokine\u2019, \u2018Inflammatory response\u2019, \u2018Host-virus interaction\u2019 and \u2018Host cell receptor for virus entry\u2019 to all be enriched ( ). Analysis of putative protein-protein interactions (physical and functional associations) through the STRING option of OnTheFly  uncovered a cluster of interacting cytokines and other immune components that is pertinent to the \u2018cytokine storm\u2019 of severe COVID-19 (Figure  ). Cytokines are also a recurring theme in the publication enrichment results, which as one would hope further included several COVID-19 studies ( ). \n  \nAnalysis of clinical biomarkers of severe COVID-19 using OnTheFly . (  A  ) List of enriched pathways from the KEGG, Reactome and WikiPathways databases. (  B  ) Analysis of putative protein-protein interactions through the STRING option of OnTheFly . A cluster of interacting components of inflammatory/immune pathways, each represented by a different color, is shown. \n  \nCellular/extracellular components predicted to be associated with biomarkers of severe COVID-19 included extracellular space (GO:0005615, GO:0005576), plasma membrane (GO:0009897, GO:0009986, GO:0098552) and, interestingly, membrane microdomains (also called \u2018membrane rafts\u2019; GO:0098857, GO:0045121) ( ). The latter emerge as important cellular components implicated in (i) the initial binding of SARS-CoV-2 to ACE2 receptor, (ii) virus internalization and (iii) cell-to-cell transmission [reviewed in ( )]. Pertinent to knowledge discovery, this biological information was extracted in the absence of specific reference to membrane microdomains in any of the six meta-analysis reports that were interrogated. \n\nSeveral relevant KEGG pathways were also extracted ( ), including \u2018coronavirus disease - COVID-19\u2019 (KEGG: 05171;  ), \u2018viral protein interaction with cytokine and cytokine receptor\u2019 (KEGG: 04061) and \u2018cytokine-cytokine receptor interaction\u2019 (KEGG: 04060). Interestingly, \u2018Yersinia infection\u2019 (KEGG: 05135) was also identified as a relevant KEGG pathway with high probability (  P  -value<10 ).   Yersinia pestis   is the causative pathogen for pneumonic plague, one of the world\u2019s deadliest infectious diseases.   Yersinia pestis   infects pneumocytes and alveolar macrophages, triggering inflammasome-mediated IL-1\u03b2/IL-18 cytokine release ( ) that is followed by neutrophil influx, exaggerated inflammation and lung tissue damage ( ). These immune and lung tissue reactions to   Yersinia pestis   are reminiscent of those to severe SARS-CoV-2 infection ( ) and warrant further insights into the immunological mechanisms of response to these unrelated pathogens. Of additional interest is the predicted involvement of the \u2018IL-17 signaling pathway\u2019 (KEGG: 04657) in severe COVID-19 ( ) which is supported by a recent study reporting T cell skewing towards Th17, a specialized CD4  effector T cell lineage characterized by secretion of IL-17 and IL-17F cytokines in patients with COVID-19 pneumonia ( ). \n\nWe also explored the REACTOME option of OnTheFly  to map and analyze biological pathways that are over-represented in the validation example. As shown in  , several cytokine pathways were predicted to be significantly associated with biomarkers of severe COVID-19. We note that predicted REACTOME pathways included \u2018cellular senescence\u2019 despite the absence of specific references to this biological term in any of the six annotated meta-analysis reports under study. In line with this prediction, COVID-19 pneumonia has recently been associated with immunosenescence ( ) and accelerated aging of pneumocytes ( ). Overall, the aforementioned analyses underscore the practical utility of OnTheFly  to rapidly extract biological information from texts and hence assisting knowledge discovery (Figure  ). \n\n\n\n## DISCUSSION \n  \nOnTheFly  has been redeveloped to use current technologies and overcome many of the problems of its predecessor ( ). The GUI has been completely rewritten to no longer rely on a Java applet and instead using R, Shiny, CSS, HTML and JavaScript technologies. The backend document format conversion has also been considerably improved, replacing commercial Windows-based converters with open-source, Unix-based ones, which furthermore do a much better job preserving the original document layout. Moreover, compared to its predecessor, OnTheFly  comes with a broader spectrum of term types it can identify and supports OCR technology for processing images. Uploaded files are only stored temporarily in the OnTheFly  server just for parsing and no file backups, copies or personal data are kept. A more detailed comparison between OnTheFly  and OnTheFly  is presented in Table  . \n\n\n## CONCLUSIONS \n  \nOnTheFly  is a powerful tool for identifying terms in locally stored documents varying from texts and PDFs to Office and image files. Users can identify terms such as proteins, genes, chemical compounds, organisms, tissues, environments, diseases, phenotypes and gene ontologies and perform a functional enrichment and network analysis upon selecting a set of biomedical entities. Furthermore, popup windows with informative summaries about a term and its links to external repositories are also generated. OnTheFly  can aid researchers in annotating locally stored documents and further exploring and analyzing their identified biomedical entities in a fully automated way. We believe that due to its offered capabilities and ease of use, OnTheFly  will reach a broad spectrum of users varying from experimentalists to bioinformaticians. \n\n\n## DATA AVAILABILITY \n  \nOnTheFly  is available at:   and  . \n\nThe source code and instructions about the necessary dependencies can be found at  . \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 8494211, "text_md5": "afce2553c0ab47f73692e836afc08df7", "field_positions": {"authors": [0, 261], "journal": [262, 281], "publication_year": [283, 287], "title": [298, 449], "keywords": [463, 463], "abstract": [476, 1863], "body": [1872, 28150]}, "batch": 1, "pmid": 34632381, "doi": "10.1093/nargab/lqab090", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8494211", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8494211"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8494211\">8494211</a>", "list_title": "PMC8494211  OnTheFly2.0: a text-mining web application for automated biomedical entity recognition, document annotation, network and functional enrichment analysis"}
{"text": "Harkema, Henk and Roberts, Ian and Gaizauskas, Rob and Hepple, Mark\nComp Funct Genomics, 2005\n\n# Title\n\nA Web Service for Biomedical Term Look-Up\n\n# Keywords\n\n\n\n# Abstract\n \nRecent years have seen a huge increase in the amount of biomedical information\nthat is available in electronic format. Consequently, for biomedical researchers\nwishing to relate their experimental results to relevant data lurking somewhere within\nthis expanding universe of on-line information, the ability to access and navigate\nbiomedical information sources in an efficient manner has become increasingly\nimportant. Natural language and text processing techniques can facilitate this task\nby making the information contained in textual resources such as MEDLINE\nmore readily accessible and amenable to computational processing. Names of\nbiological entities such as genes and proteins provide critical links between different\nbiomedical information sources and researchers' experimental data. Therefore,\nautomatic identification and classification of these terms in text is an essential\ncapability of any natural language processing system aimed at managing the wealth\nof biomedical information that is available electronically. To support term recognition\nin the biomedical domain, we have developed Termino, a large-scale terminological\nresource for text processing applications, which has two main components: first, a\ndatabase into which very large numbers of terms can be loaded from resources such\nas UMLS, and stored together with various kinds of relevant information; second,\na finite state recognizer, for fast and efficient identification and mark-up of terms\nwithin text. Since many biomedical applications require this functionality, we have\nmade Termino available to the community as a web service, which allows for its\nintegration into larger applications as a remotely located component, accessed through\na standardized interface over the web. \n \n\n# Body\n\n", "metadata": {"pmcid": 2448598, "text_md5": "ead13a4bcd5fb60b756489c643e76d05", "field_positions": {"authors": [0, 67], "journal": [68, 87], "publication_year": [89, 93], "title": [104, 145], "keywords": [159, 159], "abstract": [172, 1938], "body": [1947, 1947]}, "batch": 1, "pmid": 18629294, "doi": "10.1002/cfg.459", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2448598", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=2448598"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2448598\">2448598</a>", "list_title": "PMC2448598  A Web Service for Biomedical Term Look-Up"}
{"text": "Li, Xusheng and Fu, Chengcheng and Zhong, Ran and Zhong, Duo and He, Tingting and Jiang, Xingpeng\nBMC Bioinformatics, 2019\n\n# Title\n\nA hybrid deep learning framework for bacterial named entity recognition with domain features\n\n# Keywords\n\nNamed entity recognition\nBiomedical text mining\nConditional random field\nDeep learning\n\n\n# Abstract\n \n## Background \n  \nMicrobes have been shown to play a crucial role in various ecosystems. Many human diseases have been proved to be associated with bacteria, so it is essential to extract the interaction between bacteria for medical research and application. At the same time, many bacterial interactions with certain experimental evidences have been reported in biomedical literature. Integrating this knowledge into a database or knowledge graph could accelerate the progress of biomedical research. A crucial and necessary step in interaction extraction (IE) is named entity recognition (NER). However, due to the specificity of bacterial naming, there are still challenges in bacterial named entity recognition. \n\n\n## Results \n  \nIn this paper, we propose a novel method for bacterial named entity recognition, which integrates domain features into a deep learning framework combining bidirectional long short-term memory network and convolutional neural network. When domain features are not added, F1-measure of the model achieves 89.14%. After part-of-speech (POS) features and dictionary features are added, F1-measure of the model achieves 89.7%. Hence, our model achieves an advanced performance in bacterial NER with the domain features. \n\n\n## Conclusions \n  \nWe propose an efficient method for bacterial named entity recognition which combines domain features and deep learning models. Compared with the previous methods, the effect of our model has been improved. At the same time, the process of complex manual extraction and feature design are significantly reduced. \n\n \n\n# Body\n \n## Background \n  \nMicroorganisms are ubiquitous in nature. Human beings are exposed to microorganisms from birth to death and are associated with microorganisms during all stages of life. The human body together with its microbiome constitutes a super-species, forming our own exclusive microbial community [ ]. Studies have shown that microbial diversity is associated with various human diseases, including allergy, diabetes, obesity, arthritis, inflammatory bowel disease, and even neuropsychiatric diseases [ \u2013 ]. Therefore, the diversity of microbial communities and the interaction between microorganisms and the host immune system play crucial role in guaranteeing human healthy. Microorganisms in microbial communities interact with other members actively which ensures the stability and diversity of microbial communities [ ]. Thus it is important to explore the microbial interaction for understanding the structure of microbial community and applying these results to the biomedical field. In the past, the method of extracting microbial relationships traditionally is to culture bacteria separately in biological laboratory. However, most microbes cannot be cultured experimentally as well as it is time-consuming and expensive. Recently, computational approaches can alleviate above problems to some extent thanks to the development of high-throughput sequencing technologies. At present, there are several kinds of computational methods for this task including exploring microbial interactions from metagenomic data, inferring microbial interaction from genomic information and mining microbial interaction from biomedical literature [ ]. The two former computational approaches are widely explored; however, extracting the microbial interaction from the biomedical literature is less popular. There are rich relevant researches published in the literature confirming certain microbial interactions through direct experiments. It will be a valuable resource to explore the microbial interaction by mining biomedical literatures and integrate these knowledge into a database or knowledge graph. Nevertheless, the rapid growth in the volume of biomedical literature and the variety of microorganisms make manual interaction extraction barely possible. \n\nIn previous work, Freilich [ ] proposed a microbial interaction extraction method based on the co-occurrence model. They first extracted the species names from the intestinal microbial abundance data. Then, they retrieved articles with the two species in PubMed and calculated the co-occurrence probability of the species. Finally, a microbial co-occurrence network was constructed to predict microbial interaction. Similarly, Lim [ ] obtained the data in the same way and put forward an automated microbial interaction extraction method based on support vector machine (SVM). What they had in common was the process to get the species from microbial abundance data of the human gut, which might result in the omission of certain potential interactions due to the different standards of spelling species names. \n\nIn recent years, with the development of natural language processing (NLP), text mining strategy makes it possible to extract microbial interaction from unstructured texts. Furthermore, named entity recognition (NER) is the core task of interaction extraction (IE). The purpose of NER is to extract words with special meaning from the text, such as   Person  ,   Location  . Various methods about NER have been proposed as the advancement of computer technology, which are mainly based on following three categories:(1) rule-based method [ ]; (2) machine learning-based method [ ], 3) neural network-based method [ ]. It is not portable and universal that rule-based way needs to design rules in specific domain with experts. The second approach based on statistical machine learning has strong portability and excellent performance, but it requires complex feature engineering and large-scale labeling. Furthermore, neural network based method has the highlighting performance without cumbersome process of feature design as well as large-scale tagging data. Although the method of NER in the general domain has fully developed, it is a challenging task in the domain of bacterial name identification on account of complexity of microbial names. \n\nWang [ ,  ] proposed a method of bacterial named entity recognition based on conditional random fields (CRF) and dictionary, which contains more than 40 features (word features, prefixes, suffixes, POS, etc.). The model effect was optimized after selecting the best combinations of 35 features, in the meanwhile, the computing efficiency of this model was greatly improved by deploying the model on Spark platform. Unfortunately, CRF and dictionary-based method need manually design features and additionally dictionary resources, and the result of the model depend on the quality of the annotated data and the rationality of the feature design. \n\nIn the last few years, deep learning has been widely utilized and has achieved great performance in many fields, such as image [ ]; speech recognition [ ]; machine translation [ ]; reading comprehension [ ] and so on. Similarly, the method based on deep learning has attracted extensive attention in the field of NER. Lample [ ] first adopted Bi-LSTM -CRF for NER, Ma [ ] introduced Bi-LSTM-CNN- CRF for NER, in which CNN was used to extract character-level features. Since then, more and more deep learning algorithms are used for NER. Also, the biomedical text mining contest was organized to accelerate the research on biomedical [ ,  ], and many of top participating systems utilized deep learning in biomedical text [ ,  ]. Li [ ] shown that deep learning-based method could acquire well performance in bacterial NER. However, his work did not take advantage of the existing biological resources and incorporate them as features into the model. \n\nIn this paper, we propose a method combining domain features and deep learning for bacterial NER, which achieves excellent performance in dataset. When adopting POS features only, the F1-measure of the model reaches 89.4%. With POS features and dictionary features are both added, the F1-measure is up to 89.7%. The experimental results demonstrate that external resources can contribute to the improvement of the result of the model. \n\n\n## Materials and methods \n  \nAs shown in Fig. , we build a model mainly divided into the following three layers: embedding layer, encoding layer and decoding layer. Firstly, we concatenate pre-trained word embedding, character-level embedding extracted by convolution neural network, POS embedding and dictionary embedding and input it into the encoding layer. Then the encoding layer is used for parameter learning. In the end, we can predict the best output path of sentence through the decoding layer.\n   \nThe model proposed in this paper. The concatenated word-level embedding, char-level embedding, pos embedding and dict embedding are input into encoding layer for learning, then the output of encoding layer are input into decoding layer for predict \n  \n\n### Embedding layer \n  \n#### Word embedding \n  \nAccording to a recent study, word embedding has achieved outstanding results in the field of NLP. Compared with the traditional encoding method, the word embedding technique can fully exploit semantic information between words, for example \u201cking\u201d \u2013 \u201cman\u201d\u2009+\u2009\u201cwoman\u201d\u2009=\u2009\u201cqueen\u201d, as well as using a low-dimensional continuous vector to represent the vector of words. This not only solves the sparse problem of the vector, but also obtains semantic information of the word. Currently, there are some well-performed word embedding tools which are widely used, such as fastText [ ], glove [ ], Word2vec [ ]. At the same time, Moen [ ] pre-trained a word embedding PubMed2vec with word2vec in the field of biomedical text mining. In our work, in order to obtain higher quality of word vectors, we downloaded more than 400 thousand abstracts about bacteria from PubMed and then used them together with our corpus to train word vectors. We adopted the skip-gram model of word2vec provided in gensim [ ] to train our corpus. \n\n\n#### Char embedding \n  \nAs shown by previous studies, character-level features have been proved to be work well in many NLP tasks. Kim [ ] used CNN to obtain character representation and then utilized LSTM to train a language model. Santos and Chiu [ ] showed that CNN could extract word morphological features (prefix and suffix etc.) effectively and encoded them into neural network. Lample [ ] also demonstrated that LSTM could extract morphological features of words. But, experiment results show that CNN is better than LSTM in the task of NER. As a consequence, in this paper, we use the CNN to obtain the character-level features of words. Figure\u00a0  illustrates detailed process of our method. Given a word W=  , T is the length of sequence,   c   represents the character of the word, e(  c  ) is the character vector for each character. In order to acquire morphological features of words, we use N times of convolution kernels X to perform convolution operations. The size of convolution kernels is k. The calculation formula of\u00a0  O   output for each convolution can be written as:\n   \nThe method to get char-level embedding in our paper. The characters in a word are transfer to vectors, then though a convolution layer and a max-pooling, finally the output are concatenated to represent the word \n  \n\nWhere   W   denote the weight matrix and   b   denote the bias vector,   X  \u2009=\u2009[  e  (  c  ),\u2009\u2026,\u2009  e  (  c  ),\u2009\u2026,\u2009  e  (  c  )], relu denote the activation function. Finally, for each convolution kernel output\u00a0  O  , \u2026,   O  , \u2026,   O  , the max-pooling operation is performed to obtain the character vector representation of the word. The j-th vector representing   W   can be computed as:\n \n\n\n#### Domain features \n  \nInspired by the related work of Chiu [ ] and Huang [ ], some artificial designed features and domain knowledge can also promote the effectiveness of the neural network model. Consequently, in this paper, we discuss the influence of POS and dictionary features on the neural network model. \n\nIn fact, although the model of neural network can extract feature automatically to some extent, some linguistic features cannot be well learned on account of the complicacy of natural language processing. We use the nltk [ ] tool to get the POS features of each word, and bidirectional maximum matching algorithm (BDMM) [ ] to obtain dictionary features. UMLS [ ] is a unified medical database, which contains volume of standardized names and abbreviations for diseases, proteins, genes and microorganisms. Hence we extract all the bacterial names from UMLS and integrate them into a bacterial dictionary. Table\u00a0  gives an example of our preprocessing data.\n   \nThe example of the data format in our paper \n  \n\n\n#### Encoding layer \n  \nThe long short-term memory network is a [ ] variant of recurrent neural network (RNN). It solves the problems of the gradient disappearance and the gradient explosion in the training process of RNN [ ,  ]. In the practical application process, LSTM can handle the time series problem and the long-distance dependence problem well. It mainly consists of three gates: input gate, output gate and forget gate. The main formula is as follows:\n \n\nWhere   \u03c3   denote sigmoid function,   x   denote the input of LSTM,   h   denote the output of LSTM,   W  ,\u00a0  W  ,\u00a0  W  ,\u00a0  W   denote the weight matrix in the process of training ,   b  ,\u00a0  b  ,\u00a0  b  , b is the bias vector. \n\nFor many sequence labeling tasks, we should consider the context information of the word at the same time, but a single LSTM structure can only obtain the historical information of the word. For this reason, Dyer [ ] proposed a bidirectional long short-term memory (Bi-LSTM) network for acquiring the history information and future information of words. At first, given a sequence X=  , n represents the length of sequence,   x   is the input vector at time t, use a forward LSTM to obtain historical information   =LSTM ( ,   x  ). Then a backward LSTM to obtain future information  . Finally, the outputs from both directions are concatenated to represent the word information   learned at time t. \n\n\n#### Decoding layer \n  \nFor the task of sequence labeling, we should consider the dependency problem between words, because the neighboring words of the current word contribute to the labeling of the word, so we introduce the conditional random fields (CRF) [ ] on the top of encoding layer. CRF has been proved to have a good effect on sequence labeling. Given the input of a sentence:\n \n\nWhere   x   denote the vector representation of\u00a0the\u00a0output of encoding layer. We define P as the score matrix output by Bi- LSTM, the size of the matrix P is n \u00d7 m, n represents the length of the sentence, m is the number of types of output tags and   P   represents the probability of the j-th tag of the i-th word. The output of the definition sentence is:\n \n\nWhere   y   represents the output prediction for each word. The score we define for the sentence is:\n \n\nWhere T represents the tag transition matrix, for example,   T   represents the transition probability from tag i to tag j.   y   and   y   denote the start and end that we add to the matrix, so the size of T is m\u2009+\u20092. T is learned during training. Then, softmax function is used to normalize the output path y:\n \n\nWhere Y is the set of all possible output sequences of sentence X, and we maximize log-probability of the correct output sequence during the training, which can represented as follows:\n \n\nIn the decoding stage, we predict the best output path through maximizing the score function:\n \n\nThis process can be implemented by dynamic programming and inferred by Viterbi algorithm [ ]. \n\n\n#### Dataset \n  \nIn this paper, we utilize the dataset proposed by Wang [ ] . They used \u201cbacteria\u201d, \u201coral\u201d and \u201chuman\u201d as keywords to retrieve relevant abstracts from PubMed for nearly 10\u2009years. At last they selected 1030 abstracts as train set and 314 abstracts as test set. The statistics about dataset are shown in Table\u00a0 . In order to evaluate the performance of the model, we divided it into training set, validation set and test set, in which 20% of the original training set was taken as validation set. We downloaded all abstracts related to \u201cbacteria\u201d from PubMed in the past decade and then trained word vectors along with the dataset.\n   \nThe statistics of the dataset in our experiment \n  \n\n\n#### Tagging scheme \n  \nIn this experiment, our task is to give each word in the sentence a tag. As we investigated, a bacterial entity in a sentence may be composed of multiple words, so we need a set of identifiers to represent it. Currently, there are three main types of tagging scheme: IOB2, BIOE and BIOES. To compare the performance with other models, we use the IOB2 format as our tagging scheme. In the IOB2 tagging method, B-label represents the starting word of an entity, I-label represents the inside word of an entity, and O represents the word is not in entity. \n\n\n#### Training and hyper-parameter settings \n  \nIn this experiment, the following four parts constitute the input of our model: word embedding, character embedding, pos embedding, dict embedding. The word embedding is trained by word2vec with the dimension is 300, and the character embedding is trained by CNN. The initial input of the characters vector are 25-dimensional. The dimensions of the pos embedding and the dict embedding are 25, 5, respectively. The input embeddings all randomly initialized with uniform samples from   where   dim   is the dimension of embeddings [ ]. The convolutional layers and fully connect layers were initialized with glorot uniform initialization [ ], bias vectors are initialized with 0. Then the four embeddings are concatenated to input the model for parameter learning. During the training, we use the back propagation algorithm to update the parameters. Our optimization function is Adam [ ] algorithms with a learning rate of 0.001 and a decay rate of 0.9. \n\nWe introduce dropout [ ] and early stopping [ ] technology to the model during the process of training. The purpose of the dropout technique is to prevent over-fitting of the model by randomly dropping some hidden nodes during the training process. We introduce dropout technology both before and after the decoding layer, which set dropout rate\u2009=\u20090.5. The principle of early stopping technology is to stop training when the result of the validation set is no longer improved within a tolerance range class, and record the parameters of model which has best result. It can prevent over-fitting of the model and select the best iteration number effectively. In this experiment, we set patience\u2009=\u20095. The detailed parameters are shown in Table\u00a0 .\n   \nThe hyper-parameter in our experiment \n  \n\n\n\n### Evaluation metrics \n  \nIn order to evaluate the performance of the model proposed in this paper, we choose P (precision), R (recall) and F1 (F1-measure) as experiment metrics.\n \n\nWhere TP is the number of entities correctly identified and FP is the number of non-entities identified as entities. F1-measure is the harmonic average of P and R. \n\n\n\n## Results and discussion \n  \nThe experimental results are shown in Table\u00a0 . Model 1 and Model 2 were proposed by Wang [ ,  ], and their models were based on traditional machine learning methods. Therefore, they manually extracted 43 groups of features, and then achieved good results on the dataset through feature combination and selection. Besides, the model based on Spark was greatly improved in speed. The model we proposed previously was based on neural network and did not need to extract features manually [ ]. It was an end-to-end model and had enhanced the effect of the bacteria NER to some extent, but it did not make full use of the linguistic features and existing resources. In this paper, we consider the influence of domain features on the model. The experimental results show that the F1-measure of the model achieves 89.4% when adding the POS feature. With dictionary features and POS features are added, the model\u2019s F1-measure is up to 89.7%. From the above, we can include that these two features can effectively improve the effect of the model.\n   \nThe result of our model \n  \n\nIn order to evaluate the impact of word embedding on the model, we compare the performance of four pre-trained word embedding: glove [ ], fastText [ ], word2vec [ ] and PubMed2vec [ ] as well as random initialization in our model. Among them, glove and fastText are trained on Wikipedia which the dimension are 300, Pubmed2vec is 200 dimension which is trained on PubMed and PMC articles, and word2vec is based on the bacterial abstract training we downloaded from PubMed for 10 years. The experimental results are shown in Fig.\u00a0 . As can be seen from the figure, the use of the word embedding in the general domain has a certain effect on the model compared with the random initialization and the performance is better than the model based on machine learning. Also, we can know that the result of using the medical field word vector is better than the general domain word vector, although it is not reach the highest. However, the F1-measure is the best when using the word vector of the bacterial field. As a result, the experiment proves that word vectors in different fields should be used for different professional problems, so that the model effect can be optimal and the error rate will be reduced.\n   \nThe influence of different embedding in model \n  \n\nTo evaluate the practicability of our model, we utilize the model for named entity recognition on real data. We downloaded more than 400 thousand bacteria-related abstracts from PubMed for bacterial NER, and then compared the identified entity with the bacterial dictionary. UMLS [ ] has collected nearly 4.5 million bacterial entities, which is relatively a large database of bacterial entities. Therefore, we extracted all bacterial entities from UMLS to construct a bacterial dictionary. Figure\u00a0  is a comparison of experiments. Compared with 4.5 million bacterial entities in UMLS, more than 500 thousand bacterial entities are not in the dictionary when exact matching; however, when appending some rules, there still have more than 300 thousand entities not in the dictionary. Analyzing the entities predicted by our model shows that even though some predicted entities may be misidentified, our model can still largely predict mainly bacterial strains and bacteria in different ways of writing, and most of them are not updated or included in current dictionary.\n   \nThe performance of our model in real dataset \n  \n\n\n## Conclusion and outlook \n  \nThis paper proposes a method for bacterial named entity recognition based on deep learning and domain features, integrating convolutional neural network, long short-term memory network, and conditional random fields. The experimental results demonstrate that the use of POS features and dictionary features can well promote the recognition of bacterial named entities. At the same time, we also compare the effects of different word embedding on the experimental results. The results illustrate that domain-specific embedding is more effective for bacterial named entity recognition. \n\nRecently, language models have been widely used in the field of natural language, these models have achieved good results in many NLP tasks. In the future, we will combine the language model with bacterial named entity recognition, improve the effect of bacterial named entity recognition, and combine our task with interaction extraction. \n\n \n", "metadata": {"pmcid": 6886245, "text_md5": "582d67ca4a1fe843c901b88e9f3b8df3", "field_positions": {"authors": [0, 97], "journal": [98, 116], "publication_year": [118, 122], "title": [133, 225], "keywords": [239, 326], "abstract": [339, 1926], "body": [1935, 23692]}, "batch": 1, "pmid": 31787075, "doi": "10.1186/s12859-019-3071-3", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6886245", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6886245"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6886245\">6886245</a>", "list_title": "PMC6886245  A hybrid deep learning framework for bacterial named entity recognition with domain features"}
{"text": "Xu, Kai and Zhou, Zhanfan and Gong, Tao and Hao, Tianyong and Liu, Wenyin\nBMC Med Inform Decis Mak, 2018\n\n# Title\n\nSBLC: a hybrid model for disease named entity recognition based on semantic bidirectional LSTMs and conditional random fields\n\n# Keywords\n\nBiomedical informatics\nText mining\nMachine learning\nNeural networks\n\n\n# Abstract\n \n## Background \n  \nDisease named entity recognition (NER) is a fundamental step in information processing of medical texts. However, disease NER involves complex issues such as descriptive modifiers in actual practice. The accurate identification of disease NER is a still an open and essential research problem in medical information extraction and text mining tasks. \n\n\n## Methods \n  \nA hybrid model named Semantics Bidirectional LSTM and CRF (SBLC) for disease named entity recognition task is proposed. The model leverages word embeddings, Bidirectional Long Short Term Memory networks and Conditional Random Fields. A publically available NCBI disease dataset is applied to evaluate the model through comparing with nine state-of-the-art baseline methods including cTAKES, MetaMap, DNorm, C-Bi-LSTM-CRF, TaggerOne and DNER. \n\n\n## Results \n  \nThe results show that the SBLC model achieves an F1 score of 0.862 and outperforms the other methods. In addition, the model does not rely on external domain dictionaries, thus it can be more conveniently applied in many aspects of medical text processing. \n\n\n## Conclusions \n  \nAccording to performance comparison, the proposed SBLC model achieved the best performance, demonstrating its effectiveness in disease named entity recognition. \n\n \n\n# Body\n \n## Background \n  \nMedical named entities are prevalent in biomedical texts, and they play critical roles in boosting scientific discovery and facilitating information access [ ]. As a typical category of medical named entities, disease names are widely used in biomedical studies [ ], including disease cause exploration, disease relationship analysis, clinical diagnosis, disease prevention and treatment [ ]. Major research tasks in biomedical information extraction depend on accurate disease named entity recognition (NER) [ \u2013 ], and how to accurately identify disease named entities is a fundamental and essential research problem in medical information extraction and text mining tasks. \n\nDisease NER involves many complex issues, which induce difficulties in actual practice [ ]. Disease names are usually generated by combining Greek and Latin roots and affixes, e.g.,   hemo-chromatosis  . More and more unknown names are difficult to identify from a morphology aspect. Many disease names also frequently contain disease descriptive modifiers, e.g.,   liver cancer  . These modifiers may be related to human body parts or degrees of disease, e.g.   recurrent cat-eye syndrome  . This may cause difficulties in identifying modifiers from other types of medical named entities (e.g.,   syndrome  ). Moreover, disease names may have multiple representation forms. For instance,   hectical complaint   and   recurrent fever   are the same disease but represented differently. Finally, there exist a large amount of disease name abbreviations in medical texts. Some of them may not be standard, such as those user-defined abbreviations listed in the appendix of clinical trial texts. \n\nThere are large number of biomedical texts, e.g., PubMed, PMC OA full texts, and Wikipedia. In order to effectively obtain the semantic information from the texts, word embedding training method named Negative Sampling (NEG) Skip-gram [ ] was proposed by Mikolov et al. to learn high quality vector representations from a large number of unstructured texts. This method could speed up the vector training process and generate better word embeddings. The method simplified the traditional neural network structure, and thus could adapt to a large number of texts. It could also automatically generate semantic representations of words in text context. Recently, many deep neural networks, such as the Long Short Term Memory network (LSTM) model [ ], have been widely used to extract text context features. A variety of relevant models that integrate LSTM to train word contextual features and Conditional Random Field (CRF)-based methods to optimize word sequence parameters have been widely used in NER tasks [ ]. These models improved the feature extraction process by reducing the work-load of feature selection. In addition, word embeddings have been proved to be effective in NER tasks [ ]. Motivated by both the effectively applied LSTM model and the usefulness of word embeddings, this paper combines the word embeddings containing the semantics of disease named entities with LSTM to improve the performance of disease NER tasks. \n\nTo this purpose, we propose a new model named SBLC for disease NER. The model is based on word embeddings, bidirectional LSTM and CRF. As a multi-layer neural network, the model consists of three layers. The first layer is word embedding, which is generated from medical resources through massive medical text training. The second layer is Bi-LSTM, which is used to obtain the context of semantic structures. The third layer is CRF, which captures relationship among token labels. We evaluate the SBLC model by comparing it with the state-of-the-art methods including NCBI, UMLS, CMT, MeSH, cTAKES, DNorm and TaggerOne. Based on the standard publicly available NCBI disease dataset that contains 6892 disease named entities, the SBLC model achieves an F1 score of 0.862, outperforming all the other baseline methods. \n\nThe major contributions of this paper lie in the following two aspects. First, the proposed SBLC model systematically combines word embedding, bidirectional LSTM and CRF for disease NER tasks. Second, this revised model by integrating Ab3P improves the current performance compared with state-of-the-art methods on a publically available dataset. \n\nThe rest of the paper is organized as follows: The section   gives a brief overview of the background of the disease NER and related work. The section   introduces the methodology of the SBLC model. The section Result presents the evaluation of the proposed SBLC model. The section   analyzes error cases, discusses properties of medical semantic words, and points out the limitations of our model. Finally, the section   concludes this study. \n\n\n## Related work \n  \n### Disease NER \n  \nIn medical domain, most existing studies on disease NER mainly used machine learning methods with supervised, unsupervised or semi-supervised training. For example, Dogan et al. [ ] proposed an inference-based method which linked disease names mentioned in medical texts with their corresponding medical lexical entries. The method, for the first time, used Unified Medical Language System (UMLS) [ ] developed by the National Library of Medicine in the NCBI disease corpus. Some similar systems, such as MetaMap [ ], cTAKES [ ], MedLEE [ ], SymText / MPlus [ ], KnowledgeMap [ ], HiTEX [ ] have been developed utilizing UMLS. Although UMLS could cover a wide range of medical mentions, many of these methods failed to identify disease mentions not appearing in the UMLS. In addition, the NER efficiency in terms of accuracy was not sufficiently high for practical usage. For example, the F1 in NCBI dataset of official MetaMap was only 0.559 as reported in [ ]. \n\nDNorm [ ] was one of the recent studies using a NCBI disease corpus and a MEDICS vocabulary. It combined MeSH [ ] and OMIM [ ]. DNorm learned the similarity between disease names directly from training data, which was based on the technology of paired learning to rank (pLTR) strings normalization. Instead of solely relying on medical lexical resources, DNorm adopted a machine learning approach including pattern matching, dictionary searching, heuristic rules. By defining a vector space, it converted disease mentions and concepts into vectors. DNorm achieved an F1 score of 0.809 on the NCBI disease corpus. \n\nIn 2016, Leaman and Lu proposed the TaggerOne [ ]. It was a joint model that combined NER and normalized machine learning during training and predicting to overcome the cascading error of DNorm. TaggerOne consisted of a semi-Markov structured linear classifier for NER and a supervised semantic index for normalization, and ensured high throughput. Based on the same NCBI disease corpus, TaggerOne achieved an F1 score of 0.829. \n\nWith respect to the methods applying deep learning to NER, some neural network models that could automatically extract word representation characteristics from raw texts have been widely used in the NER field (e.g., [ ]). Using deep learning, some sequence annotation methods were also proposed and applied to disease NER tasks (e.g., [ ,  ]). As a typical method, Pyysalo et al. [ ] used word2vec to train a list of medical resources, and obtained a better performance on a NCBI Disease corpus. Recently, Wei et al. proposed a multi-layer neural network, DNER [ ], which used GENIA Tagger [ ] to extract a number of word features including words, part-of-speech tags, words chunking information, glyphs, morphological features, word embeddings, and so on. After extraction, the word features were embedded as inputs to a bidirectional Recurrent Neural Network model, and other features like POS tags were used for a CRF model. The normalization method of dictionary matching and the vector space model (VSM) were used together to generate optimized outputs. The overall performance of the model in terms of F1 score was 0.843 on the NCBI disease corpus. To our knowledge, DNER was the best performance deep learning-based method. \n\nMotivated by the benefits of word embedding and deep learning from the existing research, we intend to utilize external medical resources for word representation and combine bidirectional LSTM and CRF for NER recognition. We use a large number of medical resources to train the word embeddings model in an unsupervised manner, and combine the deep learning techniques for disease NER tasks. \n\n\n### Word embedding training \n  \nSuccess of machine learning algorithms usually depended on appropriate data representation, since different representations could capture different features of the data. Distributed word representation proposed by Hinton [ ], has been widely used. The word distribution hypothesis held that the words in a similar context have similar meanings, which convey similarities in semantic dimensions. Along with the recent development of machine learning techniques, more and more complex models have been trained on larger datasets and achieved superior performance [ ]. \n\nMikolov et al. [ ] proposed a skip-gram method for calculating vector representations of words in large data sets. The compositions of disease named entities often contained rare medical words. In order to improve the computational efficiency, the Skip-gram model removed the hidden layer so that all words in input layer shared a mapping layer. In the skip-gram method, Negative Sampling (NEG) was used. It was a simplified version of Noise Contrastive Estimation (NCE) [ ]. NEG simplified NCE by guaranteeing word vector quality and improving training speed. NEG no longer used a relatively complex Huffman tree, but rather a relatively simple random negative sample, which could be used as an alternative for hierarchical softmax. \n\nMotivated by the related work, particularly from Mikolov et al. [ ,  ], we apply the NEG skip-gram method for disease NER. The method is described as follows. Given a training text sequence   w  , \u2026,   w  , at position   t  , the distribution score   s  (  w  ,\u2009  c  ;\u2009  \u03b8  ) for the true probability model was calculated using Eq. ( ). The target of   w   was a set of context words   w  , \u2026,   w  ,   w  , \u2026,   w  . \n\nWhen using the negative sampling method,   k   negative cases ( ) were randomly sampled in the noise distribution   Q  (  w  ) for each positive case (  w  ,\u2009  c  ).   \u03c3   was a logistic function. The negative function for negative samples was shown in Eq. ( ): \n\nThe value   k   was determined by the size of the data. Normally,   k   ranged within [ ,  ] in a small-scale data, while decreased to [ ,  ] in a large-scale data [ ]. Equation ( ) could be solved by a random gradient rise method. \n\n\n### Bi-LSTM & CRF \n  \nAs a typical deep learning method, the long and short memory network (LSTM) [ ] was usually used for annotation tasks of text sequences. LSTM, as shown in Eq. ( ), could capture long distance information by adding several threshold cells which controlled the contribution of each memory cell. Therefore, LSTM enhanced the ability of keeping long distance context information. Longer contextual information could help the model to learn semantics more precisely. \n\nBidirectional LSTM (Bi-LSTM) could simultaneously learn forward and backward information of input sentences and enhance the ability of entity classification. A sentence   X   containing multiple words could be represented as a set of dimension vectors (  x  ,\u2009  x  ,\u2009\u2026,\u2009  x  ).  denoted the forward LSTM and   denotes the backward LSTM.   and   were calculated by capturing from the LSTM the preceding and following information of the word   t  , respectively. The overall representation was achieved by generating the same backend sequence in LSTM. This pair of forward and backward LSTMs was Bi-LSTM. This representation preserved the context information for the word   t  . \n\nSince there was more and more research focusing on Bi-LSTM and Conditional Random Field (CRF) in NER tasks, the following of this subsection described CRF. It was first introduced as a sequence data tag recognition model by Lafferty et al. [ ]. Considering that the target of NER was label sequences, linear chain CRF could compute the global optimal sequence, thus it was widely used to solve NER problems. The objective function of a linear chain CRF was the conditional probability of the state sequence   y   given the input sequence   x, as   shown in Eq. ( ). \n\n f  (  y  ,\u2009  y  ,\u2009  x  ) was a characteristic function.   \u03bb   denoted the learning weights of the function features, while   y   and   y  referred to the previous and the current states, respectively.   Z  (  x  ) was the normalization factor for all state sequences, as shown in Eq. ( ). \n\nThe maximum likelihood method and numerical optimization L-BFGS algorithm were used to solve the parameter vector   in training process. The viterbi algorithm was used to find the most likely hidden state sequences from observed sequences [ ]. \n\n\n\n## Methods \n  \nThis paper presents a new model SBLC for disease named entity recognition based on semantic word embedding, bidirectional LSTM, and CRF. The model consists of three layers: 1) a semantic word embedding layer, 2) a bidirectional LSTM layer, and 3) a CRF and Ab3p layer. The overall architecture of the SBLC model shown in Fig.\u00a0 .   \nThe overall architecture of the proposed SBLC model including three layers: The first layer is word embedding containing word embeddings trained on three large-scale datasets. The second layer is Bi-LSTM used to learn context information. The third layer is CRF and Ab3p capturing the relationship among word part-of-speech labels \n  \n\nIn the model, we first train semantic word vectors on three corpora including PubMed, PMC OA full text and Wikipedia. The trained word vectors are then projected to the vectors trained on a standard NCBI corpus. The word vectors containing text semantic information are input to the Bi-LSTM layer. The NCBI training corpus is further used for Bi-LSTM parameter training. We optimize sequence parameters by the CRF layer. Finally, the model identifies disease abbreviations using an Ab3P module. \n\nThe first layer is word embedding. The Skip-gram model based on Negative Sampling is used to train word embeddings on the three large-scale medical datasets. Based on a previous work [ ], we extract the texts from PubMed, PMC Open Access (OA), and Wikipedia. A total of 22,120,000 abstract records from PubMed, 672,000 full-texts from PMC OA, and 3,750,000 articles from Wikipedia are retrieved by the end of 2013. The finally extracted texts as a corpus contain a total of 5.5 billion words. The corpus is then used as the training dataset for word embedding generation. \n\nThe second layer is Bi-LSTM, which is used to learn context information. LSTM captures long distance information through a threshold unit, thus it can learn more semantic features through longer contextual information. Using the Bi-LSTM structure can simultaneously learn the context information of preceding and following sentences. From our previous empirical studies, the Bi-LSTM can enhance entity classification performance. \n\nThe third layer is CRF and Ab3p, which captures the relationship among word part-of-speech labels. We use NLTK toolkit [ ], a widely used natural language processing tool, for part-of-speech labeling. In the CRF, the Viterbi algorithm is used to solve the global optimal sequence problem. Finally, the BIO method is used for NER annotation and the Ab3P is used to identify additional disease abbreviations. \n\nIn general, a disease NER task can be regarded as a process of assigning named entity tags to words. A single named entity may consist of multiple words in order. Accordingly, we use the BIO method for sequenced-word labeling. Each word is marked with BIO labels. A word is tagged with a   B   label if it is at the beginning of a named entity. If the word is inside the entity but not at the beginning, it is tagged as   I  . Words that are not named entities are marked as   O  . \n\nThe labels of named entities are mutually dependent. For example, an   I  -PERSON cannot appear after a   B  -LOCATION label. Therefore, the BIO labels cannot be tagged independently. We use a CRF method to calculate the possibility score of each label from the Bi-LSTM output. The objective function   s  (  X  ,  y  ), as shown in Eq. ( ), is used to calculate the probability of each label. The higher the value, the higher probability of the predicted label to be chosen. \n\nFor an input sentence set   X  \u2009=\u2009(  x  ,\u2009  x  ,\u2009\u2026,\u2009  x  ),   P   is a score matrix, which is the output of the bidirectional LSTM network containing the medical semantic features.   P   is of size   n  \u2009\u00d7\u2009  k  , where   k   is the number of different BIO labels and it is set to 3 in this paper.   A   is a matrix of transition scores and   A   represents the transition score from the BIO   label   to   label  .   y   and   y   are the beginning and ending labels of a sentence, respectively. \n\nWe use a softmax function   p  (  y  |  X  ) to calculate the probability of sequence   y   from all possible label sequences, as shown in Eq. ( )  .  \n\nThe final computation task is to find the point estimate   y*   of all possible outputs   y   such that the conditional log-likelihood probability   P  (  y|X  ) is maximized, as shown in Eq. ( ). \n\nIn the task of disease NER, disease abbreviations are often interfered by other non-disease abbreviations. For example, a disease name CT appearing in a clinical text may refer to Computed Tomography (non-disease) or Copper Toxicosis (Wilson disease). Thus, the identification of CT as Computed Tomography is incorrect. \n\nThe abbreviation recognition is not effective using solely word embeddings generated by the NEG skip-gram training, since the disease abbreviations are easily conflicted with other types of non-disease abbreviations. Taking the same example, CT is expected to be classified as Copper Toxicosis (ID 215600 in OMIM (Online Mendelian Inheritance in Man)). However, the most similar vocabularies associated with the word embeddings are the following 5 ranked tuples (noncontrast CT, 0.8745), (MDCT ray, 0.8664), (Computed tomography, 0.8643), (non-contrast, 0.8621), and (unenhanced, 0.8505), where the first tuple element refers to the words relevant to CT and the second element is their similarity values. However, the similarity between CT and target word Copper Toxicosis is as low as 0.003, causing the difficulty in the identification of disease abbreviation Copper Toxicosis. To that end, we use Ab3P [ ], available at  , to identify disease abbreviations. Evident in previously reported results, Ab3P has an F1 score of 0.9 and 0.894 \u200b\u200bon the Medstract corpus and the MEDLINE annotation set, respectively. It defines short form (SF) as abbreviations and long form (LF) as the full representations of the abbreviations. Ab3P uses relaxed length restrictions and tried to find the best LF candidates by searching for the most reliable strategy out of seventeen strategies. For example, strategy FC denotes that a SF character matches the 1st character of a word in LF. Strategy FCG denotes that a SF character matches the character following a non-alphanumeric and non-space character in LF. \n\nThe BIO labels for the identified abbreviations by SBLC and Ab3P are   Set   and   Set  , respectively. The final label sets are computed as  Set  \u2009\u222a\u2009  Set  . If there is no identification output for an abbreviation using SBLC, the identified label by Ab3P is applied as the final result. In cases the identified labels from SBLC and Ab3P are different, the labels by Ab3P are taken as the correct identification. In this way, Ab3P in identifying abbreviations of disease named entities is used to supply the SBLC, thus improving the overall NER performance. \n\n\n## Results \n  \n### Dataset \n  \nWe use a publicly available dataset, the NCBI disease corpus [ ], to evaluate the performance of the proposed SBLC model. The dataset is developed and annotated by the research groups from American National Center for Biotechnology Information (NCBI) and American National Institutes of Health (NIH). It has been frequently used in disease NER tasks [ ,  ,  ]. The dataset contains 793 article abstracts from PubMed, and includes over 6000 sentences and 2136 unique disease concepts. The dataset is manually annotated by 14 persons having medical informatics research backgrounds and medical text annotation experiences. The dataset consists of three sub-datasets: a training data set (593 texts), a development data set (100 texts), and a test data set (100 texts). Detailed statistics information of the NCBI dataset is shown in Table\u00a0 .   \nThe statistics of the NCBI dataset for disease NER \n  \n\n\n### Baseline \n  \nTo evaluate the effectiveness of the SBLC, the following 9 baseline methods are used in performance comparison:   \nDictionary look-up method [ ]. It uses Norm from the SPECIALIST lexical tools to identify disease names in the MEDIC lexicon. \n  \ncTAKES [ ]. The cTAKES NER component implements a dictionary look-up algorithm within a noun-phrase look-up window. The dictionary is a subset of UMLS, including SNOMED CT and RxNORM concepts guided by extensive consultations with clinical researchers and practitioners. Each named entity is mapped to a concept from the terminology. The cTAKES is available at  . In the comparison, we use the latest version cTAKES 4.0. \n  \nMetaMap [ ]. MetaMap is based on lexical look-up to identify the UMLS Metathesaurus concepts in biomedical texts. In the experiment, we use MetaMap MEDIC filtering to restrict output results to disease names. \n  \nThe Inference Method [ ]. It tries to link diseases to their corresponding medical lexical entries. It designs string matching rule combinations that map annotated strings to standard disease dictionaries. The method was tested by the manually annotated AZDC disease corpus and the PubMed abstract texts. \n  \nDNorm [ ]. The method is based on pairwise learning to rank (pLTR), which has been successfully applied to large optimization problems in information retrieval. It learns similarities between mentions and concept names, including synonymy and polysemy. \n  \nCRF\u2009+\u2009UMLS, CRF\u2009+\u2009CMT, CRF\u2009+\u2009MeSH [ ]. These are several hybrid combination strategies involving CRF and UMLS, CRF and Convergent Medical Terminology (CMT), as well as CRF and Medical Subject Headings (MeSH). \n  \nC-Bi-LSTM-CRF [ ]. It extracts the prefix and suffix information for each word at the character-level in training text. The method consists of three layers. The first layer is a character-based Bi-LSTM layer designed to learn character-level expressions of words. The second layer is a word-based Bi-LSTM layer. The third layer is a CRF layer, which captures the relations among labels. \n  \nTaggerOne [ ]. This method is developed by the National Center for Biotechnology Information, USA. It uses a semi-Markov structured linear classifier for NER and normalization, simultaneously performs NER and normalization during training and prediction. \n  \nDNER [ ]. Based on a deep learning method Bi-RNN, this method recognizes named entities using a support vector machine classifier. Dictionary matching and vector space model based normalization method are used to align the recognized mention-level disease named entities in MeSH. \n  \n\nWe further analyze the functional characteristics of all the baseline methods in terms of using \u201cdictionary look-up\u201d, \u201cdisease name normalization\u201d, \u201cword embedding\u201d, \u201cLSTM\u201d, and \u201cCRF\u201d, as shown in Table\u00a0 . \u201cY\u201d means that a method contains a specific function and \u201cN\u201d means not. As can be seen in the table, most of the methods use disease name normalization approach and half of them use CRF. Only SBLC and C-Bi-LSTM-CRF use LSTM. SBLC is the only method that uses word embedding and it does not rely on dictionary look-up nor disease name normalization.   \nParameter combination comparison \n  \n\n\n### Evaluation metrics \n  \nWe use three widely used evaluation metrics, precision, recall and F1-score, in disease NER studies [ ,  ,  ,  ,  ] and other types of NER studies [ ,  ,  ]. There are four possible outcomes for an instance in a testing data: An instance will be classified as a disease when it is truly a disease (true positive, TP); it will be classified as a disease when it is actually a non-disease (false positive, FP); it will be classified as a non-disease when it is actually a disease (false negative, FN); or it will be classified as a non-disease and it is truly a non-disease (true negative, TN). Based on these 4 possible outcomes, precision, recall and F1-score are defined as follows: \n\nPrecision: the proportion of instances that are correctly labeled as diseases among those labeled as diseases. \n\nRecall: the proportion of disease instances that are correctly labeled. \n\nF1 score: the harmonic mean of precision and recall. \n\n\n### Parameter tuning \n  \nIn SBLC, there are a number of parameters. In the parameter tuning process, we try different combinations of the parameters and record the corresponding performances in terms of F1 scores based on the training dataset. Eventually, we obtain a list of optimized parameter values, as shown in Table\u00a0 .   \nThe optimized parameter settings of the LSTM network \n  \n\nIn addition, the increase of the hidden layer dimension of Bi-LSTM network may lead to high computational complexity. To optimize the network layers, we have tried different dimensions of hidden layers ranging from 50 to 200 incrementally, with a step of 50, to test the performance of the Bi-LSTM network on the training dataset. From the result shown in Table\u00a0 , the F1 score is 0.768 using 50 dimensions of hidden layers and is increased to 0.802 using 100 dimensions of hidden layers. However, the F1 score drops to 0.753 and 0.768 when the dimension number of the hidden layers is increased to 150 and 200, respectively. In order to have a lower computational complexity, we select 100 as the best dimension number of hidden layers for the Bi-LSTM network.   \nEffects of dimension settings of hidden layer dimension in Bi-LSTM \n  \nThe highest values are denoted in bold type \n  \n\nThe number of word embedding dimensions may also affect the method performance and computational complexity. Similarly, we set the word embedding dimensions from 50 to 200, with a step of 50. From the result shown in the Table\u00a0 , the highest F1 score is 0.862 when the dimension equals to 200. Consequently, we use 200, which is also commonly used in many other NER tasks as the best dimension setting in word embedding generation.   \nEffects of different parameter settings of word embedding dimensions \n  \nThe highest values are denoted in bold type \n  \n\n\n### Results \n  \nDuring word embedding training, different training data sources may affect the quality of generated word embedding. We use three datasets: 1) A PubMed dataset composed of 22,120,000 paper abstracts. 2) A PMC dataset containing 672,000 full-text publications, and 3) A Wikipedia dataset containing 3,750,000 articles. \n\nWe test the performance of disease NER using different combinations of the datasets. As shown in Table\u00a0 , with respect to F1 score, using the PubMed (abstract) and the PMC (full text) separately achieve an F1 score of 0.843 and 0.861, respectively. Using the PubMed (abstract)\u2009+\u2009PMC (full text) obtains the best F1 performance.   \nPerformance comparison using different combinations of external training datasets \n  \nThe highest values are denoted in bold type \n  \n\nFrom the result, Wikipedia is not effective on both independent usage and combination. This might be caused by our incomplete Wikipedia training dataset, since the dataset contained only part of disease named entries and some disease names were not being covered. Moreover, Wikipedia is not a specialized medical corpus thus much non-medical content were involved. The reason was also reported by [ ] similarly. We therefore use the combination of the PubMed (abstract) and the PMC (full text) as the external datasets for word embedding pre-training. \n\nIn order to verify the robustness of the proposed SBLC model, we evaluate the performance using different sizes of the test dataset increasing from 10 to 100 abstracts with a step of 10. We apply a bootstrap sampling method on the test data set using put-back sampling method for 100 times. After that, we assess the statistical significance of F1 scores by computing confidence intervals at the 95% level. In each round, five different strategies by setting different SBLC parameters are used for comparison. As mentioned above, SBLC was the method with the full functions; SBLC(\u2212 semantic word embedding) represented SBLC without semantic word embedding layer; SBLC(\u2212 word embedding) represents the SBLC without word embedding in the training process; SBLC(\u2212 Bi-LSTM) denoted SBLC without Bi-LSTM network; and SBLC(\u2212 CRF) denoted the SBLC without the CRF layer. \n\nWithout Bi-LSTM, the model acquires the widest range of variability and poor robustness. It shows that Bi-LSTM contributes a lot to the robustness of the SBLC model. The performances of the models without semantic word embedding nor word embedding are close to each other. The robustness of the SLBC model is generally smoother, compared to the two methods. The F1 scores using different numbers of testing texts are shown in Fig.\u00a0 .   \nThe performance of SBLC using different numbers of testing texts. The lines are the averaged F1 for 100 times testing and the shaded areas are at the 95% confidence level \n  \n\nIn addition, we test the performance of SBLC by comparing it with different strategies considering contributions from four parts: Ab3p, CRF, Bi-LSTM, Word Embedding. The comparison results are shown in Table\u00a0 . CRF uses the CRF layer structure only for NER. The precision, recall, F1 score is 0.701, 0.675 and 0.688. Bi-LSTM uses the Bi-LSTM layer structure only. The precision, recall, F1 score is 0.600, 0.425 and 0.498. While adding Ab3p on the basis of CRF, Ab3p\u2009+\u2009CRF obtains a precision and a recall of 0.726 and 0.689, respectively. By adding abbreviations on the basis of Bi-LSTM, Ab3p\u2009+\u2009Bi-LSTM obtains a precision and a recall of 0.645 and 0.452, respectively. Utilizing both CRF and Bi-LSTM layers, Bi-LSTM + CRF achieves a precision, a recall, and an F1 score of 0.806, 0.800 and 0.803, which improves the overall performance. Combining Ab3p, Bi-LSTM and CRF layers, Ab3p\u2009+\u2009Bi-LSTM + CRF improves the precision, recall, and F1 score to 0.813, 0.808 and 0.811. Combining Word Embedding and Bi-LSTM layers, Word Embedding + Bi-LSTM achieves a precision, a recall, and an F1 score of 0.675, 0.501 and 0.575. Word Embedding + CRF obtains a precision, a recall, and an F1 score of 0.821, 0.772 and 0.796. Combining Word Embedding, Bi-LSTM and CRF layers, Word Embedding + Bi-LSTM + CRF obtains a precision, a recall, and an F1 score of 0.842, 0.828 and 0.835. Ab3p\u2009+\u2009Word Embedding + Bi-LSTM, by combining Ab3p, Word Embedding and Bi-LSTM layers, obtains a precision, a recall, and an F1 score of 0.613, 0.689 and 0.648. Combining Ab3p, Word Embedding and CRF layers, Ab3p\u2009+\u2009Word Embedding + CRF obtains a precision, a recall, and an F1 score of 0.846, 0.786 and 0.815. Ab3p\u2009+\u2009Word Embedding + Bi-LSTM + CRF (SBLC) obtains the highest precision, recall, and F1 score of 0.866, 0.858 and 0.862.   \nEffects of different parameter settings and the final optimized result \n  \nThe highest values are denoted in bold type \n  \n\nThe fourth experiment compares the performances of the proposed SBLC model with those of the above mentioned 9 baseline methods. For MetaMap, we further consider the usage of two filtering strategies: semantic type filtering and MEDIC filtering. For TaggerOne, we further use normalization leveraging external resource. Comparison results are shown in Table\u00a0 . The widely-used cTAKES obtain an F1 score of 0.506 and the MetaMap increased the F1 score to 0.559. The inference method acquires an F1 score of 0.637. The three combinations of CRF strategies CRF\u2009+\u2009CMT, CRF\u2009+\u2009MeSH and CRF\u2009+\u2009UMLS obtain F1 scores of 0.735, 0.746 and 0.756. The state-of-the-art methods DNorm and TaggerOne, both developed by NIH, achieve relatively higher F1 scores as 0.798 and 0.829, respectively. The deep learning-based method C-LSTM-CRF obtains an F1 of 0.802, while the recent DNER has an F1 score of 0.843. Our SBLC achieves the highest F1 score of 0.862, outperforming all the baseline methods. The comparison results show the effectiveness of our proposed SBLC method.   \nThe performance comparison of our SBLC model with the baseline methods on the same NCBI test dataset \n  \nThe highest values are denoted in bold type \n  \n\n\n\n## Discussion \n  \n### Error analysis \n  \nWe analyze all the error cases from our SBLC method, and summarize the error cases as the following three types. \n\n1) The complex compound words cause difficulties in disease NER. For example, the disease name \u201cinsulin-dependent diabetes mellitus\u201d (MeSH ID D003922) has a joint mark \u201c-\u201d but SBLC can recognize \u201cdiabetes mellitus\u201d only. This might be due to the insufficient amount of training data, which cause the incorrect identification of complex disease named entities and compound words. \n\n2) Long disease mentions might cause NER failures. For example, \u201cdemyelination of the cerebral white matter\u201d (D003711) and \u201cdisorder of glycoprotein metabolism\u201d (DiseaseClass, D008661) are two long disease names failed to be recognized by SBLC. We further identify the length of these error cases with long disease names, and find that the unidentified disease names usually contain more than 3 words. This is a challenge for disease NER, particularly with the appearance of more and more disease names. \n\n3) Some rare disease names appear in the testing dataset only. For example, Non-Hodgkins lymphoma (D008228) is not appeared in the training dataset, thus it is missed in the NER on the testing dataset. \n\n\n### Medical semantic word embedding \n  \nIn a medical NER task, word is a fundamental unit and word semantics is proved to be useful. The trained semantics could be further enhanced as a feature for higher-level neural network training. For example, the disease NER result on a PubMed article (PID 9949209) in the testing dataset is shown in Fig.\u00a0 . The words with colored background in purple, blue, gray and yellow denote the four identified unique disease mentions. These mentions are further normalized to standard concepts marked with associated rectangle boxes containing unique concept id.   \nThe annotations of the identified disease named entities \n  \n\nIn SBLC, NEG skip-gram is used to train word embeddings and the trained embeddings could reflect the semantic distances among the learned disease concepts. For example, based on the same example above, SBLC calculates the similarities among all the identified disease concepts using the Cosine similarity measure. The results are reported in Table\u00a0 . Words in different capitalization and tense, or synonymy are identified and assigned with a similarity weights. In order to view the similarity among the identified disease concepts, we map the concepts to a two-dimensional space, as shown in Fig.\u00a0 . The closer the words, the more semantically similar they become. For example, the closest semantics to the word \u201cliver\u201d are \u201ckidney\u201d, \u201chepatic\u201d, \u201cpancreas\u201d, \u201ckidneys\u201d, and \u201clivers\u201d.   \nThe semantic similarity among the identified disease concepts using Cosine similarity measure \n    \nThe example word embedding projected to a two-dimensional space \n  \n\n\n\n## Conclusions \n  \nIn this paper, we proposed a new deep learning-based model named as SBLC. The model utilized semantic word embeddings, bidirectional LSTM, CRF, and Ab3P. Based on a standard NCBI disease dataset, we compared the SBLC with 9 state-of-the-art methods including MetaMap, cTAKES, DNorm, and TaggerOne. The results showed that the SBLC model achieved the best performance, indicating the effectiveness of SBLC in disease named entity recognition. \n\n \n", "metadata": {"pmcid": 6284263, "text_md5": "009517d4f74b630dea0fd31e399d8ad5", "field_positions": {"authors": [0, 73], "journal": [74, 98], "publication_year": [100, 104], "title": [115, 240], "keywords": [254, 322], "abstract": [335, 1626], "body": [1635, 37710]}, "batch": 1, "pmid": 30526592, "doi": "10.1186/s12911-018-0690-y", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6284263", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6284263"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6284263\">6284263</a>", "list_title": "PMC6284263  SBLC: a hybrid model for disease named entity recognition based on semantic bidirectional LSTMs and conditional random fields"}
{"text": "Kumar, Ashutosh and Sharaff, Aakanksha\nSN Comput Sci, 2023\n\n# Title\n\nPubExN: An Automated PubMed Bulk Article Extractor with Affiliation Normalization Package\n\n# Keywords\n\nArticles extraction\nAffiliation normalization\nBulk extraction\nNCBI PubMed\nPython package\nBiomedical text mining\n\n\n# Abstract\n \nBiomedical article extraction is the preliminary step for every biomedical application. These applications are helpful in finding the gene, disease, chemical, drugs, protein entities. Finding entities relation such as gene\u2013gene entities, drug-disease interaction, and chemical protein relation the PubExN can be helpful for these types of biomedical applications. In most cases, domain experts do this extraction process on their own. Human interference makes this process time-consuming and there is a high probability, that documents can be missed during the extraction process. To get rid of these complicated processes a python package is introduced to automate the process of bulk extraction from the PubMed database. The extraction process covers all the citation information with the associated abstract. The batch approach is used to extract the bulk extraction. The motivation for the development of PubExN was to provide flexibility for the extraction process of biomedical article\u2019s text data from NCBI\u2019s PubMed database. Basically, NCBI\u2019s PubMed database article contains the article id or can say PubMed-id (PMID), the title of the article, abstract, authors information, etc. This package will benefit many biomedical texts mining research including biomedical named entity recognition, biomedical relation extraction, literature discovery, knowledgebase creation, and various biomedical Natural Language Processing (NLP) tasks. In addition, it could be used in the author name disambiguation problems and new drug discoveries. This package will help save time and extra effort for the extraction and normalization process of PubMed articles. \n \n\n# Body\n \n## Introduction \n  \nImprovement in the healthcare systems is a basic need for patients as well as for researchers [ ]. The most complicated task is to improve it continuously. The improvement can only be made if we keep measuring the deficits in the system through appropriate metrics and use quality indicators for the following system. The Research methods are broadly classified into two categories qualitative and quantitative [ ]. The use of methods such as interviews, observations, and data analysis in qualitative research has shown to be an efficient way of answering some of these complex problems [ ]. Over time, data are drastically exploding in almost every field. Text data brings new opportunities in various domain. Scientific and technical publications where the new research is going on. Figure\u00a0  shows the statistics of country-wise publication produce a volume of scientific and technical journal from 2000 to 2018. Biomedical text mining is currently a trending topic nowadays due to the increasing new opportunities in the field. When a researcher works on any problem statement, their primary issue is retrieving the structured data, e.g., patient-id, prescription, lab results. The maximum percentage of data available is unstructured or freely written without context [ ]. Researchers cannot directly utilize the data produced and stored on the internet for modeling or any extensive scale application. Retrace, annotating or transforming unstructured data into structured data is extensive and laborious work, and it is not worth the time [ ]. The development of an information extraction package, which we have coined PubExN, is an essential component in making more complex quantitative research on the quality of healthcare possible. This is because there are now no widely accessible tools available for biomedical researchers, despite having already discussed the significance, expense, and arduous nature of this activity [ ]. Such tools would be straightforward in an ideal world, even for researchers who had no prior knowledge of natural language processing (NLP) or software engineering.   \nPublication over the year 2000\u20132018 \n  \n\nThe environment and another requirement to successfully fetch the data are done using the entire API [ ]; the personal NCBI key is required to access the database and maintain the authentication while extracting the data [ ]. The approach our package utilizes is batch extraction. The package can be installed in any operating system having python 3.8 or above versions. For python 3.8, we need to create a conda environment. \n\nThe package can directly be installed using the standard python-pip command. However, the library we have developed can be used as a python package; There is a similar platform named Canary developed by [ ], which is an NLP Based platform and software for data extraction. It was designed to match the above criteria and developed to process and extract the data from clinically supported documents. The package we have developed is similar in some ways, but at the same time, it is implemented as a python package and fulfill the objectives described. Python programing language is used to develop this package and we have used the python 3.9 version for developing our wheel and final package. As we have developed this package from the scratch, we have plugged some existing web scraping dependencies which reduced our efforts up to some extent. The major reason for using this. \n\nThere is always a high probability of missing out on the information relevant to their area of interest and essential for implementation. At that time, it is necessary to refer to all the relevant resources to gain a 360-degree of view (all the possible information on a particular topic), means while extracting the biomedical articles from the PubMed database all the possible information will be able to create a knowledge source. One of the most positive aspects of having software-based data extraction, for now, is that the researchers are now able to collect the pool of evidence systematically, structure their research and study their area of interest to create systematic Literature Reviews (SLR) along with systematic Mapping (SM). Whenever the goal is to precisely collect information that researchers need to have in line with the research questions posed by the SM/SLR study and to extract the necessary data from the primary studies, according to multiple studies, the SLR is not very accurate in their objective study and increases the chances of errors. Hence this issue can be tackled better with the introduction of this software. When young researchers and juniors start to work on research, as they are not very experienced, it might increase the chances of deviating from the objective and learning irrelevant information; hence, it requires heuristics and predefined guidelines for efficient and effective data extraction. Here, effectiveness means the extraction of accurate and quality data from studies. Meanwhile, efficiency implies data extraction speed, which can only be made possible through specific and targeted packages like PubExN. The software has drastically reduced the search time for researchers to find relevant studies on the internet. Previously all types of researchers had to spend much time on the internet to find out the relevant studies of their domain; now, they can directly access content by typing out the keywords. Apart from it, they can have complete information about the topic with a reduced risk of missing out on anything important or relevant. The human utility factor is a significant aspect of explaining the success of any software mainly; there are two significant challenges: low team synergy and language barriers. People new to research find it challenging to implement software usage into their work. However, after gaining some user experience, they get over their hesitation in utilizing the package. Apart from that, the people doing any research have very little or less knowledge of the programming language in which the software package is etched. Second, for the non-native English speaker, the package is bounded to be used for English speakers and etched to extract the English content. However, we need to restructure the package to retrieve the data in other major languages. The package we have created focuses on the extraction of related medical purposes. However, the utilization of our package majorly is adopted by the pharmaceutical industries for their in-lab research; they can fetch the data in bulk to understand any disease, issue or any field of research without wasting their time, the unique selling point for the pharma industry will be there will be lack of chances to miss out any vital information if they switch to automated package for the retrieval of data. \n\nUnique features of the package are given as follows:   \nSome methods are available scientific community, but the uniqueness of the PubExN is the bulk extraction of articles. \n  \nThe PubExN can provide the facility to extract as much as or required article information from the PubMed database which is not provided by the other methods. \n  \nThe PubExN gives the flexibility of normalizing the extracted data which is not present in the other methods. \n  \nPubExN, further normalize the affiliation details which will be helpful for knowledge base creation. \n  \nPubExN uses parallelization for faster execution or parallel extraction process. \n  \n\n\n## Methodology \n  \nHere in this section, we will describe the high-level software architecture; Fig.\u00a0  illustrates the high-level building components used in the creation of PubExN.   \nExplain end to end pipeline development of the package \n  \n\nThe process starts with the NCBI\u2019s PubMed database. The reason of chosen the PubMed index biomedical database because the PubExN is made for mostly biomedical research and only required abstract-level information. PubMed provides the abstract, author and citation information. This information can easily extract from the PubMed index database without any consent or permission and it will be useful in various biomedical applications like biomedical entity extraction, biomedical entities relation extraction, literature base discovery, knowledge graph creation and many more. PubMed stores this information in a systematic order. The researches based on the targeted data need to extract these data from the biomedical database. PubExN can take \u201ckeyword\u201d or \u201csingle PMID\u201d or \u201cList of PMIDs\u201d to extract data from the PubMed database. Extracted PubMed data in the XML format which provides \u201cPMID\u201d, \u201carticle title\u201d, \u201cauthor first name\u201d, \u201cauthor middle name\u201d, \u201cauthor last name\u201d, \u201caffiliation details\u201d and \u201cabstract of the articles\u201d. Full-text affiliation details passes to the affiliation parser for further normalization of the \u201cCountry\u201d, \u201cDepartment name\u201d, \u201cEmail\u201d, \u201cLocation\u201d, \u201cZip code\u201d. PubMed also holds many other information like other citation information like no. of citation, cited paper,publication data, PMCID (PubMed Central) information, etc. The PubExN can also be used for \u201cPubMed Central\u201d which stores almost full detail of the article (Title, abstract, Full text of the article(missing in PubMed database) and author citation information), but in most use cases \u201cPMID\u201d, \u201carticle title\u201d, \u201cauthor first name\u201d, \u201cauthor middle name\u201d, \u201cauthor last name\u201d, \u201caffiliation details\u201d and \u201cabstract of the articles\u201d are the most important component of any natural language processing task in biomedical domain [ ,  ]. \n\n High-level software architecture description:    \n Purpose:   By utilising a variety of architectural views to illustrate various facets of the system, this article presents a thorough architectural overview of the entire system. The purpose of this document is to record and communicate the major architectural choices made during the development of the system. \n  \n Scope:   A high-level overview of how the software behind article extraction and normalization is structured is provided in this Software Architecture Document. The text mining research team of NIT Raipur is working on PubExN to aid in the article extraction process in biomedical text mining. This document was made using the PubExN Analysis Design Model found in PubMed. The bulk of the content was culled from PubMed utilising the E utility API and the format of the Software Architecture Document. \n  \n Definitions, Acronyms and Abbreviations:   The PubExN is a software to designed to reduce the cost and time of the biomedical literature extraction process. We have defined the acronyms and abbreviations throughout the research paper. \n  \n Architectural Representation:   The architecture is laid out in this document from several different perspectives, including the use case view, the logical view, the process view, and the deployment view. Specifically, this document does not detail an implementation view. All of these are perspectives on a model created in the Unified Modeling Language (UML) \n  \n Architectural Goals and Constraints:   Explanation of the software architecture from the perspective of its use cases. The scenarios and/or use cases that are prioritised during an iteration often depend on the information provided by the Use Case View. It specifies the scenarios and/or use cases that model crucial core features. It also details the collection of use cases and/or scenarios that either provide extensive architectural coverage (by exercising many architectural elements) or highlight a particularly nuanced aspect of the architecture. \n  \n Use-Case View:   Fig.\u00a0  shows the Use-case view of the proposed architecture. \n    \nExplanation of Use-Case View \n  \n\nArticles can be searched by PubExn but the affiliation of the author or the citation information is required when you are creating a knowledge base for any biomedical application. So that is the reason we are extracting article information along with the affiliation of the author. Basically, for clustering the domain of the particular area (suppose when we are interested that how many articles are published by a particlcular author then that time it will help to find the domain expert of the particular field).\n   \n Process View   In Fig.   explaining the architectural process view. \n  \n Size and Performance  \n    \nExplanation of Process View of the proposed architecture \n  \n\n  \nThe framework can endorse N number of simultaneous users against the central database or local servers. \n  \nThe client portion shall require memory disc space according to the articles download size and at least 32\u00a0GB of RAM required for smooth operation. \n  \nNo more than a 10-s delay in accessing the original course catalogue database is allowed. \n  \n\n  \n Quality  The performance indicators mentioned in the Additional Standard are sup- ported by the software architecture. \n  \n\n  \nThe desktop user interface shall be windows, linux or any other platform. \n  \nSystem should be python supported. \n  \nInternet should be mandated throughout the process. \n  \n\n### Article Extraction Process \n  \nAlgorithm 1 will be explaining the workflow for the extraction of data. PubExN can be extracting PMID, title, first name, middle name, last name, affiliation, and abstract of the paper. The package utilizes an API that permits access to NCBI\u2019s dataset and downloads the data without any blockage. The data is fetched through research (research is mainly used to determine the total number of documents present.) within the batches of 300 approx. The NCBI website does not permit downloading approx more than 300 papers at a time. Key and API tackle this fetching hindrance to make sure the fetching operation keeps on running and does not stop in the middle. Let us try to understand the architecture with the help of an example. Suppose we aim to fetch the entire collection of papers written on covid\u201319; we will use the command to input our keyword covid \u2013 19, and the algorithm will use research to search for all the papersstored in the PubMed database [ ,  ]. Based on a request. Get started putting together our search answer right now. When we are finished with this search answer, make search soup. After finding all the papers of the targeted keyword or field, the package will start to fetch the papers in batches with all the details like PMID, title, author name, and affiliation (it is the information related to the source of the paper, e.g., university, journals), abstract of the paper in a structured [ ]. The papers will be fetched into batches of 300 papers at once with the help of the APIs we have defined. The response for the fetch is in XML. The final response is collected after the response is in the dictionary format. However, we programmed the package to normalize the affiliations of the paper because affiliation is in the full-text format. By increasing the size of the dataset, we may transform the response into a data frame. After the data had been converted into a data frame, the normalized author function was implemented, which helped eliminate the ambiguity in the author\u2019s name\u2014after that, finally used the affiliation normalization technique to normalize the affiliation into the different sections (department, school, institute and country, email, location, zip code) [ ]. \n\n\n\n#### Affiliation Extraction Code Explanation \n  \nThe explanation of the affiliation extraction code is as follows.   \n Import Libraries  : Code starts with the essential importing packages like BeautifulSoup, requests, XML, dotenv, JSON, pandas, etc. BeautifulSoup is a python package mainly used to fetch data from HTML and XML files. It integrates with your preferred parser to provide you idiomatic access to the parse tree for navigation, searching, and editing. Typically, it saves programmers hours or days of effort. We have used the \u201crequests python package\u201d for request HTTP requests using python. XML documents can be parsed in Python with the help of two modules: XML.etree.ElementTree and Minidom (Minimal DOM Implementation). To \u201cparse\u201d an XML file, you must first read the file and then divide the data into individual tokens based on the characteristics of the XML document. To store the system environment variable, we have used the python dotenv package. JSON (JavaScript Object Notation) is used to encode and decode JSON data. \n  \n Driver Function  : In the driver function, create a \u201cfunction\u201d to take \u201cPMID\u201d or \u201cList of PMID\u201d or \u201cKeywords\u201d or \u201cauthor name\u201d as input. Then inside this function, declared another function called the author detail in which the pmid item (single pmid, list of pmid and keyword) passes in an iteration and stores the output in the database in a structural manner. \n  \n Main Function  : In the author detail function pmid as a keyword, then research variable that stores the access of the NLM databases. PubExN uses NLM\u2019s NCBI PubMed database through eutils API. Then prepare a search response with a combination of research, keyword, and API key. Then create a search soup by search response and XML features. if search soup has any value function, the process goes further; otherwise, it terminates the program. Once we got the value in the \u201cwebenv\u201d to find the text from the search soup. Then create a batch to fetch all the article information which we have given in the pmid item. Because there is a limit of 300 articles at once, so we have created maximum of 2000 batches. When the 2001 batch comes next iteration will be started. Then with efetch PubExN fetch all the information one by one. From the efetch we have extracted \u201carticle title\u201d, \u201cpmid\u201d, \u201cauthorlist\u201d, \u201cabstract\u201d, \u201cauthor\u201d, \u201cforename\u201d, \u201clastname\u201d, \u201cinitials\u201d and, \u201caffiliation\u201d. In each iteration it goes through try and except; if it matches it goes to try otherwise it goes to except block. Finally, the value which we have gotten from the process is stored in the dictionary. After that dictionary convert into pandas data frame to store in the normalized form in Mongo or postgres SQL database. \n  \n\n\n\n### Affiliation Normalization Process \n  \nAlgorithm 2 shows step wise step process of the affiliation normalization process. The algorithm starts with downloading the GRID database. Global Research Identifier Database (GRID), an open database of one-of-a-kind research-related organisation identifiers that the company had developed in-house over the course of several years and made available for public use by the research community. If the Grid dataset is available, it shows function will continue with the further process otherwise first it downloads the dataset and then move to the further functionality. In the second step, package preprocess the full-text affiliation by applying the RegEx methods (A regular expression technique has been with the help of the python RegEx method. We did not use any context-sensitive grammar for designing the PubExN. A simple rule-based approach has been used for exact matching to word matching has been applied). In the third step, it extracts various components by a parsing method. \n\nAlgorithm 3 is the driver function of the functions defined in Algorithm 2. \n\n\n\n#### Code Explanation of Affiliation Normalization \n  \n  \n Importing Packages:   The program starts with important packages such as re, string unidecode, and NumPy required in the affiliation parser. \u201cre\u201d stands for a regular expression known as RegEx can be used to check if a string contains the specified search pattern. Unidecode. This library provides functions to transliterate Unicode characters to an ASCII approximation. Unidecode. Decoder. This module takes care of transliterating a single grapheme. It\u2019s only documented so you can use a better strategy to transliterate larger texts. NumPy (short for \u201cNumerical Python\u201d) is a package that contains objects representing multidimensional arrays and a set of procedures for working with those arrays. NumPy performs mathematical and logical array operations. \n  \n Important Function Explanation:   Affiliation parser required important function like \u201creplace institution abbreviation\u201d, \u201cappend institute city\u201d, \u201cclean text\u201d, \u201cfind country\u201d, \u201ccheck country\u201d, \u201cparse email\u201d, \u201cparse zipcode\u201d and \u201cparse location\u201d to complete the parsing process of affiliation parser. In \u201creplace institution abbreviation\u201d, the abbreviation is replaced by its full-text institution name. Suppose we have an abbreviation \u201cNITRR\u201d so it replaces NITRR to \u201cNational Institute of Technology Raipur\u201d. We used GRID database for the exact matching of abbreviations. Then in \u201cappend institution city\u201d function we append the city to the university that has multiple campus if exist. In \u201cclean text\u201d cleans the text of the affiliation text with the abbreviation. If there is any abbreviation i.e., \u201cDept.\u201d it will replace it by \u201cDepartment\u201d similarly if the parser \u201cUniv.\u201d so it replaces with \u201cUniversity\u201d and so on. \u201cfind country\u201d function is used to find the country from the string. suppose in the full-text article like \u201cIndian Institute of Technology Delhi, India\u201d so this function will separate the country name from the full text. Country function check if any states sting from USA or UK. \u201cparse email\u201d function parse email and separate from the full-text article. \u201cParse zipcode\u201d function parses the zipcode from the full-text affiliation. \u201cparse location\u201d function parse state and country from the full-text affiliation. \n  \n Driver Function:   \u201cparse affil\u201d is the driver function and collectively call all the above-defined function and create a dictionary once the dictionary is completed then convert it to the pandas dataframe to store the normalized data in the MongoDb or any SQL databases. \n  \n\n\n\n\n## Results \n  \nThe package we have developed, PubExN, for extracting data from NCBI\u2019s PubMed database for retrieving the research article, has an immense impact on Biomed, medical researchers and students. Although applying this batch approach will help them directly extract the entire text surrounding their area of interest by just using the keyword. \n\nIn addition, the result obtained using the PubExN package is underlined in this subsection. \n\n### Article Extraction Results \n  \nThe article extraction process starts with importing the package into the pro gram\u2019s main module. Select the option based on user requirements to get the desired output. In the first option, if the use-case needed is \u201csearch by keyword,\u201d then pass the keyword and can get all the associated results from that particular keyword. \n\nSelect the option \u201ca\u201d then pass any keyword. We take \u201ccovid-19\u201d as a keyword example first, it gives the total number of articles available with the associated keyword in the PubMed database, and then it produces output in the form of tabulation. Figure\u00a0  shows the extracted abstract from the following PMIDs [ \u2013 ].   \nOutput of the keyword search \n  \n\nUsers or researcher can also search for single article information by passing a single \u201cPMID\u201d of the article. In that case user import the PubMed extract and select option \u201cb\u201d. The function asks to take input in the form of PMID; if the PMID is valid, it will produce the output. \n\nThe output was generated in the form of tabulation, by taking the example PMID [ ] and the results appear as shown in Fig.\u00a0 :   \nPubMed article count by \u201ccovid-19\u201d keyword \n  \n\nIf the requirement of extraction of multiple PMIDs, then choose the option \u201cc\u201d. It takes multiple PMIDs in a list format and passes the list items one by one to take multiple inputs. Multiple PMIDs article information and author details are produced in the tabulated format. The results seem as by taking the example PMIDs: \n\nThe compiled results of multiple \u201cPMID\u201d entries [ \u2013 ] shows in Fig.\u00a0 :   \nPubMed article count by \u201ccovid-19\u201d keyword \n  \n\n\n### Affiliation Normalization Results \n  \nAffiliation normalization module plays an important role in generating structured data. Whenever any use-case requires affiliation normalization in the module, they can easily install and import the pubmed extract package and can use the functionality of affiliation normalization in their module. \n\nAfter successful import of the package then select the choice \u201cd\u201d and enter the full text affiliation. It will generate the normalized affiliation from the full text. Normalized full-text affiliation break into \u201cdepartment\u201d, \u201cinstitution\u201d, \u201cemail\u201d, \u201czip code\u201d, \u201clocation\u201d, \u201ccountry\u201d and it store in the dictionary with the same keys. The final output seems like this: \n\n\n### Reducing Batch Timing \n  \nSome significant delays we have noticed in the PubExN because the proposed software package uses a batch technique, and these methods suffer delay when the process is increases. We have tested the package using one parallelization technique called RAY. Ray is an open-source parallel distributed method. We have tested our package with ray and found a drastic time reduction in the extraction process. Undoubtedly batch approach is a time-consuming process, but if we use it with parallel or distribution methods, it performs well. \n\nModern applications use parallel and distributed computing. To make efficient or to run them on a larger scale, we will need to make use of numerous cores on different machines. The infrastructure that is responsible for crawling the biomedical literature responding to search queries is not a collection of single-threaded applications that are executed on someone\u2019s rather a collection of services that communicate and interact with one another. \n\nRay has the following advantages which are as follows.   \nRay provides the facility to run the same code with multiple machines. Although we have not run the code on multiple machines, we have tested PubExN on a single machine with multiple cores, and for 5 million records, it took around 13\u00a0h. \n  \nBuilding micro-services and actors that have a state and can communicate. \n  \nIt can also handle network or service failure. In case of any failure, if the code is stopped in between whenever the services resume, it starts from that point only. \n  \nIt can efficiently handle large objects and numeric data. \n  \n\n\n### Comparison with the Existing Methods \n  \n \nTable   shows the comparison of the proposed work with existing techniques used to extract the raw data from biomedical databases. The proposed package provided the facility to normalize the raw biomedical text, and also PubExN provides the facility to further normalize author-named and affiliation normalization. \n   \nProposed model comparison with the existing method \n  \n\n\n### Execution Time \n  \nExecution time and the search time both gradually increase as the number of articles increases in PubExN. Table   shows the execution time of the PubExN for a particular number of articles. The execution time includes searching the article information from the PubMed database, normalization process and dumping the extracted data into local database (MySQL/SOLR).   \nExecution Time of the PubExN for the given articles \n  \n\n\n### Searching Time \n  \nTable   shows the time taken by the PubExN for searching the articles from the PubMed database. The time taken by searching the articles is very less because PubExN used API based searching technique which reduces the searching time of the article from the PubMed database.   \nSearching time of the article from the PubMed database \n  \n\n\n### Application of the Proposed Model \n  \nThe PubExN is an essential biomedical text-mining tool that can be used in almost every biomedical application. The following are the most used application by PubExN.   \n Biomedical Named Entity Recognition:   Extraction of a biomedical entity from biomedical literature is a significant task of biomedical named entity recognition (BioNER). Training a BioNER model requires substantial biomedical literature. This bio literature is fetched through the biomedical literature databases (PubMed, PubMed Central). Extraction from these databases is a complicated and hectic process. The PubExN provides flexibility to extract as much raw data from these biomedical literature databases. \n  \n Biomedical Entity Relation Extraction:   Similarly, to find the relation between the biomedical entities large volume of the biomedical structural database is required. PubExN brings down the extra efforts done in the extraction process and is also helpful in reducing the time of the extraction process. \n  \n Author Name Disambiguation Problem:   The Author Name Disambiguation problem as one of the challenging tasks of biomedical text mining. Some- times an author writes intentionally or unintentionally a different name in the articles that time identification of the authorship might be difficult. To solve this problem Microsoft research team is currently working on the Author Name Disambiguation problem. In this problem, the researcher extracts data from a biomedical database like PubMed, PubMed Central, and Some Patent, and Grant databases. Basically, the researcher extracted author information, citation information, a keyword from the abstract, and material methods to check the similarities between the articles to identify the exact author. \n  \n Literature-Based Knowledge Discovery:   In biomedical text mining, literature-based knowledge discovery (LBD) is one of the significant applications of the biomedical domain. In LBD, the title, author information, citation information, abstract and material, and method section are extracted from biomedical databases like PubMed, then tried to find the relations between similar articles. \n  \n\n\n\n## Conclusion \n  \nThis package has been designed to automate the PubMed article data extraction process with affiliation-level normalization. It also follows open-source development and rigorous operational testing and validation standards. We have designed comprehensive documentation with step-wise step guidance to use this package to further aid researchers or users. We envision that this package will benefit a wide range of biomedical text mining development and could aid in the further development of the PubMed-based bulk extraction process. In the future, a deep learning model can be trained to normalized the affiliation information using the Named Entity Recognition model (NER) so that affiliation information can be extracted from these databases. The latest version of the python package is available on PyPI Website  . You can easily install the package using the pip command locally accordingly to your data extraction need. \n\n \n", "metadata": {"pmcid": 10132428, "text_md5": "36b1b760a91d595145e0ae79d2ac9955", "field_positions": {"authors": [0, 38], "journal": [39, 52], "publication_year": [54, 58], "title": [69, 158], "keywords": [172, 284], "abstract": [297, 1958], "body": [1967, 32464]}, "batch": 1, "pmid": 37128512, "doi": "10.1007/s42979-023-01687-3", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10132428", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=10132428"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10132428\">10132428</a>", "list_title": "PMC10132428  PubExN: An Automated PubMed Bulk Article Extractor with Affiliation Normalization Package"}
{"text": "Tsai, Richard Tzong-Han and Sung, Cheng-Lung and Dai, Hong-Jie and Hung, Hsieh-Chuan and Sung, Ting-Yi and Hsu, Wen-Lian\nBMC Bioinformatics, 2006\n\n# Title\n\nNERBio: using selected word conjunctions, term normalization, and global patterns to improve biomedical named entity recognition\n\n# Keywords\n\n\n\n# Abstract\n \n## Background \n  \nBiomedical named entity recognition (Bio-NER) is a challenging problem because, in general, biomedical named entities of the same category (e.g., proteins and genes) do not follow one standard nomenclature. They have many irregularities and sometimes appear in ambiguous contexts. In recent years, machine-learning (ML) approaches have become increasingly common and now represent the cutting edge of Bio-NER technology. This paper addresses three problems faced by ML-based Bio-NER systems. First, most ML approaches usually employ singleton features that comprise one linguistic property (e.g., the current word is capitalized) and at least one class tag (e.g.,   B-protein  , the beginning of a protein name). However, such features may be insufficient in cases where multiple properties must be considered. Adding conjunction features that contain multiple properties can be beneficial, but it would be infeasible to include all conjunction features in an NER model since memory resources are limited and some features are ineffective. To resolve the problem, we use a sequential forward search algorithm to select an effective set of features. Second, variations in the numerical parts of biomedical terms (e.g., \"2\" in the biomedical term IL2) cause data sparseness and generate many redundant features. In this case, we apply numerical normalization, which solves the problem by replacing all numerals in a term with one representative numeral to help classify named entities. Third, the assignment of NE tags does not depend solely on the target word's closest neighbors, but may depend on words outside the context window (e.g., a context window of five consists of the current word plus two preceding and two subsequent words). We use global patterns generated by the Smith-Waterman local alignment algorithm to identify such structures and modify the results of our ML-based tagger. This is called pattern-based post-processing. \n\n\n## Results \n  \nTo develop our ML-based Bio-NER system, we employ conditional random fields, which have performed effectively in several well-known tasks, as our underlying ML model. Adding selected conjunction features, applying numerical normalization, and employing pattern-based post-processing improve the F-scores by 1.67%, 1.04%, and 0.57%, respectively. The combined increase of 3.28% yields a total score of 72.98%, which is better than the baseline system that only uses singleton features. \n\n\n## Conclusion \n  \nWe demonstrate the benefits of using the sequential forward search algorithm to select effective conjunction feature groups. In addition, we show that numerical normalization can effectively reduce the number of redundant and unseen features. Furthermore, the Smith-Waterman local alignment algorithm can help ML-based Bio-NER deal with difficult cases that need longer context windows. \n\n \n\n# Body\n \n## Background \n  \nThe exponential growth of large-scale molecular sequence databases and PubMed scientific literature has prompted active research in biological literature mining and information extraction to facilitate genome/proteome annotation and improve the quality of biological databases [ ]. Critical tasks in biomedical literature mining include named entity recognition (NER), tokenization, relation extraction, indexing, and categorization/clustering. Using various techniques developed for these tasks, we create a set of tools to assist biomedical researchers in exploiting the stream of publications that are flooding Medline at a rate of 1,500 abstracts a day [ ]. \n\nNamed entity recognition (NER) is a fundamental task that involves the identification of words or phrases that refer to specific entities in texts and their classification into different categories. NER was first defined in the general-language domain in the contests of the Message Understanding Conferences [ ]. NER for the biomedical domain (Bio-NER) is a specialized area of NER that has received increased attention in recent years. For natural language processing (NLP) researchers, this field presents a different set of challenges to those found in general NER. In general-language domains, sets of named entities tend to be fairly heterogeneous, ranging from names of individuals to monetary amounts, whereas in the biomedical domain, a set of entities is often restricted to proper biomedical names, such as proteins (e.g., CD28 surface receptor), DNA (e.g., IL-2 gene), RNA (e.g., IL-2 alpha mRNA). Depending on the underlying application, Bio-NER systems can extract objects ranging from protein/gene names to disease/virus names. \n\nBio-NER presents unique challenges because, in general, biomedical named entities of the same category (e.g., protein, DNA) do not follow one standard nomenclature [ ] and can comprise long compound words and short abbreviations [ ]. Some NEs contain various symbols and spelling variations [ ]. In addition, irregularities often occur in named entities; for example, an NE can have unknown acronyms and contain hyphens, digits, letters, and Greek letters; adjectives preceding an NE may or may not be part of that NE, depending on the context and application; NEs with the same orthographical features may fall into different categories; an NE may belong to multiple categories; and an NE of one category may contain an NE of another category. \n\nInitially, Bio-NER used handcrafted patterns [ ] to recognize the various NE forms; however this approach suffered from lack of portability and scalability. Later, machine learning (ML) models were introduced to tackle the Bio-NER problem \u2013 first simple classifiers [ , ] and then more complex probabilistic sequence models [ - ]. The first step in an ML approach is to break the input sentence into tokens, usually individual words or hyphenated compounds. Then, each token is assigned a class tag containing its type (e.g., protein, DNA) and position. For example,   B-protein   and I  -protein   are class tags that respectively represent the beginning (  B  ) and the internal/ending (  I  ) token of a protein name. To make these predictions, ML models rely on linguistic features, i.e., functions that represent linguistic properties and class tags. A linguistic property is a function that indicates the value corresponding to a specific linguistic attribute of the current token, as shown by the following binary function: \n\n\n\nExamples of linguistic information that can be referenced in linguistic properties include the affixes of current or neighbor tokens, part-of-speech tags, phrase tags, and specific words. Linguistic features frequently operate within a limited range on either side of the current token, usually two tokens preceding and two following the current token [ ]. Such features are categorized as either singleton features or conjunction features [ ]. \n\nSingleton features are only conditioned on one linguistic property and joined by at least one class tag. For example, a simple binary singleton feature for token-tagging may be \"if the current word= 'factor' AND current tag=  I-protein   THEN feature value = 1.\" Conjunction features, however, are conditioned on multiple linguistic properties and joined by at least one class tag. A conjunction feature similar to the above singleton feature would be \"if current word='factor' AND previous word ='transcription' AND current tag=  I-protein   THEN feature value = 1,\" in which the multiple linguistic properties are current word='factor' and previous word ='transcription'. In most ML systems, singleton features far outnumber conjunction features because the latter occupy a lot of memory. For example, if   n   linguistic properties are used in the ML model, the space required for a conjunction feature is O(  n  ) compared to O(  n  ) for a singleton feature. \n\nTo improve the ML model for Bio-NER, we consider three issues. The first is to choose an effective set of features, both singleton and conjunction, for a given application and hardware configuration. In particular, we want to choose effective conjunction features since they present interesting possibilities [ , ]; however, this is a challenging task. \n\nThe second issue we address relates to variations in the numerical parts of biomedical terms (e.g, \"2\" in the protein name IL2). Such numerical affixes cause data sparseness and generate many redundant features. Furthermore, in the biomedical domain, proteins or genes of the same family may only differ in their numerical parts. For example, interleukin-2 and interleukin-3 belong to the same family \u2013 interleukin. In Bio-NER, they are usually annotated as the same NE class. Our solution is to convert all numerals into representative place-holders. \n\nThe third issue involves distant tag dependencies in Bio-NER. Most ML models follow the Markov assumption that the current NE tag only depends on the previous tag. However, in Bio-NER, there are many exceptions to this assumption as an NE tag may depend on the previous tag or the next tag, or the words in between. Take \"IL-1, IL-2, and AP-1\" for example. AP-1's NE class depends on the conjunction \"and\" and the previous two NEs, so these three NEs should be placed in the same class. However, ML models that operate in a limited context window cannot represent this dependency and are generally weaker at estimating distant features. Furthermore, they may fail if there are dependencies beyond the context window. Therefore, we need a different strategy to model such dependencies. \n\n### Related work \n  \nBiomedical NER solution methods fall into three general classes: dictionary-based approaches, rule-based approaches, and machine-learning-based approaches. One might think that systems relying solely on dictionary-based recognition could achieve a satisfactory performance. However, dictionary-based approaches cannot handle unseen NEs and ambiguous contexts effectively [ ]. Rule-based approaches, e.g., Fukuda's PROPER system [ ], generally rely on combinations of regular expressions (templates) to define patterns that match biomedical NEs and rules for extending NE boundaries to the right and/or left of an expression. For example, a rule-based approach might use a regular expression such as \" [a-z]+ [0-9]+ \" (a sequence of one or more lower-case letters followed immediately by a sequence of one or more digits) to recognize that p53 is a gene name. One can also create a rule that uses categorical nouns to classify biomedical named entities. For example, compound words ending in \"mRNA\" have a high probability of being RNA. While rules of this type can be quite effective, they suffer from the weakness of being domain-specific. Thus, if the system is ported to a new domain, many rules would probably need to be modified. \n\nMachine-learning-based approaches are divided into two categories: classifier-based and sequence-model-based. The former include na\u00efve Bayes classifiers and Support Vector Machines (SVM) [ ]; and the latter include hidden Markov models (HMM) [ ], Maximum Entropy Markov Models (MEMM) [ ], and Conditional Random Fields (CRFs) [ ]. In the following sections, we discuss the differences between classifier-based and sequence-model-based approaches, and then explain why we choose CRFs for sequence tagging over classifier-based models and other sequence-based models. \n\n#### Formulation \n  \nTo perform ML-based NER, all sentences must be broken into tokens, which are then given tags. From the numerous token/tag formats available, we adopt the IOB2 format, which is has a proven track record for sequence tagging problems [ ]. In IOB2, each word in a sentence is regarded as a token, and each token is associated with a tag that indicates the category of the NE and whether the given token is at the beginning (  B  ), or inside (  I  ) of the NE. For example, in   B_c  ,   I,_c  , where   c   is an NE category,   B_  and,   I   denote, respectively, the first token and the subsequent token of an NE in category   c  . In addition, we use the tag   O   to indicate that a token does not belong to any NE. Once we have tokenized a sentence, we can define NER as the assignment of one of 2  m  +1 tags to each token, where   m   is the number of NE categories. For example, the following phrase annotated in XML format: \n\n\"<DNA> IL-2 gene </DNA> expression, <Protein> CD28 </Protein>, and <Protein> NF- kappa B </Protein>\" \n\nis transformed to the following IOB2 format: \n\n\"IL-2/  B-DNA   gene/I-DNA expression/O,/O CD28/B-protein,/O and/O NF- kappa/B-protein B/I-protein\". \n\n\n#### Classifier-based approaches \n  \nIn classifier-based approaches, each token is classified into a tag class. For binary classifiers, the training data can be used to train 2  m  +1 classifiers and calculate the token's score for each tag class. For multi-label classifiers, the score for each NE class can be derived directly. After obtaining the score of each tag for each token, the best total tagging sequence for the input sentence using the Viterbi search algorithm [ ], which outputs the valid tag sequence with the highest score. In a valid sequence, each   I-  tag must follow either another   I-  tag or a   B-  tag of the same class. For example,   I  -  protein   should follow   I  -  protein   or   B  -  protein  . An invalid tag sequence would be one containing a   B  -  protein   followed by an   I  -  DNA  . \n\n\n#### Sequence-based models \n  \nSequence-based models can use the same token/tag format as classifier-based models. However, in sequence models, features must refer to tags in the context window. In most cases, they refer to the current, preceding or subsequent tags. We can divide sequence models into generative and discriminative types. \n\n\n#### HMM and MEMM \n  \nThe Hidden Markov Model (HMM) is a generative type of sequence-based model. In the following explanation of sequence models,   x   refers to the input token sequence and y refers to the output tag sequence. Generative models generally find the best tag sequence by computing the generative form of the probability   p  (  x  ,  y  ). In HMM, the probability of   p  (  y  |  x  ) can be rewritten as a calculation utilizing its generative form   p  (  x  ,  y  ) according to the Bayes theorem: \n\n\n\nAssuming the current tag   y  depends on the previous tag   y  , and the current token   x  depends on the current tag   y  , then   p  (  x,y  ) can be rewritten as: \n\n\n\nwhere   n   is the number of tokens in   x  . \n\nSince the objective is to find the best   p  (  y  |  x  ), and   p  (  x  ) is an a priori probability that remains the same for each possible tag class, we only need to compare   p  (  x  ,  y  ). \n\nThe problem with Equation (2) lies in data sparseness caused by   p  (  x   |y  ). Ideally, we would have sufficient training data for every possible value of   x  in order to calculation of   p  (  x   |y  ). In reality, however, there is rarely enough training data to compute accurate probabilities when decoding new data. This problem is often solved by using the na\u00efve Bayes approach. \n\nIn the the na\u00efve Bayes approach, we decompose   p  (  x   |y  ) as follows: \n\n\n\nwhere   f  is the value of   x  's   j  feature. \n\nEven with the above solution, HMMs suffer from two limitations. The first arises from the na\u00efve Bayes assumption of HMMs for solving NER, which would benefit from a richer representation of observations, especially a representation that describes observations in terms of many overlapping features, such as capitalization, affixes, part-of-speech (POS) tags, in addition to surface word features. For example, when trying to extract unseen company names from a newswire article, knowing whether the word is capitalized and associated with a POS noun tag would be useful. However, a na\u00efve Bayes assumption might fail in this case because these features are not independent of each other. The second problem with HMM is that it sets its parameters to maximize the likelihood of the observation sequence, but the task is to predict the state sequence given the observation sequence. In other words, HMM inappropriately uses a generative joint model to solve a conditional problem in which the observations are given. \n\nMaximum entropy Markov models (MEMMs) [ ] have been proposed to address both of the above issues. To allow for non-independent observation features that are difficult to enumerate, MEMM replaces the generative, joint probability parameterization employed by HMMs with a conditional model that represents the probability of reaching a state given an observation feature and the previous state. The probability of   p  (  y|x  ) in MEMM is calculated as follows: \n\n\n\n\n\n\n\nwhere   Z  (  y  ,   x  ) is the normalization factor that ensures the probability of all state   y  sum to one,   h  (  y  ,   y  ,   x  ,   c  ) is usually a binary-valued feature function and   \u03bb  is its weight. Large positive   \u03bb  values indicate a preference for such an event, whereas large negative values make the event unlikely. \n\nHowever, despite their advantages, MEMMs suffer a label bias problem in that the transitions leaving a given state only compete against each other, rather than against all other transitions in the model [ , ]. The Markovian assumptions in MEMMs ignore the dependencies between the current state and other states, except for the previous state. To resolve this problem, Lafferty et al. [ ] developed conditional random fields (CRFs), a sequence modeling framework that retains the advantages of MEMMs, but avoids the label bias problem. \n\n\n#### Conditional Random Fields \n  \nCRFs are undirected graphical models, in which each node represents a state that is trained to maximize a conditional probability [ ]. A linear-chain CRF with parameters \u039b = {  \u03bb  ,   \u03bb  , ...} defines a conditional probability for a state sequence   y   =   y  ...  y  given a length-  n   input sequence   x   =   x  ...  x  as follows: \n\n\n\n\n\nwhere   Z  (  x  ) is the normalization factor that ensures the probability of all state sequences sum to one,   C   is the set of all cliques in the target sentence, and   c   is any single clique. A clique is a fully connected subset of nodes. Note that   h  (  y  ,   x  ,   c  ) is usually a binary-valued feature function and   \u03bb  is its weight. Large positive   \u03bb  values indicate a preference for such an event, while large negative values make the event unlikely. \n\nThe size of   c   determines which states   h  can refer to. If   c   contains only the current state, then   h  can only refer to the current state. However, if   c   contains the current and the previous states, then   h  can refer to all of them. Given   x  , the conditional probability of   y   is equal to the exponential sum of   \u03bb   h  in all cliques. Since the average length of a biomedical NE is between two and three tokens, we use two instead of three for the clique size to economize on memory space. The following are two examples of current word features. The first refers to the pair   y  and   y  and the second refers to   y  individually: \n\n\n\nwhere   y  stands for the tag sequence in   c  , and [  i  -1,   i  ] means that the clique   c   ranges from the   i  -1 token to the   i  token. \n\nFigure   shows a graphical representation of \"activate STAT5 proteins in\" tagged as \" [  O  ,   B-protein  ,   I-protein  ,   O  ]\". To calculate the probability of the phrase \"activate STAT5 proteins in\" being tagged as [  O  ,   B-protein  ,   I-protein  ,   O  ], we consider the three cliques involved: [activate, STAT5], [STAT5, proteins], and [proteins in], as shown in Figure  . Then, given the input sequence, the conditional probability of the specified tag sequence can be calculated as follows: \n\n\n\nThe most probable label sequence for   x  , \n\n\n\ncan be efficiently determined using the Viterbi algorithm [ ]. \n\nThe parameters can be estimated by maximizing the conditional probability of a set of label sequences, given each of their corresponding input sequences. The log-likelihood of a training set {(  x  ,   y  ):   l   = 1, ...,   M  } is written as: \n\n\n\nTo optimize the parameters in CRFs, we use a quasi-Newton gradient-climber BFGS [ ]. \n\n\n#### Comparison of HMM, MEMM, and CRF \n  \nThe critical difference between CRF and MEMM is that the latter uses per-state exponential models for the conditional probabilities of next states given the current state, whereas CRF uses a single exponential model to determine the joint probability of the entire sequence of labels, given the observation sequence. Therefore, in CRF, the weights of different features in different states compete against each other. \n\nAfter comparing the various models, we chose CRF as our framework. Its primary advantage over HMM is its conditional nature, which allows for the relaxation of the independence assumptions that HMM requires to ensure tractable inferences. Additionally, CRF avoids the label bias problem [ ] inherent in MEMM [ ] and other conditional Markov models based on directed graphical models [ ]. CRF outperforms both MEMM and HMM in a number of real-world sequence labelling tasks [ , , ]. In addition, CRF uses exponential weighed sums to combine the influences of many correlated features, including overlapping and co-dependent features. As a result, we can use multiple features with CRF more easily than with HMM. \n\n\n\n\n## Results and Discussion \n  \n### Datasets \n  \nIn our experiment, we employ the dataset used in the JNLPBA 2004 shared task [ ], which was converted from the GENIA corpus. The GENIA corpus was formed by a controlled search of MEDLINE using the MeSH terms \"human\", \"blood cells\" and \"transcription factors\". In that search, 2,000 abstracts were selected and annotated manually according to a taxonomy of 48 classes, of which 36 were used for annotation. Several biomedical NER systems use the GENIA corpus as training and test data [ , ]. \n\nIn the JNLPBA 2004 shared task, the GENIA corpus was still used as training data. However, the original 36 classes were simplified to 5 classes: protein, DNA, RNA, cell line, and cell type. To reduce the annotation task to a simple linear sequential analysis problem, embedded structures have been removed leaving only the outermost structures. Consequently, a group of coordinated entities involving ellipsis are annotated as one structure, as shown by the following example: \n\n... in [lymphocytes] and [T- and B-lymphocyte] count in ... \n\nIn the example, the term \"T- and B-lymphocyte\" is annotated as one structure even though it involves two entity names, \"T-lymphocyte\" and \"B-lymphocyte\", whereas \"lymphocytes\" is annotated as one entity. \n\nTo ensure that the evaluation was objective, the JNLPBA task organizers provided 404 newly-annotated MEDLINE abstracts from the GENIA project as test data. They were annotated with the same five entity categories as the training dataset. Half of the abstracts were from the same domain as the training data and the other half were from the super-domain of \"blood cells\" and \"transcription factors\". Here, a domain refers to a specific subject or area of knowledge, while a super domain is a broader subject area than a domain. The super-domain of \"blood cells\" and \"transcription factors\", for example, includes more general terms, such as cells and proteins. Testing on the super-domain can provide an important measure of the generality of the methods used. We also used this dataset as the test set. The basic statistics for the training and test data are summarized in Table  . \n\n\n### Evaluation methodology \n  \nResults are reported as F-scores using JNLPBA's evaluation script, which is a modified version of the evaluation script of the CoNLL-03 shared task [ ]. The F-score is defined as F = (2PR)/(P + R), where P denotes the precision and R denotes the recall, defined as follows: \n\n\n\nThe evaluation script outputs three sets of F-scores according to the exact boundary matching, right-boundary matching, and left-boundary matching [ ]. In the right-boundary matching, we only examine whether the right boundaries of entities match the true NEs, without considering the left boundaries. Left-boundary matching is performed in a similar manner. \n\n\n### Feature denotation \n  \nBefore describing the system and experimental results in depth, we explain how features are denoted. We group feature functions by their linguistic properties, denoted by italicized letters and subscript appearing at the left-hand side of the equation phrase. For example, the feature functions that refer to the same linguistic property   w  , such as \"  w  = IL1\", \"  w  = IL2\", and \"  w  = IL3\" are grouped as the feature group \"current word\". Feature groups can also be combined into feature types. For example, the current word, previous word, and next word (  w  ,   w  , and   w  ) form a \"word\" feature type. \n\nOur system uses six singleton feature types: word, orthographical features, part-of-speech (POS), word shape, affix, and chunk. They are described in the Methods section along with some of their conjunctions. \n\n\n\n## Results \n  \nTable   shows the improvements in NER performance achieved by incrementally adding new features to the baseline model, which defines our three methods. The model is also described in the Methods section. In the table, F denotes the feature set of all six singleton feature types, and F refers to the set of the selected word conjunction feature groups. NN indicates that numerical normalization was applied to both the training and the test data, while PBPP indicates that pattern-based post-processing was applied to the results of NN. The #1 configuration, which simply employs F (Note comma) is our baseline configuration. The configurations created by adding F NN, and PBPP to F one-by-one are denoted as #2, #3, and #4, respectively. We observe that the F-scores increase by 1.67%, 1.04%, and 0.57%, respectively. For a Bio-NER system with an F-score of over 70%, these improvements are appreciable. Configuration #4 is our system for Bio-NER, called NERBio. \n\nIn Table  , we list the precision, recall, and F-score for each category of NE. We observe that the F-scores for proteins and cell types are comparatively high. This is possibly because they comprise two of the top three most frequent categories in the training set (as shown in Table  ). However, although DNA is the second most frequent category, it does not have a high F-score. We think this discrepancy is due to the fact that DNA names are commonly used in proteins, causing a substantial overlap between these two categories. RNA's performance is comparatively low because its training set is much smaller than other categories. The performance on cell line is the lowest since it overlaps heavily with cell type and its training set is also very small. In Table  , we compare NERBio with the top three JNLPBA 2004 systems. NERBio performs better than [ ] and [ ], which use MEMM and CRF, respectively, because our conjunction features, term normalization, and pattern-based post processing are effective. \n\nSurprisingly, Zhou's HMM [ ] outperforms Settles' CRF model [ ], which seems to contradict our earlier comments. Settles' model simply uses general, non-biomedical features, such as word and orthographic features, whereas Zhou's model incorporates domain-specific resources, such as biomedical dictionaries, manually observed rules, abbreviations, and an alias database. These additional resources probably fill in gaps when the system encounters unseen words. \n\nTable   shows the F-scores for the following boundary matching criteria: exact boundary match (Exact Match), left boundary match (Left Match) and right boundary match (Right Match). By relaxing the boundary matching, the F-score improves from 3.11% (Left Match) to 6.56% (Right Match). Although this increases the tagging accuracy of some NEs that have descriptive preceding adjectives or rightmost head nouns, it also increases other types of errors. The degree of relaxation should be based on the specific NER application [ ]. \n\n\n## Discussion \n  \nRecognition disagreement between our system results and the JNLPBA corpus can be attributed to the following two factors: \n\n### Annotation problems in the JNLPBA Corpus \n  \nAlthough inter-annotator agreement results for the JNLPBA corpus are not available, some studies of inter-annotator agreement among biomedical named entities have reported agreement rates between 87% [ ] and 89% [ ]. We divide the annotation problems into four sub-problems, all of which are caused by inconsistent annotation. \n\n#### (a) Preceding adjective problem \n  \nSome descriptive adjectives are annotated as part of the subsequent NE, but some are not. In fact, it is even hard for biologists to decide whether descriptive adjectives, such as \"normal\" and \"activated\", should be part of entity names. For example, in the training data, \"human\" occurred 1,790 times either before or at the beginning of an entity, but it was not recognized as a part of an entity 110 times. In test data, however, it was only excluded from an NE once out of 130 occurrences. This irregularity confuses NER systems and reduces the reliability of evaluation results on the GENIA corpus. \n\n\n#### (b) Nested NE problem \n  \nIn the JNLPBA data, we found that, in some instances, only embedded NEs are annotated, but in other instances, only the outmost NEs are annotated. In fact, both should be tagged. However, according to the JNLPBA's simplification of NER, which removes all embedded NEs, the outmost NE should be tagged. For example, in the training set of the JNLPBA 2004 data, in 59 instances of the phrase \"IL-2 gene\", \"IL-2\" was annotated as a protein 13 times, while in the remaining 46 instances \"IL-2 gene\" was tagged as DNA. This irregularity can confuse machine learning based systems. \n\n\n#### (c) Cell-line/cell-type confusion \n  \nNEs in the cell line class are from certain cell types. For example, the HeLa cell line can be from humans or cellular products. Given the abbreviated content of an abstract, it is even difficult for an expert to distinguish between cell lines and cell types. In GENIA, most instances of \"granulocytic colonies\" are tagged as cell lines; however, in the phrase \"stimulated primary murine bone marrow cells to form granulocytic colonies in vitro\", the phrase \"granulocytic colonies\" is tagged as a cell type. \n\n\n#### (d) Missing tags \n  \nIn the training data of the JNLPBA 2004 data, some NEs of each category, especially cell lines, are not tagged. Such incorrect annotation causes a large number of false negatives. For example, we have observed many instances of \"T cell\", \"Peripheral blood neutrophil\", and \"NK cell\" not tagged as cell lines. \n\n\n\n### System recognition errors \n  \nThe other main cause of disagreement is our system's tagging errors. We categorize errors into two subtypes: \n\n#### (a) Misclassification \n  \nSome protein molecules or regions are misclassified as DNA molecules or regions. These errors may be solved by exploiting more context information. \n\n\n#### (b) False positives \n  \nSome entities appear without a specific name, e.g., \"the epitopes\" without indicating which kind of epitopes. GENIA tends to ignore these entities, but their contexts are similar to the entities with specific names. Therefore, our system sometimes incorrectly recognizes them as NEs. \n\n\n\n\n## Conclusion \n  \nIn the biomedical domain, relying solely on singleton features cannot resolve all ambiguous cases. Adding conjunction features is necessary, therefore, and has proven effective previously [ ]. However, any conjunction feature group, especially the word conjunction feature group, has many more member features than any singleton feature group. It quickly becomes infeasible to include a large number of conjunction feature groups in a Bio-NER ML model due to limited memory resources. Furthermore, including all feature groups may not be desirable since features in some feature groups occur rarely and therefore have inaccurate weights. In this paper, we successfully employ sequential forward search (described in the Methods section) to select the most effective feature groups. The next problem we address is data sparseness caused by variations in the numerical parts of Bio-NEs. Such variations generate many redundant and unseen features. To deal with these variations we apply numerical normalization. Lastly, we overcome the limits of the context window using pattern-based post-processing. A token's tag may not only depend on its adjoining neighbors, but may be determined by words beyond the context window. We use automatically generated global patterns to recognize distant relations and modify the CRF tagging results accordingly. Using the above three methods, we improve the system's overall performance by 3.28%, achieving a total F-score of 72.98%, which is higher than all current JNLPBA systems. \n\nThere are still several unsolved problems in Bio-NER. For example, it is still difficult to recognize long, complicated NEs and to distinguish between two overlapping NE classes, such as cell-lines and cell-types. Another serious problem is annotation inconsistency, which confuses machine learning models and makes evaluation difficult. \n\nCertain errors, such as those in boundary identification, are relatively tolerable if the main purpose is to discover the relations between NEs. In our future work, we will exploit more global linguistic features, e.g., semantic role labels and full-parsing features. To date, we have not exploited external databases, such as iProLink [ ]; however, we will incorporate them into our system. Finally, to reduce the requirement for human annotation and alleviate the scarcity of available annotated corpora, we will develop machine learning techniques to apply to partially annotated corpora in different biomedical domains. \n\n\n## Methods \n  \nIn this section we describe numerical normalization, the feature set of our CRF model and its selection, as well as global pattern-based correction post-processing. \n\n### Feature set \n  \nFeature selection is critical to the success of machine learning approaches. In this section, we describe the features used in our system. \n\n#### Word features \n  \nWords preceding or following the target word may be useful for determining its category. Take the sentence \"The IL-2 gene localizes to bands BC on mouse Chromosome 3\" for example. If the target word is \"IL-2\", the following word \"gene\" will help the CRF model distinguish the IL-2 gene from the protein of the same name. One might assume that the larger the context window, the better and more precise the results will be. However, widening the context window would lead to an explosion in the number of word features, often encompassing infrequent word features that are distant from the current token. In our experience, a suitable window size is five, i.e., the two preceding words, the current word, and the two following words. This window size is also suitable for most tagging problems, such as POS tagging [ ]. \n\n\n#### Orthographical features \n  \nTable   lists all the orthographical features used in our system. These features are widely used in other general NER [ ] or biomedical NER systems [ ]. Empirically it has been shown that these features can detect NE patterns. Take the ALPHANUMERIC feature for example. The digits following a sequence of English letters, e.g., '5' in the protein STAT5, usually denote the serial number of the gene, protein, or cell families. These digits can be used to distinguish Bio-NEs from general English words. \n\n\n#### Part-of-speech features \n  \nPart-of-speech (POS) information is useful for identifying named entities. Verbs and prepositions usually indicate an NE's boundaries. Nouns not found in the dictionary are usually proper nouns, which are good candidates for named entities. Setting a context window length for POS features is similar to setting the window length for word features. We found that five is also a suitable window size for POS features. The Stanford POS tagger [ ] is used to provide POS information. We trained it on GENIA 3.02 p and achieved 98.85% accuracy. \n\n\n#### Word shape features \n  \nSometimes, named entities belonging to the same category are similar, for example, HLA-A and HLA-B. We first employ a simple method to normalize all similar words: all capitalized characters are all replaced by 'A'; all digits are all replaced by '0'; non-English characters are replaced by '_' (underscore); and non-capitalized characters are replaced by 'a'. Thus, Kappa-B would be normalized as \"Aaaaa_A\". To further normalize such words we reduce consecutive strings of identical characters to a single character. This feature is the   compressed word shape feature  . For example, \"Aaaaa_A\" is normalized to \"Aa_A\". After applying the first normalization method, the two proteins \"HLA-A\" and \"HLA-B\" will normalized to the same term \"AAA_A\" and activate the same features. After applying the second method, \"IL-2\" and \"IL-21\" will be normalized to the same term \"A_1\" and activate the same features. \n\n\n#### Affix features \n  \nAn affix is a morpheme that is attached to a base morpheme, such as a root or a stem, to form a word. The type of an affix depends on its position relative to the root. Prefixes (attached before another morpheme) and suffixes (attached after another morpheme) are two of these types. Some prefixes and suffixes provide good clues for classifying named entities. For example, words that end in \"~ase\" are usually proteins. However, short prefixes or suffixes are too common to be of any help in classification. For example, it would be difficult to guess to which category a word ending in \"~es\" belongs. In our experience, the acceptable length for prefixes and suffixes is 3\u20135 characters; the longer the prefix or suffix, the fewer the number of matches that will be found. \n\n\n#### Chunk features \n  \nChunk features, which are provided by a chunker or shallow parser, are also useful for recognizing NEs. In shallow parsing, a sentence is divided into a series of chunks that include nouns, verbs, and prepositional phrases. Generally speaking, NEs are usually located in noun phrases. In most cases, either the left or right boundary of an NE is aligned with either edge of a noun phrase. For instance, in the noun phrase \"the human interleukin-2 gene\", the gene name \"human interleukin-2 gene\" aligns with the right boundary of the noun phrase. NEs rarely exceed phrase boundaries. Our system uses the GENIA tagger [ ] to obtain chunk data. \n\n\n#### Conjunction features \n  \nAs the above features are all singletons, in some cases, they are insufficient to classify tokens correctly. Consider the following two features: \n\n\n\nNeither feature is effective by itself. When the first feature is enabled, the current tag can be either   I-protein  ,   I-DNA  , or   O  , depending on whether the current word is \"factor,\" \"silencer \", or \"rate\". However, their conjunction \n\n\n\nis very effective because when the previous word is \"transcription,\" and the current word is \"factor,\" the current word is most likely the last word of a transcription factor name, which is categorized as a protein name in the GENIA ontology. \n\n\n\n### Feature group selection \n  \nEach feature type has several feature groups that differ in their positions, lengths, and patterns. Since not all feature groups are effective for Bio-NER, we need to select effective feature groups. Performing feature group selection on all possible groups is extremely time-consuming. Therefore, following previous NER studies, we only consider lengths ranging from 3 to 5 characters, and positions ranging from -3 to +3 tokens relative to the current token. Table   lists the feature types, the number of subtypes and the number of possible feature groups in each feature type. We can see that both the word and POS feature types have seven feature groups, which differ in their relative positions. Some feature types have subtypes, such as the orthographical feature type, which has 17 subtypes, giving it 7 \u00d7 17 = 119 feature groups. The word shape feature type has two subtypes, word shape and compressed word shape; while the affix feature has two subtypes, prefix and suffix. The word conjunction feature has   feature groups. The total number of all potential feature groups is 210. \n\nBecause our system has tens of thousands of singleton features, employing all possible conjunctions is infeasible. In our experience, one word conjunction feature group (e.g.,   w  =  X  AND   w  =  X  , where   X  and   X  can be any word) occupies approximately 1.5 GB of RAM. Therefore, a server with 10 GB of RAM can only accommodate about six word conjunction feature groups. Even though including all the word conjunction features is computationally feasible, it would be difficult to obtain sufficient statistics for these features since most conjunctions occur rarely, if ever. Therefore, the performance would be reduced. In [ ], it is reported that the performance achieved using all singleton features and all conjunction features is not as good as that derived by only using the best subset in the CoNLL-2003 English NER task's dataset. Given the limited memory, we need to use a suitable feature-selection algorithm to maximize the system's performance. \n\nFeature selection is often viewed as a search problem in a space of feature subsets. To carry out this search we must specify a starting point, a strategy to traverse the space of subsets, an evaluation function, and a stopping criterion. Although this formulation allows a variety of solutions to be developed, usually two families of methods are considered: filter and wrapper. Filter methods use an evaluation function that relies solely on the properties of the data, making them independent of any particular machine learning algorithm. Commonly used measurements include mutual information and information gain. In the field of CRF-based sequence tagging, [ ] describes an implementation of feature induction for CRF that automatically creates a set of useful features and conjunction features. The method scores candidate features by their log-likelihood gains. Feature induction works by iteratively considering sets of candidate singleton and conjunction features created from the initially defined set of singleton features as well as the set of current model features. Only candidates with the highest gain are included into the current set of model features. Intuitively, features with high gain provide strong evidence for many decisions. To calculate the log-likelihood gain efficiently, McCallum [ ] makes certain independence assumptions about the parameters and only includes positions in the sequence that are mislabeled by the current parameter settings. In biomedical NER, McDonald et al. [ ] applied McCallum's method and achieved a 2% increase in the F-score. \n\nWrapper methods use the actual evaluation to measure the quality of   F  , where   F   is a subset of all feature candidates. In this approach, a small portion of data is selected for training and development initially. The evaluation is then performed on the development set. Several machine learning and pattern recognition papers [ - ] have established that the wrapper method can select more appropriate features for a specific machine learning algorithm than the filter method. In the current work, we also adopted the wrapper method. \n\nDue to time limitations, it is very difficult to select a globally optimal feature set for the development set. In our system, we employ sequential forward selection to find the best feature subset. We first calculate which feature group has the highest F-score and select it as the basis for the feature pool. In each subsequent iteration, we add the feature groups to the feature pool individually and calculate their F-scores. Each time, we select the feature group with the best score and add it to the pool. This process continues until the F-score stops increasing. \n\nCurrently, we only include word conjunction features because they are more effective than other conjunctions. Table   shows the most effective word conjunction feature groups selected by our algorithm. These features specify the values of two words in the context window. For example,   w  AND   w  stands for all features that contain the two properties \"  w  =* \" and \"  w  =*\", where '* ' can be any word. The performance of the selected conjunction features is discussed in the Results section. \n\n\n### Numerical normalization \n  \nNumerical normalization is a data preprocessing method that converts numerals in each term to one representative numeral. The advantages of numerical normalization include: (1) the number of features can be substantially reduced; (2) it is possible to transform unseen features into seen features; and (3) feature weights can be estimated more accurately. Take the gene names IL2, IL3, IL4, and IL5 for example. IL2, IL3, IL4 are in the training set, but IL5 is not. If we apply numerical normalization to these terms, they will all be normalized to IL1. Therefore, the number of features corresponding to the first three terms is reduced to a minimum of 1/3. Since IL5 and IL1 are treated alike and share the same weight, this unseen feature becomes a seen feature. According to our results, normalization generally increases overall Bio-NER accuracy (Table  ). Suppose IL2 is annotated as \"gene\" three times, IL3 is annotated as \"gene\" six times, and IL4 is annotated as \"gene\" once and as \"compound\" once. The annotation of IL4 may confuse machine learning models. After numerical normalization, however, the first three terms are annotated as \"gene\" ten times and as \"compound\" only once. Therefore, the feature weights can be correctly estimated. \n\n\n### Using global pattern to improve CRF \n  \nSince dependency may exist between NEs, words among NEs, and even words beyond the context window (as described in Background section), we apply global patterns composed of NEs and surrounding words to resolve the problem. In the following subsections, we describe pattern induction, pattern filtering, and pattern-based error correction in detail. \n\n#### Global pattern induction and filtering \n  \nThe first step of creating global patterns applies numerical normalization to all sentences in the training, development, and test sets. For each pair of sentences in the training set, we apply the Smith-Waterman local alignment algorithm [ ] to find the longest common string, which is then added to the candidate pattern pool. During the alignment process, positions where the two input sequences share the same word or NE class are counted as a match. Here, NE class means a tag's NE category, such as   cell_type, protein, RNA  , or   DNA  . The similarity function used in the Smith-Waterman algorithm is: \n\n\n\nwhere   x   and   y   refer to any two compared tokens from the first and second input sentences, respectively. The similarity of two inputs is calculated by the Smith-Waterman algorithm based on this token-level similarity function. \n\nThe following two tagged sentences show how patterns are extracted from a sentence pair in the training set: \n\n\"  both  /  O   megakaryocytic/   B-cell_type    and/   I-cell_type    erythroid/   I-cell_type    lineages/  I-cell_type  /  O  \" and \n\n\"  both  /  O   myeloid/   B-cell_type    and/   I-cell_type    natural/   I-cell_type    killer/   I-cell_type    (/   I-cell_type    NK/   I-cell_type   )/   I-cell_type    cells/   I-cell_type   /O\" \n\nWe generate a pattern \"  both <cell_type>  .\" for them. Here, we put the aligned words and tags in bold font. The first and last tokens in a pattern are constrained to be words, or sentence beginning and ending symbols. \n\nThe extracted patterns are composed of a headword, an NE type, and a tail-word; for example, \"headword <NE type> tail-word\". To test the patterns' effectiveness, each one is applied to the development set to correct the NE tags of all sentences. If the pattern's error ratio exceeds a certain threshold, \u03c4, the pattern is filtered out. \n\n\n#### Complexity analysis \n  \nFor each pair of sentences in the training set (  n   sentences), using local alignment to find their longest common patterns has a complexity of   O  (  n   l  ), where   l   is the longest sentence in the training set. The evaluation of each pattern,   p  , using the development set (  m   sentences) has a complexity of O(  mnl  ). \n\n\n#### Error correction \n  \nAfter the CRF-based NER module tags an input sentence, we check if that sentence can be corrected by our global patterns. We first modify the tagged sentence for matching. The tagged sentence output by CRF is still in the IOB2 format. For the tokens assigned to an NE, we combine them with an NE-type symbol, while for others, we only keep the words. For example, \"both/  O   CD4/  O   and/  O   CD8/  B-cell_type   mature/  I-cell_type   T/  I-cell_type   cells/  I-cell_type  /  O  \" is modified to \"both CD4 and <cell_type>.\". \n\nNext, we match the transformed tagged sentence   s  ' with any pattern   t  . Basically, the matching is also implemented using the Smith-Waterman local alignment algorithm. After this matching, two aligned segments have the same beginning and end tokens (words or symbols). For each aligned pair of segments   p   in   s  ' and   q   in   t  , we check if both   p   and   q   contain the same NE symbol   \u03c3  . If they do, we check the gap ratio   \u03b3   as follows: \n\n\n\nOur correction policy is very simple. If   \u03b3   is less than a threshold \u03b6, we modify   p   to be   \u03c3  , except for beginning and end words. \n\nLet us use the above example to explain pattern matching and error correction. We align the modified segment \"both CD4 and <cell_type>\" with a pattern segment \"both <cell_type>.\". There are two gaps. Therefore,   \u03b3   is equal to 2/(8-2) = 0.33, which is less than our threshold of 0.4. Then, we modify the original tagged segment to be \"both/  O   CD4/   B_cell_type    and/   I-cell_type    CD8/   I-cell_type    mature/  I-cell_type   T/  I-cell_type   cells/  I-cell_type  /  O  \", where the modified tags are in bold font. \n\n\n\n\n## Authors' contributions \n  \nRTH Tsai designed all the experiments. CL Sung developed and implemented the pattern-based post-processing algorithm. HJ Dai implemented the sequential feature selection algorithm. RTH Tsai and HC Hung implemented the numerical normalization subroutine and conducted all experiments. TY Sung and WL Hsu guided the whole project. \n\n \n", "metadata": {"pmcid": 1764467, "text_md5": "ab425643e3a7e29b901d3a2b904c5765", "field_positions": {"authors": [0, 120], "journal": [121, 139], "publication_year": [141, 145], "title": [156, 284], "keywords": [298, 298], "abstract": [311, 3185], "body": [3194, 50878]}, "batch": 1, "pmid": 17254295, "doi": "10.1186/1471-2105-7-S5-S11", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1764467", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=1764467"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1764467\">1764467</a>", "list_title": "PMC1764467  NERBio: using selected word conjunctions, term normalization, and global patterns to improve biomedical named entity recognition"}
{"text": "Perera, Nadeesha and Dehmer, Matthias and Emmert-Streib, Frank\nFront Cell Dev Biol, 2020\n\n# Title\n\nNamed Entity Recognition and Relation Detection for Biomedical Information Extraction\n\n# Keywords\n\nnatural language processing\nnamed entity recognition\nrelation detection\ninformation extraction\ndeep learning\nartificial intelligence\ntext mining\ntext analytics\n\n\n# Abstract\n \nThe number of scientific publications in the literature is steadily growing, containing our knowledge in the biomedical, health, and clinical sciences. Since there is currently no automatic archiving of the obtained results, much of this information remains buried in textual details not readily available for further usage or analysis. For this reason, natural language processing (NLP) and text mining methods are used for information extraction from such publications. In this paper, we review practices for Named Entity Recognition (NER) and Relation Detection (RD), allowing, e.g., to identify interactions between proteins and drugs or genes and diseases. This information can be integrated into networks to summarize large-scale details on a particular biomedical or clinical problem, which is then amenable for easy data management and further analysis. Furthermore, we survey novel deep learning methods that have recently been introduced for such tasks. \n \n\n# Body\n \n## 1. Introduction \n  \nWith the exploding volume of data that has become available in the form of unstructured text articles, Biomedical Named Entity Recognition (BioNER) and Biomedical Relation Detection (BioRD) are becoming increasingly important for biomedical research (Leser and Hakenberg,  ). Currently, there are over 30 million publications in PubMed (Bethesda,  ) and over 25 million references in Medline (Bethesda,  ). This amount makes it difficult to keep up with the literature even in more specific specialized fields. For this reason, the usage of BioNER and BioRD for tagging entities and extracting associations is indispensable for biomedical text mining and knowledge extraction. \n\nNamed-entity recognition (NER), in general, (also known as entity identification or entity extraction) is a subtask of information extraction (text analytics) that aims at finding and categorizing specific entities in text, e.g., nouns. The phrase \u201c  Named Entity\u201d   was coined in 1996 at the   6th Message Understanding Conference   (MUC) when the extraction of information from unstructured text became an important problem (Nadeau and Sekine,  ). In the linguistic domain, Named Entity Recognition involves the automatic scanning through unstructured text to locate \u201c  entities,\u201d   for term normalization and classification into categories, e.g., as person names, organizations (such as companies, government organizations, committees.), locations (such as cities, countries, rivers) or date and time expressions (Mansouri et al.,  ). In contrast, in the biomedical domain, entities are grouped into classes such as genes/proteins, drugs, adverse effects, metabolites, diseases, tissues, SNPs, organs, toxins, food, or pathways. Since the identification of named entities is usually followed by their classification into standard or normalized terms, it is also referred to as \u201c  Named Entity Recognition and Classification\u201d   (NERC). Hence, both terms, i.e., NER and NERC, are frequently used interchangeably. One reason why BioNER is challenging is the non-standard usage of abbreviations, synonymous, homonyms, ambiguities, and the frequent use of phrases describing \u201c  entities\u201d   (Leser and Hakenberg,  ). An example of the latter is the neuropsychological condition   Alice in wonderland syndrome  , which requires the detection of a chain of words. For all these reasons, BioNER has undoubtedly become an invaluable tool in research where one has to scan through millions of unstructured text corpora for finding selective information. \n\nIn biomedical context, Named Entities Recognition is often followed Relation Detection (RD) (also known as relation extraction or entity association) (Bach and Badaskar,  ), i.e., connecting various biomedical entities with each other to find meaningful interactions that can be further explored. Due to a large number of different named entity classes in the biomedical field, there is a combinatorial explosion between those entities. Hence, using biological experiments to determine which of these relationships are the most significant ones would be too costly and time-consuming. However, by parsing millions of biomedical research articles using computational approaches, it is possible to identify millions of such associations for creating networks. For instance, identifying the interactions of proteins allows the construction of protein-protein interaction networks. Similarly, one can locate gene-disease relations allowing to bridge molecular information and phenotype information. As such, relation networks provide the possibility to narrow down previously-unknown and intriguing connections to explore further with the help of previously established associations. Moreover, they also provide a global view on different biological entities and their interactions, such as disease, genes, food, drugs, side effects, pathways, and toxins, opening new routes of research. \n\nDespite the importance of NER and RD being a prerequisite for many text mining-based machine learning tasks, survey articles that provide dedicated discussions of how Named Entity Recognition and Relations Detection work, are scarce. Specifically, most review articles (e.g., Nadeau and Sekine,  ; Goyal et al.,  ; Song,  ), focus on general approaches for NER that are not specific to the biomedical field or entity relation detection. In contrast, the articles by Leser and Hakenberg ( ) and Eltyeb and Salim ( ) focus only on biomedical and chemical NER, whereas (Li et al.,  ; Vilar et al.,  ) only focus on RD. To address this shortcoming, in this paper, we review both NER and RD methods, since efficient RD depends heavily on NER. Furthermore, we also cover novel approaches based on deep learning (LeCun et al.,  ), which have only recently been applied in this context. \n\nThis paper is organized according to the principle steps involved in named entity recognition and relation extraction, shown in  . Specifically, the first step involves the tagging of entities of biomedical interest, as shown in the figure for the example sentence \u201c  BRCA1 gene causes predisposition to breast cancer and ovarian cancer.\u201d   Here the tagged entities are   BRCA1, Breast Cancer  , and   Ovarian Cancer  . In the next step, relationships between these entities are inferred using several techniques, such as association indicating verbs as illustrated in the example. Here the verb   causes   is identified as pointing to a possible association. In the subsequent step, we aim to distinguish sentence polarity and strength of an inferred relationship. For instance, in the above sentence, the polarity is negative, i.e., indicating an unfavorable relation between the   BRCA1   gene and the tagged disease and the strength of relationship could be extracted by either shortest path in the sentence dependency tree or by a simple word distance as shown in the example. Finally, it is favorable to visualize these extracted relations with their responding strengths in a graph, facilitating the exploration and discovery of both direct associations and indirect interactions, as depicted in  . \n  \nAn overview of the principle steps for BioNER and Relation Detection and Analysis. As an example, the sentence \u201cBRCA1 gene causes predisposition to breast cancer and ovarian cancer\u201d is used to visualize each step. \n  \nAs such, in section 2, we survey biomedical Named Entity Recognition by categorizing different analysis approaches according to the data they require. Then we review relation inferring methods in section 3, strength, and polarity analysis in section 4 and Data Integration and Visualization in section 5. We will also discuss applications, tools, and future outlook in NER and RD in the sections that follow. \n\n\n## 2. Biomedical Named Entity Recognition (BioNER) \n  \nBioNER is the first step in relation extraction between biological entities that are of particular interest for medical research (e.g., gene/disease or disease/drug). In  , we show an overview of trends in BioNER research in the form of scientific publication counts. We extracted the details of the publications that correspond to several combinations of terms related to \u201c  Biomedical Named Entity Recognition\u201d   from the Web of Science (WoS) between 2001 and 2019 and categorize them by general BioNER keywords, i.e., gene/protein, drugs/chemicals, diseases, and anatomy/species. As a result, the counts of articles in each category were plotted chronologically. One can see that there is a steadily increasing amount of publications in general BioNER and a positive growth in nearly every sub-category since the early 2000s. By looking at  , one can predict that this trend will presumably continue into in the near future. \n  \nPublication trends in biomedical Named Entity Recognition. The numbers of the published articles were obtained from Web of Science (WoS). The legend shows different queries used for the search of WoS. \n  \nAccordingly, in the following sections, we discuss challenges in BioNER, the steps in a generic NER pipeline, feature extraction techniques, and modeling methods. \n\n### 2.1. Main Challenges in BioNER \n  \nDeveloping a comprehensive system to capture named entities, requires defining the types on NEs, specific class guidelines for types of NEs, to resolve semantic issues such as metonymy and multi-class entities, and capturing valid boundaries of a NE (Marrero et al.,  ). However, for developing a BioNER system, there are a few more additional problems to overcome than those for general NER (Nayel et al.,  ). Most of these issues are domain-specific syntactic and semantic challenges, hence extending to feature extraction as well as system evaluation. In this section, we will address some of these problems. \n\nText preprocessing and feature extraction for BioNER requires the isolation of entities. However, as for any natural language, many articles contain ambiguities stemming from the equivocal use of synonyms, homonyms, multi-word/nested NEs, and other ambiguities in naming in biomedical domain (Nayel et al.,  ). For instance, the same entity names can be written differently in different articles, e.g., \u201c  Lymphocytic Leukemia\u201d   and \u201c  Lymphoblastic Leukemia\u201d   (synonyms/British and American spelling differences). Some names may share the same head noun in an article such as in \u201c  91 and 84 kDa proteins\u201d   (nested) corresponding to \u201c  91 kDa protein\u201d and \u201c84 kDa protein\u201d  , in which case the categorization needs to take the context into account. There are various ways for resolving these ambiguities, using different techniques, e.g., name normalization and noun head resolving (D'Souza and Ng,  ; Li et al.,  ). \n\nIn addition, there are two distinct semantic-related issues resulted from homonyms, metonymy, polysemy, and abbreviations usage. While most terms in the biomedical field have a specific meaning, there are still terms, e.g., for genes and proteins that can be used interchangeably, such as   GLP1R   that may refer to either the gene or protein. Such complications may need ontologies and UMLA concepts to help resolve the class of the entity (Jovanovi\u0107 and Bagheri,  ). There are also those terms that have been used to describe a disease in layman's terms or drugs that have ambiguous brand names. For example, diseases like   Alice in Wonderland syndrome, Laughing Death, Foreign Accent Syndrome   and drug names such as   Sonata, Yasmin, Lithium   are easy culprits in confusing a bioNER system if there is no semantic analysis involved. For this reason, recent research work (e.g., Duque et al.,  ; Wang et al.,  ; Pesaranghader et al.,  ; Zhang et al.,  ) discussed techniques for word sense disambiguation in biomedical text mining. \n\nAnother critical issue is the excessive usage of abbreviations with ambiguous meanings, such as \u201c  CLD\u201d  , which could either refer to \u201c  Cholesterol-lowering Drug,\u201d \u201cChronic Liver Disease,\u201d \u201cCongenital Lung Disease,\u201d or \u201cChronic Lung Disease.\u201d   Given the differences in the meaning and BioNE class, it is crucial to identify the correct one. Despite being a subtask of word sense disambiguation, authors like (Schwartz and Hearst,  ; Gaudan et al.,  ) have focused explicitly on abbreviation resolving due to its importance. \n\nWhereas most of the above issues are a result of the lack of standard nomenclature in some biomedical domains, even the most standardized biological entity names can contain long chains of words, numbers and control characters (for example \u201c  2,4,4,6-Tetramethylcyclohexa-2,5-dien-1-one,\u201d \u201cepidemic transient diaphragmatic spasm\u201d  ). Such long named-entities make the BioNER task complex, causing issues in defining boundaries for sequences of words referring to a biological entity. However, correct boundary definitions are essential in evaluation and training systems, especially in those where penalizing is required for missing to capture the complete entity (long NE capture) (Campos et al.,  ). One of the most commonly used solutions for multi-word capturing challenge is to use a multi-segment representation (SR) model to tag words in a text as combination of   I  nside,   O  utside,   B  eginning,   E  nding,   S  ingle,   R  ear or   F  ront, using standards like IOB, IOBES, IOE, IOE, or FROBES (Keretna et al.,  ; Nayel et al.,  ). \n\nIn order to assess and compare NER systems using gold-standard corpora, it is required to use standardized evaluation scores. A frequently used error measures for evaluating NER is the   F-Score  , which is a combination of Precision and Recall (Mansouri et al.,  ; Emmert-Streib et al.,  ). \n\nPrecision, recall, and F-Score are defined as follows (Campos et al.,  ): \n\nA problem with scoring a NER system in this way is it requires to define the degree of correctness of the tagged entities for calculating precision and recall. The degree of correctness, in turn, depends on the pre-defined boundaries of the captured phrases. To illustrate this, consider the following example phrase \u201c  Acute Lymphocytic leukemia.\u201d   If the system tags \u201c  lymphocytic leukemia\u201d  , but misses \u201c  Acute\u201d  , we need to decide if it is still a \u201ctrue positive,\u201d or not. The decision depends on the accuracy requirement of the BioNER; for a system that collects information on patients with Leukemia in general, it may be possible to accept the above tag as a \u201ctrue positive.\u201d In contrast, if we are looking for rapid progressing Leukemia types, it may be necessary to capture the whole term, including   acute  . Hence, the above would be considered \u201cfalse positive.\u201d \n\nOne possible solution is to relax the matching criteria to a certain degree, since an   exact match   criterion tends to reduce the performance of a BioNER system. The effects of such approaches have been evaluated, e.g., using left or right matching, partial or approximate matching, name fragment matching, co-term matching, and multiple-tagging matching. Furthermore, some approaches apply semantic relaxation such as \u201ccategorical relaxation,\u201d which merges several entity types to reduce the ambiguity, e.g., by joining DNA, RNA, and protein categories or by combining cell lines and type entities into one class. In  , we show an example of the different ways to evaluate \u201c  Acute Lymphocytic leukemia  .\u201d For a thorough discussion of this topic, the reader is referred to Tsai et al. ( ). \n  \nAn example for different matching criteria to evaluate Named Entity Recognition. From left to right the criteria become more relaxed (Tsai et al.,  ). \n  \nUntil recently, there was also an evaluation-related problem stemming from the scarcity of comprehensively labeled data to test the systems (which also affected the training of the machine learning methods). This scarcity was a significant problem for BioNER until the mid-2000s, since human experts annotated most of the gold standard corpora, and thus were of small size and prone to annotator dependency (Leser and Hakenberg,  ). However, with growing biological databases and as the technologies behind NER evolved, the availability of labeled data for training and testing have increased drastically in recent years. Presently, there is not only a considerable amount of labeled data sets available, but there are also problem-specific text corpora, and entity-specific databases and thesauri accessible to researchers. \n\nThe most frequently used general-purpose biomedical corpora for training and testing are GENETAG (Tanabe et al.,  ), and JNLPBA (Huang et al.,  ), various BioCreative corpora, GENIA (Kim et al.,  ) (which also includes several levels of linguistic/semantic features) and CRAFT (Bada et al.,  ). In  , we show an overview of 10 text corpora often used for benchmarking a BioNER system. \n  \nBenchmark Corpora used for analyzing BioNER systems. \n  \n\n### 2.2. Principle Steps in BioNER \n  \nThe main steps in BioNER include preprocessing, feature processing, model formulating/training, and post-processing, see  . In the preprocessing stage, data are cleaned, tokenized, and in some cases, normalized to reduce ambiguity at the feature processing step. Feature processing includes different methods that are used to extract features that will represent the classes in question the most, and then convert them into an appropriate representation as necessary to apply for modeling. Importantly, while dictionary and rule-based methods can take features in their textual format, machine learning methods require the tokens to be represented as real-valued numbers. Selected features are then used to train or develop models capable of capturing entities, which then may go through a post-processing step to increase the accuracy further. \n  \nThe main steps in designing a BioNER system (with an example from manually annotated GENIA Corpus article MEDLINE:95343554\u2014 Routes and Cook,  ). \n  \n#### 2.2.1. Pre-processing \n  \nWhile for general NLP tasks, preprocessing includes steps such as data cleaning, tokenization, stopping, stemming or lemmatization, sentence boundary detection, spelling, and case normalization (Miner et al.,  ), based on the application, the usage of these steps can vary. Preprocessing in BioNER, however, comprises of data cleaning, tokenization, name normalization, abbreviation, and head noun resolving measures to lessen complications in the features processing step. Some studies follow the TTL model (Tokenization, Tagging, and Lemmatization) suggested by Ion ( ) as a standard preprocessing framework for biomedical text mining applications (Mitrofan and Ion,  ). In this approach, the main steps include sentence splitting and segmenting words into meaningful chunks (tokens), i.e., tokenization, part-of-speech (POS) tagging, and grouping tokens based on similar meanings, i.e., lemmatization using linguistic rules. \n\n\n#### 2.2.2. Feature Processing \n  \nIn systems that use rules and dictionaries, orthographic and morphological feature extraction focusing on word formations are the principle choice. Hence, they heavily depend on techniques based on word formation and language syntax. Examples of such include, regular expressions to identify the presence of words beginning with capital letters and entity-type specific characters, suffixes, and prefixes, counting the number of characters, and part-of-speech (POS) analysis to extract nouns/noun-phrases (Campos et al.,  ). \n\nFor using machine learning approaches, feature processing is mostly concerned with real-valued word representations (WR) since most machine learning methods require a real-valued input (Levy and Goldberg,  ). While the simplest of these use bag-of-words or POS tags with term frequencies or a binary representation (one-hot encoding), the more advanced formulations also perform a dimensional reduction, e.g., using clustering-based or distributional representations (Turian et al.,  ). \n\nHowever, the current state-of-the-art method for feature extraction in biomedical text mining is word embedding due to their sensitivity to even hidden semantic/syntactic details (Pennington et al.,  ). For word embedding, a real-valued vector representing a word is learned in an unsupervised or semi-supervised way from a text corpus. While the groundwork for word embedding was laid by Collobert and Weston ( ), Collobert et al. ( ), over the last few years, much progress has been made in neural network based text embedding taking into account the context, semantics and syntax for NLP applications (Wang et al.,  ). Below we discuss some of the most significant approaches for word representation and word embedding applicable to biomedical field. \n\n##### 2.2.2.1. Rich text features \n  \nThe most commonly used rich text features in BioNER are Linguistic, Orthographic, Morphological, Contextual, and Lexicon (Campos et al.,  ), all of which are used extensively, when it comes to rule-based and dictionary-based NER modeling. Still, word representation methods may use selected rich text features like char n-grams and contextual information to improve the representation of feature space as well. For instance, char n-grams are used for training vector spaces to recognize rare words effectively in fastText (Joulin et al.,  ), and CBOW in word2vec model uses windowing to capture local features, i.e., the context of a selected token. \n\nTo further elaborate,   linguistic features  , generally focus on the grammatical syntax of a given text, by extracting information such as sentence structures or POS tagging. This allows us to obtain tags that are most probable to be a NE since most named entities occur as noun phrases in a text. The   orthographic features  , however, emphasize the word-formation, and as such, attempt to capture indicative characteristics of named entities. For example, the presence of uppercase letters, specific symbols, or the number of occurrences of a particular digit might suggest the presence of a named entity and, therefore, can be considered a feature-token. Comparatively,   morphological features   prioritize the common characteristics that can quickly identify a named entity, for instance, a suffix or prefix. It also uses char n-grams to predict subsequent characters, and regular expression to capture the essence of an entity.   Contextual features   use preceding and succeeding token characteristics of a word by windowing to enhance the representation of the word in question. Finally,   Lexicon features   provides additional domain specificity to named entities. For example, systems that maintain extensive dictionaries with tokens, synonyms, and trigger words that belong to each field are considered to use lexicon features in their feature extraction (Campos et al.,  ). \n\n\n##### 2.2.2.2. Vector representations of text \n  \n One-hot vector word representation:   The one-hot-encoded vector is the most basic word embedding method. For a vocabulary of size   N  , each word is assigned a binary vector of length   N  , whereas all components are zero except one corresponding to the index of the word (Braud and Denis,  ). Usually, this index is obtained from a ranking of all words, whereas the rank corresponds to the index. The biggest issue of this representation is the size of the word vector; since for a larger corpus, word vectors are very high-dimensional and very sparse. Besides, frequency and contextual information of each word are lost in this representation but can be vital in specific applications. \n\n Cluster-based word representation:   In clustering-based word representation, the basic idea is that each cluster of words should contain words with contextually similar information. An algorithm that is most frequently used for this approach is Brown clustering (Brown et al.,  ). Specifically, Brown clustering is a hierarchical agglomerative clustering which represents contextual relationships of words by a binary tree. Importantly, the structure of the binary tree is learned from word probabilities, and the clusters of words are obtained by maximizing their mutual information. The leaves of the binary tree represent the words, and paths from the root to each leaf can be used to encode each word as a binary vector. Furthermore, similar paths and similar parents/grandparents among words indicate a close semantic/syntactic relationship among words. This approach, while similar to a one-hot vector word representation, reduces the dimension of the representation vector, reduces its sparsity, and includes contextual information (Tang et al.,  ). \n\n Distributional word representation:   The distributional word representation uses co-occurrence matrices with statistical approximations to extract latent semantic information. The first step involves obtaining a co-occurrence matrix,   F   with dimensions   V   \u00d7   C  , whereas   V   is the vocabulary size and   C   the context, and each   F   gives the frequency of a word   i   \u2208   V   co-occurring with context   j   \u2208   C  . Hence, in this approach, it is necessary for the preprocessing to perform stop-word filtering since high frequencies of unrelated words can affect the results negatively. In the second step, a statistical approximation or unsupervised learning function   g  () is applied to the matrix   F   to reduce its dimensionality such that   f   =   g  (  F  ), where the resulting   f   is a matrix of dimensions   V   \u00d7   d   with   d   \u226a   C  . The rows of this matrix represent the words in the vocabulary, and the columns give the counts of each word vector (Turian et al.,  ). \n\nSome of the most common methods used include clustering (Turian et al.,  ), self-organizing semantic maps (Turian et al.,  ), Latent Dirichlet Allocation (LDA) (Turian et al.,  ), Latent Semantic Analysis (LSA) (Sahlgren,  ), Random Indexing (Sahlgren,  ), Hyperspace Analog to Language (HAL) (Sahlgren,  ). The main disadvantage of these models is that they become computationally expensive for large data sets. \n\n\n##### 2.2.2.3. Neural network-based text embedding methods \n  \n Word2Vec:   Word2Vec is the state-of-the-art word representation model using a two-layer shallow neural network. It takes a textual corpus as the input, creates a vocabulary out of it, and produces a multidimensional vector representation for each word as output. The word vectors position themselves in the vector space, such that words with a common contextual meaning are closer to each other. There are two algorithms in the Word2Vec architecture, i.e., Continuous Bag-of-Words (CBoW) and Continuous Skip-Gram. Either can be used based on the application requirement. While the former predicts the current word by windowing its close contextual words in the space (with no consideration to the order of those words), the latter uses the current word to predict the words that surround it. The network ultimately outputs either a vector that represents a word (in CBoW) or a vector that represents a set of words (in skip-gram).   illustrates the basic mechanisms of the two architectures of word2vec; CBOW and Skip-Gram. Details about these algorithms can be found in Mikolov et al. ( , ) (parameter learning of the Word2Vec is explained in Rong,  ). \n  \nAn overview of the Continuous Bag-of-Words Algorithm and the Skip-Gram Algorithm. A CBOW predicts the current word based on surrounding words, whereas Skip-Gram predicts surrounding words based on the current word. Here   w  (  t  ) represents a word sequence. \n  \n GloVe:   GloVe (Global Vectors) is another word representation method. Its name emphasizes that global corpus-wide statistics are captured by the method, as opposed to word2vec, where local statistics of words are assessed (Pennington et al.,  ). \n\nGloVe uses an unsupervised learning algorithm to derive vector representations for words. The contextual distance among words creates a linear sub-structural pattern in the vector space, as defined by logarithmic probability. The method bases itself on how word-word co-occurrence probabilities evaluated on a given corpus, can interpret the semantic dependence between the words. As such, training uses log-bi-linear modeling with a weighted least-square error objective, where GloVe learns word vectors so that the logarithmic probability of word-word co-occurrence equals the dot product of the words. For example, if we consider two words   i   and   j  , a simplified version of an equation for GloVe is given by \n\nHere   is the word vector for word   i  ,   is the contextual word vector, which we use to build the word-word co-occurrence.   is the probability of co-occurrence between the words   i   and   j   and   X   and   X   are the counts of occurrence of word   i   with   j   and occurrence of word   i   alone in the corpus. An in-depth description of GloVe can be found in Pennington et al. ( ). \n\n fastText:   fastText, introduced by researchers at Facebook, is an extension of Word2Vec. Instead of directly learning the vector representation of a word, it first learns the word as a representation of N-gram characters. For example, if we are embedding the word   collagen   using a 3-gram character representation, the representation would be <   co, col, oll, lla, lag, age, gen, en  >, whereas < and >, indicate the boundaries of the word. These n-grams are then used to train a model to learn word-embedding using the skip-gram method with a sliding window over the word. FastText is very effective in representing suffixes/prefixes, the meanings of short words, and the embedding of rare words, even when those are not present in a training corpus since the training uses characters rather than words (Joulin et al.,  ). This embedding method has also been applied to the biomedical domain due to its ability to generalize over morphological features of biomedical terminology (Pylieva et al.,  ) and detecting biomedical event triggers using fastText semantic space (Wang et al.,  ). \n\n BERT/BioBERT:   Bidirectional Encoder Representations for Transformers (BERT) (Devlin et al.,  ), is a more recent approach of text embedding that has been successfully applied to several biomedical text mining tasks (Peng et al.,  ). BERT uses the transformer learning model to learn contextual token embeddings of a given sentence bidirectionally (from both left and right and averaged over a sentence). This is done by using encoders and decoders of the transformer model in combination with Masked Language Modeling to train the network to predict the original text. In the original work targeted for general purpose NLP, BERT was pre-trained with unlabeled data from standard English corpora, and then fine-tuned with task-specific labeled data. \n\nFor domain-specific versions of BioBERT (Peng et al.,  ; Lee et al.,  ), one uses the pre-trained BERT model, and by using its learned weights as initial weights, pre-trains the BERT model again with PubMed abstracts and PubMed Central full-text articles. Thereafter, the models are fine-tuned using benchmark corpora, e.g., mentioned in  ,   3  . The authors of BioBERT states that for the benchmark corpora, the system achieves state-of-the-art (or near) precision, recall, and F1 scores in NER and RE tasks. \n\nWe would like to highlight that a key difference between BERT, ELMo, or GPT-2 (Peters et al.,  ; Radford et al.,  ) and word2vec or GloVec is that the latter perform a context-independent word embedding whereas the former ones are context-dependent. The difference is that context-independent methods provide only one word vector in an unconditional way but context-dependent methods result in a context-specific word embedding providing more than one word vector representation for one word. \n\n\n\n#### 2.2.3. BioNER Modeling \n  \nModeling methods in BioNER can be divided into four categories: Rule-based, Dictionary-based, Machine Learning based, and Hybrid models (Eltyeb and Salim,  ). However, in recent years, the focus shifted to either pure machine learning approaches or hybrid techniques combining rules and dictionaries with machine learning methods. \n\nWhile supervised learning methods heavily dominate machine learning approaches in the literature, some semi-supervised and even unsupervised learning approaches are also used. Examples of such work will be discussed briefly later in the section below. The earliest approaches for BioNER focused on Support Vector Machines (SVM), Hidden Markov Models (HMM), and Decision Trees. However, currently, most NER research utilizes deep learning with sequential data and Conditional Random Fields (CRF). \n\n##### 2.2.3.1. Rule-based models \n  \nRule-based approaches, unlike decision trees or statistical methods, use handcrafted rules to capture named-entities and classify them based on their orthographic and morphological features. For instance, it is conventional in the English language to start proper names, i.e., named-entities, with a capital letter. Hence entities with features like upper-case letters, symbols, digits, suffixes, prefixes can be captured, for example, using regex expressions. Additionally, part-of-speech taggers can be used to fragment sentences and capture noun phrases. It is common practice, in this case, to include the complete token as an entity, if at least one part of the token identifies as a named-entity. \n\nAn example of the earliest rule-based BioNER system is PASTA (Protein Active Site Template Acquisition, Gaizauskas et al.,  ), in which entity tagging was performed by heuristically defining 12 classes of technical terms, including scope guidelines. Each document is first analyzed for sections with technical text, split into tokens, analyzed for semantic and syntactic features, before extracting morphological and lexical features. The system then uses handcrafted rules to tag and classify terms into 12 categories of technical terms. The terms are tagged with respective classes using the SGML (Standard Generalized Markup Language) format. Recently, however, there is not much literature on pure handcrafted rule-based BioNER systems, and instead, papers such as Wei et al. ( ) and Eftimov et al. ( ) present how combining heuristic rules with dictionaries may result in higher state-of-the-art f-scores. The two techniques complement each other by rules compensating for exact dictionary matches, and dictionaries refining results extracted through rules. \n\nThe main drawbacks of rule-based systems are the time-consuming processes involved with handcrafting rules to cover all possible patterns of interest and the ineffectiveness of such rules toward unseen terms. However, in an instance where an entity class is well-defined, it is possible to formulate thorough rule-based systems that can achieve both high precision and recall. For example, most species entity tagging systems rely on binomial nomenclature (two-term naming system of species), which provides clearly defined entity boundaries, qualifying as an ideal candidate for a rule-based NER system. \n\n\n##### 2.2.3.2. Dictionary-based models \n  \nDictionary-based methods use large databases of named-entities and possibly trigger terms of different categories as a reference to locate and tag entities in a given text. While scanning texts for exactly matching terms included in the dictionaries is a straightforward and precise way of named entity recognition, recall of these systems tends to be lower. Such is the result of increasingly expanding biomedical jargon, their synonyms, spelling, and word order differences. Some systems have been using an inexact or fuzzy matching, by automatically generating extended dictionaries to account for spelling variations and partial matches. \n\nOne prominent example of a dictionary-based BioNER model is in the association mining tool   Polysearch   (Cheng et al.,  ), where the system keeps several comprehensive dictionary thesauri, to make tagging and normalization of entities rather trivial. Another example is Whatizit (Rebholz-Schuhmann,  ), a class-specific text annotator tool available online, with separate modules for different NE types. This BioNER is built using controlled vocabularies (CV) extracted from standard online databases. For instance,   WhatizitChemical   uses a CV from ChEBI and OSCAR3,   WhatizitDisease   uses disease terms CV extracted from MedlinePlus,   whatizitDrugs   uses a CV extracted from DrugBank,   WhatizitGO   uses gene ontology terms and   whatizitOrganism   uses a CV extracted from the NCBI taxonomy. The tool also includes options to extract terms using UniProt databases when using a combined pipeline to tag entities. LINNAEUS, Gerner et al. ( ) is also a dictionary-based NER package designed explicitly to recognize and normalize species name entities in text and includes regex heuristics to resolve any ambiguities. The system has a significant recall of 94% at the mention-level and 98% at the document level, despite being dictionary-based. \n\nMore latest state-of-the-art tools have shown preference in using dictionary-based hybrid NER as well, attributing to its high accuracy of performance with previously known data. Moreover, since it involves exact/inexact matching, the main requirement for high accuracy is only a thoroughly composed dictionary of all possible related jargon. \n\n\n##### 2.2.3.3. Machine learning models \n  \nCurrently, the most frequently used methods for named entity recognition are machine learning approaches. While some studies focus on purely machine learning-based models, others utilize hybrid systems that combine machine learning with rule-based or dictionary-based approaches. Overall these present state-of-the-art methods. \n\nIn this section, we discuss three principal machine learning methodologies utilizing supervised, semi-supervised, and unsupervised learning. These also include Deep Neural Networks (DNN) and Conditional Random Fields (CRF), because newer studies focused on using LSTM/Bi-LSTM coupled with Conditional Random Fields (CRF). Furthermore, in section 2.2.3.4, we will discuss hybrid approaches. \n\n Supervised methods:   The first supervised machine learning methods used were Support Vector Machines (Kazama et al.,  ), Hidden Markov models (Shen et al.,  ), Decision trees, and Naive Bayesian methods (Nobata et al.,  ). However, the milestone publication by Lafferty et al. ( ) about Conditional Random Fields (CRF) taking the probability of contextual dependency of words into account shifted the focus away from independence assumptions made in Bayesian inference and directed graphical models. \n\nCRFs are a special case of conditionally-trained finite-state machines, in which the final result is a statistical-graphical model that performs well with sequential data, therefore making it ideal for language modeling tasks such as NER (Settles,  ). In Lafferty et al. ( ), the authors stated that given a text sequence   X   = {  x  ,   x  , ...,   x  } and its corresponding state label   S   = {  s  ,   s  , ....,   s  }, the conditional probability of state S for given X can be expressed as: \n\nHere,   s   can be an entity class label (  l   \u2208   L  ) for each text   x   (such as a gene or protein),   f  (  s  ,   s  ,   x, i  ) is the feature function and \u03bb  is the weight vector of   f  . Ideally, the learned \u03bb  for   f   must be positive for features that correlate to a target label, negative for anti-correlation and zero for irrelevant features. Overall, the learning process for a given training set   D   = {\u2329  x, l  \u232a , \u2329  x, l  \u232a , ....., \u2329  x, l  \u232a } can be expressed as a log likelihood maximization problem given by: \n\nModified Viterbi algorithm assigns respective labels for the new data, after the training process (Lafferty et al.,  ). \n\n Deep learning:   In the last 5 years, there is a shift in the literature toward general deep neural network models (LeCun et al.,  ; Emmert-Streib et al.,  ). For instance, feed-forward neural networks (FFNN) (Furrer et al.,  ), recurrent neural networks (RNN), or convolution neural networks (CNN) (Zhu et al.,  ) have been used for BioNER systems. Among these, frequent variations of RNNs are, e.g., Elman-type, Jordan-type, unidirectional, or bidirectional models (Li et al.,  ). \n\nThe Neural Network (NN) language models are essential since they excel at dimension reduction of word representations and thus help improve performances in NLP applications immensely (Jing et al.,  ). Consequently, Bengio et al. ( ) introduced the earliest NN language model as a feed-forward neural network architecture focusing on \u201cfighting the curse of dimensionality.\u201d This FFNN that first learns a distributed continuous space of word vectors is also the inspiration behind CBOW and Skip-gram models of feature space modeling. The generated distributed word vectors are then fed into a neural network, that estimates the conditional probability of each word occurring in context to the others. However, this model has several drawbacks, first being that it is limited to pre-specifiable contextual information. Secondly, it is not possible to use timing and sequential information in FFNNs, which would facilitate language to be represented in its natural state, as a sequence of words instead of probable word space (Jing et al.,  ). \n\nIn contrast, convolutional neural networks (CNN) are used in literature as a way of extracting contextual information from embedded word and character spaces. In Kim et al. ( ), such a CNN has been applied to a general English language model. In this setup, each word is represented as character embeddings and fed into a CNN network. Then the CNN filters the embeddings and creates a feature vector to represent the word. Extending this approach to Biomedical text processing, Zhu et al. ( ), generates embeddings for characters, words, and POS tagging, which are then combined to represent words and fed to a CNN level with several filters. The CNN outputs a vector representing the local feature of each term, which can then be tagged by a CRF layer. \n\nTo facilitate language to be represented as a collection of sequential tokens, researchers have later started exploring recurrent neural networks for language modeling. Elman-type and Jordan-type networks are such simple recurrent neural networks, where contextual information is fed into the system as weights either in the hidden layers in the former type or the output layer in the latter-type. The main issue with these simple RNNs is that they face the problem of vanishing gradient, which makes it difficult for the network to retain temporal information long-term, as benefited by in a recurrent language model. \n\nLong Short-Term Memory (LSTM) neural networks compensate for both of the weaknesses mentioned in previous DNN models and hence are most commonly used for language modeling. LSTMs can learn long-term dependencies through a special unit called a   memory cell  , which not only can retain information long time but has gates to control which input, output, and data in the memory to preserve and which to forget. Extensions of this are bi-directional LSTMs, where instead of only learning based on past data, as in unidirectional LSTM, learning is based on past and future information, allowing more freedom to build a contextual language model (Li et al.,  ). \n\nFor achieving the best results, Bi-LSTM and CRFs models are combined with a word-level and character-level embedding in a structure, as illustrated in   (Habibi et al.,  ; Wang et al.,  ; Giorgi and Bader,  ; Ling et al.,  ; Weber et al.,  ; Yoon et al.,  ). Here a pre-trained look-up table produces word embeddings, and a separate Bi-LSTM for each word sequence renders a character-level embedding, both of which are then combined to acquire   x  ,   x  , ....,   x   as word representation (Habibi et al.,  ). These vectors then become the input to a bi-directional LSTM, and the output of both forward and backward paths,   h  ,   h  , are then combined through an activation function and inserted into a CRF layer. This layer is ordinarily configured to predict the class of each word using an IBO-format (Inside-Beginning-Outside). \n  \nStructure of the Bi-LSTM-CRF architecture for Named Entity Recognition. \n  \nIf we consider the hidden layer   h   in  , first, the embedding layer embeds the word   gene   into a vector   X  . Next, this vector is simultaneously used as input for the forward LSTM   and the backward LSTM  , of which the former depends on the past value   h   and the latter on the future value   h  . The combined output resulting from the backward and the forward LSTMs is then passed through an activation function (  tanh  ) that results in the output   Y  . The CRF layer on the top uses   Y   and tags it as either I-inside, B-Beginning, or O-Outside of a NE (named entity). Consequently, in this example,   Y   is tagged as   I-gene  , i.e., a word inside of the named entity of a gene. \n\n Semi-supervised methods:   Semi-supervised learning is usually used when a small amount of labeled data and a larger amount of unlabeled data are available, which is often the case when it comes to Biomedical collections. If labeled data is expressed as   X  (  x  ,   x  , ....,   x  )\u2212>  L  (  l  ,   l  , ...,   l  ) where X is the set of data and L is the set of labels, the task is to develop a model that accurately maps   Y  (  y  ,   y  , ...,   y  )\u2212>  L  (  l  ,   l  , ...,   l  ) where   m  >  n   and Y is the set of unlabeled data that needs mapping to labels. \n\nWhereas literature using a semi-supervised approach is lesser in BioNER, Munkhdalai et al. ( ) describes how domain knowledge has been incorporated into chemical and biomedical NER using semi-supervised learning by extending the existing BioNER system BANNER. The pipeline runs the labeled and unlabeled data in two parallel lines wherein one line labeled data is processed through NLP techniques to extract rich features such as word and character n-grams, lemma, and orthographic information as in BANNER. In the second line, the unlabeled data corpus is cleaned, tokenized, and run through brown hierarchical clustering and word2vec algorithms to extract word representation vectors, and clustered using k-means. All of the extracted features from labeled and unlabeled data are then used to train a BioNER model using conditional random fields. The authors of this system emphasize that the system does not use lexical features or dictionaries. Interestingly, BANNER-CHEMDNER has shown an 85.68% and an 86.47% F-score on the testing sets of CHEMDNER Chemical Entity Mention (CEM) and Chemical Document Indexing (CDI) sub-tasks and shown a remarkable 87.04% F-score in the test set of the BioCreative II gene-mention task. \n\n Unsupervised methods:   While unsupervised machine learning has potent in organizing new high throughput data without previous processing and improving the ability of the existing system to process previously unseen information, it is not very often the first choice for developing BioNER systems. However, Zhang and Elhadad ( ) introduced a system, which uses an unsupervised approach to BioNER with the concepts of   seed knowledge   and   signature similarities   between entities. \n\nFirst, for the seed concepts, semantic types and semantic groups are collected from UMLS (Unified Medical Language System) for each entity type, e.g., protein, DNA, RNA, Cell type, and cell line, to represent the domain knowledge. Second, the candidate corpora are processed using a noun phrase chunker and an inverse document frequency filter, which formulates word sense disambiguation vectors for a given named entity using a clustering approach. The next step generates the signature vectors for each entity class with the intuition that the same class tends to have contextually similar words. The final step compares the candidate named entity signatures and entity class signatures by calculating similarities. As a result, they found the highest F-score of 67.2 for proteins and the lowest at 19.9 for cell-line. Sabbir et al. ( ) used a similar approach, where they implement a word sense disambiguation with an existing knowledge base of concepts extracted through UMLS to develop an unsupervised BioNER model with over 90% accuracy. These unsupervised methods tend to work well when dealing with ambiguous Biomedical entities. \n\n\n##### 2.2.3.4. Hybrid models \n  \nCurrently, there are several state-of-the-art applications of BioNER, that combine the best aspects of all the above three methods. Most of these methods combine machine learning with either dictionaries or sets of rules (heuristic/derived), but other approaches exist which combine dictionaries and rule sets as well. Since machine learning approaches have shown to result in better recall values, whereas both dictionary-based and rule-based approaches tend to have better precision values, the former method shows improved F-scores. \n\nFor instance, OrganismTagger (Naderi et al.,  ) uses binomial nomenclature rules of naming species to tag organism names in text and combines this with an SVM to assure that it captures organism names that do not follow the binomial rules. In contrast, SR4GN (Wei et al.,  ), which is also a species tagger, utilizes rules to capture species names and a dictionary lookup to reevaluate the accuracy of the tagged entities. \n\nFurthermore, state of the art tools such as Gimli (Campos et al.,  ), Chemspot (Rockt\u00e4schel et al.,  ), and DNorm (Leaman et al.,  ) use Conditional Random fields with a thesaurus of own field-specific taxonomy to improve recall. In contrast, OGER++ (Furrer et al.,  ), which performs multi-class BioNER, utilizes a feed-forward neural network structure followed by a dictionary lookup to improve precision. \n\nOn the other hand, some systems have been able to combine statistical machine-learning approaches with rule-based models to achieve higher results, as described in this more recent work (Soomro et al.,  ). This study uses the probability analysis of orthographic, POS, n-gram, affixes, and contextual features with Bayesian, Naive-Bayesian, and partial decision tree models to formulate rules of classification. \n\n\n\n#### 2.2.4. Post Processing \n  \nWhile not all systems require or use post-processing, it can improve the quality and accuracy of the output by resolving abbreviation ambiguities, disambiguation of classes and terms, as well as parenthesis mismatching instances (Bhasuran et al.,  ). For example, if a certain BioNE is only tagged in one place of the text, yet the same or a co-referring term exist elsewhere in the text, untagged, then the post-processing would make sure these missed NEs are tagged with their respective class. Also, in the case of a partial entity being tagged in a multi-word BioNE, this step would enable the complete NE to be annotated. In the case where some of the abbreviations are wrongly classified or failed to be tagged, some systems use tools such as the BioC abbreviation resolver (Intxaurrondo et al.,  ) at this step to improve the annotation of abbreviated NEs. Furthermore, failure to tag NE also stems from unbalanced parenthesis in isolated entities, which also can be addressed during pre-processing. Interestingly, Wei et al. ( ) describes using a complete rule-based BioNER model for post-processing in disease mention tagging to improve the F-score. \n\nAnother important sub-task that is essential at this point, is to resolve coreferences. This may be also important for extracting stronger associations between entities, discussed in the next section. Coreferences are those terms that refer to a named entity without using its proper name, but by using some form of anaphora, cataphora, split-reference or compound noun-phrase (Sukthanker et al.,  ). For example in the sentence \u201c   BRCA1    and    BRCA2    are proteins expressed in breast tissue where    they    are responsible for either restoring or, if irreparable, destroying damaged DNA,\u201d   the anaphora   they   refers to the proteins   BRCA1   and   BRCA2  , and resolving this helps to associate the proteins with their purpose. When it comes to biomedical coreference resolution, it is important to note that generalized methods may not be very effective, given that there are fewer usages of common personal pronouns. Some approaches that have been used in the biomedical text mining literature are heuristic rule sets, statistical approaches and machine learning-based methods. Most of the earlier systems commonly used mention-pair based binary classification and rule-sets to filter coreferences such that only domain significant ones are tagged Zheng et al. ( ). While the rule set methods have provided state-of-the-art precision they often do not have a high recall. Hence, a sieve-based architecture Bell et al. ( ) has been introduced, which arranges rules starting from high-precision-low-recall to low-precision-high-recall. Recently, deep learning methods have been used for coreference resolution in general domain successfully without using syntactic parsers, for example in Lee et al. ( ). The same system has been applied to biomedical coreference resolution in Trieu et al. ( ) with some domain-specific feature enhancements. Here, it is worth mentioning that the CRAFT corpus, earlier mentioned in  , has an improved version that can be used for coreference resolution for biomedical texts (Cohen et al.,  ). \n\nIn the biomedical literature coreference resolution is sometimes conducted (e.g., Zheng et al.,  ,  ; Uzuner et al.,  ), but in general underrepresented. A reason for this could be that biomedical articles are differently written in the sense that, e.g., protagonistic gene or protein names are more clearly used and referred to due to their exposed role. However, if this is indeed the reason or if there is an omission in the biomedical NER pipeline requires further investigations. \n\n\n\n\n## 3. Inferring Relations \n  \nAfter BioNER, the identification of associations between the named entities follows. For establishing such associations, the majority of studies use one of the following techniques (Yang et al.,  ): Co-occurrence based approaches, rule-set based approaches, or machine learning-based approaches. \n\n### 3.1. Co-occurrence Based Approaches \n  \nThe simplest of these methods, co-occurrence based approaches, consider entities to be associated if they occur together in target sentences. The hypothesis is that the more frequent two entities occur together, the higher the probability that they are associated with each other. In an extension of this approach, a relationship is deemed to exist between two (or more) entities if they share an association with a third entity acting as a reciprocal link (Percha et al.,  ). \n\n\n### 3.2. Rule-Based Approaches \n  \nIn a rule-based approach, the relationship extraction depends highly on the syntactic and semantic analysis of sentences. As such, these methods rely on part-of-speech (POS) tagging tools to identify associations, e.g., by scanning for verbs and prepositions that correlate two or more nouns or phrases serving as named entities. For instance, in Fundel et al. ( ), the authors explain how syntactic parse trees can be used to break sentences into the form   NounPhase  \u2212  AssociationVerb  \u2212  NounPhrase  , where the   noun phrases   are biomedical entities associated through an   association verb  , and therefore indicates a relationship. In this approach, many systems additionally incorporate a list of verbs that are considered to show implications between nouns, i.e., for example, verbs such as   elevates, catalyzes, influences, mutates  . \n\nIn  , an example of a syntactic sentence parse tree created by POS tagging, is shown. In this figure, nodes signify syntax abbreviations, i.e., S = sentence, NP = Noun Phrase, VP = Verb Phrase, PP = Preposition Phrase, OP = Object of Preposition, CONJ = conjunction ADJ = Adjective, N = Noun, V = Verb, and O = Object. The method first fragments a sentence into noun phrases and verb phrases, and each of these phrases is further segmented to adjectives, nouns, prepositions, and conjunctions for clarity of analysis. More details of the strength of associations will include in section 4.2 \n  \nAn example of a syntax parse tree for the sentence \u201c  BRCA1 gene causes predisposition to breast cancer and ovarian cancer\u201d  . \n  \n\n### 3.3. Traditional Machine Learning Approaches \n  \nThe most commonly used machine learning approaches use an annotated corpus with pre-identified relations as training data to learn a model (supervised learning). Previously, the biggest obstacle for using such machine learning approaches for relation detection was acquiring the labeled training and testing data. However, data sets generated through biomedical text mining competitions such as BioCreative and BioNLP have moderated this problem significantly. Specifically, in  , we list a few of the main gold-standard corpora available in the literature for this task. \n  \nBenchmark corpora for biomedical entity relation detection. \n  \n These data sets contain labeled data that can be used for the training and testing of methods  . \n  \nHistorically, SVMs have been the first choice for this task due to their excellent performance in text data classification with a low tendency for overfitting. Furthermore, they have also proven to be good with sentence polarity analyzing for extracting positive, negative, and neutral relationships as described by Yang et al. ( ). Of course, in SVM based approaches, feature selection acts as the strength-indicator for accuracy and, therefore, is considered a crucial step in relationship mining using this approach. \n\nOne of the earliest studies using an SVM was (\u00d6zg\u00fcr et al.,  ). This study used a combination of methods for evaluating an appropriate kernel function for predicting gene-disease associations. Specifically, the kernel function used a similarity measure incorporating a normalized edit-distances between the paths of two genes, as extracted from a dependency parse tree. In contrast to this, the study by Yang et al. ( ) used a similar SVM model, however, for identifying the polarity of food-disease associations. For this reason, their SVM was trained with positive, negative, neutral, and irrelevant relations, which allowed assigning the polarity in the form of \u201c  risk  .\u201d For instance, particular food can either increase risk, reduce risk, be neutral, or be irrelevant for a disease. Recently, Bhasuran and Natarajan ( ) extended the study by \u00d6zg\u00fcr et al. ( ) using an ensemble of SVMs trained with small samples of stratified and bootstrapped data. This method also included a word2vec representation in combination with rich semantic and syntactic features. As a result, they improved F-scores for identifying disease-gene associations. \n\nAlthough SVMs appear to take predominance in this task, other machine learning methods have been used as well. For instance, in Jensen et al. ( ), a Naive-Bayes classifier has been used for identifying food-phytochemical and food-disease associations based on TF-IDF (term frequency-inverse document frequency) features. Whereas, in Quan and Ren ( ), a Max-entropy based classifier with Latent Dirichlet Allocation (LDA) was used for inferring gene-disease associations, and in Bundschus et al. ( ) a CRF was used for both NER and relation detection, for identifying disease-treatment and gene-disease associations. \n\n\n### 3.4. Deep Learning Approaches \n  \nDue to the state of the art performance and less need for complicated feature processing, deep learning (DL) methods are becoming increasingly popular for relation extraction in the last five years. The most commonly used DL approaches include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and hybrids of CNN and RNN (Jettakul et al.,  ; Zhang et al.,  ), most of which are also able to classify relation-type as well. \n\nThe feature inputs to DL models may include sentence-level, word-level, and lexical-level features represented as vectors (Zeng et al.,  ), positions of the related entities, and the class label of the relation type. The vectors are looked up from pre-trained word and positional vector space on either a single corpus or multiple corpora (Quan et al.,  ). Significantly, the majority of deep learning methods use sentence dependency graphs mentioned in the rule-based approach ( ) to extract the shortest path between entities and relations as features for training (Hua and Quan,  , ; Zhang et al.,  ; Li et al.,  ). Other studies have used POS tagging, and chunk tagging features in combination with position and dependency paths to improve performance (Peng and Lu,  ). The models are trained to either distinguish between sentences with relations or to output the type of relation. \n  \nDependency graph for the sentence \u201c  BRCA1 gene causes predisposition to breast cancer and ovarian cancer\u201d   generated using Standford coreNLP parser (Manning et al.,  ) (nsubj-nominal subject, dobj-direct object, nmod-nominal modifier, amod-adjectival modifier, conj-conjunction, CC-coordinating conjunction, JJ-adjective, NN-noun). \n  \nThe earliest approaches use Convolutional Neural Networks (CNN), where the extracted features e.g., dependency paths/sentences, are represented using the word vector space. Since CNN's require every training example to be of similar size, instances are padded with zeros as required (Liu et al.,  ). After several layers of convolutional operations and pooling, these methods are followed by a fully connected feed-forward neural layer with soft-max activation function (Hua and Quan,  ). \n\nSubsequently, LSTM networks, including bi-LSTM, have been used in Sahu and Anand ( ) and Wang et al. ( ), to learn latent features of sentences. These RNN based models perform well with relating entities that lie far apart from each other in sentences. Whereas, CNNs requires restrictive sized inputs, the RNNs have no such restrains and are useful when long sentences are available, since the input is sequentially processed. These models have been used to extract drug-drug and protein-protein interactions (Hsieh et al.,  ). Extending this further, Zhang et al. ( ) experiments with bidirectional RNN models using two hierarchical layers, one with two simple RNNs, one with two GRUs, and last with two LSTMs. Here the hierarchical bi-LSTM has shown a better performance. \n\nIn recent years, there have also been studies that use a novel approach, i.e., graph convolutional networks (GCN) (Kipf and Welling,  ) for relation extraction using dependency graphs (Zhang et al.,  ; Zhao et al.,  ). Graph convolutional networks use the same concept of CNN, but with the advantage of using graphs as inputs and outputs. By using dependency paths to represent text as graphs, GCNs can be applied to relation extraction tasks. In Zhao et al. ( ), the authors use a hybrid model that combines GCNs preceded by bidirectional gated recurrent units (bi-GRU) layer to achieve significant F-measures. Furthermore, for identifying drug-drug interactions, a syntax convolutional neural network has been evaluated for the DDIExtraction 2013 corpus (Herrero-Zazo et al.,  ) and found to outperform other methods (Zhao et al.,  ). Conceptually similar approaches have been used in Su\u00e1rez-Paniagua et al. ( ); Wei et al. ( ). \n\nIn extension, Zheng et al. ( ) uses a hierarchical hybrid model that resembles a reverse CRNN (convolutional recurrent neural network), where a CNN and a soft-max layer follow two bi-LSTM layers. The method has been used to extract chemical-disease relations, and have been trained and evaluated on CDR corpus (Li et al.,  ). Whereas, authors of Zhang et al. ( ) uses two CNNs and a bi-LSTM simultaneously to learn from word/relation dependency and sentence sequences, to extract disease-disease and protein-protein relations. These hybrid methods aim to combine the CNN's efficiency in learning local lexical and syntactic features (short sentences) with RNN's ability to learn dependency features over long and complicated sequences of words (long sentences). Both of the above models have been found to perform well with their respective corpora. \n\n\n### 3.5. Graph-Based Approaches \n  \nGraph-based representation preserves the sentence structure by converting the text directly into a graph, where biomedical named entities are vertices and other syntactic/semantic structures connecting them are edges. While complex sentence structures may lead to nested relations, this method facilitates identifying common syntactic patterns indicating significant associations (Luo et al.,  ). \n\nOnce the named entities are tagged, the next steps involve splitting sentences, annotating them with POS, and processing other feature extractions as required. Graph extraction is usually performed at this point as a part of the feature extracting process. Once the graphs including concepts and their syntactic/semantic relations are mined, these can be used as kernels, training data for deep learning approaches, or for generating rule sets with the help of graph search algorithms (Kilicoglu and Bergler,  ; Ravikumar et al.,  ; Panyam et al.,  ; Bj\u00f6rne and Salakoski,  ). For example, in Liu et al. ( ), approximate subgraph matching has been used to extract biomolecular relations from key contextual dependencies and input sentence graphs. A similar approach has been used in MacKinlay et al. ( ). The paper by Luo et al. ( ) provides a good review including a wide array of examples for which graph-based approaches are used in biomedical text mining. \n\n\n### 3.6. Hybrid Approaches \n  \nAlso, the combination of machine learning and graph-based approaches have been studied with great success. For instance, in Kim et al. ( ), a linear graph kernel based on dependency graphs for sentences has been used in combination with an SVM to detect drug-drug interactions. In order to enrich the information captured by kernels, Peng et al. ( ) uses an extended dependency graph that has also been defined to include information beyond syntax. Furthermore, in Panyam et al. ( ), chemical-induced disease relations have been studied by comparing tree kernels (subset-tree kernel and partial-tree kernel) and graph kernels (all-path-graph and approximate-subgraph-matching). As a result, they found that the all-path-graph kernel performs significantly better in this task. \n\n\n### 3.7. Others Approaches \n  \nIn this section, we discuss methods that do not fit in either of the above categories but provide interesting approaches. In Zhou and Fu ( ), an extended variant of the frequency approach is studied, which combines co-occurrence frequency and Inverse Document Frequency (IDF) for relations extraction. The study sets the first precedence to entity co-occurrence in MeSH terms and second to those in the article title, and third to the ones in the article abstract by assigning weights to each precedence level. A vector representation for each document sample is created using these weights for calculating the score of each key-term-association by multiplying IDF with PWK (penalty weight for the keyword, depending on the distance from MeSH root). Next, by comparing with the dictionary entries for relevance, each gene and disease is converted into vectors (  V  ,   V  ), and the strength of a relation is calculated through the cosine similarity given by  . The authors then evaluate the system by comparing precision, recall, and cosine similarity. \n\nIn contrast, the study by Percha and Altman ( ) introduces an entirely novel algorithm to mine relations between entities called Ensemble Clustering for Classification (EBC). This algorithm extract drug-gene associations by combining an unsupervised learning step and a lightly supervised step that uses a small seed data set. In the unsupervised step, all co-occurrences of gene-drugs pairs (  n  ) and all dependency path between the pairs (  m  ) are mined to create a matrix of   n   \u00d7   m   which is then clustered using Information-Theoretic Co-Clustering. The supervised step follows by comparing how often the seed set pairs and test set pairs co-cluster together using a scoring function, and relationships are ranked accordingly. The same authors have extended this method further in Percha and Altman ( ), by applying hierarchical clustering after EBC to extract four types of association between gene-gene, chemical-gene, gene-disease, and chemical-disease. Incidentally, this hierarchical step has enabled additional classification of these relationships into themes such as ten different types of chemical-gene relations or seven distinct types of chemical-disease associations. \n\n\n\n## 4. Analyzing Polarity and Strength of Relations \n  \nA further refinement following a relation detection is an analysis of the polarity and the strength of the identified associations, providing additional information about the relations and, hence, enhances extracted domain-specific knowledge. \n\n### 4.1. Polarity Analysis \n  \nA polarity analysis of relations is similar to a sentiment analysis (Swaminathan et al.,  ; Denecke and Deng,  ). For inferring the polarity of relations, similar machine learning approaches can be used, as discussed in section 3.3. However, a crucial difference is that for the supervised methods, appropriate training data need to be available, providing information about the different polarity classes. For instance, one could have three polarity classes, namely, positive associations (e.g.,   decreases risk, promotes health  ), neutral associations (e.g.,   does not influence, causes no change  ), and negative associations (e.g.,   increases risk, mutates cell  ). In general, a polarity analysis opens new ways to study research questions of how entities interact with each other in a network. For example, the influence of a given food metabolite on certain diseases can be identified, which may open new courses of food-based treatment regiments (Miao et al.,  , ). \n\n\n### 4.2. Strength Analysis \n  \nA strength analysis comes after identifying associations between entities in a text since all extracted events might not be considered significant associations. Especially in simple co-occurrences based method to identify relationships, strength analysis can be vital, since just a simple mention of two entities in a sentence with no explicit reciprocity, may result in them wrongly defined as associations. Some of the most common methods employed in the literature include distance analysis and dependency path analysis, or an extension of those methods. \n\nAn example of a method that implements a word distance analysis is Polysearch (Liu et al.,  ). Polysearch is essentially a biomedical web crawler focusing on entity associations. This tool first estimates co-occurrence frequencies and the association verbs to locate content that is predicted to have entity associations. Next, using the word-distances between entity-pairs in the selected text, content relevancy (i.e., the strength of association) score is calculated. Incidentally, this system is currently able to search in several text corpora and databases, using the above method, to find relevant content for over 300 associative combinations of named entity classes. \n\nIn Coulet et al. ( ), the authors created syntactic parse trees, as shown in  , by analyzing sentences selected by the entity co-occurrences approach. Each tree then converts into a directed and labeled dependency graph, whereas nodes are words, and edges are dependency labels. Next, by extracting shortest paths between node pairs in the graph, they transform associations into the form   Verb  (  Entity  ,   Entity  ), such that   Entity   and   Entity   are connected by   Verb  . This approach, which is an extension of the association-identifying method described in section 3.1, hypothesizes that the shortest dependency paths indicate the strongest associations. Other studies that use a dependency analysis of sentences to determine the strength of the associations include (Quan and Ren,  ; Kuhn et al.,  ; Mallory et al.,  ; Percha and Altman,  ). Many systems using machine learning approaches, also tend to define syntactic and dependency paths analysis of sentences as a feature selection method before training relation mining models, as discussed in \u00d6zg\u00fcr et al. ( ); Yang et al. ( ), and Bhasuran and Natarajan ( ). \n\n\n\n## 5. Visualization and Integrating Relations \n  \n### 5.1. Network Visualization \n  \nAfter individual relations between biomedical entities have been inferred, it is convenient to assemble these in the form of networks (Skusa et al.,  ; Li et al.,  ; Kolchinsky et al.,  ). In such networks, nodes (also called vertices) correspond to entities and edges (also called links) to relations between entities. The resulting networks can be either weighted or unweighted. If polarity or strength of relations has been obtained, one can use this information to define the weights of edges as the strength of the relations, leading to weighted networks. Polarity information and relation type classifications can further be used to label edges. For example, these labels could be   positive regulation, negative regulation  , or   transcription  . In this case, edges tend to be directed indicating which entity is influenced by which. Such labeled and/or weighted networks are usually more informative than unweighted ones because they carry more relevant domain-specific information. \n\nThe visualization of interaction networks often provides a useful first summary of the results extracted from the relation extraction task. The networks are either built from scratch or automatically by using software tools. Two such commonly used tools for the network visualization are Cytoscape (Franz et al.,  ) and Gephi (Bastian et al.,  ), both providing open-source java libraries. Cytoscape can also be used interactively via a web-interface, while Gephi can be used for 3D rendering of graphs and networks. There are also several libraries specifically developed for network visualization in different languages. For instance, NetbioV (Tripathi et al.,  ) provides an R package and Graph-tool (Peixoto,  ) a package for Python. \n\n\n### 5.2. Network Analysis \n  \nThe networks generated in the above way can be further analyzed to reconfirm known associations, and further explore new ones (\u00d6zg\u00fcr et al.,  ; Quan and Ren,  ). Measures frequently used for biomedical network analysis include node centrality measures, shortest paths, network clustering, and network density (Sarangdhar et al.,  ). The measures selected to analyze a graph predominantly depend on the task at hand; for example, shortest path analysis is vital for discovering signaling pathways, while clustering analysis helps identify functional subnetwork units. Further commonly used metrics are centrality measures and network density methods, e.g., for identifying the most influential nodes in the network. Whereas graph density compares the number of existing relations between the nodes vs. all possible connections that can be formed in the network, centrality measures are commonly used to identifying the importance of an entity within the entire network (Emmert-Streib and Dehmer,  ). \n\nThere are four main centrality measures, namely, degree, betweenness, closeness, and eigenvector centrality (Emmert-Streib et al.,  ). Degree centrality, the simplest of the above measures, corresponds just to the number of connections of a node. Closeness centrality is given by the reciprocal of the sum of all shortest path lengths between a node and all other nodes in the network, as such it measures the spread of information. Also betweenness centrality utilizes shortest paths by taking into account the information flow of the network. This is realized by counting shortest paths through pairs of nodes. Finally, eigenvector centrality is a measure of influence where each node is assigned a score based on how many other influential nodes are connected to it. \n\nFor instance, consider  , a disease-gene network. Here blue nodes correspond to genes and pink nodes represent diseases. For instance, blue nodes with a higher degree centrality correspond to those genes associated with a higher number of diseases. Similarly, pink nodes with a high degree centrality correspond to diseases that are associated with more genes. Furthermore, the genes with a high closeness centrality are important because they have a direct or indirect association to the largest number of other genes and diseases. Further, if a gene X that is connected to a large number of diseases, and is furthermore connected to gene Y with a high eigenvector centrality, it may be worth exploring if there are diseases in the neighborhood of gene X, that are possibly also associated to gene Y and vice versa. Hence, based on centrality measures, one may be able to find previously undiscovered relations between certain diseases and genes. \n  \nDisease-gene network created for selected 300 entries from the DisGeNet Database (Bauer-Mehren et al.,  ) with Cytoscape v3.7.2. Genes are shown as blue nodes and diseases as pink nodes. \n  \n\n\n## 6. Tools and Data Resources \n  \nIn this section, we will discuss some of the main benchmark tools and resources available for Named-Entity Recognition and Relation Extraction used in the biomedical domain. \n\nWhile the training corpora for machine learning methods in BioNER and BioRD both have been discussed extensively in the sections above, here we mention some of the databases with entities and relation mappings. These are crucial for dictionary-based methods and in post-processing, and as such, are often used for biomedical text mining research. \n\nSome of the Named-Entity specific databases that have comprehensive collections of jargon include   Gene Ontology   (Consortium,  ),   Chemical Entities of Biological Interest   (Shardlow et al.,  ),   DrugBank   (Wishart et al.,  ),   Human Protein Reference Database   (Keshava Prasad et al.,  ),   Online Mendelian Inheritance in Man   (Amberger et al.,  ),   FooDB   (Wishart,  ),   Toxins and Toxin-Targets Database   (Wishart et al.,  ),   International Classifications of Disease (ICD-11) by WHO   (World Health Organization,  ),   Metabolic Pathways and Enzymes Database   (Caspi et al.,  ),   Human Metaboleme Database   (Jewell et al.,  ), and USDA food and nutrients database (Haytowitz and Pehrsson,  ). The majority of these has been used by Liu et al. ( ) to compile their thesauri and databases. \n\nDatabases for known Entity-Relations in Biomedical research include DISEASES (Pletscher-Frankild et al.,  ) and DisGeNet (Bauer-Mehren et al.,  ) providing gene-disease relations, CTD (Davis et al.,  ) with relations between chemicals, genes, phenotypes, diseases, exposures and pathways, SIDER (Kuhn et al.,  ) providing drug-side effect relations, STRING (Szklarczyk et al.,  ) with protein-protein interactions, ChemProt (Kringelum et al.,  ) with chemical-protein interactions and PharmGKB (Hewett et al.,  ) providing drug-gene relations. These databases have been used by various authors to evaluate relation extraction systems. \n\nIn  , we provide an overview of BioNER tools that are available for different programming languages. While there are several other tools, our selection criterion was to cover the earliest successful implementations, benchmark tools as well as the most recent tools using novel approaches. \n  \nAn overview of approaches for BioNER tools. \n    \nThe improvement of resources and techniques for biomedical annotation has also brought about an abundance of open source tools that have simplified the information extraction for relation mining in biomedical texts. Many of these are general-purpose text mining tools that can be easily configured to process biomedical texts. For instance, Xing et al. ( ) used the open information extraction tool OLLIE (Schmitz et al.,  ) to identify relations between genes and phenotypes, whereas (Kim et al.,  ) identified gene-disease relations utilizing DigSee (Kim et al.,  ). Other useful tools that can be application adaptable and have higher F-score measures are; DeepDive (Niu et al.,  ): an information extraction system developed for structuring unstructured text documents, RelEx (Fundel et al.,  ): a dependency parsing based relations extractor applicable to biomedical free text, and PKDE4J (Song et al.,  ): an extractor that combines rule-based and dictionary-based approaches for multiple-entity relation extraction. \n\nFurthermore, there are general NLP tools heavily used in BioNER and BioRD alike for pre-processing and syntactic analysis. These include Stanford CoreNLP (Manning et al.,  ) for general pre-processing, Stanford POS Tagger (Toutanova et al.,  ) and Stanford dependency parser (Chen and Manning,  ) for syntactic and semantic sentence analysis, Splitta (Gillick,  ) for sentence splitting, GENIA tagger (Tsuruoka et al.,  ) for POS tagging and semantic analysis and Verbnet (Palmer et al.,  ) for verb extraction. \n\n\n## 7. Applications \n  \nOne of the most important applications of BioNER and BioRD is narrowing down the search space when exploring millions of online biomedical journal articles. Often, one needs to find articles that do not merely include a search term but also include contextual information. For example, if \u201csequenced genes in chromosome 9\u201d is the query, all the articles that contain different gene names should also appear in the search results. That would only be possible if the search method knows how to locate genes as well as classify them as chromosome 9 related. \n\nAnother application is for disease diagnosis and treatment, where mining prior treatment data and research work could assist in narrowing down the diagnosis and possibly effective treatment regiments for a given complicated set of symptoms presented by a patient (Zhu et al.,  ; Bello et al.,  ). In recent years, there has been much attention to designing automated healthcare chatbot systems that are configured to respond to user queries and provide advice or support. Healthcare chatbots use various biomedical text mining techniques to process queries, match them to answers in their knowledge base to either provide medical advice or to refer them (Chawla and Anuradha,  ; Ghosh et al.,  ). Such systems require the ability to process entities and relations such as diseases, drugs, symptoms, body parts, diagnosis, treatments, or adverse effects (Ghiasvand and Kate,  ; Wang et al.,  ). \n\nAnother notable application of relation detection is for generating biological interaction networks Azam et al. ( ). For instance, a query like \u201call drugs associated with prostate cancer treatment\u201d requires knowing which tokens refer to drugs and which phrases point to prostate cancer treatment. Once such associations are established, they can be summarized as a network representing prostate cancer gene interactions or drug-to-drug interactions with side effects. These networks not only provide a summation of thousands of research articles and a visualization but also allow us to derive novel hypotheses. \n\nFurthermore, relation extraction can be a vital tool in Adverse Drug Reaction (ADR) and Drug-Drug Interaction (DDI) analysis. It is not practical and ethical to conduct drug trials in a way that all possible DDIs and ADRs are discovered. As such, creating a network with known interactions extracted from research would allow us to explore other possible interactions between drugs and adverse effects Luo et al. ( ). \n\n\n## 8. Discussion \n  \nFrom a general point of view, the task of performing Named Entity Recognition (NER) and Relation Detection (RD) are data science problems (Emmert-Streib and Dehmer,  ). That means an optimal combination of data and methods is required for achieving the best results. Regarding the data, most current studies are based on information from abstracts of scientific articles as provided, e.g., by PubMed. However, such articles contain much more information, which is only accessible if one would have access to full-text publications. For journals having an open access policy like PLoS, Frontiers, or MDPI, this does not constitute an obstacle. However, many articles are still hidden behind a paywall, e.g., most articles from Nature and Science. A related problem refers to capturing information from tables or Supplementary Files. Especially the latter possess new challenges because most publishers do not provide formatting guidelines for Supplementary Files rendering such texts as unstructured. Importantly, information extracted from such full-text publications or Supplementary Files could not only lead to additional information but to redundant information that could be utilized for correcting errors obtained from using journal abstracts solely. Hence, one could expect to improve the quality of the analysis performance by using additional input texts as provided by full-text publications or Supplementary Files. Another problem relates to the extraction of italicized or quoted text which may not be captured. \n\nA common question asked is what is the performance of a method and how does it compare to other related methods? Since the papers reviewed in this article have all been published in either scientific journals or conferences or preprint servers all of them have been studied numerically, at least to some extend. However, for any serious application the information required is the generalization error (GenErr) Emmert-Streib and Dehmer ( ) and the dependence of the GenErr on variations of the data. The statistical estimation of the GenErr is in general challenging and not straight forward. This implies that this error may be considerably different to the numerical results provided in the reviewed papers and, hence, a case-by-case analysis is required to select the proper method for a given application domain. For this reason, as a warning, we would like to remark that despite the fact that we provided throughout the paper information about obtained F-scores or recall values, such information needs to be considered cautiously. Hence, such values should not be seen as an absolute indicator of performance but as guideline for your own dedicated context-specific performance analysis. \n\nFrom a methodological point of view, deep learning approaches are still relatively new, leaving plenty of room for improvement (Yadav and Bethard,  ). A general reason for the popularity of these methods is that deep learning neural networks require no/little feature selection but perform such a mechanism internally within their hidden layers. Certainly, this characteristic is not entirely domain and data-independent (Smolander et al.,  ), and it remains to be seen if this also holds for text data, especially when the number of samples is not in the millions. Interestingly, recent results for patient phenotyping from electronic health records (eHRs) show that this might be the case (Yang et al.,  ). Regarding methods, unsupervised and semi-supervised methods have the most significant potential for improvement because annotated benchmark corpora are still relatively small; see   and the information about the available sample sizes. Hence, methods that operate, at least partially, unsupervised would be very beneficial because they do not require such annotations yet can harvest from the millions of available publications. This could also be connected to learning representations of sentences or words. A good example of this direction is an extension of BERT (Devlin et al.,  ), where unsupervised pre-training with large-scale biomedical corpora is used followed by task-specific fine tuning. The resulting method called BioBERT (Lee et al.,  ) has been shown to result in state-of-the-art performance in a number of different biomedical tasks, including biomedical named entity recognition, biomedical relation extraction and biomedical question answering. \n\nLooking back, methods for word embedding made tremendous progress in recent years starting with word2vec and the improvement by BERT. These results have been enabled by exploiting different neural network architectures (e.g., bidirectional transformers for BERT and LSTMs for ELMo). It seems natural to further explore this direction, e.g., by using nested architectures or introducing additional training or pre-training steps for combined network architectures. \n\nRelated to the last point above is learning new sentence representations in the form of trees or general graphs (Luo et al.,  ). A potential advantage of such a representation is that the rich information from network studies about graph energy, graph entropy, molecular descriptors, or network comparisons could be utilized (Todeschini et al.,  ; Li et al.,  ; Dehmer et al.,  ; Emmert-Streib et al.,  ). For instance, starting from a dependency parse, refined representations could be learned using unsupervised approaches, e.g., autoencoders, to enhance the captured features (Eisenstein,  ). Importantly, not only deep learning methods might be relevant but also SVMs by deriving new graph kernels from such refined graph representations and, e.g., graph descriptors (Vishwanathan et al.,  ; Panyam et al.,  ). Furthermore, we would like to note that NLP methods can contain subjective notions. For instance, for a polarity analysis there is no objective way to derive the meaning of a \u201cpositive\u201d or \u201cnegative\u201d association. Instead, this information needs to be defined by the user. Hence, such an analysis captures the definition of the user. \n\nFinally, another recent development is provided by end-to-end learning Li and Ji ( ). For end-to-end learning the NER and RD tasks are jointly learned, as opposed to pipeline-based systems, because this has been shown to minimize error propagation and improve performance (Giorgi et al.,  ). Generally, end-to-end systems can be either trained as a sequentially (Li et al.,  , ; Bekoulis et al.,  ) or as a simultaneous learning process for both NER and RD. The latter approach is more recent, yet successful with state-of-the-art performance (Li et al.,  ; Bekoulis et al.,  ). While it is common for most end-to-end approaches to get some help from external NLP tools for auxiliary tasks, e.g., dependency parsers, Giorgi et al. ( ) proposed a model to be truly end-to-end with no external help. Problems current systems struggle with are nested entities and inter-sentence relations. Both issues provide ample opportunities for future research. \n\n\n## 9. Conclusion \n  \nIn this paper, we reviewed methods for Named Entity Recognition (NER) and Relation Detection (RD) allowing, e.g., to identify interactions between proteins and drugs or genes and diseases. Over the years, many methods have been introduced and studied for resolving a variety of problems in biomedical, health, and clinical sciences. For this reason, we aimed for a systematic presentation by categorizing methods according to their main characteristics. Importantly, recent progress in artificial intelligence via deep learning provided a new perspective on NER and RD, and further advances can be expected in this direction in the near future. \n\n\n## Author Contributions \n  \nFE-S conceived the study. All authors wrote the article and approved the final version. \n\n\n## Conflict of Interest \n  \nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n \n", "metadata": {"pmcid": 7485218, "text_md5": "e3b7029a962cc1182f7260a2edcdf0ee", "field_positions": {"authors": [0, 62], "journal": [63, 82], "publication_year": [84, 88], "title": [99, 184], "keywords": [198, 358], "abstract": [371, 1339], "body": [1348, 91819]}, "batch": 1, "pmid": 32984300, "doi": "10.3389/fcell.2020.00673", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7485218", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7485218"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7485218\">7485218</a>", "list_title": "PMC7485218  Named Entity Recognition and Relation Detection for Biomedical Information Extraction"}
{"text": "Munkhdalai, Tsendsuren and Li, Meijing and Batsuren, Khuyagbaatar and Park, Hyeon Ah and Choi, Nak Hyeon and Ryu, Keun Ho\nJ Cheminform, 2015\n\n# Title\n\nIncorporating domain knowledge in chemical and biomedical named entity recognition with word representations\n\n# Keywords\n\nFeature Representation Learning\nSemi-Supervised Learning\nNamed Entity Recognition\nConditional Random Fields\n\n\n# Abstract\n \n## Background \n  \nChemical and biomedical Named Entity Recognition (NER) is an essential prerequisite task before effective text mining can begin for biochemical-text data. Exploiting unlabeled text data to leverage system performance has been an active and challenging research topic in text mining due to the recent growth in the amount of biomedical literature. \n\nWe present a semi-supervised learning method that efficiently exploits unlabeled data in order to incorporate domain knowledge into a named entity recognition model and to leverage system performance. The proposed method includes Natural Language Processing (NLP) tasks for text preprocessing, learning word representation features from a large amount of text data for feature extraction, and conditional random fields for token classification. Other than the free text in the domain, the proposed method does not rely on any lexicon nor any dictionary in order to keep the system applicable to other NER tasks in bio-text data. \n\n\n## Results \n  \nWe extended BANNER, a biomedical NER system, with the proposed method. This yields an integrated system that can be applied to chemical and drug NER or biomedical NER. We call our branch of the BANNER system BANNER-CHEMDNER, which is scalable over millions of documents, processing about 530 documents per minute, is configurable via XML, and can be plugged into other systems by using the BANNER Unstructured Information Management Architecture (UIMA) interface. \n\nBANNER-CHEMDNER achieved an 85.68% and an 86.47% F-measure on the testing sets of CHEMDNER Chemical Entity Mention (CEM) and Chemical Document Indexing (CDI) subtasks, respectively, and achieved an 87.04% F-measure on the official testing set of the BioCreative II gene mention task, showing remarkable performance in both chemical and biomedical NER. BANNER-CHEMDNER system is available at:  . \n\n \n\n# Body\n \n## Background \n  \nAs biomedical literature on servers grows exponentially in the form of semi-structured documents, biomedical text mining has been intensively investigated to find information in a more accurate and efficient manner. One essential task in developing such an information extraction system is the Named Entity Recognition (NER) process, which basically defines the boundaries between typical words and biomedical terminology in a particular text, and assigns the terminology to specific categories based on domain knowledge. \n\nNER performance in the newswire domain is indistinguishable from human performance, because it has an accuracy that is above 90%. However, performance has not been the same in the biomedical and chemical domain. It has been hampered by problems such as the number of new terms being created on a regular basis, the lack of standardization of technical terms between authors, and often by the fact that technical terms, such as gene names, often occur with other terminologies [ ]. \n\nProposed solutions include rule-based, dictionary-based, and Machine Learning (ML) approaches. In the dictionary-based approach, a prepared terminology list is matched through a given text to retrieve chunks containing the location of the terminology words [ , ]. However, medical and chemical text can contain new terminology that has yet to be included in the dictionary. \n\nThe rule-based approach defines particular rules by observing the general features of the entities in a text [ ]. In order to identify any named entity in text data, a rule-generation process has to process a huge amount of text to collect accurate rules. In addition, the rules are usually collected by domain experts, requiring a lot of effort. \n\nSince the machine learning approach was adopted, significant progress in biomedical and chemical NER has been achieved with methods like the Markov Model [ ], the Support Vector Machine (SVM) [ - ] the Maximum Entropy Markov Model [ , ], and Conditional Random Fields (CRF) [ , - ]. However, most of the studies rely on supervised machine learning, and thus, system performance is limited by the training set that is usually built by a domain expert. Studies have shown that the word, the word   n  -gram and the character   n  -gram, and the traditional orthographic features are the base for NER, but are poor at representing domain background. \n\nIn order to incorporate domain knowledge into an ML model, Semi-Supervised Learning (SSL) techniques have been applied to NER. SSL is an ML approach that typically uses a large amount of unlabeled and a small amount of labeled data to build a more accurate classification model than would be built using only labeled data. SSL has received significant attention for two reasons. First, preparing a large amount of data for training requires a lot of time and effort. Second, since SSL exploits unlabeled data, the accuracy of classifiers is generally improved. There have been two different directions in SSL methods: 1) semi-supervised model induction approaches, which are the traditional methods and which incorporate domain knowledge from unlabeled data into the classification model during the training phase [ , ], and 2) supervised model induction with unsupervised, possibly semi-supervised, feature learning. The approaches in the second research direction induce better feature representation by learning from a large unlabeled corpus. Recently, the studies that apply the word representation features induced on the large text corpus have reported improvement over baseline systems in many Natural Language Processing tasks [ - ]. \n\nSeveral BioCreative shared tasks have been organized in order to evaluate researcher advancements in chemical and biomedical text mining [ , ]. BioCreative IV CHEMDNER Track consists of two subtasks: the Chemical Document Indexing subtask, where participants are asked to provide a ranked list of chemical entities found in each of the PubMed documents, and the Chemical Entity Mention recognition subtask, where they are asked to submit the start and end indices corresponding to all the chemical entities mentioned in a particular document [ ]. This study extends our previous work participated in CHEMDNER task in the following ways. First, the unlabeled data used to build unsupervised models is now enriched with a large collection of PMC articles. Second, we induce word vector class models from word vectors as word representations. Third, we explore several variations of the unsupervised models built with a larger text collection than the one used before. Finally, we take a step towards a unified NER system in biomedical, chemical and medical domain by training and evaluating a biomedical NER model. These changes lead an improvement outperforming our official entries for CHEMDNER CEM and CDI subtasks by a 0.93% and a 0.73% F-measure, respectively. \n\nIn order to incorporate domain knowledge into the machine learning model to leverage overall system performance, we propose a semi-supervised learning method that efficiently exploits unlabeled data. The proposed method includes NLP tasks for text preprocessing and learning word representation features from a large amount of raw text data, in addition to the word, the word   n  -gram, the character   n  -gram, and the traditional orthographic information (baseline features) for feature extraction, and applies CRF for token labeling. Our method does not rely on any lexicon, nor any dictionary other than the free text in the domain, in order to keep the system applicable to the other NER tasks in bio-text data, even though the usage of such resources is reported to considerably boost system performance. \n\nDuring the development, we extended the BANNER system [ ] with the proposed method, since that system is used in many biomedical text mining systems [ - ], showing state-of-the-art performance in biomedical Named Entity Recognition. BANNER extracts the most fundamental features for NER such as orthographic, letter   n-gram   and word prefix features, builds on top of a CRF model, and includes two types of postprocessing rule, namely parenthesis matching and abbreviations resolving. Our extension yields an integrated system that can be applied to chemical and drug NER or biomedical NER. We call our branch of the BANNER system BANNER-CHEMDNER, which is scalable and configurable, and can easily be plugged into other systems. BANNER-CHEMDNER shows an 85.68% and an 86.47% F-measure on the testing sets of CHEMDNER CEM and CDI subtasks, respectively, and an 87.04% F-measure on the official testing set of the BioCreative II gene mention task. \n\n\n## Methods \n  \nOur chemical and drug NER system design is shown in Figure  . First, we perform preprocessing on MEDLINE and PMC document collection and then extract two different feature sets, a base feature set and a word representation feature set, in the feature processing phase. The unlabeled set of the collection is fed to unsupervised learning of the feature processing phase to build word classes. Finally, we apply the CRF sequence-labeling method to the extracted feature vectors to train the NER model. These steps will be described in subsequent sections. \n  \n System design for chemical and drug Named Entity Recognition  . The solid lines represent the flow of labeled data, and the dotted lines represent the flow of unlabeled data. \n  \n### Preprocessing \n  \nPreprocessing is where text data is cleaned and processed via NLP tasks and is a preparatory task for feature processing. \n\nFirst, the text data is cleansed by removing non-informative characters and replacing special characters with corresponding spellings. The text is then tokenized with a tokenization tool. We evaluated two different tokenization strategies: a simple white space tokenizer and the BANNER simple tokenizer. The white space tokenizer splits the text simply, based on blanks within it, whereas the BANNER tokenizer breaks tokens into either a contiguous block of letters and/or digits or a single punctuation mark. Finally, the lemma and the part-of-speech (POS) information were obtained for a further usage in the feature extraction phase. In BANNER-CHEMDNER, BioLemmatizer [ ] was used for lemma extraction, which resulted in a significant improvement in overall system performance. \n\nIn addition to these preprocessing steps, special care is taken to parse the PMC XML documents to get the full text for the unlabeled data collection. \n\n\n### Feature processing \n  \nWe extract features from the preprocessed text to represent each token as a feature vector, and then an ML algorithm is employed to build a model for NER. \n\nThe proposed method includes extraction of the baseline and the word representation feature sets. The baseline feature set is essential in NER, but is poor at representing the domain background because it only carries some morphological and shallow-syntax information of words. On the other hand, the word representation features can be extracted by learning on a large amount of text and may be capable of introducing domain background to the NER model. \n\nThe entire feature set for a token is expanded to include features for the surroundings with a two-length sliding window. The word, the word   n  -gram, the character   n  -gram, lemma and the traditional orthographic information are extracted as the baseline feature set. The regular expressions that reveal orthographic information are matched to the tokens to give orthographic information. These baseline features are summarized in Table  . \n  \nThe baseline features. \n  \nFor word representation features, we train Brown clustering models [ ] and Word Vector (WV) models [ ] on a large PubMed and PMC document collection. Brown clustering is a hierarchical word clustering method, grouping words in an input corpus to maximize the mutual information of bigrams. Therefore, the quality of a partition can be computed as a sum of mutual information weights between clusters. It runs in time O(V \u00d7 K ), where V is the size of the vocabulary and K is the number of clusters. \n\nThe VW model is induced via a Recurrent Neural Network (RNN) and can be seen as a language model that consists of   n  -dimensional continuous valued vectors, each of which represents a word in the training corpus. The RNN instance is trained to predict either the middle word of a token sequence captured in a window (CBOW) or surrounding words given the middle word of the sequence (skip-gram) depending on the model architecture [ ]. The RNN becomes a log-linear classifier, once its non-linear hidden layer is removed, so the training process speeds up allowing millions of documents to process within an hour. We used a tool implemented by Mikolov et al. [ ] to build our WV model from the PubMed collection. \n\nFurther, the word vectors are clustered using a K-means algorithm to drive a Word Vector Class (WVC) model. Since Brown clustering is a bigram model, this model may not be able to carry wide context information of a word, whereas the WVC model is an   n  -gram model (usually   n   = 5) and learns broad context information from the domain corpus. We drive the cluster label prefixes with 4, 6, 10 and 20 lengths in the Brown model by following the experiment of Turian et al. [ ], and the WVC models induced from 250-dimension WVs as word representation features. \n\nFor feature extraction, we do not rely on any lexicon nor any dictionary other than the free text in the domain in order to keep the system applicable to other NER tasks in bio-text data, even though the usage of such resources is reported to considerably boost system performance. Most of the top performing systems participated in CHEMDNER task use the domain lexicon and observed a considerable performance boost [ ]. \n\n\n### Supervised learning \n  \nCRF - a probabilistic undirected graphical model has been used successfully in a large number of studies on NER, because it takes advantage of sequence labelling by treating each sentence as a sequence of tokens. We apply a second-order CRF model, where the current label is conditioned on the previous two using a Begin, Inside, Outside (BIO) tagging format of the tokens. In the BIO tagging format, each token is classified either at the beginning, inside or outside of a named entity, and a postprocessing task forms the named entity mentions by merging the tagged tokens. \n\nWe use a Machine Learning for Language Toolkit (MALLET) library [ ] for training the CRF model, because the BANNER system provides a convenient interface to work with it. The BANNER system also includes two types of general postprocessing that could be useful for any NER tasks in bio-text data. The first type is based on the symmetry of parenthesis, brackets or double quotation marks. Since these punctuation marks are always paired, BANNER drops any named entity mention containing mismatched parentheses, brackets or double quotation marks. The second type of postprocessing is dedicated to resolving abbreviations of named entities. \n\n\n\n## Results \n  \n### Dataset \n  \nBecause the proposed method is a semi-supervised learning method exploiting unlabeled data during feature extraction, we prepared a large document collection of domain text in addition to annotated datasets. \n\nWe evaluated the system for chemical and drug NER with a CHEMDNER dataset provided by the BioCreative IV CHEMDNER task organizers. The dataset consists of 10,000 annotated documents subdivided into training and development sets of 3,500 documents each, and a testing set of 3,000 documents. The BioCreative II Gene Mention (BC2GM) dataset was used to compare the system against existing systems of the biomedical NER. The BC2GM dataset originally splits into subsets of 15,000 and 5,000 sentences for training and testing, respectively. \n\nFor the unlabeled data, we collected around 1.4 million PubMed abstracts and full text articles from the whole PMC database available at the time (over 2 million documents). After preprocessing, we derived two different text corpora: a PubMed abstract corpus consisting of a vocabulary of 1,136,085 entries for induction of Brown clustering models, and a merged corpus of both resources with a vocabulary of 4,359,932 entries for training WV models. Given the limited resources and time, we were able to induce the Brown clustering models only with the PubMed abstract corpus. We would like to build a Brown model with the full corpus. However, this would take several months, making it impossible to test on the CHEMDNER testing set in the given period. \n\n\n### Evaluation measure \n  \nFor both CDI and CEM subtasks, an exact matching criteria was used to examine three different result types. False negative (  FN  ) and False positives (  FP  ) are incorrect negative and positive predictions. True positives (  TP  ) results corresponded to correct positive predictions, which are actual correct predictions. The evaluation is based on the performance measures   p   (precision),   r   (recall), and   F  . \n\nRecall denotes the percentage of correctly labeled positive results over all positive cases and is calculated as:   r   =   TP/(TP+FN)  . \n\nPrecision is the percentage of correctly labeled positive results over all positive labeled results and is calculated as:   p   =   TP/(TP+FP)  . \n\nThe F-measure is the harmonic average of precision and recall, and a balanced F-measure is expressed as:   F = 2pr/(p+r)  . \n\n\n### Chemical and drug named entity recognition \n  \nWe trained the second-order CRF models with different features on the training set and evaluated the models on the development set. Consequently, after noticing the best settings for the hyperparameters, we trained the models on a merged set of the training and the development sets and reported the performance on the testing set. Table   and Table   show the performance comparison of the different runs with varied feature settings for CDI and CEM subtasks. Both tasks are interconnected, and their performance measures are interchangeable. \n  \nCDI subtask evaluation results of different runs with varied features. \n  \nFeature groups are separated by (+). The parameters followed Brown and WVC are the number of classes induced in each model. Pre: Precision, Rec: Recall, F-scr: F-score. \n    \nCEM subtask evaluation results of different runs with varied features. \n  \nFeature groups are separated by (+). The parameters followed Brown and WVC are the number of classes induced in each model. Pre: Precision, Rec: Recall, F-scr: F-score. \n  \nWe started conducting a run with a basic feature setting and a BANNER setup, and gradually increased the complexity of the feature space for further runs. The inclusion of lemma by BioLemmatizer as a feature (baseline feature setup) in addition to BANNER feature set yielded a significant improvement on the development set. Surprisingly, the model based on the baseline features converged to an optimum, possibly a local optimum in the first 30 iterations of the training, and reported a worthless performance when evaluating the testing set. A Brown model with a larger number of clusters tended to obtain a higher F-measure. Unlike Brown clustering, a large or a lower number of WVCs degraded the performance. We found the WVC model with 500 different classes the best performing one on this task. During the induction of the WVC models, we set the WV dimension to 250 which is a trade-off value between computational cost and WV quality. Further, the combination of the different WVC models significantly improved the F-measure. We achieved the best performance, an 85.68% F-measure for CEM and an 86.47% F-measure for CDI subtasks, with the model based on the baseline feature set, the 1000-Brown clustering, and 300, 500 and 1000 WVCs (the baseline + Brown 1000 + WVC 300 + WVC 500 + WVC 1000 setup). This result is 0.93% and 0.73% higher than our best entries for CHEMDNER CEM and CDI subtasks [ ]. The word representation features extracted from the unlabeled data boosted performance by a 1.37% and a 1.14% F-measure for CEM and CDI subtasks, respectively. \n\n\n### Gene and protein mention recognition \n  \nWe trained BANNER-CHEMDNER on the BC2GM training set and evaluated it on the testing set, following the convention of the BioCreative II gene mention task. Table   lists the different runs with varied features included in the NER model. We expected similar results to be obtained in chemical and drug NER. However, the combination of the features did not show the expected improvement, and the best result (an 87.04% F-measure) that we obtained was a run with a model based on the baseline feature set, the 1000-Brown clustering and 500 WVCs (baseline + Brown 1000 + WVC 500). \n  \nBioCreative II gene mention evaluation results of different runs with varied features. \n  \nFeature groups are separated by (+). The parameters followed Brown and WVC are the number of classes induced in each model. Pre: Precision, Rec: Recall, F-scr: F-score. \n  \nWe finally compared our best result to the task entries and the BANNER system result (Table  ). \n  \nComparison of different systems on the BioCreative II testing set. \n  \nAndo focused on a semi-supervised approach, alternating structure optimization. The system learns better feature representations from a large collection of PubMed text and uses a regularized linear classifier during the supervised training on annotated data. Kuo et al. [ ] utilized domain lexicons and bi-direction parsing models of CRFs. The results from left-right and right-left parsing models were combined to produce a set of higher recall mention answers. Huang et al. [ ] explored an ensemble of SVM and CRFs models. They applied intersection to the tagging results of the two SVM models and then union with the tagging results of the CRF model in their ensemble approach. \n\n\n\n## Discussion \n  \nSeveral different runs other than the ones reported in the results section were carried out. We observed that the whitespace tokenizer performs better than the BANNER simple tokenizer for extraction of word representation features. A model with an additional feature set of raw WVs was trained and evaluated. However, we found that the word WVs did not always improve performance, and in some cases, degraded system performance with the CRF model. That is, the continuous valued WV features add some level of complexity to the model, overfitting the CRF model. Even though the WV features with the CRF model did not achieve improvement, those continuous valued features could be useful in conjunction with other classifiers, such as perceptron and support vector machines [ ]. Another finding is that the Brown cluster features always improve F-measure, and the improvement is significant when the model is built on the domain text corpus. \n\n\n## Conclusions \n  \nWe proposed a semi-supervised learning method that exploits unlabeled data efficiently in order to incorporate domain knowledge into a Named Entity Recognition model and to leverage overall system performance. The key feature of the method is learning word representations from a large amount of text data for feature extraction. The generally applicable word representation features were reported to boost system performance significantly for both chemical and biomedical NER. \n\nWe extended BANNER, a biomedical NER system, with the proposed method. This yields an integrated system that can be applied for chemical and drug NER or biomedical NER. We call our branch of the BANNER system BANNER-CHEMDNER. \n\nBANNER-CHEMDNER achieves an 85.68% and an 86.47% F-measure on the testing set for CHEMDNER CEM and CDI subtasks, respectively, and an 87.04% F-measure on the official testing set of the BioCreative II gene mention task, showing remarkable performance for both chemical and biomedical NER. \n\nOur future work should be towards a unified NER system in biomedical, chemical and medical domain, based on the generally applicable word representations. \n\n\n## Competing interests \n  \nThe authors declare that they have no competing interests. \n\n\n## Authors' contributions \n  \nTM conceived the study, participated in its design, developed the extension program, and drafted the manuscript. ML carried out calculations and helped draft the manuscript. KB participated in data analysis and helped draft the manuscript. HAP participated in study design. NHC helped draft the manuscript. KHR provided system design, valuable guidance, editing, and research grant. All authors read and approved the final manuscript. \n\n \n", "metadata": {"pmcid": 4331699, "text_md5": "c07343c8b862f45321d4cc974cb5b771", "field_positions": {"authors": [0, 121], "journal": [122, 134], "publication_year": [136, 140], "title": [151, 259], "keywords": [273, 381], "abstract": [394, 2274], "body": [2283, 24883]}, "batch": 1, "pmid": 25810780, "doi": "10.1186/1758-2946-7-S1-S9", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331699", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4331699"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331699\">4331699</a>", "list_title": "PMC4331699  Incorporating domain knowledge in chemical and biomedical named entity recognition with word representations"}
{"text": "Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo\nBioinformatics, 2019\n\n# Title\n\nBioBERT: a pre-trained biomedical language representation model for biomedical text mining\n\n# Keywords\n\n\n\n# Abstract\n \n## Motivation \n  \nBiomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. \n\n\n## Results \n  \nWe introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. \n\n\n## Availability and implementation \n  \nWe make the pre-trained weights of BioBERT freely available at  , and the source code for fine-tuning BioBERT available at  . \n\n \n\n# Body\n \n## 1 Introduction \n  \nThe volume of biomedical literature continues to rapidly increase. On average, more than 3000 new articles are published every day in peer-reviewed journals, excluding pre-prints and technical reports such as clinical trial reports in various archives. PubMed alone has a total of 29M articles as of January 2019. Reports containing valuable information about new discoveries and new insights are continuously added to the already overwhelming amount of literature. Consequently, there is increasingly more demand for accurate biomedical text mining tools for extracting information from the literature. \n\nRecent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing (NLP). For instance, Long Short-Term Memory (LSTM) and Conditional Random Field (CRF) have greatly improved performance in biomedical named entity recognition (NER) over the last few years ( ;  ;  ;  ). Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction (RE) ( ;  ) and question answering (QA) ( ). \n\nHowever, directly applying state-of-the-art NLP methodologies to biomedical text mining has limitations. First, as recent word representation models such as Word2Vec ( ), ELMo ( ) and BERT ( ) are trained and tested mainly on datasets containing general domain texts (e.g. Wikipedia), it is difficult to estimate their performance on datasets containing biomedical texts. Also, the word distributions of general and biomedical corpora are quite different, which can often be a problem for biomedical text mining models. As a result, recent models in biomedical text mining rely largely on adapted versions of word representations ( ;  ). \n\nIn this study, we hypothesize that current state-of-the-art word representation models such as BERT need to be trained on biomedical corpora to be effective in biomedical text mining tasks. Previously, Word2Vec, which is one of the most widely known context independent word representation models, was trained on biomedical corpora which contain terms and expressions that are usually not included in a general domain corpus ( ). While ELMo and BERT have proven the effectiveness of contextualized word representations, they cannot obtain high performance on biomedical corpora because they are pre-trained on only general domain corpora. As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks, adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches. \n\n\n## 2 Approach \n  \nIn this article, we introduce BioBERT, which is a pre-trained language representation model for the biomedical domain. The overall process of pre-training and fine-tuning BioBERT is illustrated in  . First, we initialize BioBERT with weights from BERT, which was pre-trained on general domain corpora (English Wikipedia and BooksCorpus). Then, BioBERT is pre-trained on biomedical domain corpora (PubMed abstracts and PMC full-text articles). To show the effectiveness of our approach in biomedical text mining, BioBERT is fine-tuned and evaluated on three popular biomedical text mining tasks (NER, RE and QA). We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora, and analyze the effect of each corpus on pre-training. We also provide in-depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies.\n \n  \nOverview of the pre-training and fine-tuning of BioBERT \n  \nThe contributions of our paper are as follows:\n   \nBioBERT is the first domain-specific BERT based model pre-trained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs. \n  \nWe show that pre-training BERT on biomedical corpora largely improves its performance. BioBERT obtains higher F1 scores in biomedical NER (  0.62  ) and biomedical RE (  2.80  ), and a higher MRR score (  12.24  ) in biomedical QA than the current state-of-the-art models. \n  \nCompared with most previous biomedical text mining models that are mainly focused on a single task such as NER or QA, our model BioBERT achieves state-of-the-art performance on various biomedical text mining tasks, while requiring only minimal architectural modifications. \n  \nWe make our pre-processed datasets, the pre-trained weights of BioBERT and the source code for fine-tuning BioBERT publicly available. \n  \n\n\n## 3 Materials and methods \n  \nBioBERT basically has the same structure as BERT. We briefly discuss the recently proposed BERT, and then we describe in detail the pre-training and fine-tuning process of BioBERT. \n\n### 3.1 BERT: bidirectional encoder representations from transformers \n  \nLearning word representations from a large amount of unannotated text is a long-established method. While previous models (e.g. Word2Vec ( ), GloVe ( )) focused on learning context independent word representations, recent works have focused on learning context dependent word representations. For instance, ELMo ( ) uses a bidirectional language model, while CoVe ( ) uses machine translation to embed context information into word representations. \n\nBERT ( ) is a contextualized word representation model that is based on a masked language model and pre-trained using bidirectional transformers ( ). Due to the nature of language modeling where future words cannot be seen, previous language models were limited to a combination of two unidirectional language models (i.e. left-to-right and right-to-left). BERT uses a masked language model that predicts randomly masked words in a sequence, and hence can be used for learning bidirectional representations. Also, it obtains state-of-the-art performance on most NLP tasks, while requiring minimal task-specific architectural modification. According to the authors of BERT, incorporating information from bidirectional representations, rather than unidirectional representations, is crucial for representing words in natural language. We hypothesize that such bidirectional representations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus ( ). Due to the space limitations, we refer readers to   for a more detailed description of BERT. \n\n\n### 3.2 Pre-training BioBERT \n  \nAs a general purpose language representation model, BERT was pre-trained on English Wikipedia and BooksCorpus. However, biomedical domain texts contain a considerable number of domain-specific proper nouns (e.g. BRCA1, c.248T>C) and terms (e.g. transcriptional, antimicrobial), which are understood mostly by biomedical researchers. As a result, NLP models designed for general purpose language understanding often obtains poor performance in biomedical text mining tasks. In this work, we pre-train BioBERT on PubMed abstracts (PubMed) and PubMed Central full-text articles (PMC). The text corpora used for pre-training of BioBERT are listed in  , and the tested combinations of text corpora are listed in  . For computational efficiency, whenever the Wiki + Books corpora were used for pre-training, we initialized BioBERT with the pre-trained BERT model provided by  . We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora (e.g. BioBERT (+ PubMed)).\n \n  \nList of text corpora used for BioBERT \n    \nPre-training BioBERT on different combinations of the following text corpora: English Wikipedia (Wiki), BooksCorpus (Books), PubMed abstracts (PubMed) and PMC full-text articles (PMC) \n  \nFor tokenization, BioBERT uses WordPiece tokenization ( ), which mitigates the out-of-vocabulary issue. With WordPiece tokenization, any new words can be represented by frequent subwords (e.g.   Immunoglobulin   =>   I ##mm ##uno ##g ##lo ##bul ##in  ). We found that using cased vocabulary (not lower-casing) results in slightly better performances in downstream tasks. Although we could have constructed new WordPiece vocabulary based on biomedical corpora, we used the original vocabulary of BERT  for the following reasons: (i) compatibility of BioBERT with BERT, which allows BERT pre-trained on general domain corpora to be re-used, and makes it easier to interchangeably use existing models based on BERT and BioBERT and (ii) any new words may still be represented and fine-tuned for the biomedical domain using the original WordPiece vocabulary of BERT. \n\n\n### 3.3 Fine-tuning BioBERT \n  \nWith minimal architectural modification, BioBERT can be applied to various downstream text mining tasks. We fine-tune BioBERT on the following three representative biomedical text mining tasks: NER, RE and QA. \n\n Named  \u2002  e   ntity  \u2002  r   ecognition   is one of the most fundamental biomedical text mining tasks, which involves recognizing numerous domain-specific proper nouns in a biomedical corpus. While most previous works were built upon different combinations of LSTMs and CRFs ( ;  ;  ), BERT has a simple architecture based on bidirectional transformers. BERT uses a single output layer based on the representations from its last layer to compute only token level BIO2 probabilities. Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora ( ;  ), BioBERT directly learns WordPiece embeddings during pre-training and fine-tuning. For the evaluation metrics of NER, we used entity level precision, recall and F1 score. \n\n Relation  \u2002  e   xtraction   is a task of classifying relations of named entities in a biomedical corpus. We utilized the sentence classifier of the original version of BERT, which uses a [CLS] token for the classification of relations. Sentence classification is performed using a single output layer based on a [CLS] token representation from BERT. We anonymized target named entities in a sentence using pre-defined tags such as @GENE$ or @DISEASE$. For instance, a sentence with two target entities (gene and disease in this case) is represented as \u201c  Serine at position 986 of @GENE$ may be an independent genetic predictor of angiographic @DISEASE$  .\u201d The precision, recall and F1 scores on the RE task are reported. \n\n Question  \u2002  a   nswering   is a task of answering questions posed in natural language given related passages. To fine-tune BioBERT for QA, we used the same BERT architecture used for SQuAD ( ). We used the BioASQ factoid datasets because their format is similar to that of SQuAD. Token level probabilities for the start/end location of answer phrases are computed using a single output layer. However, we observed that about 30% of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given passages. Like  , we excluded the samples with unanswerable questions from the training sets. Also, we used the same pre-training process of  , which uses SQuAD, and it largely improved the performance of both BERT and BioBERT. We used the following evaluation metrics from BioASQ: strict accuracy, lenient accuracy and mean reciprocal rank. \n\n\n\n## 4 Results \n  \n### 4.1 Datasets \n  \nThe statistics of biomedical NER datasets are listed in  . We used the pre-processed versions of all the NER datasets provided by   except the 2010 i2b2/VA, JNLPBA and Species-800 datasets. The pre-processed NCBI Disease dataset has fewer annotations than the original dataset due to the removal of duplicate articles from its training set. We used the CoNLL format ( ) for pre-processing the 2010 i2b2/VA and JNLPBA datasets. The Species-800 dataset was pre-processed and split based on the dataset of Pyysalo ( ). We did not use alternate annotations for the BC2GM dataset, and all NER evaluations are based on entity-level exact matches. Note that although there are several other recently introduced high quality biomedical NER datasets ( ), we use datasets that are frequently used by many biomedical NLP researchers, which makes it much easier to compare our work with theirs. The RE datasets contain gene\u2013disease relations and protein\u2013chemical relations ( ). Pre-processed GAD and EU-ADR datasets are available with our provided codes. For the CHEMPROT dataset, we used the same pre-processing procedure described in  . We used the BioASQ factoid datasets, which can be converted into the same format as the SQuAD dataset ( ). We used full abstracts (PMIDs) and related questions and answers provided by the BioASQ organizers. We have made the pre-processed BioASQ datasets publicly available. For all the datasets, we used the same dataset splits used in previous works ( ;  ;  ) for a fair evaluation; however, the splits of LINAAEUS and Species-800 could not be found from   and may be different. Like previous work ( ), we reported the performance of 10-fold cross-validation on datasets that do not have separate test sets (e.g. GAD, EU-ADR).\n \n  \nStatistics of the biomedical named entity recognition datasets \n      \nStatistics of the biomedical relation extraction datasets \n      \nStatistics of biomedical question answering datasets \n  \nWe compare BERT and BioBERT with the current state-of-the-art models and report their scores. Note that the state-of-the-art models each have a different architecture and training procedure. For instance, the state-of-the-art model by   trained on the JNLPBA dataset is based on multiple Bi-LSTM CRF models with character level CNNs, while the state-of-the-art model by   trained on the LINNAEUS dataset uses a Bi-LSTM CRF model with character level LSTMs and is additionally trained on silver-standard datasets. On the other hand, BERT and BioBERT have exactly the same structure, and use only the gold standard datasets and not any additional datasets. \n\n\n### 4.2 Experimental setups \n  \nWe used the BERT  model pre-trained on English Wikipedia and BooksCorpus for 1M steps. BioBERT v1.0 (+ PubMed + PMC) is the version of BioBERT (+ PubMed + PMC) trained for 470\u2009K steps. When using both the PubMed and PMC corpora, we found that 200K and 270K pre-training steps were optimal for PubMed and PMC, respectively. We also used the ablated versions of BioBERT v1.0, which were pre-trained on only PubMed for 200K steps (BioBERT v1.0 (+ PubMed)) and PMC for 270K steps (BioBERT v1.0 (+ PMC)). After our initial release of BioBERT v1.0, we pre-trained BioBERT on PubMed for 1M steps, and we refer to this version as BioBERT v1.1 (+ PubMed). Other hyper-parameters such as batch size and learning rate scheduling for pre-training BioBERT are the same as those for pre-training BERT unless stated otherwise. \n\nWe pre-trained BioBERT using Naver Smart Machine Learning (NSML) ( ), which is utilized for large-scale experiments that need to be run on several GPUs. We used eight NVIDIA V100 (32GB) GPUs for the pre-training. The maximum sequence length was fixed to 512 and the mini-batch size was set to 192, resulting in 98\u00a0304 words per iteration. It takes more than 10\u2009days to pre-train BioBERT v1.0 (+ PubMed + PMC) nearly 23\u2009days for BioBERT v1.1 (+ PubMed) in this setting. Despite our best efforts to use BERT , we used only BERT  due to the computational complexity of BERT . \n\nWe used a single NVIDIA Titan Xp (12GB) GPU to fine-tune BioBERT on each task. Note that the fine-tuning process is more computationally efficient than pre-training BioBERT. For fine-tuning, a batch size of 10, 16, 32 or 64 was selected, and a learning rate of 5e\u22125, 3e\u22125 or 1e\u22125 was selected. Fine-tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by  . On the other hand, it takes more than 20 epochs for BioBERT to reach its highest performance on the NER datasets. \n\n\n### 4.3 Experimental results \n  \nThe results of NER are shown in  . First, we observe that BERT, which was pre-trained on only the general domain corpus is quite effective, but the micro averaged F1 score of BERT was lower (2.01 lower) than that of the state-of-the-art models. On the other hand, BioBERT achieves higher scores than BERT on all the datasets. BioBERT outperformed the state-of-the-art models on six out of nine datasets, and BioBERT v1.1 (+ PubMed) outperformed the state-of-the-art models by 0.62 in terms of micro averaged F1 score. The relatively low scores on the LINNAEUS dataset can be attributed to the following: (i) the lack of a silver-standard dataset for training previous state-of-the-art models and (ii) different training/test set splits used in previous work ( ), which were unavailable.\n \n  \nTest results in biomedical named entity recognition \n    \nThe RE results of each model are shown in  . BERT achieved better performance than the state-of-the-art model on the CHEMPROT dataset, which demonstrates its effectiveness in RE. On average (micro), BioBERT v1.0 (+ PubMed) obtained a higher F1 score (2.80 higher) than the state-of-the-art models. Also, BioBERT achieved the highest F1 scores on 2 out of 3 biomedical datasets.\n \n  \nBiomedical relation extraction test results \n    \nThe QA results are shown in  . We micro averaged the best scores of the state-of-the-art models from each batch. BERT obtained a higher micro averaged MRR score (7.0 higher) than the state-of-the-art models. All versions of BioBERT significantly outperformed BERT and the state-of-the-art models, and in particular, BioBERT v1.1 (+ PubMed) obtained a strict accuracy of 38.77, a lenient accuracy of 53.81 and a mean reciprocal rank score of 44.77, all of which were micro averaged. On all the biomedical QA datasets, BioBERT achieved new state-of-the-art performance in terms of MRR.\n \n  \nBiomedical question answering test results \n    \n\n\n## 5 Discussion \n  \nWe used additional corpora of different sizes for pre-training and investigated their effect on performance. For BioBERT v1.0 (+ PubMed), we set the number of pre-training steps to 200K and varied the size of the PubMed corpus.   shows that the performance of BioBERT v1.0 (+ PubMed) on three NER datasets (NCBI Disease, BC2GM, BC4CHEMD) changes in relation to the size of the PubMed corpus. Pre-training on 1 billion words is quite effective, and the performance on each dataset mostly improves until 4.5 billion words. We also saved the pre-trained weights from BioBERT v1.0 (+ PubMed) at different pre-training steps to measure how the number of pre-training steps affects its performance on fine-tuning tasks.   shows the performance changes of BioBERT v1.0 (+ PubMed) on the same three NER datasets in relation to the number of pre-training steps. The results clearly show that the performance on each dataset improves as the number of pre-training steps increases. Finally,   shows the absolute performance improvements of BioBERT v1.0 (+ PubMed + PMC) over BERT on all 15 datasets. F1 scores were used for NER/RE, and MRR scores were used for QA. BioBERT significantly improves performance on most of the datasets.\n \n  \n(  a  ) Effects of varying the size of the PubMed corpus for pre-training. (  b  ) NER performance of BioBERT at different checkpoints. (  c  ) Performance improvement of BioBERT v1.0 (+ PubMed + PMC) over BERT \n  \nAs shown in  , we sampled predictions from BERT and BioBERT v1.1 (+PubMed) to see the effect of pre-training on downstream tasks. BioBERT can recognize biomedical named entities that BERT cannot and can find the exact boundaries of named entities. While BERT often gives incorrect answers to simple biomedical questions, BioBERT provides correct answers to such questions. Also, BioBERT can provide longer named entities as answers.\n \n  \nPrediction samples from BERT and BioBERT on NER and QA datasets \n    \n\n## 6 Conclusion \n  \nIn this article, we introduced BioBERT, which is a pre-trained language representation model for biomedical text mining. We showed that pre-training BERT on biomedical corpora is crucial in applying it to the biomedical domain. Requiring minimal task-specific architectural modification, BioBERT outperforms previous models on biomedical text mining tasks such as NER, RE and QA. \n\nThe pre-released version of BioBERT (January 2019) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes ( ), human phenotype-gene RE ( ) and clinical temporal RE ( ). The following updated versions of BioBERT will be available to the bioNLP community: (i) BioBERT  and BioBERT  trained on only PubMed abstracts without initialization from the existing BERT model and (ii) BioBERT  and BioBERT  trained on domain-specific vocabulary based on WordPiece. \n\n\n## Funding \n  \nThis research was supported by the National Research Foundation of Korea(NRF) funded by the Korea government (NRF-2017R1A2A1A17069645, NRF-2017M3C4A7065887, NRF-2014M3C9A3063541). \n\n \n", "metadata": {"pmcid": 7703786, "text_md5": "015be63eb38f6773b5d4241a3c8ccc49", "field_positions": {"authors": [0, 115], "journal": [116, 130], "publication_year": [132, 136], "title": [147, 237], "keywords": [251, 251], "abstract": [264, 2072], "body": [2081, 22912]}, "batch": 1, "pmid": 31501885, "doi": "10.1093/bioinformatics/btz682", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7703786"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7703786\">7703786</a>", "list_title": "PMC7703786  BioBERT: a pre-trained biomedical language representation model for biomedical text mining"}
{"text": "Luo, Ling and Lai, Po-Ting and Wei, Chih-Hsuan and Arighi, Cecilia N and Lu, Zhiyong\nBrief Bioinform, 2022\n\n# Title\n\nBioRED: a rich biomedical relation extraction dataset\n\n# Keywords\n\nrelation extraction\nnamed entity recognition\nbiomedical dataset\nbiomedical natural language processing\n\n\n# Abstract\n \nAutomated relation extraction (RE) from biomedical literature is critical for many downstream text mining applications in both research and real-world settings. However, most existing benchmarking datasets for biomedical RE only focus on relations of a single type (e.g. protein\u2013protein interactions) at the sentence level, greatly limiting the development of RE systems in biomedicine. In this work, we first review commonly used named entity recognition (NER) and RE datasets. Then, we present a first-of-its-kind biomedical relation extraction dataset (BioRED) with multiple entity types (e.g. gene/protein, disease, chemical) and relation pairs (e.g. gene\u2013disease; chemical\u2013chemical) at the document level, on a set of 600 PubMed abstracts. Furthermore, we label each relation as describing either a novel finding or previously known background knowledge, enabling automated algorithms to differentiate between novel and background information. We assess the utility of BioRED by benchmarking several existing state-of-the-art methods, including Bidirectional Encoder Representations from Transformers (BERT)-based models, on the NER and RE tasks. Our results show that while existing approaches can reach high performance on the NER task (  F  -score of 89.3%), there is much room for improvement for the RE task, especially when extracting novel relations (  F  -score of 47.7%). Our experiments also demonstrate that such a rich dataset can successfully facilitate the development of more accurate, efficient and robust RE systems for biomedicine. \n\nAvailability: The BioRED dataset and annotation guidelines are freely available at  . \n \n\n# Body\n \n## Introduction \n  \nBiomedical natural language processing (BioNLP) and text-mining methods/tools make it possible to automatically unlock essential information published in the medical literature, including genetic diseases and their relevant variants [ ,  ], chemical-induced diseases [ ] and drug response in cancer [ ]. However, two crucial and building block steps in the general biomedical information extraction pipeline remain challenging. The first is named entity recognition and linking (NER/NEL), which automatically recognizes the boundary of the entity spans (e.g. ESR1) of a specific biomedical concept (e.g. gene) from the free text and further links the spans to the particular entities with database identifiers (e.g. NCBI Gene ID: 2099). The second is relation extraction (RE), which identifies an entity pair with certain relations. \n\nTo facilitate the development and evaluation of NLP and machine learning methods for biomedical NER/NEL and RE, significant efforts have been made on relevant corpora development [ ]. However, most existing corpora focus only on relations between two entities and within single sentences. For example, Herrero-Zazo   et al  . [ ] developed a drug\u2013drug interaction (DDI) corpus by annotating relations only if both drug names appear in the same single sentence. As a result, multiple individual NER/RE tools need to be created to extract biomedical relations beyond a single type (e.g. extracting both DDI and gene\u2013disease relations). \n\nAdditionally, in the biomedical domain, extracting novel findings that represent the fundamental reason why an asserted relation is published as opposed to background or ancillary assertions from the scientific literature is of significant importance. To the best of our knowledge, none of the previous works on (biomedical) relation annotation, however, included such a novelty attribute. \n\nIn this work, we first give an overview of NER/NEL/RE datasets and show their strengths and weaknesses. Furthermore, we present a rich biomedical relation extraction dataset (BioRED). We further annotated the relations as either novel findings or previously known background knowledge. We summarize the unique features of the BioRED corpus as follows: (i) BioRED consists of biomedical relations among six commonly described entities (i.e. gene, disease, chemical, variant, species and cell line) in eight different types (e.g. positive correlation). Such a setting supports developing a single general-purpose RE system in biomedicine with reduced resources and improved efficiency. More importantly, several previous studies have shown that training a machine learning algorithm on multiple concepts simultaneously on one dataset, rather than multiple single-entity datasets, can lead to better performance [ ]. We expect similar outcomes with our dataset for both NER and RE tasks. (ii) The annotated relations can be asserted either within or across sentence boundaries. For example, as shown in   (relation R5 in pink), the variant \u2018D374Y\u2019 of the PCSK9 gene and the causal relation with the disease \u2018autosomal dominant hypercholesterolemia\u2019 are in different sentences. This task, therefore, requires relations to be inferred by machine reading across the entire document. (iii) Finally, our corpus is enriched with novelty annotations. This novel task poses new challenges for (biomedical) RE research and enables the development of NLP systems to distinguish between known facts and novel findings, a greatly needed feature for extracting new knowledge and avoiding duplicate information toward the automatic knowledge construction in biomedicine. \n  \nAn example of a relation and the relevant entities displayed on TeamTat ( ). \n  \nTo assess the challenges of BioRED, we performed benchmarking experiments with several state-of-the-art methods, including Bidirectional Encoder Representations from Transformers (BERT)-based models. We find that existing deep-learning systems perform well on the NER task but only modestly on the novel RE task, leaving it an open problem for future NLP research. Furthermore, the detailed analysis of the results confirms the benefit of using such a rich dataset toward creating more accurate, efficient and robust RE systems in biomedicine. \n\n\n## Overviews of NER/NEL/RE datasets \n  \n### Named entity recognition and named entity linking \n  \nExisting NER/NEL datasets cover most of the key biomedical entities, including gene/proteins [ ], chemicals [ ,  ], diseases [ ,  ], variants [ ], species [ ,  ] and cell lines [ ]. Nonetheless, NER/NEL datasets usually focus on only one concept type; the very few datasets that annotate multiple concept types [ ,  ] do not contain relation annotations.   summarizes some widely used gold standard NER/NEL datasets, including the annotation entity type, corpus size and the task applications. \n  \nOverview of gold standard NER/NEL datasets \n  \nDue to the limitation of the entity type in NER datasets, most of the state-of-the-art entity taggers were developed individually for a specific concept. A few studies (e.g. PubTator [ ]) integrate multiple entity taggers and apply them to specific collections or even to the entire PubMed/PubMed Central (PMC). In the development process, some challenging issues related to integrating entities from multiple taggers, such as concept ambiguity and variation emerged [ ]. Moreover, the same articles need to be processed multiple times by multiple taggers. Huge storage space also is required to store the results of the taggers. In addition, based on clues from previous NER studies [ ,  ], we realized that a tagger trained with other concepts performs as well or even better than a tagger trained on only a single concept, especially for highly ambiguous concepts. A gene tagger GNormPlus trained on multiple relevant concepts (gene/family/domain) boosts the performance of a gene/protein significantly. Therefore, a rich NER corpus can help develop a method that can recognize multiple entities simultaneously to reduce the hardware requirement and achieve better performance. Only a very few datasets [ ,  ] curate multiple concepts in the text, but no relation is curated in these datasets. \n\n\n### Relation extraction \n  \nA variety of RE datasets in the general domain have been constructed to promote the development of RE systems [ ]. Many of the RE datasets focus on extracting relations from a single sentence. Since many relations cross sentence boundaries, moving research from the sentence level to the document level (e.g. DocRED [ ], DocOIE [ ]) became a popular trend recently. In the biomedical domain, most existing RE datasets [ ,  ,  ] focus on sentence-level relations involving a single pair of entities. However, multiple sentences are often required to describe an entire biological process or a relation. We highlight several commonly used biomedical RE datasets in   (a complete dataset review can be found in  , see Supplementary Data available online at  ). But only very few datasets contain relations across multiple sentences (e.g. BC5CDR dataset [ ]). Most of the datasets [ ,  ], which were widely used for the RE system development [ ], focus on the single entity pair only (e.g. AIMed [ ] to protein\u2013protein interaction). Some of those datasets annotated the relation categories more granular. For example, DDI13 [ ] annotated 4 categories (i.e. advise, int, effect and mechanism) of the DDI, ChemProt [ ] annotated 5 categories of the chemical\u2013protein interaction and DrugProt [ ], an extension of ChemProt, annotated 13 categories. Recently, ChemProt and DDI13 are widely used in evaluating the abilities of biomedical pretrained language models [ ] on RE tasks. \n  \nA summary of biomedical RE and event extraction datasets. The value of \u2018-\u2019 means that we could not find the number in their papers or websites. The SEN/DOC Level means whether the relation annotation is annotated in \u2018Sentence\u2019, \u2018Document\u2019 or \u2018Cross-sentence\u2019. \u2018Document\u2019 includes abstract, full-text or discharge record. \u2018Cross-sentence\u2019 allows two entities within a relation to appear in three surrounding sentences \n  \nDuring the curation of the relations at the sentence level, curators usually do not access the context of the surrounding sentences. Besides, most sentence-level RE datasets do not link the entity names to the concept identifiers (e.g. NCBI Gene ID) in the external resources/databases. Instead, the RE dataset development at the document level relies highly on the concept identifiers. But it is extremely time-consuming, and very limited biomedical datasets annotate the related entities to the concept identifiers. BC5CDR dataset [ ] is a widely used dataset with chemical-induced disease relations at the document level. All of the chemicals and diseases are linked to the concept identifiers. However, BC5CDR did not annotate the relations (e.g. treatment) out of the chemical-induced disease category. Peng   et al  . [ ] developed a cross-sentence n-ary relation extraction dataset with the relations among drug, gene and mutation. But the dataset is constructed via distant supervision with the inevitable wrong labeling problem [ ] instead of manual curation. Moreover, BioNLP shared task datasets [ ] provide fine-grained biological event annotations to promote biological activity extraction. In  , we compare BioRED to representative biomedical relation extraction datasets. BioRED covers more types of entity pairs than those datasets. \n  \nComparison of the BioRED corpus with representative relation extraction datasets \n  \nD\u2009=\u2009Disease, G\u2009=\u2009Gene, C\u2009=\u2009Chemical and V\u2009=\u2009Variant. \n  \n\n\n## Methods \n  \n### Annotation definition/scope \n  \nWe first analyzed a set of public PubMed search queries by tagging different entities and relations. This data-driven approach allowed us to determine a set of key entities and relations of interest that should be most representative, and therefore the focus of this work. Some entities are closely related biologically and are thus used interchangeably in this work. For instance, protein, mRNA and some other gene products typically share the same names and symbols. Thus, we merged them to a single gene class, and similarly merged symptoms and syndromes to a single disease class. In the end, we have six concept types: (i) Gene: for genes, proteins, mRNA and other gene products; (ii) Chemical: for chemicals and drugs; (iii) Disease: for diseases, symptoms and some disease-related phenotypes; (iv) Variant: for genomic/protein variants (including substitutions, deletions, insertions and others); (v) Species: for species in the hierarchical taxonomy of organisms and (vi) CellLine: for cell lines. Due to the critical problems of term variation and ambiguity, entity linking (also called entity normalization) is also required. We linked the entity spans to specific identifiers in an appropriate database or controlled vocabulary for each entity type (e.g. NCBI Gene ID for genes). \n\nBetween any of two different entity types, we further observed eight popular associations that are frequently discussed in the literature: <D,G\u2009>\u2009for <Disease, Gene>; <D,C\u2009>\u2009for <Disease, Chemical>; <G,C\u2009>\u2009for <Gene, Chemical>; <G,G\u2009>\u2009for <Gene, Gene>; <D,V\u2009>\u2009for <Disease, Variant>; <C,V\u2009>\u2009for <Chemical, Variant>; <C,C\u2009>\u2009for <Chemical, Chemical> and\u2009<\u2009V,V\u2009>\u2009for <Variant, Variant>. For relations between more than two entities, we simplified the relation to multiple relation pairs. For example, we simplified the chemicals co-treat disease relation (\u2018bortezomib and dexamethasone co-treat multiple myeloma\u2019) to three relations: <bortezomib, multiple myeloma, treatment>, <dexamethasone, multiple myeloma, treatment> and\u2009<\u2009bortezomib, dexamethasone, cotreatment> (treatment is categorized in the Negative_Correlation). Other associations between two concepts are either implicit (e.g. variants frequently located within a gene) or rarely discussed. Accordingly, in this work we focus on annotating those eight concept pairs, as shown in solid lines in  . To further characterize relations between entity pairs, we used eight biologically meaningful and nondirectional relation types (e.g. positive correlation; negative correlation) in our corpus as shown in  . The details of the relation types are described in our annotation guidelines. \n  \nRelations annotated in BioRED corpus. (  A  ) Categorized relations between concepts. The patterns of the lines between the concepts present the categories: (  \u2014  ) Popular associations: The concept pairs are frequently discussed in the biomedical literature. (\u2550\u2550) Implied associations, e.g. the name of a gene can imply the corresponding species. (-----) Rarely discussed associations: Some other relation types are rarely discussed in the biomedical text (and this is why the concept of Cell Line is not listed here). (  B  ) The mapping between the concept pairs and the relation types. The frame widths of the concept pairs/relation types and the bold lines between the two sides proportionally represent the frequencies. \n  \n\n### Annotation process \n  \nTo be consistent with previous annotation efforts, we randomly sampled articles from several existing datasets (i.e. NCBI Disease [ ], NLM-Gene [ ], GNormPlus [ ], BC5CDR [ ] and tmVar [ ,  ]). A small set of PubMed articles were first used to develop our annotation guidelines and familiarize our annotators with both the task and TeamTat [ ], a web-based annotation tool equipped to manage team annotation projects efficiently. Following previous practice in biomedical corpus development, we developed our annotation guidelines and selected PubMed articles consistently with previous studies. Furthermore, to accelerate entity annotation, we used previous annotations combined with automated preannotations (i.e. PubTator [ ]), which can then be edited based on human judgment. Unlike entity annotation, each relation is annotated from scratch by hand with an appropriate relation type, except the chemical-induced-disease relations that were previously annotated in BC5CDR. \n\nEvery article in the corpus was first annotated by three annotators with a background in biomedical informatics to prevent erroneous and incomplete annotations (especially relations) due to manual annotation fatigue. If an entity or a relation cannot be agreed upon by the three annotators, this annotation was then reviewed by another senior annotator with a background in molecular biology. For each relation, two additional biologists assessed whether it is a novel finding versus background information and made the annotation accordingly. We annotated the entire set of 600 abstracts in 30 batches of 20 articles each. For each batch, it takes approximately 2\u00a0h per annotator to annotate entities, 8\u00a0h for relations and 6\u00a0h for assigning novel versus background label. The details of the data sampling and annotation rules are described in our annotation guidelines. \n\n\n### Data characteristics \n  \nThe BioRED corpus contains a total of 20\u00a0419 entity mentions, corresponding to 3869 unique concept identifiers. We annotated 6503 relations in total. The proportion of novel relations among all annotated relations in the corpus is 69%.   shows the numbers of the entities (mentions and identifiers) and relations in the training, development and test sets. \n  \nNumber of entity (mention and identifier) and relation annotations in the BioRED corpus, the IAA and the distribution between the training, development and test sets. The parenthesized numbers are the unique entities linked with concept identifiers. \n  \nIn addition, we computed the inter-annotator-agreement (IAA) for the entity, relation and novelty annotations, where we achieved 97.01, 77.91 and 85.01%, respectively.   depicts the distribution of the different concept pairs in the relations. \n  \nThe distribution of concept pairs and relation types in the BioRED corpus. \n  \nWe also analyzed dataset statistics per document. The average document length consists of 11.9 sentences or 304 tokens. Furthermore, 34 entity spans (3.8 unique entity identifiers) and 10.8 relations are annotated per document. Among the relation types, 52% are associations, 27% are positive correlations, 17% are negative correlations and 2% are involved in the triple relations (e.g. two chemicals co-treat a disease). \n\n\n### Benchmarking methods \n  \nTo assess the utility and challenges of the BioRED corpus, we conducted experiments to show the performance of leading RE models. For the NER task, each mention span was considered separately. We evaluate three state-of-the-art NER models on the corpus including bidirectional long short-term memory-conditional random field (BiLSTM-CRF), BioBERT-CRF and PubMedBERT-CRF. The input documents are first to split into multiple sentences and encoded into a hidden state vector sequence by BiLSTM [ ], BioBERT [ ] and PubMedBERT [ ], respectively. The models predicted the label corresponding to each of the input tokens in the sequence, then computed the network score using a fully connected layer and decode the best path of the tags in all possible paths by using CRF [ ]. Here, we used the BIO (Begin, Inside, Outside) tagging scheme for the CRF layer. \n\nWe chose two BERT-based models, BERT-GT [ ] and PubMedBERT [ ], for evaluating the performance of current RE systems on the BioRED corpus. The first model is BERT-GT, which defines a graph transformer by integrating a neighbor\u2013attention mechanism into the BERT architecture to avoid the effect of the noise from the longer text. BERT-GT was specifically designed for document-level relation extraction tasks and utilizes the entire sentence or passage to calculate the attention of the current token, which brings significant improvement to the original BERT model. PubMedBERT is a trained biomedical language model based on transformer architecture. It is currently a state-of-the-art text-mining method, which applies the biomedical domain knowledge (biomedical text and vocabulary) for the BERT pretrained language model. In the benchmarking, we used the text classification framework for the RE model development. \n\nFor both NER and RE evaluations, the training and development sets were first used for model development and parameter optimization before a trained model is evaluated on the test set. Benchmark implementation details are provided in  . Standard precision, recall and   F  -score metrics are used. To allow approximate entity matching, we also applied relaxed versions of the   F  -score to evaluate NER. In this case, as long as the boundary of the predicted entity overlaps with the gold standard span, it is considered as a successful prediction. \n\n\n\n## Results \n  \n### NER results on the test set \n  \n shows the evaluation of NER on the test set. The first run is evaluated by strict metrics. The concept type and boundary of the entity should exactly match the entity in the text. The second run is evaluated by relaxed metrics: The entity boundary should overlap, and the same entity type is required. Unlike BiLSTM-CRF, the BERT-based methods contain well pretrained language models for extracting richer features, hence achieving better performance overall. Further, PubMedBERT performs even better than BioBERT on genes, variants and cell lines. BioBERT uses the original BERT model\u2019s vocabulary generated from general domain text, which causes a lack of understanding of the biomedical entities. On the contrary, PubMedBERT generates the vocabulary from scratch using biomedical text, and it achieves the highest   F  -score (89.3% in strict metrics). Among these entity types, the PubMedBERT-CRF achieves the highest performance of 97% in F1 score to species entity recognition as less term ambiguity and variation issues are found in species names. \n  \nPerformance of NER models on test set. All numbers are   F  -scores. \n  \n\n### RE results on the test set \n  \nWe also evaluated performance on the RE task by different benchmark schemas: (i) entity pair: to extract the pair of concept identifiers within the relation, (ii) entity pair + relation type: to recognize the specific relation type for the extracted pairs and (iii) entity pair + relation type + novelty: to further label the novelty for the extracted pairs. In this task, the gold-standard concepts in the articles are given. We applied BERT-GT and PubMedBERT to recognize the relations and the novelty in the test set. \n\nAs shown in  , the overall performance of PubMedBERT is higher than that of BERT-GT in all schemas. Because the numbers of relations in <D,V>, <C,V>\u2009and\u2009<V,V>\u2009are low, their performance is not comparable to that of other concept pairs, especially <V,V>\u2009(the   F  -score is 0% for two models). In the first schema, BERT-GT and PubMedBERT can achieve performance above 72% for the   F  -scores, which is expected and promising in the document-level RE task. To predict the relation types (e.g. positive correlation) other than entity pairs, however, is still quite challenging. The best performance on the second schema is only 58.9%, as the number of instances in many relation types is insufficient. The performances of different relation types of our best model using PubMedBert are provided in  . The performance on the third schema dropped to 47.7%. In some cases, the statements of the relations in abstracts are usually concise, and the details of the related mechanism can only be found in the full text. \n  \nPerformance on RE task for the first schema: extracting the entity pairs within a relation, second schema: extracting the entity pairs and the relation type and the third schema: further labeling the novelty for the extracted pairs. All numbers are   F  -scores. The <G,D\u2009>\u2009is the concept pair of the gene (G) and the disease (D). The columns of those entity pairs present the RE performance in   F  -scores. \n  \nG\u2009=\u2009gene, D\u2009=\u2009disease, V\u2009=\u2009variant and C\u2009=\u2009chemical. \n  \n\n### Benefits of multiple entity recognition and relation extraction \n  \nTo test the hypothesis that our corpus can result in a single model with better performance, we trained multiple separate NER and RE models, each with an individual concept (e.g. gene) or relation (e.g. gene\u2013gene) for comparison. We used PubMedBERT for this evaluation since it achieved the best performances in both the NER and RE tasks. As shown in  , both models trained on all entities or relations generally perform better than the models trained on most of the entities or relations, while the improvement for RE is generally larger. The performance on NER and RE tasks is both obviously higher in the single model. Especially for entities and relations (e.g. cell lines and chemical\u2013chemical relations) with insufficient amounts, the model trained on multiple concepts/relations can obtain larger improvements. The experiment demonstrated that training NER/RE models with more relevant concepts or relations not only can reduce resource usage but also can achieve better performance. \n  \nThe comparison of the models trained on all entities/relations to the models trained on individual entity/relation. The <G,D\u2009>\u2009is the relation of the gene (G) and the disease (D). All models are evaluated by strict metrics. \n  \nG\u2009=\u2009gene, D\u2009=\u2009disease, C\u2009=\u2009chemical, V\u2009=\u2009variant, S\u2009=\u2009species and CL\u2009=\u2009cell line. \n  \n\n\n## Discussion \n  \nThe relaxed NER results in   for overall entity type are over 92% for all methods, suggesting the maturity of current tools for this task. If considering the performance of each concept individually, the recognition of genes, species and cell lines can reach higher performance (over 90% in strict   F  -score) since the names are often simpler and less ambiguous than other concepts. The best model for genomic variants achieves an   F  -score of 87.3% in strict metrics and 94.5% in relaxed metrics, which suggests that the majority of the errors are due to incorrect span boundaries. Most variants are not described in accordance with standard nomenclature (e.g. \u2018ACG\u2014\u2009>\u2009AAG substitution in codon 420\u2019), thus it is difficult to exactly identify the boundaries. Similar to genomic variants, diseases are difficult to be identified due to term variability and most errors are caused by mismatched boundaries. For example, our method recognized a part (\u2018papilledema\u2019) of a disease mentioned (\u2018bilateral papilledema\u2019) in the text. Disease names also present greater diversity than other concepts: 55.4% of the disease names in the test set are not present in the training/development sets. Chemical names are extremely ambiguous with other concepts: half of the errors for chemicals are incorrectly labeled as other concept types (e.g. gene), since some chemicals are interchangeable with other concepts, such as proteins and drugs. Moreover, we merged the annotations matched by the dictionary to the results of the PubMedBERT-CRF model. However, the performance of the dictionary method heavily depends on the difficulties of the term variation and ambiguity issues. Especially, there are many ambiguous terms in the dictionary, such as \u2018B1\u2019, \u2018Beta\u2019 and \u201898\u20134.9\u2019 in Cellosaurus. Although the F1 score of the dictionary cannot compete with the machine learning method, merging the results from both methods can improve the recall of all the concepts (see details in  ). \n\nExperimental results in   show that the RE task remains challenging in biomedicine, especially for the new task of extracting novel findings. In our observation, there are three types of errors in novelty identification. First, some abstracts do not indicate which concept pairs represent novel findings, and instead, provide more details in the full text. Such cases confused both the human annotators and the computer algorithms. Second, when the mechanism of interaction between two relevant entities is unknown, and the study aims to investigate it but the hypothesized mechanism is shown to be false. Third, the authors frequently mention relevant background knowledge within their conclusion. As an example, \u2018We conclude that Rg1 may significantly improve the spatial learning capacity impaired by chronic morphine administration and restore the morphine-inhibited LTP. This effect is NMDA receptor-dependent.\u2019 in the conclusion of the PMID:18308784, the Rg1 responded to morphine as background knowledge. But it is mentioned together with the novelty knowledge pair <Rg1, NMDA receptor>. In this case, our method misclassified the pair < Rg1, morphine> as novel. We also conducted an experiment to evaluate the effect of section information for novelty detection. The experimental results show that the structured section information (e.g. TITLE, PURPOSE, METHODS, RESULTS, \u2026 ) can be useful for novelty classification by boosting the best F1 score from 47.7% to 48.9% (see details in  ). However, this result was obtained on a subset of 191 abstracts with structured section information due to limited availability. \n\nThe results in   demonstrate that training NER/RE models on one rich dataset with multiple concepts/relations simultaneously can not only make the trained model simpler and more efficient, but also more accurate. More importantly, we notice that for the entities and relations with a lower number of training instances (e.g. cell lines and chemical\u2013chemical relations), simultaneous prediction is especially beneficial for improving performance. Additionally, merging entity results from different models often poses some challenges, such as ambiguity or overlapping boundaries between different concepts. \n\n\n## Conclusion \n  \nIn the past, biomedical RE datasets were typically built for a single entity type or relation. To enable the development of RE tools that can accurately recognize multiple concepts and their relations in biomedical texts, we have developed BioRED, a high-quality RE corpus, with one-of-a-kind novelty annotations. Similar to other commonly used biomedical datasets, e.g., BC5CDR [ ], we expect BioRED to serve as a benchmark for not only biomedical-specific NLP tools but also for the development of RE methods in the general domain. Additionally, the novelty annotation in BioRED proposes a new NLP task that is critical for information extraction in practical applications. Recently, the dataset was successfully used by the NIH LitCoin NLP Challenge ( ) and a total of over 200 teams participated in the Challenge. \n\nThis work has implications for several real-world use cases in medical information retrieval, data curation and knowledge discovery. Semantic search has been commonly practiced in the general domain but much less so in biomedicine. For instance, several existing studies retrieve articles based on the co-occurrence of two entities [ ] or rank search results by co-occurrence frequency. Our work could accelerate the development of semantic search engines in medicine. Based on the extracted relations within documents, search engines can semantically identify articles by two entities with relations (e.g. 5-FU-induced cardiotoxicity) or by expanding the user queries from an entity (e.g. 5-FU) to the combination of the entity and other relevant entities (e.g. cardiotoxicity, diarrhea). \n\nWhile BioRED is a novel and high-quality dataset, it has a few limitations. First, we are only able to include 600 abstracts in the BioRED corpus due to the prohibitive cost of manual annotation and limited resources. Nonetheless, our experiments show that except for a few concept pairs and relation types (e.g. variant\u2013variant relations) that occur infrequently in the literature, its current size is appropriate for building RE models. Our experimental results in   also show that in some cases, the performance of entity class with a small number of training instances (e.g. Cell Line) can be significantly boosted when training together with other entities in one corpus. Second, the current corpus is developed on PubMed abstracts, as opposed to the full text. While full text contains more information, data access remains challenging in real-world settings. More investigation is warranted on this topic in the future. \n \n### Key Points \n    \nFirst review on publicly available biomedical named entity recognition and relation extraction (RE) datasets. \n  \nWe present a first-of-its-kind biomedical relation extraction dataset (BioRED) with multiple entity types and relation pairs at the document level. \n  \nThe novelty RE task is proposed to differentiate between a novel finding or previously known background knowledge. \n  \nSeveral cutting-edge deep learning models are evaluated on BioRED, and results show that there is much room for improvement for the RE task. \n  \n \n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 9487702, "text_md5": "8633126316b16eecc7554f04121884b0", "field_positions": {"authors": [0, 84], "journal": [85, 100], "publication_year": [102, 106], "title": [117, 170], "keywords": [184, 287], "abstract": [300, 1947], "body": [1956, 32631]}, "batch": 1, "pmid": 35849818, "doi": "10.1093/bib/bbac282", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9487702", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9487702"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9487702\">9487702</a>", "list_title": "PMC9487702  BioRED: a rich biomedical relation extraction dataset"}
{"text": "Schaeffer, Camille and Interdonato, Roberto and Lancelot, Renaud and Roche, Mathieu and Teisseire, Maguelonne\nData Brief, 2022\n\n# Title\n\nLabeled entities from social media data related to avian influenza disease\n\n# Keywords\n\nText mining\nNamed entity recognition\nSocial networks\nEpidemiology\nAvian influenza\n\n\n# Abstract\n \nThis dataset is composed by spatial (e.g. location) and thematic (e.g. diseases, symptoms, virus) entities concerning avian influenza in social media (textual) data in English. It was created from three corpora: the first one includes 10 transcriptions of YouTube videos and 70 tweets manually annotated. The second corpus is composed by the same textual data but automatically annotated with Named Entity Recognition (NER) tools. These two corpora have been built to evaluate NER tools and apply them to a bigger corpus. The third corpus is composed of 100 YouTube transcriptions automatically annotated with NER tools. The aim of the annotation task is to recognize spatial information such as the names of the cities and epidemiological information such as the names of the diseases. An annotation guideline is provided in order to ensure a unified annotation and to help the annotators. This dataset can be used to train or evaluate Natural Language Processing (NLP) approaches such as specialized entity recognition. \n \n\n# Body\n \n## Specification Table \n  \n  \n\n\n## Value of the Data \n  \n  \nThis dataset contributes to the available resources for Natural Language Processing (NLP) on specialized domains and more precisely in the field of epidemiology. \n  \nThis dataset is useful for computer scientists for NLP and data mining tasks. \n  \nThis dataset can be used for evaluation or training purposes for the entity recognition task. \n  \nThe annotators have identified a variety of entities (e.g. diseases, symptoms, virus, hosts). These entities are relevant to recognize epidemiological information. \n  \n\n\n## Data Description \n  \nAfter an extraction process from YouTube and Twitter (i.e. primary data sources), the dataset is constituted of five table data files (.tab) normalized (i.e. secondary data sources) and annotated (i.e. final data sources). One annotation guide (.pdf) details the instruction to the annotators as well as the choices that were made while designing the study. \n\nThe python codes to reproduce the transformation of the documents for the annotation step is available on github. \n\nThe five data files are distributed as follows: \n\n### Manual annotation of spatial and thematic entities - Corpus 1 (small) \n  \n  \nA table data file containing the YouTube transcription data, manually annotated, from corpus 1, named corpus 1Y (datafile_1Y_manual_annotation.tab); \n  \nA table data file containing the Twitter data, manually annotated, from corpus 1, named corpus 1T (datafile_1T_manual_annotation.tab). \n  \n\n\n### Automatic annotation of spatial and thematic entities - Corpus 2 (small) \n  \n  \nA table data file containing the YouTube transcription data, automatically annotated, from corpus 2, named corpus 2Y (datafile_2Y_automatic_annotation.tab); \n  \nA table data file containing the Twitter data, automatically annotated, from corpus 2, named corpus 2Y (datafile_2T_automatic_annotation.tab). \n  \n\n\n### Automatic annotation of spatial and thematic entities - Corpus 3 (big) \n  \n  \nA table data file containing the transcriptions YouTube data annotated automatically from corpus 3, named corpus 3Y (datafile_3Y_automatic_annotation.tab). \n  \n\nThe files from the three corpora include data organized in tables. An example of the YouTube transcription data row is given in   and an example of the Twitter data row is reported in  .   \nExample of a row in the transcriptions YouTube data files. \n  Table 1     \nExample of a row in the Twitter data files. \n  Table 2   \n\n The data are described through a set of features from YouTube transcription data of the   three corpora:    \nid: the id of the data on the social media; \n  \npublication_date: the date of publication of the data on the social media; \n  \nraw_text: the raw text (no transformation) of the data; \n  \nnormalized_text: the normalized text (after applying transformations) of the data; \n  \nspatial_entity: A list of spatial entities annotated, with their labels (GPE, LOC, FAC); \n  \nthematic_entity: A list of annotated thematic entities, with their labels (Disease or Syndrome, Virus, Sign or Symptom). \n  \n\n The data are described through a set of features from Twitter data of both corpora:    \nid: the id of the data on the social media; \n  \npublication_date: the date of publication of the data on the social media; \n  \nspatial_entity: A list of annotated spatial entities, with its label (GPE, LOC, FAC); \n  \nthematic_entity: A list of annotated thematic entities, with its label (Disease or Syndrome, Virus, Sign or Symptom). \n  \n\nThe annotation guideline (annotation_guide_spatial_thematic_entities.pdf) presents the instructions to the annotators. These instructions and the choices made are summarized in the next section. The annotation framework defines several tags to annotate the texts. \n\nSpatial concepts are annotated with three tags:   \n GPE   (Geopolitical entity) is used to annotate entities representing countries, cities, states etc; \n  \n LOC   (Non-GPE locations) is used to annotate entities representing mountain ranges, bodies of water, etc; \n  \n FAC   (Faculty) is used to annotate entities representing buildings, airports, highways, bridges, etc. \n  \n\nThematic concepts are annotated with three tags:   \n Disease or Syndrome   is used to annotate entities representing a disease or a syndrome; \n  \n Virus   is used to annotate entities representing a virus; \n  \n Sign or Symptom   is used to annotate entities representing a sign or symptom. \n  \n\nNumber and distribution of annotated information in the corpora are given in   and  .   \nDistribution of the data in the dataset files Corpus 1 (i.e. manual annotation). \n  Table 3           \nDistribution of the data in the dataset files Corpus 2 (i.e. automatic annotation). \n  Table 4           \nDistribution of the data in the dataset files Corpus 3 - larger corpus and only automatic annotation. \n  Table 5   \n\n\n\n## Experimental Design, Materials and Methods \n  \n### Acquisition and pre-processing \n  \nThe corpora were obtained automatically from the platforms YouTube and Twitter, thanks to their dedicated APIs. The texts from the web were stored in.txt files, with the aim to obtain distinct files to annotate. \n\nThe pre-processing consists of four main steps applied on the raw corpora, detailed below. First, in order to optimize the recognition of entities, the raw text of these files has been normalized with the automatic addition of punctuation by using the python library punctuator  and with the automatic addition of capital letters, by using the POS tagging provided by the python library SpaCy . \n\nThen, the text has been cleaned by deleting non Unicode symbols and eventual noise originated by the transcription process. As a final normalization step of the raw text, a correction of specific terms (e.g. disease names) is applied by using regular expressions  . \n\nAfter the normalization task, the manual and automatic annotation of the spatial and thematic entities can be performed on the text. \n\n\n### Manual data labeling \n  \nIn order to have a unified annotation of the spatial and thematic entities, a guideline was created. This annotation guide was written by a specialist in NLP. Three other persons (specialists in epidemiology) validated and adjusted these choices. By following this guide, one person (NLP specialist) annotated manually the spatial entities on the raw text data of 70 tweets and 10 transcriptions. In the same way, one person (data scientist specialized in epidemiology) manually annotated the thematic entities on the same data. This process results in the corpus 1 (1Y and 1T) and represents the ground truth data. \n\n\n### Automatic data labeling \n  \nIn parallel, the spatial and thematic entities were automatically annotated on the same raw text data (70 tweets and 10 transcriptions) with NER tools. We applied SpaCy   on the data to recognize the spatial entities and SciSpaCy   combined with UMLS   in order to recognize the thematic entities. The result of this process is the corpus 2 (2T and 2Y). \n\nOnce these two annotations (manual and automatic) were performed, we evaluated the automatic recognition of the spatial and thematic entities on corpus 2, by comparing the annotation results of the corpus 1 and the corpus 2, and measured the evaluation with three metrics: Precision, Recall and F-Measure. \n\nThe evaluation is presented in  . The Precision value of spatial and thematic entity recognition is quite good, with a result of 0.7 and 0.6 respectively. The Recall for automatic recognition of thematic entities is low (0.2), the tool does not recognize many thematic entities that are not included in the thesaurus associated with SciSpaCy.   \nEvaluation of the automatic recognition of spatial and thematic entities on corpus 1 \n  Table 6   \n\nWe then apply the processes of   Acquisition and pre-processing   and   Automatic data labeling   on a larger corpus (corpus 3) called 3Y (100 transcriptions). This enables to check the scalability of the proposed process. \n\nAll text related information such as copyright, texts, authors list or references were discarded. \n\nThe dataset is composed of the extracted annotations. The extraction program, available in the dataset, preserves the text and adds the textual entities and their annotated categories at the end of the text. \n\n\n\n## Ethics Statement \n  \nFor Twitter data, we keep the message identifiers and the content of message is not stored in accordance to Twitter\u2019s terms of use. The content of YouTube data (i.e. transcriptions) is anonymized by removing the user names and the names of person in transcripts (using SpaCy for name recognition). \n\nNo conflict of interest exists in this submission. The authors declare that the work described in this paper is original and not under consideration for publication elsewhere, in whole or in part. Its publication is approved by all the authors listed. \n\n\n## CRediT authorship contribution statement \n  \n Camille Schaeffer:   Methodology, Software, Investigation, Resources, Data curation, Writing \u2013 original draft.   Roberto Interdonato:   Methodology, Writing \u2013 review & editing.   Renaud Lancelot:   Methodology, Writing \u2013 review & editing.   Mathieu Roche:   Methodology, Writing \u2013 review & editing.   Maguelonne Teisseire:   Methodology, Writing \u2013 review & editing. \n\n\n## Declaration of Competing Interest \n  \nThe authors declare that they have no financial or personal interests that could influence the work reported in this paper. \n\n \n", "metadata": {"pmcid": 9184875, "text_md5": "ca78b080c0c3ae7a8d727e74be31cb27", "field_positions": {"authors": [0, 109], "journal": [110, 120], "publication_year": [122, 126], "title": [137, 211], "keywords": [225, 307], "abstract": [320, 1346], "body": [1355, 10803]}, "batch": 1, "pmid": 35692611, "doi": "10.1016/j.dib.2022.108317", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9184875", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9184875"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9184875\">9184875</a>", "list_title": "PMC9184875  Labeled entities from social media data related to avian influenza disease"}
{"text": "Reddy, Sandeep and Bhaskar, Ravi and Padmanabhan, Sandosh and Verspoor, Karin and Mamillapalli, Chaitanya and Lahoti, Rani and Makinen, Ville-Petteri and Pradhan, Smitan and Kushwah, Puru and Sinha, Saumya\nComput Methods Programs Biomed Update, 2021\n\n# Title\n\nUse and validation of text mining and cluster algorithms to derive insights from Corona Virus Disease-2019 (COVID-19) medical literature\n\n# Keywords\n\nCOVID-19\nText Mining\nCluster Algorithms\nBiomedical NLP\n\n\n# Abstract\n \nThe emergence of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) late last year has not only led to the world-wide coronavirus disease 2019 (COVID-19) pandemic but also a deluge of biomedical literature. Following the release of the COVID-19 open research dataset (CORD-19) comprising over 200,000 scholarly articles, we a multi-disciplinary team of data scientists, clinicians, medical researchers and software engineers developed an innovative natural language processing (NLP) platform that combines an advanced search engine with a biomedical named entity recognition extraction package. In particular, the platform was developed to extract information relating to clinical risk factors for COVID-19 by presenting the results in a cluster format to support knowledge discovery. Here we describe the principles behind the development, the model and the results we obtained. \n \n\n# Body\n \n## Introduction \n  \nThe first coronavirus disease 2019 (COVID-19) cases emerged in Wuhan city in China as a result of infection from severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).  Since then the disease has spread across the world resulting in a pandemic.  To contain the disease, there is a world-wide effort to study the virus and devise treatments and vaccine candidates.  Clinicians and researchers are in urgent need of rapid and quality information that will help them to inform diagnostics and therapeutics relating to the disease. However, the volume of studies concerning COVID-19 has substantially increased since the beginning of 2020 ( \n).  When you couple these studies with pre-existing studies relating to other coronaviruses, immunology, genomics, proteomics and therapeutics the volume increases markedly.   \nIncrease in pre-print and peer reviewed publications relating to COVID-19 February-May 2020 (Source: NIH, 2020). \n  Fig 1   \n\nWhile not everything in this collection of medical literature is of proven value, it is nevertheless an essential source of information in the pandemic context.  In this context, researchers and clinicians require a reliable approach to mining published literature for novel insights, emerging risk factors and therapeutics to inform their work in combating the COVID-19 pandemic. Biomedical natural language processing (NLP) or text mining can help mine useful information and knowledge from large volumes of biomedical literature.  The objective of text mining is to derive implicit knowledge that hides in such literature and present it in an explicit form. In this context, we offer an innovative text mining and analytical tool that will aid clinicians and researchers in extracting valuable insights from large datasets of literature. In the following sections, we describe the principles, techniques and results of our approach. \n\n\n## Methods \n  \nThe first step in our text mining process was information retrieval (IR). The dataset we relied on to retrieve information in this step was the COVID-19 Open Research Dataset (CORD-19), which is a resource of over 60,000 scholarly articles, including over 47,000 publications covering subjects such as COVID-19 and coronavirus related research.  The CORD-19 dataset was initially made available for machine learning practitioners as an open-access database by a coalition including the White House, National Institutes of Health, and other partners but is now maintained by the Allen Institute for AI. The dataset gets updated weekly, and we last retrieved the dataset, to assess the utility of our model, in the first week of January 2021. \n\nAs part of the pre-processing seven sections in the full text of the publications were indexed for information retrieval including 'Abstract', 'Introduction', 'Background', 'Discussion', 'Results', 'Results and Discussion' and 'Methods'. The sections were established through an internal consultation process with the medical researchers in the team. Solr, an open-source enterprise search platform, was employed as a foundational query search engine for information extraction. Solr, uses okapi BM25 a ranking function used by search engines and provides distributed indexing and load-balanced querying, which suited our objectives. \n\nThe input for the search engine was the CORD-19 dataset, with files in JavaScript Object Notation Format. The search engine allowed both free text and Boolean queries, while allowing unlimited queries. Written into the code was a process to eliminate duplicates of the retrieved information like for example similar abstracts. Weighted queries ensured only the most relevant articles were retrieved. The search platform was enjoined with scispaCy, a package tailored to identify within the search results biomedical or clinical terms.  scispaCy builds on the Python-based spaCy library, which has a number of tools to aid text processing in multiple languages. scispaCY customises these tools to support primary text processing requirements for the biomedical, scientific and clinical areas. Initially, key terms relating to COVID-19 including all its associated synonyms were built into the search process to filter results. The search process then incorporated the scispaCy model to filter the results further and extract the medical terms. \n\nKnowledge about the relations between biomedical factors are embedded in literature and extracting information about the relationships can be useful for medical research.  Following retrieval of results from the search process, there is a requisite to obtain clarity from the results, which necessitated us to explore various analytical approaches. It has been identified that clustering is a useful analytical approach for gaining insights from biomedical data mining as it enables dimensionality reduction and visualisation of similarity indices. Clustering is a core task in many machine learning and text mining approaches.  Clustering algorithms are known to be useful to explore large scale data.  Cluster analysis when applied provides insights regarding underlying data similarity structures. The clustering technique groups similar terms into the same cluster thus assisting with knowledge discovery. \n\nFor the purpose of this project, we used spectral clustering (SC), which is stated to achieve better results than generative models.  For dimensionality reduction and to organise retrieved results into clusters, we initially employed spectral embedding (SE). SE is generally employed for non-linear dimensionality reduction and works by forming an affinity matrix and applying spectral decomposition to the graph Laplacian. KNN algorithm was used to create heat kernels, which served as input for SE. SC uses the top eigenvectors of a matrix that is resulting from the distance between points. SC conserves linguistically motivated similarities. Also SC identifies communities of nodes in the dataset based on the edges connecting them.  Unlike traditional approaches, SC utilises proximity or connectivity to cluster data instead of distance or compactness. SC represents data as graphs where samples are vertices and the similarity between samples is represented as edge weights.  SC adopts an approach where given a data set X\u00a0=\u00a0{xi} , the purpose of SC is to separate X into c clusters. The cluster assignment matrix is represented by Y\u00a0=\u00a0[y1, y2, ..., yn]  \u2208 B  , where y  \u2208 B   (1 \u2264 i \u2264 n) is the assigned vector for the pattern xi. Following this, the j-th element of y  is represented as 1 and if the pattern xi is assigned to the j-th cluster it is 0. Thus, the main task of a clustering algorithm is to learn the cluster assignment matrix Y. \n\nTo enable a more comprehensive approach and a comparison of clustering approaches we also utilised agglomerative clustering (AC) in our project. AC or hierarchical clustering as it is also known is another technique used for undertaking exploratory data analysis.  AC commences with a large number of small clusters and through an iterative process selects two clusters with the greatest affinity until a stopping condition is attained.  In other words, a binary merge tree commencing from the data elements stored at the leaves, which proceeds by merging two by two the nearest subsets until the root of the tree is arrived at. AC is conceptually easy to understand and execute while producing an informative clustering approach. \n\nThe sequence of our biomedical NLP, which included information retrieval, entity recognition of relevant biomedical entities, medical content information extraction, knowledge discovery of latent relationships, data visualisation (described in the Results section), and validity measures (described in the Results section) is outlined in the  \n.   \nOur biomedical NLP sequence. \n  Fig 2   \n\n\n## Results \n  \nTo illustrate the utility of our biomedical NLP, we searched for \u2018lymphocytopenia ' and 'anosmia\u2019' terms from the CORD-19 dataset. Both lymphocytopenia and anosmia have been found to be associated with COVID-19   Since the beginning of the pandemic, lymphocyte count has been an important marker of clinical progression of the disease.  Numerous studies have now shown that lymphocytopenia is associated with poor clinical outcomes for COVID-19 patients. Another prominent sign that has been associated with COVID-19 patients is anosmia, which can present suddenly and sometimes without other symptoms.  While anosmia has been widely reported as a clinical sign, the pathogenic mechanism of olfactory dysfunction and it clinical characteristics is not clear. Therefore, we considered identifying literature pertaining to these two COVID-19 manifestations would showcase the usefulness of our biomedical data mining model to biomedical and clinical researchers. Outlined below are the representation of the data clusters of the representation including spectral and agglomerative cluster representation of the two terms \u2018Lymphocytopenia\u2019 and \u2018Anosmia\u2019 ( ,  ,  \n). In   and  , the papers relating to the terms are reflected as dots and in  , the cluster hierarchy for each search term is presented as a dendrogram (tree diagram).   \nSpectral and Agglomerative Cluster representation for search term \u2018Lymphocytopenia'. Each dot represents a paper relating to the term. \n  Fig 3     \nSpectral and Agglomerative Cluster representation and confusion matrix for search term 'Anosmia\u2019. Each dot represents a paper relating to the term. \n  Fig 4     \nDendrogram for agglomerative clustering of search terms \u2018Lymphocytopenia\u2019 and 'Anosmia\u2019 respectively. \n  Fig 5   \n\nValidity measures for clustering approaches can be internal and external.  For this project, we used internal measures as they only rely on information in the data unlike external validation measures, which need information external to the data. As we did not have ground true label of data, we relied just on internal clustering validation. For the quality metrics we used three internal validation measures: Silhouette, Calinski-Harabasz (CH) and Davies-Bouldin (DB) index scores ( \n). Internal validity measures quantify the quality of the clustering relying on properties intrinsic to the dataset.  They mathematically measure what a good clustering looks like. They also allow for the comparison of partition between clusters. Silhouette index authenticates the clustering performance based on the pairwise variance of between and within cluster distances.  By maximising the value of this index, the optimal cluster number can be determined. The CH index, also known as the Variance Ratio, assesses the validity of clusters based on the average between and within cluster sum of squares.  DB signifies the average similarity between clusters by comparing the distance between clusters with the size of the clusters. In other words it identifies clusters, which are far from each other and compact.   \nQuality metrics of dimensionality reduction and cluster coherence. \n  Table 1   \n\nThe Silhouette score is bound between -1 and +1 so the higher the score relates to better defined clusters. We notice for both search term \u2018lymphocytopenia\u2019 spectral clustering achieves slightly lower performance with Silhouette index than agglomerative clustering indicating that the clusters obtained are not relatively compact. On the other hand, we observed that agglomerative clustering creates tight clusters but well separated both for \u2018lymphocytopenia\u2019 and \u2018anosmia\u2019. The same pattern recurs for agglomerative clustering with CH scores, which scores dense but well separated clusters higher. Spectral clustering tries to maintain nearest neighbours in reduced dimensions causing the dilution of clusters but still achieving decent CH scores. For agglomerative clustering, DB scores are also better as it forms tight clusters trading off nearest neighbour similarity unlike spectral clustering. This is because DB emphasizes separation which becomes pronounced when inter-cluster distances are very low. \n\n\n## Discussion \n  \nInformation retrieval, in a text mining context, involves a user submitting a query to a search engine and receiving relevant results aligning with their submitted question in return.  In a pandemic context, information extraction from medical literature involves the identification of entities such as diseases, as well as the identification of complex relationships between these entities.  Query results extracted from the literature may be used to populate databases or data curation.  From these extractions, knowledge bases can be built that contain the collected statements with references to the literature. Knowledge discovery involves identifying undiscovered or hidden knowledge by applying data-mining algorithms to the collection of facts gathered from the literature. From here, text mining results may be used to suggest new hypotheses which can be used to either validate or disprove existing hypotheses or to help direct future research.  \n\nThe text-mining and clustering visualisation model we have developed assists with the knowledge discovery process by using clustering approaches and uncovering latent relationship between entities aiding researchers and clinicians in their pursuit of appropriate treatment and management of COVID-19 cases. This process is achieved by retrieving articles that mention relevant biomedical terms relating to COVID-19 and categorising them for their relevance to the clinical risk factors. The assumption here is that supplied databases like CORD-19 have relevant information suitable for extraction. A valuable aspect to the model is its information extraction process, which involves both a robust information retrieval engine and biomedical named entity recognition. Knowledge representation was achieved through dimensionality reduction and feature affinity clustering algorithms, which is a novel process not found generally in other biomedical NLP models. Further to this, the garnered findings are presented in visually aesthetic and a readable manner that ensure pertinent insights are conveyed to a broad spectrum of healthcare professionals. While the tool we developed here was customised to identify COVID19 related risk factors, this model can be potentially customised to extract other biomedical terms and assist with knowledge discovery. \n\n\n## Conclusion \n  \nTo develop the COVID-19 biomedical NLP, we brought together data scientists, software engineers, clinicians, and medical researchers to enable an informed approach and to develop a well-rounded knowledge extraction process. The multi-disciplinary effort emulates the real-world clinical inductive reasoning that utilises a stepped approach to evaluate, extract and prioritise insights to enable evidence-based medicine. \n\nWhile our biomedical NLP extracts pertinent entities from the biomedical literature and provides a high degree of cluster coherency, it has limitations associated with the biomedical named entity recognition library. This restricts the ability to extract all the necessary biomedical aligned entities. Biomedical concept recognition is an area of active research, and improved methods targeting a broad range of entity and concept types can be substituted. We demonstrated our project to frontline clinicians and the feedback was positive. This is in terms of being able to rapidly access accurate and appropriate information from the COVID infodemic in a visually insightful interface. The COVID-19 pandemic has created a unique situation where there is a need for rapid access to evolving clinical knowledge rapidly from the exponentially increasing publications of variable quality. We strongly believe because of these features the platform may support deeper investigation of the scientific literature related to COVID-19. \n\n\n## Declaration of competing interest \n  \nNone. \n\n \n", "metadata": {"pmcid": 8050406, "text_md5": "ae73c93e0f177d76c0cd33dfab84767b", "field_positions": {"authors": [0, 205], "journal": [206, 243], "publication_year": [245, 249], "title": [260, 396], "keywords": [410, 465], "abstract": [478, 1376], "body": [1385, 17250]}, "batch": 1, "pmid": 34337589, "doi": "10.1016/j.cmpbup.2021.100010", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8050406", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8050406"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8050406\">8050406</a>", "list_title": "PMC8050406  Use and validation of text mining and cluster algorithms to derive insights from Corona Virus Disease-2019 (COVID-19) medical literature"}
{"text": "Abdelmageed, Nora and L\u00f6ffler, Felicitas and Feddoul, Leila and Algergawy, Alsayed and Samuel, Sheeba and Gaikwad, Jitendra and Kazem, Anahita and K\u00f6nig-Ries, Birgitta\nBiodivers Data J, 2022\n\n# Title\n\nBiodivNERE: Gold standard corpora for named entity recognition and relation extraction in the biodiversity domain\n\n# Keywords\n\nentity annotation\nrelation annotation\nNamed Entity Recognition (NER)\nRelation Extraction (RE)\nInformation Extraction (IE)\nbiodiversity research\ngold standard\n\n\n# Abstract\n \n## Background \n  \nBiodiversity is the assortment of life on earth covering evolutionary, ecological, biological, and social forms. To preserve life in all its variety and richness, it is imperative to monitor the current state of biodiversity and its change over time and to understand the forces driving it. This need has resulted in numerous works being published in this field. With this, a large amount of textual data (publications) and metadata (e.g. dataset description) has been generated. To support the management and analysis of these data, two techniques from computer science are of interest, namely Named Entity Recognition (NER) and Relation Extraction (RE). While the former enables better content discovery and understanding, the latter fosters the analysis by detecting connections between entities and, thus, allows us to draw conclusions and answer relevant domain-specific questions. To automatically predict entities and their relations, machine/deep learning techniques could be used. The training and evaluation of those techniques require labelled corpora. \n\n\n## New information \n  \nIn this paper, we present two gold-standard corpora for Named Entity Recognition (NER) and Relation Extraction (RE) generated from biodiversity datasets metadata and abstracts that can be used as evaluation benchmarks for the development of new computer-supported tools that require machine learning or deep learning techniques. These corpora are manually labelled and verified by biodiversity experts. In addition, we explain the detailed steps of constructing these datasets. Moreover, we demonstrate the underlying ontology for the classes and relations used to annotate such corpora. \n\n \n\n# Body\n \n## Introduction \n  \nThe increasing amount of scientific datasets in public data repositories calls for more intelligent systems that automatically analyse, process, integrate, connect or visualise data. An essential building block in the evolution of such computer-supported analysis tools is Information Extraction with its sub-tasks, Named Entity Recognition (NER) and Relation Extraction (RE). That process aims to automatically identify important terms (entities) and groups of terms/expressions, which can fall into a certain category in the data (NER), as well as relationships between these entities (RE). However, the advancement of such tools is applicable if   gold standards  , manually labelled test corpora, are available. This supports the training of machines (for machine-learning approaches) and allows an evaluation of the developed tool. For applied domains, such as biodiversity research, gold standards are very rare. \n\nIn this work, we present a novel gold standard for biodiversity research. We provide a NER corpus, based on scientific metadata files and abstracts with manual annotations of important terms, such as species (ORGANISM), environmental terms (ENVIRONMENT), data parameters and variables measured (QUALITY), geographic locations (LOCATION), biological, chemical and physical processes (PHENOMENA) and materials (MATTER), for example, chemical compounds. In addition, we provide an RE corpus, based on a portion of the same data that consists of important binary and multi-class relationships amongst entities, such as OCCUR_IN (Organism, Environment), INFLUENCE (Organism, Process) and HAVE/OF (Quality, Environment). We also added these identified entities and relationships to a conceptual model developed in our previous work ( ). \n\nOur contribution is threefold: \n\n  \na NER corpus, based on metadata and abstracts with the following entities: ORGANISM, ENVIRONMENT, QUALITY, LOCATION, PHENOMENA, MATTER \n  \nan RE corpus, based on a portion of the same data, with the following relationships containing the entities identified in the NER corpus: OCCUR_IN, INFLUENCE and OF/HAVE \n  \na conceptual model that integrates all concepts and relations. \n  \n\nWe provide the results in formats that allow easy further processing for various Natural Language Processing (NLP) tasks, based on machine-learning and deep learning techniques. The code and the data are publicly available as follows: \n\n  \nThe data DOI:  \n  \nGithub Repo for the scripts:  \n  \n\n### Background \n  \nBiodiversity research is a sub-research domain of the Life Sciences that comprehends the totality and variability of organisms, their morphology and genetics, life history and habitats and geographical ranges ( ). Scientific data generated in biodiversity research are very heterogenous and can occur in multiple formats. This is an obstacle for machine processing, which needs additional information for data integration, data search or data visualisation. Therefore, primary research data are described by metadata and descriptive information along the W-questions (what, who, when, where and why). Such metadata are mostly provided in structured formats, such as JSON or XML. \n\nNatural Language Processing (NLP), with its sub-task Information Extraction, is a research field that uses these structured data or scientific publications. The aim is to develop systems that automatically identify important terms and phrases in the text. That supports scholars in obtaining a quick overview of unknown texts, for example, in search or allows improved filtering. In the Life Sciences, Information Extraction has a long history ( ). Driven by a series of workshops and shared tasks, such as  ,   and   in the scope of  , multiple corpora and tools for various purposes were developed to extract main entities from text and relations amongst them automatically. However, determining what a relevant entity or relation in a document or data depends on the domain of focus. While scholars looking for biomedical data are mainly interested in data types, such as diseases, biological processes and organisms ( ) and related entities, such as genes and proteins, in biodiversity research, other categories are of relevance, namely: organisms, environmental terms, geographic locations, measured data parameters, materials, biological, physical and chemical processes and data types ( ). \n\n\n### Previous Resources Analysis \n  \nIn the first step, we had to figure out which categories (or entity types) are relevant for biodiversity research. In addition, we also had to explore occurring relations amongst these entities. Therefore, we selected two sources from our previous works: 1) BiodivOnto ( ) and a biodiversity research-related question corpus ( ). In this section, we describe how we decided on the classes and relations to be used in the annotation process. We also elaborate on how we came along with a reconciled model representing the final conceptual model we used in this work. In addition, we introduce the underlying data sources for the development of the novel gold standards. \n\n Biodiversity Questions  \n\nThe biodiversity question corpus consists of 169 questions provided by around 70 scholars of three biodiversity research-related projects ( ). Concerning the topics and granularity, the questions are very diverse and reflect different information needs. While some questions ask for facts, such as \"What butterfly species occur on calcareous grassland?\", others are more complex and aim to obtain an answer on associations, for example, How do autotrophic microorganisms influence carbon cycling in groundwater aquifers? The noun entities of these questions were manually labelled (including nested entities, such as adjectives, for example, autotrophic microorganisms). Nine biodiversity scholars grouped the labelled nouns and phrases into 13 proposed categories. Each annotator classified all 169 questions, which resulted in 592 total annotations. It turned out that seven categories (entity types) were mentioned very often (at least 89 times per category): ORGANISM (e.g. plants, fungi, bacteria), ENVIRONMENT (environments species live in), QUALITY (characteristics to be measured), MATERIAL (e.g. chemical compounds), PROCESS (re-occurring biological and physical processes), LOCATION (geographic location) and DATA TYPE (research results, e.g. lidar data). All annotations for which the inter-rater agreement was larger than 0.6 (representing a substantial agreement ( )) were exported to a final  . \n\nThe identified relevant entity types from this question corpus were aligned with the detected categories of classes from BiodivOnto in several discussion rounds. The final outcome (see Table  ) was used to inspect the annotated questions again. This inspection consists of manually detecting the relations between the already annotated entities in each question. We omitted questions that do not possess any annotation of the final classes or provide only one class. We only considered questions that contain at least two annotations of the entity types in Table  . In total, 91 questions were utilised for the relation detection in the question corpus. \n\nThe main idea for the relation detection process was to come up with categorisation for relations similar to the categories for noun entities. The detection process was conducted in several rounds. In the first pilot phase, three scholars analysed only a few questions about the existence of relations. The initial instruction was to manually inspect the questions and to identify binary relations between the occurring entities. Scholars were also advised to inspect the given verbs (which mainly describe a relation) and to think about suitable categories for the relations. In a second round, the proposed relation categories were discussed. The outcome was used for the final detection round. The final agreed relation categories are: \n\n  \ninfluence (an entity influences another entity, for example, an ORGANISM influence PHENOMENA), \n  \noccur (an entity occurs in another entity, for example, PROCESS occur ENVIRONMENT), \n  \nof (inverse relation of have: an entity of an entity or an entity has another entity, for example, QUALITY of ORGANISM) \n  \n\nComplex questions with several entities were split into several relations. For example, the question \"How do (autotrophic microorganisms)[ORGANISM] influence (carbon cycling) (PHENOMENA) in (groundwater aquifers)[ENVIRONMENT]\"? This resulted in detecting two relations: influence (autotrophic microorganisms  , carbon cycling  ) and occur (carbon cycling  , groundwater aquifers  ). Fig.   presents the outcome of the relation detection of the question corpus. The most frequent relation patterns are ORGANISM   occur   ENVIRONMENT and ORGANISM   occur   LOCATION, with 13 mentions each. This result served as input for the conceptual model, as well as for the subsequent relation annotation of metadata and abstracts. \n\n BiodivOnto  \n\nBiodivOnto is a conceptual model of the core concepts and relations in the biodiversity domain. The first version of BiodivOnto ( ) was developed in 2021, whereas the most recent ontology version is given by ( ). Such core or general concepts represent the classes of annotation utilised. The proposed class names were discussed with two biodiversity experts who are also authors of this paper. We finally agreed on, for example, ORGANISM, PHENOMENA and MATTER as tags for the NER corpus. However, BiodivOnto contains subclasses as well, like Ecosystem and Landscape, which are subclasses of the Environment class. To facilitate the annotation process, we decided to use the top-level classes only. In this case, both Ecosystem and Landscape are substituted by the ENVIRONMENT class. The same applies to Trait and Quality, where only QUALITY was used as an annotating class. LOCATION has appeared as a common concept in the Biodiversity Questions (see above); we included it as well as a core concept in the BiodivOnto. Table   summarises the final selected classes of interest that were used in the NER annotation. \n\nBiodivOnto initially had the following relations: \n\n  \nhave: that appeared between ORGANISM-ENVIRONMENT, ORGANISM-QUALITY, ENVIRONMENT-QUALITY and MATTER-QUALITY. \n  \noccur_in: that appeared between PHENOMENA-ENVIRONMENT. \n  \n\nHowever, we merged the outcome from the analysis of the Biodiversity Questions as we did for classes. Thus, we included new relations as follows: \n\n  \noccur_in linking MATTER-ENVIRONMENT, ORGANISM-LOCATION, ORGANISM-ORGANISM, PHENOMENA-LOCATION and ENVIRONMENT-LOCATION. \n  \ninfluence relating ORGANISM-PHENOMENA, ORGANISM-MATTER, PHENOMENA-PHENOMENA, PHENOMENA-QUALITY, PHENOMENA-ENVIRONMENT and QUALITY-QUALITY. \n  \n\nOn the other hand, BiodivOnto initially included both \"part_of\" and \"is_a\" relations. However, we do not include them in the new ontology version since the most common relations in the Biodiversity Questions lack them. We picked on the relations that appear in both sources only. \n\nFig.   illustrates the reconciled version of BiodivOnto, based on the old BiodivOnto model and the Biodiversity Questions. It consists of six classes and 17 relations we used in the annotation process. \n\n Data Sources  \n\nTo construct our corpora, we re-used our previous work's collected metadata and abstracts ( ). Thus, metadata files are gathered from two data sources with very different characteristics (  and  ). The Semedico search engine ( ) retrieves relevant abstracts from  , a source with more than 32M abstracts. To ensure the relevance of the crawled data from Semedico, we have followed an iterative way of revision. We started with the initial keywords set that we used to crawl. Then, we manually revised it to guarantee relevance. More details on the collection and crawling, license verification, and biodiversity relevance checking are already explained in ( ) and go beyond the scope of this paper. Initially, these collected data were meant to extract biodiversity-related keywords. However, in this work, we use them for the purpose of developing NER and RE corpora. \n\n\n### Related Work \n  \nThe loss of biodiversity has a lot of concerns and it considers a major issue in our life ( ,  ). Research in this domain has recently seen accelerated growth, leading to the big data scenario of the biodiversity literature. For instance, the   currently holds over 55 million digitised pages of legacy biology text from the 15 -21  centuries, representing a huge amount of textual content ( ). Extracting core knowledge, i.e. entities and relations between these entities, from myriads of available resources, allows a better overview of the data and thus supports fact discovery. In this section, we outline the state-of-the-art related work towards building such gold standards in the Life Sciences, focusing on biodiversity research. \n\n Named Entity Recognition (NER) Corpora  \n\nBIOfid ( ) is a Specialised Information Service for Biodiversity Research launched to mobilise valuable biological data from printed literature hidden in German libraries for the past 250 years. First, historical literature was converted into text using OCR and plants, birds and butterfly occurrences were annotated. A training dataset was then generated for named entity recognition and taxa recognition from biological documents. After that, this training dataset was used to create a global standard for taxa recognition in the German biodiversity literature. Even though BIOfid represents a global standard, it is a limited resource for the following reasons: (i) input resources are limited to German literature only, (ii) the entity identification process focuses only on taxa and other more generic categories, such as person and location. \n\nCOPIOUS ( ) is another gold standard corpus covering a wide range of biodiversity entities. The corpus has 668 documents downloaded from the Biodiversity Heritage Library with over 26K sentences and more than 28K entities. Only two annotators manually annotated the corpus with five categories of entities, i.e. taxon names, geographical locations, habitats, temporal expressions and person names. The proposed gold standard supported the development of named entity recognition and relation extraction using two different machine-learning techniques. \n\nSpecies-800 ( ) is based on 800 PubMed abstracts, such as 100 abstracts from journals in eight categories: bacteriology, botany, entomology, medicine, mycology, protistology, virology, and zoology. Similar to ( ), Species-800 is annotated with taxon entities and normalised to the   database. \n\nLinnaeus ( ) is a 100 full-text documents from the PubMed Central Open Access (PMC OA) document set randomly selected and annotated for species mentions. The corpus was only annotated for species (except for the cases where genus names were incorrectly used when referring to species). Same as the case with COPIOUS and Species-800, all mentions of species terms were manually annotated and normalised to the NCBI taxonomy IDs of the intended species, except for terms where the author did not refer to the species. \n\nQEMP ( ) is the only corpus that is based on biodiversity metadata files. It provides annotations for four main categories: Organism, Material for chemical compounds, Process for chemical, biological and natural processes, Environment that represents the habitat of organisms, Quality for data measures and Location. \n\nThe existing datasets have several limitations. They focus on species only, like the case of BioFid and COPIOUS. They are based on legacy data, as in COPIOUS and BioFID. They rely on only Pubmed abstracts like the case of Species800 and Linnaeus. They miss one important concept in the field, like the case of QEMP; it does not contain species. In this work, we create an NER corpus that contains various biodiversity classes for abstracts and metadata files. \n\n Relation Extraction (RE) Corpora  \n\nIdentifying the important entities is the first step in creating an RE gold standard. Based on this information, relationships amongst the entities in a sentence can be determined in a second step. There is a variety of approaches in the biomedical domain to identify relations amongst genes, diseases, proteins and drugs BioInfer ( ), BioRelEx ( ), EU-ADR ( ) and its successor GAD ( ). All of them use biomedical abstracts or full articles from PubMed as data sources. In contrast, some approaches do not identify the exact mention of relation but only determine the existence of a binary relation between entities ( ,  ,  ). Other gold standards distinguish between four main relation types, such as \"causal\", \"is_a\", \"part_of\" and \"observation\" ( ). They also developed a large ontology to describe the entities and their relations semantically. \n\nThere are only two approaches for relation extraction in the biodiversity domain: BacteriaBiotop ( ) and COPIOUS ( ). The former defines a binary \"lives_in\" relation between Taxons and Habitats. The latter uses a pattern-based system that can identify any binary relations between entities within a single sentence to detect four relations: Taxon \"occur\" Habitat, Taxon \"occur\" Temporal Expression, Taxon \"occur\" Geographic Location and Taxon \"seen by\" Person. \n\nTo the best of our knowledge, there is no gold standard with relations also from dataset metadata. The introduced corpora only have the main focus on species, habitats and locations. However, biodiversity research is a diverse research field with other important categories, such as data parameters, processes, materials and data types ( ). Therefore, we aim to develop a gold standard that supports both multiple and binary relations and goes beyond the annotation of species, habitats, and geographic locations. \n\n\n\n## General description \n  \n### Purpose \n  \nThis project aims at constructing two corpora for NER and RE tasks, based on abstracts and metadata files from Biodiversity datasets. \n\n\n### Additional information \n  \n Methodology  \n\nIn this section, we describe the process of constructing the NER and RE corpora. \n\n BiodivNER Construction Pipeline  \n\nIn this section, we explain the construction pipeline of the NER corpus as shown in Fig.  . Our process consists of seven steps. It starts with the annotation guidelines to describe what we annotate and is followed by the data preparation step in which the originally collected data is transformed into the required data format used for annotation. In the pilot phase, we carry out an initial annotation task to check whether we have to modify the annotation guidelines or whether we have to invest more time in the annotators' training. Afterwards, the actual annotation task takes place. The outcome is evaluated with the computation of the inter-rater agreement. Finally, we discuss the mismatches with biodiversity experts in the reconciliation phase. \n\n  \nAnnotation Guidelines \n  \n\nWe followed a modified version of our previous project guidelines to construct the QEMP corpus ( ). We set the current sentence as the only available context to annotate. We did not consider the entire document as in the gold standard construction process in NLP. Since the main purpose of this work is to develop a corpus for NER, we consider only noun entities and discard adjective entities. In addition, we gave higher attention to the complex words and minimised the chance of having two valid annotations for one term. Thus, we followed the longest span annotation and avoided nested entities annotation. For example, \"benthic oxygen uptake rate\" is annotated as [QUALITY], while we ignored any simple word annotation inside such span. Conjunctions are handled as two separate entities. For example, \"(phylogenetic diversity)[QUALITY] of (bacteria)[ORGANISM]\". We included more existing external resources than the ones used in QEMP to find proper annotations. For example, we considered the following ontologies that were used for constructing the original version of BiodivOnto:   and   for environmental-related terms,   and   for phenomena-related keywords. In addition, we utilise   and   for species and phenotype annotation, respectively. Moreover, we used the   ontology to capture any missing terms from the previous ontologies. Our last option to find annotations from existing sources is a reference to the   detected and summarised by ( ). Such a mixture of selected resources facilitated the detection of a wide range of terms that vary in their granularity (too specific vs. too general terms). \n\n  \nData Preparation \n  \n\nWe parsed the original data collection into sentences. For each sentence, we tokenised it into a set of words using   library. Since our used annotation format is  , where a word is annotated either with   B-tag   as a beginning of an entity or,   I-tag   as an inside of entity or,   O   as outside of the entity, each word is initialised with an O tag. Each sentence as a set of words with O tags is stored vertically in a CSV file, as shown in Fig.  a. Afterwards, we split the entire corpus into two halves to enable the double annotation process. \n\n  \nPilot Phase and Participant Guidance \n  \n\nFour authors of this paper were responsible for annotating the corpus. Two authors have previous experience with biodiversity text annotation. The four annotators received periodical guidance from two biodiversity experts. Initially, we established a trial or a pilot phase before the actual annotation process took place. The purpose of this phase is to ensure the training of the annotators (participant guidance) as well as to revise the annotation guidelines. Around 2% (450 sentences) of the entire corpus is assigned to each annotator pair. Each annotator labelled a local copy of the pilot phase data in an Excel file. During this process, each annotator was asked to annotate a relevant term with one and only one tag from the provided tags. The results of this process are represented in Fig.  b. After the end of the Pilot Phase, we held a \"Share Thoughts\" meeting to discuss the outcome. At this stage, we realised that we need a modified version of the guidelines. For example, at the beginning, not all annotators followed the 'longest span' rule and annotated every single word separately. Thus, we have settled on the longest span sequence to avoid or minimise such inconsistencies. In addition, we have decided to add the SWEET ontology to include missing terms from the other used ontologies. \n\n  \nAnnotation Process and Agreement \n  \n\nAfter the pilot phase, we familiarised ourselves with the annotation process and the guidelines. Each half of the corpus was assigned to an annotator pair. We followed the same procedure as in the pilot phase. Each annotator from the annotators' pair worked blindly on a local copy of the sheet. We refer to blindly as without access to the annotation of the other colleague. This procedure ensures the higher quality of annotated data and allows the calculation of the inter-rater agreement. Each annotator was asked to complete the annotation of half of the corpus. This annotation process was time-consuming and lasted for several months. Annotating a term is considered to be done if the annotator found the target tag in the selected existing data sources. However, if the annotator was unsure about the correct annotation, the term with a suggested tag was kept in a separate sheet named \"Open Issues\". We held various meetings with the biodiversity experts during this stage to solve the open issues. Since we had two annotator pairs, let's say, team A and B for two different sheets, where each sheet represented half of the corpus, we were able to calculate the inter-rater agreement for each team. We used Kappa's score for the agreement computation since it is one of the most common statistics to test inter-rater reliability ( ). The scores are 0.76 and 0.70 for teams A and B, respectively, with an average score of 0.73. In addition, we calculated both precision, recall and F1-score for both teams, as shown in Figs  ,  . Team A reached an average precision, recall and F1-score of 0.73, 0.65 and 0.67 respectively. However, Team B gained average scores: 0.66, 0.74 and 0.67 for both precision, recall and F1-score respectively. \n\n  \nReconciliation \n  \n\nWe have extracted the mismatches in a separate sheet per annotator pair. A sheet contained the actual sentence with each of the annotator's answers. The task of each annotator pair was to reconcile their mismatches and to reach a final annotation that the two agreed on. We noticed that a significant cause for the mismatches was the rule of longest text span consideration in the annotation guidelines. For example, one annotator labelled the entire phrase \"Secondary Metabolites\" as MATERIAL, while the other tagged only \"Metabolites\" as MATERIAL. Such cases were the easiest to solve. However, other cases, where an annotator pair could not agree on one correct annotation were discussed with the biodiversity experts. For example, \"Soil lipid biomass\" seemed to be confusing as it could be either classified as MATTER or QUALITY. In such a case, we followed the biodiversity expert's opinion and settled on MATTER. \n\n BiodivRE construction Pipeline  \n\nIn this section, we describe our pipeline of constructing the binary and multi-class RE corpus on top of the BiodivNER. Initially, we transformed the annotated data for NER to suit the RE annotations process. Then, we tried to sample a subset of sentences to obtain a reasonable size of the RE corpus to be annotated. For each sampling method, we detailed its advantages and disadvantages. Afterwards, we explained the annotation process for the RE corpus. \n\n  \nInitial Construction \n  \n\nWe considered the final NER corpus as an input for the RE corpus construction. We prepared the data in such a way to be more readable. Each sentence is represented by one row, followed by its corresponding NER annotations in the following line. The NER corpus contains sentences with multiple tags. However, an RE corpus should be designed in a way that each sentence contains exactly two tags. We generated all possible combinations for sentences with more than two tags, including exactly two tags. Fig.   illustrates an example where one sentence with three tags generates three sentences with two labels. This operation generated a large-scale corpus with more than 52K sentences. We expect a high rate of FALSE (no relation) statements in the generated corpus. However, our task aims at creating an RE corpus with a good balance between TRUE (existing relation) and FALSE sentences. To achieve this, we have to choose a suitable sampling strategy to achieve the best balance amongst the selected sentences. Therefore, we have explored two different sampling methods. We discuss them in the following sections. \n\n  \nRandom Sampling \n  \n\nIn the pilot phase of BiodivRE construction, we used a random sampling mechanism amongst the created corpus. We did not consider any selection criteria. We directly stacked the entire corpus in a list, shuffled it and randomly picked \"n\" sentences. We started annotating the resultant smaller corpus and, by doing so, we encountered two issues. At first, we found long sentences with too far tags, i.e. have many words between them, which makes the existence of a relation between the two tags impossible. Second, some of the relation pairs in the ontology have not appeared in the corpus at all. There are two reasons for the second issue. Either such kinds of relations do not appear in the original corpus or they are missed by the sampler since it depends purely on the random selection. The conclusion from the pilot phase is the need for changing the sampling strategy. \n\n  \nBalance-Biased Sampling \n  \n\nWe developed a Balance-Biased sampler to have more control over what to include in the final RE corpus. It is inspired by the   scheduler. We grouped the sentences from the initial construction by tag-pair, where a valid pair is the one appearing in the BiodivOnto and the unsupported co-occurrences were grouped into a new category, \"Other\". At this stage, we handled the relations bidirectionally between entities of interest to cover cases like ENVIRONMENT have QUALITY and QUALITY of ENVIRONMENT. Afterwards, we iterated over the groups, including the entire set of tag-pairs, as well as the \"Other\" group. We picked one sentence from each group until a threshold was reached. By this means, we avoided any bias that could be caused by a random sampler. In our case, we selected 4000 sentences as a threshold. An additional criterion is that we limit the number of words between the two entities of interest to a certain value, for example, 30 words. In this way, we solved the two problems that appeared using the random sampling method. At first, we guarantee that we cover all the relations of the BiodivOnto, if it exists in the text, in the final corpus. Second, we avoid cases with FALSE sentences due to too far entities, since it is clear that no relation could exist between them. \n\n  \nAnnotation Process \n  \n\nWe directly referred to BiodivOnto and limited the accepted relations to those supported by the ontology. On the one hand, for each sentence, we checked whether there is a relation between its two named entities. On the other hand, whether this relation has a semantic correspondence in the BiodivOnto. For example, a verb relation \"has an impact on\" is considered a synonym for the ontological relation \"influence\". FALSE examples would be either the relation is not supported by the BiodivOnto or it has a different meaning than the ontological relation. For example, \"Climate change (B-Phenomena I-Phenomena) impacts the carbon dioxide (B-Matter I-Matter)\" is a FALSE sentence since there is no ontological relation between PHENOMENA-MATTER. Such a sentence would appear since we also choose from the \"Other\" group in the selected sampling method. Another FALSE example might occur between two entities with a relation in the BiodivOnto. \"Trees (B-Organism) with extrafloral nectaries (B-Matter I-Matter)\" is a FALSE statement since the word with does not imply the relation influence between ORGANISM and MATTER. \n\nSimilar to our procedure to construct the NER corpus, we also applied a pilot phase for RE annotation. Two of the authors annotated the same 50 sentences that were randomly picked. Afterwards, we calculated the inter-rater agreement (Kappa's score), which resulted in 0.94. Due to this high score, we decided to split the corpus and individually continue the annotation. \n\nDuring the real annotation phase, we encountered issues regarding the entity tags, especially for the longest span annotation. This rule does not seem to be correctly followed during the annotation of the NER corpus. For example, \"earthworm invasion\" was annotated as \"B-Organism\" \"B-Phenomena\", instead of \"B-Phenomena\" \"I-Phenomena\". For those cases, we fixed them to follow the rule of the annotation declared originally in the NER guidelines. Fig.   shows samples from an annotation sheet. The first column holds the actual relation label from BiodivOnto that will be used for the multi-class RE corpus. Then, it is followed by a binary relation tag (0- no relation, 1- existing relation). Yellow cells highlight the relation between the two entities of interest in the text. Red cell indicates that there is a relation based on the sentence, but not supported by BiodivOnto. In this sentence, the verb \"degrade\" has an \"influence\" meaning implicitly. However, we expect to have a relationship that semantically means \"have\"; thus, the sentence is tagged with a \"0\". Other sentences, like the last one, indicate no relation at all. \n\n\n\n## Geographic coverage \n  \n### Description \n  \nNot Applicable \n\n\n\n## Usage licence \n  \n### Usage licence \n  \nCreative Commons Public Domain Waiver (CC-Zero) \n\n\n\n## Data resources \n  \n### Data package title \n  \nBiodivNERE \n\n\n### Resource link \n  \n\n\n\n### Number of data sets \n  \n3 \n\n\n### Data set 1. \n  \n#### Data set name \n  \nBiodivNER \n\n\n#### Data format \n  \nCSV \n\n\n#### Download URL \n  \n\n\n\n#### Description \n  \nThree files per named entity recognition (NER) represent train, dev and test splits. \n  \n\n\n### Data set 2. \n  \n#### Data set name \n  \nBiodivRE \n\n\n#### Data format \n  \nCSV \n\n\n#### Download URL \n  \n\n\n\n#### Description \n  \nThree files for Relation Extraction (RE) represent train, dev and test splits. \n  \n\n\n### Data set 3. \n  \n#### Data set name \n  \nBiodivRE_MultiClass \n\n\n#### Data format \n  \nCSV \n\n\n#### Download URL \n  \n\n\n\n  \n\n\n\n## Additional information \n  \n Results and Discussion  \n\nIn this section, we give an overview of our final NER and RE corpora. We illustrate the characteristics of each corpus, for example, the class distribution in the NER corpus. In addition, we compare them to existing state-of-the-art corpora. \n\n BiodivNER Characteristics  \n\nThe final version of the NER corpus consists of three folds: train, dev and test because our corpus mainly addresses various tasks in NLP that could be solved, based on machine-learning techniques. We followed the split of 80%, 10% and 10% for the train, dev and test sets, respectively. All files are given in a CSV format, each of which consists of three entries Sentence#, Word and Tag, as shown in Fig.  b. Fig.   provides an overview of the category distribution inside the BiodivNER corpus in the tree data folds. QUALITY represents the most occurring mention in the corpus, followed by ORGANISM and ENVIRONMENT, respectively. However, LOCATION is the least frequent one. The overall distribution reflects a diverse corpus of the most important classes in the biodiversity domain. \n\nMoreover, we compare our BiodivNER to the existing common corpora. Table   shows the comparison overview. We compared in terms of the used data source, collected data type, number of annotated documents, number of statements, words, categories and mentions. Mentions represent how many words are annotated. We also provide the number of unique mentions. COPIOUS corpus is the largest in terms of all aspects, except the number of categories. However, BiodivNER covers the greatest number of categories. In addition, BiodivNER is the largest corpus that is based on metadata files of biodiversity datasets as a data source. \n\nCOPIOUS has two categories closely related to biodiversity (Habitat and Taxon) and two general Categories (TemporalExpression and GeographicalLocation). QEMP has four categories derived from the biodiversity domain (Environment, Material, Process and Quality). As there is already a variety of corpora for species, we only concentrated on missing categories in QEMP. BiodivNER also covers such an essential category in addition to the same closely-related classes as QEMP and a general domain LOCATION category. \n\n BiodivRE Characteristics  \n\nSimilar to BiodivNER, we created three folds in a CSV format for both binary and multi-class RE corpus. The files consist of two columns: (1) the relationship either in a binary or label form and (2) the sentence where the actual named entities are encoded with their tags. An example line in the file of binary relations: \"1 Our study shows a significant decline of the @QUALITY$ of @ENVIRONMENT$.\". However, it would be in the multi-relations files as: \"have, Our study shows a significant decline of the @QUALITY$ of @ENVIRONMENT$.\" This format will facilitate the training procedure for any machine-learning technique. We followed the same split setting for 80%, 10%, 10% of the train, dev and test sets, respectively. \n\nFig.   shows the category pairs distribution of the BiodivRE corpus. We have calculated the frequencies in a bidirectional order. For example, ORG-ENV represents the total of such a pair and ENV-ORG as well. Since QUALITY is the most frequent class in the NER corpus, this is also reflected in the category pairs ORG-QUA and ENV-QUA. The self-relations that appear in ENV-ENV and PHE-PHE are the least frequent in our corpus. Other category pairs that the BiodivOnto support do not appear in the text used for creating the RE corpus. For example, ORG-ORG and ORG-LOC. The \"Other\" group represents any co-occurrences that appear in the text and do not exist in the BiodivOnto. In addition, Figs  ,   depict the binary and multi-class annotation distribution of the BiodivRE in the three folds of the benchmark. Such that \"have\" followed by \"occur_in\" are the most common relations in the corpus. \n\nTable   identifies our RE corpus and the biomedical corpora GAD, EU-ADR and BioRelEx. We selected these corpora for comparison since the data are publicly available and the scope of the annotation is limited to only one sentence, as was the case of our BiodivRE corpus. For example, the COPIOUS corpus discusses the RE part, but the data are unavailable. In addition, BioCreative V ( ) uses the entire abstract as a context of annotation and, thus, we skip it here. For BioRelEx, in the original dataset paper, they have -1, 1 and 0 classes. We use them here as the former two classes map to TRUE, while the latter maps to FALSE classes. BiodivRE has a second-place amongst the existing corpora concerning the number of sentences (4K) with a higher rate of FALSE sentences. There are two reasons behind this high number of FALSE statements. On the one hand, we found that most metadata sentences have a listing format of entities and we could not guess the relation amongst them (the most frequent sentences). On the other hand, BiodivOnto is still incomplete; some relations are missing from it. For example, \"Trees (B-ORGANISM) with extrafloral nectaries (B_MATTER, I-MATTER)\" holds a meaning of contains, but we look for influence. \n\n Conclusions and Future Work  \n\nWe introduced BiodivNERE as a package for two corpora for NER and RE tasks that are based on abstracts and metadata from the biodiversity domain. We manually annotated and revised them with biodiversity experts. BiodivNER, the NER corpus, consists of six important classes in the biodiversity domain. BiodivRE is a binary and multi-class benchmark containing three relations from the domain. Both classes and relations are derived from the analysis of our previously-developed work (Biodiversity Questions and BiodivOnto). We release our corpora and code as publicly available. \n\nFuture Work \n\nWe see multiple areas to extend this work. We plan to include more classes and relations from the biodiversity domain. For example, we restore the dropped relations from BiodivOnto, for example, \"part_of\" and \"is_a\". In addition, we include more data sources to cover a broader range of the domain. Moreover, we evaluate them in terms of the quality of the annotations. Last but not least, we apply both corpora to a machine-learning model to bring them to the actual use case. \n\n \n", "metadata": {"pmcid": 9836593, "text_md5": "36f65793efd1631ba1b592cb8931e9b7", "field_positions": {"authors": [0, 167], "journal": [168, 184], "publication_year": [186, 190], "title": [201, 314], "keywords": [328, 486], "abstract": [499, 2200], "body": [2209, 41045]}, "batch": 1, "pmid": 36761617, "doi": "10.3897/BDJ.10.e89481", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9836593", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9836593"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9836593\">9836593</a>", "list_title": "PMC9836593  BiodivNERE: Gold standard corpora for named entity recognition and relation extraction in the biodiversity domain"}
{"text": "Zulkarnain, nan and Putri, Tsarina Dwi\nHeliyon, 2021\n\n# Title\n\nIntelligent transportation systems (ITS): A systematic review using a Natural Language Processing (NLP) approach\n\n# Keywords\n\nIntelligent transportation system\nNatural language processing\nCustom named entity recognition\nLatent dirichlet allocation\nWord embedding\nContinuous skip-gram\nSystematic review\n\n\n# Abstract\n \nIntelligent Transportation Systems (ITS) is not a new concept. Notably, ITS has been cited in various journal articles and proceedings papers around the world, and it has become increasingly popular. Additionally, ITS involves multidisciplinary science. The growing number of journal articles makes ITS reviews complicated, and research gaps can be difficult to identify. The existing software for systematic reviews still relies on highly laborious tasks, manual reading, and a homogeneous dataset of research articles. This study proposes a framework that can address these issues, return a comprehensive systematic review of ITS, and promote efficient systematic reviews. The proposed framework consists of Natural Language Processing (NLP) methods i.e., Named Entity Recognition (NER), Latent Dirichlet Allocation (LDA), and word embedding (continuous skip-gram). It enables this study to explore the context of research articles and their overall interpretation to determine and define the directions of knowledge growth and ITS development. The framework can systematically separate unrelated documents and simplify the review process for large dataset. To our knowledge, compared to prior research regarding systematic review of ITS, this study offers more thorough review. \n  \nIntelligent transportation system; Natural language processing; Custom named entity recognition; Latent dirichlet allocation; Word embedding; Continuous skip-gram; Systematic review. \n \n\n# Body\n \n## Introduction \n  \nITS first appeared in 1996 [ ]. In this book, the writer outlines ITS and then discusses the early history of their emergence. However, systematic reviews regarding ITS as a field are considerably rare. A few recent systematic reviews of ITS have used only a few journal articles, resulting in a fragmented perspective [ ,  ,  ,  ,  ]. Moreover, the number of research articles regarding ITS is increasing every year.   shows the increase based on search results from Scopus with the keyword \u2018intelligent transportation system\u2019. The ITS research field has experienced growth and development in an interdisciplinary manner. Various research articles have introduced numerous concepts spanning technology (both application and core science), policy, management and strategy, method development, and other topics. These broad ranges of concepts and scientific fields can lead to confusion in understanding the direction of ITS knowledge development.   \nThe number of ITS publications from 1971\u20132020. \n  Figure\u00a01   \n\nIn 2020, Scopus alone provided more than 22,000 research articles with the keyword 'intelligent transportation system'. Note that the keyword consists of 3 words, which might lead to other journal articles and proceedings papers that are not ITS related being returned. Additionally, there is an absolute needed for homogeneous journal articles and proceeding papers datasets to perform a systematic review. Thus, the systematic review screening phase is always laborious, and this study intends to address this issue first. \n\nThe dataset was taken from the Scopus database with an API key. The keywords resulted in as many as 23,823 titles and abstracts from English journal articles and proceeding papers from 1974 to 2020. A tag was used for each document with words (terms) in the ITS context. Terms such as traffic management systems, area traffic controls, traveler information systems, bus information systems, and other terms might be represented or written differently. It is impossible to read and tag all papers in the dataset manually. Therefore, the study uses a custom NER method and constructs all the possible terms as a data train, classifies each document, and tags words in documents. Thus, each document can be labelled and documents them based on their labels. \n\nThe study involves a reproducible data train and test, and this approach could be updated. This approach allows the addition of tags that are already available in NER for the English model database. Therefore, the method is commonly known as custom NER. Custom NER can effectively extract information from large or regular datasets with reasonable accuracy. Some studies have verified NER for various purposes [ ,  ,  ,  ]. \n\nThe next stage after document screening is systematic review. Although the first phase succeeds in removing unrelated documents with ITS terms, the number of documents in the corpora is still approximately 20,000, which will complicate the systematic review process. Therefore, this study combines other methods to obtain at least an overview of the essence of journal articles and paper proceedings. A conventional method such as bibliometric mapping is an effective way to explore research gaps and topics. Notably, the more significant the scale of mapping is, the more complex the visualization will be [ ]. This relation correspond to the increase in ITS emergence in journal articles and proceedings papers. Consequently, we need an efficient way to perform systematic reviews. \n\nThe existing tools for bibliometric mapping (such as VxOrd) cannot provide complete specifications that maximize graph mapping results [ ]. Another tool, VosViewer, uses the frequency concept and then looks for similarities among concepts using the Jaccard Index [ ]. However, to obtain complete results, many parameters are required. Inevitably, the systematic review process is highly complicated and laborious. Dachyar   et\u00a0al.   [ ] conducted a systematic review of the Internet of Things (IoT) with a significant number of documents. In the process, first, they needed to ensure that a large homogeneous dataset was used. Subsequently, VosViewer was used to visualize the scientometric analysis. However, they still had to interpret and manually read every document that had a high impact value, and this number was not small. This study proposes the combination of other NLP methods to solve the systematic review problem. The methods are LDA and word embedding, which are based on the idea that journal articles and proceedings papers often have similarities or interrelationships based on topics or themes. \n\nAsmussen and Moller [ ] describe the variety of LDA applications and specifically describe LDA as a tool for exploratory literature review. Grouping articles based on the likelihood of each article helps to dissect the topics in the dataset systematically. In other words, each article group has similarities regarding the research theme/topic based on its bag-of-word. On the other hand, the distribution hypothesis states that context can be interpreted as a series of texts surrounding the reference text in a continuous vector space. The word embedding (continuous skip-gram) function works based on this hypothesis. Therefore, based on these two things, this study decided to use word embedding to help shorten the manual reading process to find the main context of each group of articles. \n\nThe core NLP methods used in this study are text classification, topic modeling, and word extraction. Generally, the study constructs the proposed methods as a framework, and we expect it to be more advanced and thorough than the conventional systematic review in the following ways:   \nIt is able to screen journal articles and proceedings papers given the importance of document homogeneity in a systematic review dataset. \n  \nIt is able to establish effective topic representations for systematic reviews. \n  \nIt is able to eliminate one or several processes to make laborious systematic reviews more efficient. \n  \n\nTherefore, there are two utilization tasks that this study tries to produce. First, document classification eliminates unrelatable documents with ITS terms. Second, systematic reviews are used to effectively and efficiently examine ITS knowledge growth based on a collection of journal articles and proceedings papers and to identify research gaps in the long term. \n\nAs guidance for the review process, the following predetermined research questions were asked:   \nHow does the core knowledge of ITS evolve over time? \n  \nWhat are the substantial approaches used in research regarding ITS? \n  \n\nThe paper structure is as follows. First, the background that led to the research questions is discussed. Second, the research methodology, data collection procedure, and results are presented in the context of the proposed framework. Third, the results are analyzed and interpreted, the direction of knowledge growth is discussed, and ITS development is assessed. The final section presents conclusions and suggestions for future research. \n\n\n## Methodology and proposed framework \n  \nThe data were collected from Scopus using an API key with the R programming language as the interface, the keyword \u201cintelligent transportation system\u201d, and the publication time interval from 1974 to 2020. The dataset is structured (tabular) with attributes consisting of the publication year, author, title, abstract, citations, and affiliations collected for as many as 23,823 journal articles and proceedings papers. The study adopts and improves a previously designed framework [ ]. \n\nPutri [ ] initially proposed a systematic review framework based on analysis of content-based recommendation system using word embedding (continuous skip-gram). It uses various keywords to collect research articles dataset with publication time range between 1931 to 2020, and it also used various hyperparameters. The method captured context from each research article and calculated similarity to display its magnitude toward other research articles. The similarity value was then used as the basis of a content-based recommendation system for research articles. The hyperparameter variety generated various results, which were then compared and analyzed. The main goal was to determine whether the assigned method can extend its utility. Putri [ ] determines which word embedding hyperparameters were suitable for systematic review and proposed a framework. The initial framework consists of text preprocessing, word embedding to separate research articles that do not have similarities with the target word \"intelligent transportation system\", clustering, topic modeling, and bibliometric analysis. \n\nThis study found major obstacles in conducting a systematic review during the data processing phase. Firstly, the target word was semantically too general for large datasets (as explained in the introduction section, several terms are also considered as part of ITS, such as traffic management system). Thus, word embedding was not adequate to separate research articles that were not really related to the term ITS. Secondly, the quality of dataset homogeneity and considerable time for the reading process. Therefore, this study added NER, LDA, and word embedding is used in smaller topic clusters to make it easier to capture the various contexts inside the cluster.   depicts the adopted improved proposed framework. The extent of the usefulness of the framework will be explored.   \nThe proposed framework. \n  Figure\u00a02   \n\nThe framework consists of the following components:   \nNER to classify and screen documents that contain ITS terms; \n  \nLDA to cluster document topics related to ITS terms; \n  \nWord embedding to capture all words that provide context by using a target word. In this way, the framework can extract the context to reduce the reading process; \n  \nCitation analysis to determine the direction of ITS knowledge core development and the overall research trend. \n  \n\n### Text preprocessing \n  \nThis study uses the title and abstract of journal articles and proceedings papers as input variables for the NER embedding layers. This stage eliminates words and characters that are irrelevant to the context in the input variables, such as \"@Elsevier Ltd. Rights Reserved\", \"Published @IEEE\", etc. Irrelevant characters such as numbers, punctuation, and extra spaces are also eliminated. The process continues with tokenization, spelling correction, and lemmatization to give the words in the dataset a consistent structure and context. An example result is as follow (see  ).   \nText preprocessing results (Example). \n  Table\u00a01   \n\n\n### Custom NER \n  \nThis process begins by creating a data train for all terms that fall within the ITS scope. Custom NER parse the terms by utilizing the IOB (inside-outside-beginning) form and reserved 2% of the original data as a data train development set. The study uses the en_core_web_md English model from the SpaCy library in the Python programming language and stochastic gradient descent (SGD) optimizer for the NER pipeline with 10% dropout. \n\nThe custom NER model is evaluated by using a data test, our in-house evaluation set, by randomly picking 23 sentences. The study used the loss number, F-1 score, precision, and recall as evaluation metrics; subsequently, the results are 61, 85%, 90%, and 83%, respectively. \n\nThe custom NER result is POS (part of speech) tagging for ITS terms, and later, the tags are used as labels to separate documents. Documents that do not have an ITS-POS tag are unrelated documents with ITS terms. Thus, this process eliminates them and create a new dataset containing only documents related to ITS terms. The example result is as follows (see  ).   \nCustom NER results (Example). \n  Table\u00a02   \n\n\n### LDA \n  \nThe new dataset consists of 21,995 documents. The study clusters the new dataset by topic, and then the movement of a topic cluster over time can be assessed. It uses coherence (c_v measurement) for model evaluation. Topic coherence reflects how semantically the high score word tokens fit within each topic cluster. In the first step, it proposes an initial model using k (number of topics) = 10 to establish a coherence baseline, and the result is 0.41. Then, hyperparameter tuning to determine the hyperparameters k, alpha, and beta with default alpha and beta values of 1 and k values ranging from 2\u201311. We choose the k, alpha, and beta values that yield the highest coherence. The results are k = 10, alpha = 0.3 and beta = 0.9, which yield a coherence score of 0.45. \n\nIn the second step, the study creates another initial model with k = 10, alpha = 0.3, and beta = 0.9. Subsequently, the model creates a graphic to see which k has the highest coherence ( ). Based on the chart, several k numbers give the best coherence scores. If we use a small number for k, the dataset may be overgeneralized. However, if k is too high, there terms in each cluster will be limited, which might affect the review process using word embedding. Additionally, there is a possibility that topic groups may become redundant. Therefore, to choose the final k number, we directly use the \u201ceyeball\u201d approach for the topic content in addition to considering the coherence number. The model decision is based on coherence and whether the topics have redundant information or contain research articles with certain similarities. Hereafter, the final hyperparameters are k = 6, iteration number = 2000, alpha = 0.3, and beta = 0.9.   \nBest topic number (k). \n  Figure\u00a03   \n\nLDA will produce phi and theta values. Phi is the distribution of word tokens within a topic, while theta is the distribution of topics in a document. This study uses theta to help determine the dominant topics in research articles in each topic cluster. This approach simplifies the systematic review process to determine the direction of ITS knowledge development based on these dominant topics. Additionally, as an a priori approach, we choose documents with theta \u22650.5 in each topic cluster. In this way, it distinguishes the prevailing context in every topic cluster.   lists the topic modeling results.   \nTopic groups. \n  Table\u00a03   \n\nThis study found that several topic groups (topic groups one and three) have very distinct research objects. Although they share the same research theme, due to differences in research objects, they have the result of review in different directions. For example, in topic one, this group clearly discusses control and safety. However, we found that these articles discuss 'safety' from various objects, namely 'vehicle', 'drive behaviour', 'traffic signal', and 'alarms'. Therefore, we decided to repeat the LDA process on these topic groups to make it easier to comprehend the research. \n\n\n### Word embedding and citation analysis \n  \nThe study uses word embedding and citation analysis to help minimize the manual reading process. Word embedding is used to find specific words from each topic cluster as a context using certain words as a keyword that we want to check. As an example, we use \"method\" as a keyword. We manage word embedding to show words from each cluster that implied the same context as the keyword. Keywords such as \"Markov chain\", \"neural network\", \"reinforcement learning\", and \"deep learning\" were used. In this way, word embedding can find the dominant context within the cluster topics without reading them individually. The study uses citation analysis to identify documents with a more significant impact than other documents in each cluster. The same treatment applies to each cluster to dive deeper into each topic and answer the research questions. \n\nThe word embedding method is the Skip-Gram method because the purpose of this study is to use one or more keywords to obtain words (context) similar to these keywords. The first problem is that the words that construct the context are phrases or combinations of two words, such as deep learning, molecular cameras, intelligent transportation, highway systems, and others, that cannot be separated. Therefore, before entering the modeling and pretraining process, we create a bigram dictionary from each topic cluster. For word embedding, the study uses intrinsic evaluation, and the following results were obtained (see  ).   \nIntrinsic evaluation results. \n  Table\u00a04   \n\nThe study uses the citation numbers to determine which articles are mostly referred to by other researchers. Broadly speaking, after completed the modelling and pretraining Skip-Gram, we look for the context with keywords as references (research questions as the guidelines and scope). Hereafter, we look for articles that contain the context. Articles that can be considered influential or have the greatest impact are articles with considerable citation numbers. Additionally, to keep up to date with the most sophisticated technology and the latest methods, we limit the reading process. We heavily focus on research articles published after 2010 and 2015. Old publications were only retain in certain cases requiring us to know the origin or initial source of knowledge within the ITS scope. \n\n\n\n## Results and discussions \n  \nITS integrates systems (users, road infrastructures, and vehicles) to significantly improve safety, efficiency, and convenience and preserve the environment by creating good traffic patterns. To achieve this concept, ITS utilizes and combines state-of-the-art information systems and telecommunication technologies. In other words, ITS involves a collection of various disciplines to achieve the desired concepts and goals. Therefore, it is mandatory to examine research articles in each topic cluster to obtain complete details about the disciplines that have been researched and developed concerning ITS. We determine the topic distribution according to the publication year to obtain an aggregate picture of ITS research topic evolution over time ( ). Mainly, the dataset is divided into six main topic clusters, including:   \nTopic one: Traffic control and safety \n  \nTopic two: ITS in general and public/urban transport \n  \nTopic three: Detection systems \n  \nTopic four: Emissions reduction and electric vehicles, energy resource substitutions, and intelligent systems for train systems \n  \nTopic five: Vehicle communication systems \n  \nTopic six: Traffic optimization, prediction, modeling, networks, and data \n    \nTopic distributions in documents according to the publication year. \n  Figure\u00a04   \n\nVisually, topics 1, 2, and 4 displayed a decreasing trend over time, and topics 3, 5, and 6 displayed an increasing trend. \n\n### Topic 1: traffic control and safety \n  \n shows that the overall theta distribution for topic one (traffic control and safety) has decreased slowly over time. The study performs another LDA with k = 4, iteration number = 1500, alpha = 0.3, and beta = 0.9 to further assess the research themes associated with this cluster. This cluster consists of four subtopics ( ):   \nSubtopic one: Vehicle control systems and models \n  \nSubtopic two: Driving and driver behavior models \n  \nSubtopic three: Traffic signals \n  \nSubtopic four: Crash alarm systems \n    \nSubtopic distributions for topic one according to publication year. \n  Figure\u00a05   \n\n#### Vehicle control system and model \n  \nResearch related to vehicle control systems and models began to appear in the 1990s. This particular research theme has increased annually in popularity (as   shows). From its first appearance, the research theme has considered autonomous vehicles. Later, the research shifted from stand-alone vehicles to vehicle groups (platoons). Control systems and models of safety issues have focused on sensors, body parts, and even global positioning autonomous vehicles (either individuals or platoons). The goal (regarding safety issues) is to control vehicle velocities under all circumstances and on all roads through mathematical or simulation approaches. Previous studies proposed various approaches, and no study dominated one another's, whether they focused on big data, linear programming, high-fidelity simulations, model-in-loop simulations, or other research themes. \n\nMilanes   et\u00a0al.   [ ] developed a mathematical model (fuzzy controller) to enhance vehicle-to-vehicle communication through an intersection-detection system that automatically allows vehicles to take turns at an intersection while avoiding collisions. Guo   et\u00a0al.   [ ] proposed a framework for connected automated vehicle (CAV) platoons by using model predictive control (MPC) to stabilize CAV platoons under dynamic uncertainties. Both research articles used a mathematical approach, and another research article used a closed-loop simulation approach (enhancing X-in-the-loop simulation with cooperative adaptive cruise control) to test vehicle control functions in platoon mode [ ]. Another study combined both approaches, proposed a mathematical model to stabilize the follower vehicle speed in platoon mode and simulated various cases with a series of robots that acted as vehicles [ ]. \n\nHou   et\u00a0al.   [ ] used a different approach to propose a collision-avoidance system considering two types of collision avoidance system algorithms: those for collision avoidance systems with and without interrobot communication. Both were developed and tested in two main road crash scenarios: rear-end collision and junction-crossing intersection collision scenarios. Both algorithms were tested and run in both simulations and experiments; they use vehicle velocity and positioning inputs obtained through vehicle-to-vehicle communication in the current autonomous car collision avoidance system. \n\n\n#### Driving and driver behavior model \n  \nThis subtopic has been around since the 1980s. The research articles focus more on driving patterns, the physical conditions of drivers, and responses in various driving situations, especially in relation to navigation devices. In 2014, the scope of driving behavior model research expanded into enhancing sensor-based functions and vehicle-to-vehicle communication, and microscopic traffic simulation has dominated the research methods. Moreover, driver behavior models have also been developed with a focus on the use of technology to model time responses associated with disturbances (devices or hearing speech), physical responses (such as awareness, drowsiness, fatigue, stress detection, and recognition of traffic signs), decision making (lane, route, and intersection decisions), and software interference effects for drivers. \n\nDrastic changes have occurred since 2015, when most of the research themes were related to autonomous vehicles. Mathematical methods, including big data, machine learning, and technology-based methods, have become increasingly popular for use with sound and image recording devices and sensors. However, the distribution of this particular subtopic has been relatively stagnant over time. Regarding safety issues, modeling driving patterns is about the early detection of risky manual driving behaviors with or without autopilot mode and for various transition times (time to switch on and off autopilot) [ ,  ,  ]. Optimal safety measures while changing lanes, with or without autopilot model, remain a hot research theme. Lane change maneuvers are among the largest contributors to road traffic accidents. Hou   et\u00a0al.   [ ] studied this problem by creating an advanced driver assistance system with machine learning. Another study used an in-vehicle platform to detect risky behavior within a short window of time, such as in cases with tight spaces between vehicles while driving and during lane changes. A mathematical model was embedded in a neural network to improve classification [ ]. This particular research theme was then expanded to include intelligent driving assistance for autonomous vehicles, with decisions not just for changing lanes but also to avoid risk with or without autopilot mode activated [ ,  ,  ]. Overall, there are more studies regarding driving behavior modeling than driver behavior modeling. \n\nDriver behavior models are mainly related to reading cognitive and physical responses using technology to avoid accidents. Physiological signals, such as electrocardiogram, galvanic skin response, and respiration signals, were measured using in-vehicle devices and classified using sparse Bayesian learning; principal component analysis was then used to detect stress and help drivers better manage negative status situations [ ]. The study was then expanded to consider various distractions simultaneously. Another study focused on developing algorithms for detecting simultaneous distractions using only data related to vehicle dynamics. An experiment was designed that included two distracted driving scenarios and control with multiple runs for each. A medium-fidelity driving simulator was used to acquire vehicle dynamics data for each scenario and each run. Several data mining techniques (linear discriminant analysis, logistic regression, support vector machines, and random forests) were explored to investigate the performance of each method in detecting distractions [ ]. \n\nBylykbashi   et\u00a0al.   [ ] proposed an intelligent fuzzy-based driver monitoring system (FDMS) for safe driving. They presented and compared two fuzzy-based systems: FDMS1 and FDMS2. To make a decision, FDMS1 considers the vehicle's environmental temperature (VET), the environmental noise level (NL), and the driver's heart rate (HR). FDMS2 considers the driver's respiratory rate (RR) as a new parameter to determine the driver's situational awareness (DSA). The driver's ability to operate the vehicle was evaluated by monitoring their condition. The research themes mentioned above focus on the detection of disturbances and also compare methods and consider various disturbance types. Another focus is the use of machine learning and neural network methods in clustering and classifying driver behavior, responses, and potential accidents. \n\n\n#### Traffic signals \n  \nTraffic signals keep traffic moving at a stable speed and as efficiently as possible under any conditions (routine or emergency). Currently, the research focus is integrating traffic signals with vehicles using a wireless connection. Therefore, real-time data are needed to conduct this type of research. Nonetheless, these challenges have not discouraged researchers because the distribution of this subtopic has increased over time. \n\nIn this subtopic, the studies achieved integration in two specific ways. First, traffic signals were enhanced by implementing the proposed algorithms and sophisticated systems to regulate traffic. Second, vehicle-to-vehicle communication and vehicle-to-infrastructure systems have been improved (i.e., vehicles can predict each other's trajectories without overlapping or determine the traffic density). For this subtopic, word embedding helped identify the most frequent research objects: intersections, urban road networks, freeway networks, and route guidance. Conversely, ramp meters, variable message signs (VMSs), and weaving were found to be the least-common subjects. The mathematical approach is also a favorable choice for this particular subtopic, also simulation and machine learning approaches are common. \n\nOne particular study [ ], was commonly cited. In the study, cooperative vehicle intersection control (CVIC) was investigated in a connected vehicle environment to enable cooperation between vehicles and infrastructure for effective intersection operations and management when all vehicles are fully automated (i.e., no traffic signals). The goal was to avoid collisions and overlapping trajectories. Many other studies have attempted to develop proposed algorithms to achieve better results. \n\nFurther research, such as [ ], focused on connected vehicle safety issues and noted that traffic signal control strategies mainly rely on infrastructure-based vehicle detectors. However, this approach has drawbacks, and the vehicle location and speed cannot be directly measured. Data collected from connected vehicles provide a complete picture of the traffic states near an intersection and can be utilized for signal control. A real-time adaptive signal phase allocation algorithm was developed using connected vehicle data, and corresponding problems were solved as two-level optimization problems. A real-world intersection was modeled in VISSIM to validate the algorithms. Wang   et\u00a0al.   [ ] developed a joint control model that simultaneously optimizes the connected vehicle speeds and coordinate signals along an arterial road (in platoon mode to pass through intersections together with no stops, at least with a minimum stop time). Simultaneously, signal timing plans along an arterial road can be optimized to achieve reduced signal delays and increased throughput. In scenarios with or without traffic signals, connected vehicles remain a hot research theme. \n\nThe general findings for this subtopic are in line with those in [ ]. Notably, emerging technologies toward a connected vehicle-infrastructure-pedestrian environment with big data have made it easier and cheaper to collect, store, analyze, use, and disseminate multisource data. The realization of collaborative control relies on the vehicle network. A real-time communication function is utilized in a vehicle-infrastructure collaborative environment to collect and analyze vehicle information from areas near intersections. The signal timing is optimized to guide the vehicle speed to a certain extent. \n\nRegardless, in the future, the physical infrastructure of traffic signals may be reduced. Other studies have explored how to optimize and advance traffic signals (including ramp signals) to reduce wait times and traffic delays in conventional road scenarios. Tachet   et\u00a0al.   [ ] proposed slot-based intersections to solve problems regarding how intersections work. In addition, others have attempted to reduce bottlenecks at conventional intersections. HomChaudhuri   et\u00a0al.   [ ] developed a fast model predictive control (MPC)-based fuel economy strategy for a platoon operating in urban road conditions. The signal phase and timing information from traffic lights are used to reduce stopping at red lights and improve fuel economy. Ma   et\u00a0al.   [ ] developed a coordinated signal control system for urban ring roads in a vehicle-infrastructure environment. Speed guidance is provided based on four subsystems, including detection, communication, signal control, and expected speed calculation. The proposed signal control system was tested using a VISSIM simulation model. Other studies have used artificial intelligence, machine learning, neural networks (such as reinforcement learning or deep learning), and optimization methods to improve adaptive traffic signal control. Reinforcement learning is also a common method in this subtopic. \n\n\n#### Crash alarm systems \n  \n shows that this subtopic distribution has decreased over time, and the number of related studies has also decreased. Mathematical approaches dominate this subtopic, followed by machine learning and big data analysis methods. Research themes related to alarm systems for rear-end collisions also dominate this particular subtopic. The core objective is determining the optimum distance at which signals are given to the driver and the optimum distance at which an autonomous system operates at a certain speed. Such methods were developed for not only rear-end collisions but also other collision types (sideswipes, single-vehicle crashes, etc.). \n\nThe study found that the core development of alarm systems aligned with vehicle-to-X communication systems and technological advancement. There are two directions in which this subtopic develops. First, studies have developed and enhanced the current (already on the market) vehicle sensory parts and systems to avoid safety issues. Studies on vehicle sensors have calculated the crash risk and the optimum distance between vehicles for bad weather conditions and reduced visibility. Peng   et\u00a0al.   and Wu   et\u00a0al.   [ ,  ] add that different vehicles and lanes affect the percentage of traffic crash risk. Peng   et\u00a0al.   [ ] proposed a different mathematical approach (log-inverse Gaussian regression modeling) to explore the relationship between the collision time and visibility considering other traffic parameters. Other parameters, such as road geometry [ ], animal detection [ ], and vulnerable road users (VRUs) [ ], were considered in other studies to improve alarm systems. The adaptive speed limit was also included in this subtopic group; this limit can be used to control the optimum distance between vehicles, even though it is not commonly considered as a safety metric for rear-end collisions [ ]. \n\nSecond, studies are focusing on concept development for vehicle-to-X communication as part of cooperative intelligent transportation systems (C-ITS) for safety improvements. Such studies treated autonomous systems as definite embedded systems in autonomous or semiautonomous vehicles. For example [ ], presented the results of a quantitative safety impact assessment of five systems that were estimated to have high potential to improve the safety of VRUs, especially cyclists, and these systems included, blind spot detection (BSD), bicycle-to-vehicle communication (B2V), intersection safety (INS), pedestrian and cyclist detection + emergency braking (PCD + EBR) and VRU beacon systems (VBS). Ehlers   et\u00a0al.   [ ] proposed bowtie analysis as a conceptual framework for evaluating the safety of C-ITS and the prevention of road traffic accidents or consequence mitigation (using three case studies under the assumption of a single-vehicle accident). Grembek   et\u00a0al.   [ ] proposed a design for an intelligent intersection (an intersection that manages all surrounding information). \n\n\n\n### Topic two: ITS in general and public/urban transport \n  \nThe distribution of this topic has decreased dramatically over time. This topic is divided into two research perspectives. First, some studies provide a general discussion of ITS, which focus on the benefits of ITS in other fields or analyses/evaluations of ITS element functions. Subsequently, this first perspective can be divided into two categories for discussion:   \nThe most recent technology applications for ITS: This subtopic focuses on exploring the novelty of new technology that can be extended to ITS. Menouar   et\u00a0al.   [ ] discussed the potential and challenges of implementing unmanned aerial vehicles (UAVs) as part of ITS applications. Amini   et\u00a0al.   [ ] proposed big data analytics for real-time traffic control. Rathore   et\u00a0al.   [ ] explored big data graphs to design a smart digital city. The benefits of this approach were extended to designing intelligent transportation systems. Veres and Moussa [ ] presented a survey highlighting various modeling techniques within the realm of deep learning in ITS. Zichichi   et\u00a0al.   [ ] discussed Ethereum for data management and services in smart transportation systems. \n  \nReviews: This subtopic involves providing an overview of ITS element functions in whole or in part. Shladover   et\u00a0al.   [ ] introduce the history of connected and automated vehicle systems and some corresponding concepts. Guerrero-Ib\u00e1\u00f1ez   et\u00a0al.   [ ] explored various sensor technologies integrated with transportation infrastructures to achieve a sustainable ITS and discussed how safety, traffic control, and infotainment applications can benefit from multiple sensors deployed in various ITS elements. Machardy   et\u00a0al.   [ ] performed research across many topics in V2X, from historical developments to standardization activities at high levels in several important fields. Zhu   et\u00a0al.   [ ] discussed a framework for big data analytics in ITS. \n  \n\nThe second perspective of this research involves ITS as part of a city or smart city. Public/urban transport is part of the problem (this study found that this theme is uncommonly cited). Notably, an advanced public transportation system combines several elements that need to be presented into an integrated information service for the public. For example, this study find that optimal bus route selection can reduce passenger waiting times. The model output is an estimated arrival time that can be given to prospective passengers. The core development of traffic route optimization itself is described in the traffic optimization subsection, and other elements that may exist in the context of an advanced public transportation system are noted. \n\nProblems such as parking [ ] can be studied to provide insight into the guidance, monitoring, and reservation components of smart car parking and directions for future research. Ref.\u00a0[ ] proposed a mathematical approach for dynamic location-dependent parking pricing and reservation to improve system-wide performance (in an intelligent parking system). Moreover [ ], provided an extensive literature review and analysis of dynamic pricing techniques used in the ITS literature. Additionally, the discussion was expanded to bikes and car sharing to promote accessibility with available transportation systems and enhance the intramodality and utilization of nonmotorized transportation modes. Other discussions included bus arrival time prediction and route optimization (others used the IoT to propose intelligent bus transportation systems), integrated public transportation systems (using the IoT), visualization of passenger flows, transit travel times, route planning, and emergency crowd evacuation. \n\n\n### Topic three: detection systems \n  \n shows that the overall theta distribution of topic three has increased slowly over time. The study performs another LDA with k = 4, iteration number = 1500, alpha = 0.3, and beta = 0.9 to dig deeper into the research themes inside this cluster. This cluster consists of four subtopics ( ):   \nSubtopic one: Traffic detection \n  \nSubtopic two: Road/lane detection \n  \nSubtopic three: Position/navigation systems \n  \nSubtopic four: Vehicle detection \n    \nSubtopic distributions for topic three according to publication year. \n  Figure\u00a06   \n\n#### Traffic detection \n  \n shows that this subtopic has slowly decreased in popularity since 2005. A drastic change in the subtopic trend was observed between 2017 and 2019. Conventional technologies such as induction loops and magnetic sensors have become increasingly desirable since 2017. In 2020, video-based technology became dominant in collecting data. Computer vision technology has revolutionized ITS, especially in regard to detection systems. \n\nThe best methods (regarding traffic detection) can produce online detection results (in real time) over long durations and for multiple objects, and these methods are capable of multitarget trajectory tracking. Machine learning methods (clustering in particular), neural networks (especially deep learning methods), and computer vision are very prominent in this field. Additionally, novel studies such as [ ] have applied generative adversarial networks (GANs) to detect incidents. Spatial and temporal rules are used to extract variables from traffic data, and a random forest algorithm is applied to rank the importance of variables. Then, new incident samples are obtained using GANs. Finally, a support vector machine (SVM) algorithm is used as the incident detection model. Additionally [ ], investigated accident detection and proposed an integrated two-stream convolutional network architecture to perform real-time detection, tracking, and near-accident detection for road users based on traffic video data. The two-stream model consists of a spatial stream network for object detection and a temporal stream network to leverage motion features for multi object tracking. Near accidents are detected by incorporating appearance features and motion features from the two networks. \n\nAhmed   et\u00a0al.   [ ] proposed a method to generate meaningful and smooth synopses of long-duration videos of traffic monitoring because outputs are often summarized and include redundant content or activities that may not help the observer. Ospina and Torres [ ] proposed different approaches, such as reidentification (ReID) and multitarget single-camera tracking (MTSC), to extract vehicle attributes. Analysis of these attributes can promote multi-target multi camera tracking. Notably, the vehicles that travel a predefined path can be counted. Therefore, in detecting traffic flows, this approach is not limited by pedestrian flow [ ], traffic incident, or traffic volume conditions. Developments for this subtopic are more likely to coincide with the methods used than the technology applied. The development of new methods can increase the effectiveness of traffic detection. \n\n\n#### Road/lane detection \n  \n shows that this subtopic has slowly decreased in popularity over time. This study found that the research results for this subtopic often overlapped with those for vehicle detection. The road/lane detection process also involves detecting other entities (such as VRUs and surrounding vehicles). While this research subtopic has decreased in popularity, vehicle detection research has increased. \n\nMathematical, machine learning, and neural network methods are commonly discussed. The data collection process includes geospatial, camera-based, video-based, light detection and ranging (LIDAR), and mobile laser scanning (MLS) methods, among others. The goal is to recognize, analyze (all the markings, boundaries, and surrounding, especially in unstructured environments), and optimize trajectories. Additionally, 3-D is the desired output dimension to have a complete road/lane geometry structure in foreign areas. \n\nZai   et\u00a0al.   [ ] presented a method to automatically extract 3-D road boundaries from mobile laser scanning (MLS) data. The proposed method includes two main stages. First, a supervoxel is generated by selecting smooth points as seeds and assigning points to facets centered on these seeds using several attributes (e.g., geometry, intensity, and spatial distance). Second, 3-D road boundary extraction is performed. Wang   et\u00a0al.   [ ] presented an offline mapping algorithm for autonomous vehicles (AVs) that consisted of five key steps. First, data preprocessing was performed to calibrate the original odometry data. Second, a 2D laser scanner and the calibrated odometry data were used to build a virtual 3D LIDAR system. Third, loop closure was performed to search the revisited region and calculate the distance displacement. Fourth, an optimizer was applied to generate the final trajectories. Finally, by fusing the point cloud data from virtual 3D LIDAR and the final trajectories, a point cloud map was generated. \n\nNevertheless, other researchers are still exploring and developing different methods. No study in this subtopic has dominated one another's. However, the development direction is clear, namely, toward improved representations and lane/road detection results. For example [ ], proposed a new cumulative density function (CDF)-based symmetry verification method for lane change detection, forward collision warning, and overtaking vehicle identification. The motion cues obtained from an optical flow are used for overtaking detection. This approach was further combined with a convolutional neural network. Dairi   et\u00a0al.   [ ] addressed urban scene monitoring and tracking obstacles based on unsupervised deep-learning approaches. They designed an innovative hybrid encoder that integrates deep Boltzmann machines (DBMs) and autoencoders (AEs). This hybrid autoencoder (HAE) model combines the greedy learning features of DBMs with the dimensionality reduction capacity of AEs to detect the presence of obstacles accurately and reliably. Then, the researchers combined the proposed hybrid model with one-class support vector machines (OCSVMs) to visually monitor an urban scene. Additionally [ ], used a deep neural network architecture to detect lane markings in a complex environment by analyzing the corresponding structural information. First, they used a semantically guided channel attention (SGCA) module to select the low-level features from a deep convolutional neural network (CNN) by using high-level features for guidance. Second, a deformable pyramid convolution (PDC) module was developed to enlarge the receptive fields and capture the complex structure of lane marking by applying deformable convolution through multiple feature maps with different scales. \n\n\n#### Position/navigation systems \n  \n shows that this subtopic has slowly decreased in popularity over time. In ITS applications, high horizontal positioning accuracy is needed. Many conventional map-matching algorithms have been developed and involved topological analyses of spatial road network data, probabilistic theory, Kalman filters, fuzzy logic, belief theory, hierarchical classifiers with class unfolding paired and context-based localization, two direction-of-arrival (DOA) estimation, and many other methods. Mathematical methods dominate this subtopic. Conversely, artificial intelligence and machine learning are rarely considered. \n\nOver time, an increasing number of problems, such as frequency pairing, vehicle localization judgment, and curved road issues, have been considered in the context of meeting autonomous driving requirements, such as real time, centimeter-level accuracy, storage efficiency, and usability requirements. Gwon   et\u00a0al.   [ ] attempted to simultaneously meet these three requirements and proposed a precise and efficient lane-level road map generation system that conforms to the relevant requirements. The proposed map-building process consists of three steps: data acquisition, data processing, and road modeling. The data acquisition and processing system captures accurate 3-D road geometry data by acquiring mobile 3-D laser scanner data. \n\nLater, problems were divided into two types: navigation systems for conventional vehicles and navigation systems for autonomous vehicles; both are affected by navigation technology development and information, communication technology development, and mathematical method development. Moreover [ ], stated that to make better positioning and navigation, next generation ITS will be required to provide reports on traffic status, road condition, and driver behavior information. Their study aimed to reduce noises and biases on navigation systems (especially on road with high anomalies and vehicles that use low-cost sensors). Regarding autonomous driving, a study such as [ ] used a novel adaptive federated Kalman filter (FKF) with time-varying information sharing factors to enhance unmanned ground vehicle (UGV) integrated navigation systems for better high-precision positioning and navigation. As stated before, mathematical methods very dominated this issue. \n\n\n#### Vehicle detection \n  \nThis subtopic has displayed an increased distribution over time. Neural network methods (especially deep learning, convolutional neural networks, and long short-term memory) are dominant, also reinforcement learning, machine learning, and video surveillance technologies are used. Some other studies, such as [ ] have used different technologies and approaches; notably, an architecture was presented to detect a vehicle and compute continuous global six-degree-of-freedom (6-DoF) poses through joint 2D landmark estimation and 3D pose reconstruction. Additionally [ ], focused on developing a roadside magnetic sensor system for vehicle detection. However, since 2016, technological and other methods have rarely been applied. \n\nThe goal of vehicle detection is to analyze and recognize vehicles or other entities in real time at a large scale (multiple targets and moving objects), rapidly, and with good image quality. Thus, this subtopic is highly dependent on video surveillance technology and methods. In ITS applications, research on proposing or developing existing recognition methods is more common than research involving the development of video surveillance technology. In this particular research field, neural network methods are common. Ref.\u00a0[ ] proposed an embedded system for fast and accurate license plate segmentation and recognition using a modified single-shot detector (SSD) with a feature extractor based on depth wise separable convolutions and linear bottlenecks. Additionally [ ], proposed methods to recognize vehicle colors in complex traffic scenes, and a new network type, namely, a multiscale comprehensive feature fusion convolutional neural network (MCFF-CNN), based on residual learning was developed for color feature extraction. First, the MCFF-CNN network is used to extract the in-depth color features of vehicles. Then, a support vector machine (SVM) classifier is employed to obtain the final color recognition results. \n\n\n\n### Topic four: emissions reduction and electric vehicles, energy resource substitutions, and intelligent systems for train systems \n  \nThe distribution of this topic has decreased very sharply over time and is now close to 0. Two causes for this trend were identified. First, many paper titles and abstracts in this cluster contain only one or two words in the term 'intelligent transportation system'. It happened because we allowed the dataset to have excess article loads rather than lack of the actual number of ITS articles. LDA can reduce the excess load by using the theta variable. However, the drawback is that articles in this group can be inhomogeneous. Therefore, this section might not represent actual knowledge development in the discussion hereafter. Second, extensions of main ITS topics are given as follows:   \nAntenna and other technology specifications for communication. Those with suitable specifications for ITS applications are considered [ ,  ,  ]. \n  \nIntelligent systems for train systems. These articles do not directly discuss ITS core knowledge; instead, they discuss enhancing train systems with various developments, technologies, and elements that support ITS concepts, such as timetable management, intelligent fault detection, intelligent operation, and maintenance systems. \n  \nITS applications for emission reductions [ ,  ]. \n  \nElectric vehicle and energy source substitution applications. This subject is quite distinguishable in this cluster. The relationship between ITS and these terms is related to reducing emissions and enhancing green transportation. The scope of the discussion includes the technology used in electronic vehicles in the ITS context [ ], nanogenerators as a substitute energy source for vehicles or infrastructures [ ,  ,  ], and energy management for electric vehicles in the ITS context [ ]. \n  \n\n\n### Topic five: vehicle communication systems \n  \n shows that this topic has increased in popularity over time. Most studies in this cluster are about vehicular ad hoc networks (VANETs) and the corresponding problems (routing, security, confidentiality in data transmission, quality-of-services, etc.). The following subjects were noted in this topic cluster.   \nRouting problems \n  \n\nStudies on this subject have mainly proposed or developed routing schemes/protocols (no schemes/protocols dominate others in this topic cluster); in addition, routing optimization is often considered. The goal is to find the best scheme/protocol for the vehicular environment or any other scenarios and tackle scalability issues. Additionally, safety-critical information can be shared and routed to other intended vehicles in real time with high mobility and varying densities (in other words, the data can be used to solve highly dynamic topological problems). Moreover, the vehicular concept has evolved toward autonomous vehicles. In this context, the study was conducted solely using a word embedding approach, and there are some points worth noting.   \nThe most discussed scenarios are urban and highway scenarios. \n  \nMathematical and optimization approaches highly dominate this subject (the terms fuzzy logic algorithm and heuristic algorithm are common); very few studies use machine learning approaches. \n  \nTerms that occur a lot and are closely related to the context of 'route problem' are position-based routing (PBR), hybrid, media access control (mac) protocols, greedy forwarding method, and multihop broadcasting. \n  \nThe emergence of 5G network applications for message dissemination is notable. \n  \nHamdi   et\u00a0al.   [ ] described and reviewed the routing problem for vehicle communication systems. \n  \nCommunication channels \n  \n\nStudies on this subject often discuss channel characteristics, implications toward other subjects, models, and channel performance; propose/develop communication channels; and explore radio frequencies. Thus, this subject is associated with advances in routing and network communication technology. Method development has continued in accordance with the technological development. The goal is to deliver an ultrahigh data transmission rate with low end-to-end delay. \n\nRadio waves are still popular topics, but millimeter waves (mmWaves) in particular have begun to dominate the discussion in various studies since 2018, especially for vehicle-to-vehicle (V2V) communication applications. Since then, discussion of the application of 5G for V2X communication has been ubiquitous. He   et\u00a0al.   [ ] reviewed state of the art methods for mm-wave V2V channel measurements and modeling, described recent directional V2V channel measurements performed in the 60-GHz band, and discussed future challenges to be addressed in mm-wave V2V channel measurements and modeling.   \nSecurity for vehicle communication systems \n  \n\nEl-Rewini   et\u00a0al.   [ ] and Manivannan   et\u00a0al.   [ ] reviewed and summarized the safety issues related to vehicular communication from various angles. This study noted several crucial factors, namely, data security, user privacy/authentication, and message authentication, that can affect the security of VANETs. A security countermeasure protocol can simultaneously be created when technology is developed. Studies on this subject have focused on proposing security frameworks/protocols or new methods/technologies to improve security framework/protocols. For example [ ], presented a framework for effective user identification and authenticity to control access. Tan and Chung [ ] proposed a secure authentication and key management scheme with blockchains.   \nQuality of services (QoS) \n  \n\nThis subject is mainly related to developing network services under ITS requirements. VANETs are the relationships between roadside units (RSUs) and vehicles and among vehicles, and now the topic has expanded to include vehicle relations with other entities on the road (pedestrians and others). The direction of development is in line with advances in network and cloud computing technology, such as fog computing. This subject is focused on how networks can provide better and more efficient services in the context of the Internet of Vehicles (IoV) and vehicular cloud computing (VCC). \n\nA study worth noting [ ], proposed a novel architecture for real-time ITS big data analytics in the IoV environment. The proposed architecture merges three dimensions, including intelligent computing (i.e., cloud and fog computing), real-time big data analytics, and IoV dimensions. The IoV environment, ITS big data characteristics, lambda architecture for real-time big data analytics, and several intelligent computing technologies are described. Additionally, the opportunities and challenges associated with implementing fog computing and real-time big data analytical methods in the IoV environment are discussed. Other studies have connected network service enhancement directly to ITS elements, as in [ ], where the optimal deployment and dimensioning of a fog computing-based IoV infrastructure for autonomous driving was investigated. \n\n\n### Topic six: traffic optimization, prediction, modeling, networks, and data \n  \n#### Traffic optimization \n  \nThis cluster involves finding the optimal route and optimizing traffic signals in conventional road scenarios (traffic signals are discussed in the traffic signal subsection). The goal is to achieve the fastest travel time. Two discussion categories were identified: route optimization in environmental scenarios and conventional vehicles and route optimization in advanced environmental and vehicle scenarios (such as those with autonomous systems). \n\nIn the first category, the tendency has been to study and determine the optimal route for a specific case rather than to develop a route optimization method. Research on particular issues has included determining optimal ridesharing routes (carpooling), fleet sizes (trucks), bus routes, etc. Route selection methods are dominated by optimization, statistical, and mathematical approaches. Very few studies have suggested methods other than those discussed earlier in the paper (such as machine learning or dynamic traffic assignment). \n\nIn the second category, studies have focused on complex and broad-scale problems. The route selection methods are dominated by optimization and mathematical approaches. Hu   et\u00a0al.   [ ] presented a real-time dynamic path planning method for autonomous driving that avoids both static and moving obstacles and provides appropriate acceleration and speed constraints for a vehicle based on a mathematical approach. First, a centerline from a set of predefined waypoints is construed. A series of path candidates is generated based on the arc length and offset to the centerline in the s-\u03c1 coordinate system. Subsequently, the point coordinates are converted into Cartesian coordinates. Rossi   et\u00a0al.   [ ] did almost the same thing but with more complex objectives, namely, modeling the search for optimal ridesharing routes decoupled with rebalancing self-driving vehicles. Other studies, such as [ ], proposed traffic path planning algorithms based on data prediction to find the shortest travel time or path. Predictive models based on historical traffic data and current traffic information with load balancing have been developed based on both mathematical methods and clustering. \n\n\n#### Traffic flow prediction \n  \nThe study found two types of traffic flow prediction, namely, short-term and long-term prediction. The short-term prediction predicts traffic flow within minutes to approximately 1 h. Meanwhile, the long-term prediction is a prediction of traffic flow over a more extended period. Overall, the approaches are dominated by big data, machine learning, and neural networks: clustering (k-nearest neighbor and SVM), random forest, deep learning, long short-term memory (LSTM), artificial neural network (ANN), recurrent neural network (RNN), CNN, stack autoencoder, generative adversarial networks (GANs), gradient boosting; and a spatial-temporal approach. Statistical and time-series approaches tend to be less desirable, and very few combine all of these methods with optimization. It is important to note that all of these methods were selected and developed to predict different traffic flow characteristics. Very few studies have reviewed the relationship between traffic flow characteristics and the various methods selected and developed. \n\nThe study also found that researchers tend to use more statistical approaches, time series (ARIMA and SARIMA), machine learning, and neural networks for short-term prediction. However, some studies use machine learning and neural networks for spatial-temporal approaches or hybrid models (combining two or more different approaches). In contrast to long-term prediction, researchers are very much dominated by big data approaches, machine learning, and neural networks. Currently, the spatial-temporal approach is cited a lot. Lv   et\u00a0al.   [ ] proposed a stacked autoencoder (trained in a greedy layerwise fashion) method for traffic flow prediction, which inherently considers the spatial and temporal correlations. Zhao   et\u00a0al.   [ ] proposed a model by combining the graph convolutional network (GCN) and the gated recurrent unit (GRU). The model is called Temporal Graph Convolutional Network (T-GCN). The model is able to study complex topological structures and comprehend dynamic changes in traffic data to apprehend spatial and temporal dependence. Afterwards, they use the model to employ traffic forecasting based on the densely populated road networks. It can be said that the knowledge development direction of traffic flow prediction is highly dependent on the development of machine learning methods, neural networks, and statistics. \n\n\n#### Traffic flow networks and models \n  \nThis subtopic is closely related to the traffic lights/signals research theme described in the subtopic of traffic signals. However, this subtopic is not as ubiquitous as traffic control and traffic signals. A few recent articles have many citations compared to those published in previous years. Traffic flow can be observed microscopically, macroscopically, and at the network level. This study found that none of the three types of observations has dominated one another's. \n\nThe objectives include analyzing traffic congestion (including the different types of congestion), identifying traffic bottlenecks, controlling traffic signals (adaptive traffic signal control), placing traffic lights/signals, and selecting optimal routes in route guidance. These objectives are often linked directly as a final result rather than evaluating traffic flow networks and modeling in separate steps. The following modeling and approaches were the most common:   \nMathematical modeling based on communication systems. Such cases include the topological structure of the VANET communication scheme, the number of vehicle arrivals at a particular node, or a weighted network model. Other studies, such as [ ], have proposed cyber-physical system (CPS) sensors for autonomous vehicles to model traffic flows. Such models can also assist neighboring autonomous vehicles by communicating the required information through ad hoc network communications or a centralized cloud. \n  \nMathematical modeling based on traffic density at a particular travel time and speed-density functions. Other related models include car-following, cell transmission, intelligent driver (ID), macroscopic fundamental diagram (MFD), and Markov chain models. \n  \nOptimization approaches are the most ubiquitous of all methods and include bilevel multi-objective optimization, genetic algorithms, and ant colony optimization. \n  \nBig data, machine learning, and neural network (autoencoder stacking, reinforcement learning, deep learning, ANNs, CNNs, and LSTM networks) methods have been used to analyze vehicle densities. Other studies have extended these approaches for spatial-temporal applications. \n  \nSimulation approaches, such as VISSIM simulations. \n  \nDynamic system modeling approaches that treat traffic as a hybrid dynamic system. \n  \n\nThis study found that this subtopic mainly involves the continuous evolution of traffic congestion in various road scenarios and the development of methods to improve vehicle mobility. Variables such as speed, density, and other parametric controls continue to be explored to obtain the best modeling results. \n\n\n#### Traffic data \n  \nSome problems related to data can be identified. First, data collection within the ITS scope can performed with sensors, smart cards, communication systems, Google functionalities, surveillance cameras, etc. Smart cards are used to determine individual mobility capacities, such as through origin-destination (OD) flows. Such data can be used to design an advanced traveler information system (ATIS) or public transportation information system. OD matrixes have been increasingly used to predict individual travel flows and estimate the time of arrival. \n\nSecond, data storage is also a problem related to ITS requirements. Existing studies have attempted to make data storage more efficient. For example, a study aware that trajectory data storage may consist of traceability and vector storage. State vectors are extracted from the analysis of the original sample data (from sensing devices). The study effectively reduces the frequency of sampling data storage, reduce query and analysis operations, also the study uses a vector function that is able to transform road network into indexes. Therefore, an insignificant amount of vector data and road networks can be stored [ ]. \n\nThird, another problem is missing data. The existing studies have attempted to assess data by exploring the related spatial-temporal patterns. For missing data, singular value decomposition (SVD), singular value regression (SVR), neural network (feedforward neural network) approaches, ARIMA, Bayesian models (Bayesian augmented tensor factorization), low-rank matrix decomposition, kriging, GAN methods, unsupervised methods, and other methods have been combined with tensor decomposition to estimate missing values. Additionally, check-in data from social media, synthetic data, road network representations with embedded graphs, and GANs can be used to generate road traffic state information in real time. The most common mathematical and statistical approaches include lp-norm regularized sparse self-representations, which incorporate nonconvex lp-norm information with 0 < p < 1, and kernel sparse representations with based on a combination of the L1-norm and L2-norm. Fourth, dimension reduction and data evaluation problem. Dimension reduction can use a variational autoencoder (VAE) or similar tool to generate and infer data by exploring latent spaces for data selection. Data evaluation can use the expert system. \n\n\n\n\n## Conclusions \n  \nThe systematic review is teamwork, which is also one of the obstacles of this research. Several platforms have been developed to conduct systematic reviews. Still, the screening process remains a separate part and one of the reasons why it is teamwork. On the other hand, ITS is a combination of various disciplines within transportation system (communication technology, management systems, and others). When we generate a dataset with the keyword 'intelligent transportation system', two things happened. First, it will generate research articles containing these keywords. Second, Scopus categorizes articles that do not contain these keywords, but the research themes in it are considered related to each keyword. The fact that several articles discuss aviation being returned inside dataset as part of the keyword 'transportation'. However, these articles are not part of the 'intelligent transportation system' context. Thus, filtering in Scopus alone is not good enough as a benchmark for the quality of dataset homogeneity, and the quality of the screening process results is essential. Problems will arise if these unrelated articles have high citation numbers. If we directly use the unscreened dataset on the platform, the platform will make the unrelated articles as part of high-impact documents for review, which will cost more time. Therefore, the method used in this study is highly concerned with the dataset condition, objectives, and research questions. \n\nCustom NER is well known for tagging documents. This study utilizes the tag function to separate the unrelated research articles. Tags are proven to work well on documents containing standard terms such as medical terms 'cardiovascular' [ ,  ,  ,  ]. ITS sectors also have the exact phrase in every English-language literature, for example, 'Public Transportation System', 'Traveler Information System', and others. Therefore, custom NER method can filter documents that do not coincide with ITS terms. The obtained evaluation results include a loss value of 61, F-1 score of 85%, precision of 90%, and recall of 83%. The obtained dataset contained 23,823 documents; this method can screen as many as 21,995 documents related to ITS. Documents that have the term ITS are labeled and then separated into new datasets and clustered using topic modeling. \n\nThe topic modeling approach used in this research is LDA. LDA clustered 21,995 documents into six topic clusters. Subsequently, we divided documents into subsets with theta values \u22650.5 for each topic cluster. The benefit of this method is that the study can overcome the shortcomings of the custom NER method. There is a slight possibility that custom NER will not yield an accurate label for a document that is not in the ITS scope; however, because it contains words such as 'traffic control method', the document will be labeled as an ITS document. This example highlights the dimensional curse of the custom NER approach, in which the number and sequence of words as features in the document affect the POS tagging results. LDA will mask this issue by generating theta values that cannot represent a document included in any topic cluster. An example is given in  .   \nInaccurate results based on custom NER. \n  Table\u00a05   \n\nAlthough the combination of custom NER and LDA has yielded promising results, there might be other drawbacks of these methods that cannot be denied. The presented process is a systematic review method. One of the objectives of this study is to reduce the number of manual reading steps. For this, we used a word embedding method. Word embedding was performed for each topic cluster to generate the document information of interest. The study uses keywords such as 'optimization' and 'method' for topic cluster 6. Then, word embedding provides words as a context that have similarity with the keyword, i.e., \"genetic_algorithm\", \"ant_colony\", and \"particle_swarm\". Adhering to the distribution hypothesis, we can determine that in the topic cluster 6, which is related to route optimization, the most relevant context is provided by adjacent words. Then, we can determine which document index contains the context. Thus, by referring to the document index, we compare index citation magnitudes. The document index with the large number of significant citations can be used as a reference by other researchers studying vehicle detection. \n\nRelated to the first research question: The core knowledge of ITS has evolved over time. This study finds that detection system, communication, and traffic are topics that appear more frequently throughout the year than other topics. Broadly speaking, all research leads to integrating all possible entities in real-time and extending the communication not limited to V2V. It can be easily said that in the future, ITS will move toward a C-ITS concept. That is, information exchange will not be limited to V2V and V2I communication but also include other entities, such as road users and pedestrians. Significant changes are expected to occur in communications, sensors, and surveillance technology. Not all elements that already exist in traditional road scenarios will continued to be considered because autonomous systems are being developed. Traffic control will be viewed as a real-time control approach for large-scale network traffic scenarios. Smooth mobility and safety are the main goals of C-ITS. \n\nRelated to the second research question: The substantial approaches used in the research on ITS include mathematical, machine learning, neural network, and optimization methods applied in various ITS fields. This study also found that even though high-quality data are crucial, little research has been performed on the implications of using poor/limited amount of data. Various things related to standards and quality in the context of C-ITS in the future will need to be considered. Other terms worthy of being noted are as follows:   \nSpatiotemporal. This term emerges quite commonly, such as in the use of temporal GPS trajectory data to analyze driving behavior [ ]. \n  \nTaxi and urban ride hailing. This topic has recently attracted research interest due to its vast potential application in ITS. One of the benefits is reducing the number of private vehicles in use. Several studies have also discussed this topic in relation to autonomous vehicles. Some related research topics include route recommendation systems, cruising areas, minimizing passenger wait times, demand prediction, and rebalancing issues. \n  \n\nThe major challenge of this framework is to not overgeneralize research themes with too few topic clusters. Moreover, the purpose of a systematic review is to obtain a thorough picture of developments in a field. However, if too many topic clusters are considered, the amount of vocabulary in each cluster will be limited, which will affect the review process based on word embedding. We have to be careful and creative in conducting reviews with keywords and terms that form the existing context in topic clusters when diversity is limited. Another drawback is that this study does not cover articles with small theta and citation values. For example, traffic emergencies (accident detection, ambulance route, etc.), traffic congestion prediction, advanced traveler information systems, vehicle/traffic speed prediction, and cybersecurity for autonomous and unmanned vehicles are not covered. \n\n\n## Declarations \n  \n### Author contribution statement \n  \nZulkarnain: Conceived and designed the experiments; Performed the experiments; Analyzed and interpreted the data; Wrote the paper. \n\nTsarina Dwi Putri: Performed the experiments; Contributed reagents, materials, analysis tools or data; Wrote the paper. \n\n\n### Funding statement \n  \nThis work was supported by the   (No: NKB- 1433/UN2.RST/HKP.05.00/2020). \n\n\n### Data availability statement \n  \nData will be made available on request \n\n\n### Declaration of interests statement \n  \nThe authors declare no conflict of interest. \n\n\n### Additional information \n  \nNo additional information is available for this paper. \n\n\n \n", "metadata": {"pmcid": 8695271, "text_md5": "101120964e1f5e3ec6757522d59a04f8", "field_positions": {"authors": [0, 38], "journal": [39, 46], "publication_year": [48, 52], "title": [63, 175], "keywords": [189, 365], "abstract": [378, 1850], "body": [1859, 75843]}, "batch": 1, "pmid": 34988314, "doi": "10.1016/j.heliyon.2021.e08615", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8695271", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8695271"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8695271\">8695271</a>", "list_title": "PMC8695271  Intelligent transportation systems (ITS): A systematic review using a Natural Language Processing (NLP) approach"}
{"text": "Liu, Zengjian and Yang, Ming and Wang, Xiaolong and Chen, Qingcai and Tang, Buzhou and Wang, Zhe and Xu, Hua\nBMC Med Inform Decis Mak, 2017\n\n# Title\n\nEntity recognition from clinical texts via recurrent neural network\n\n# Keywords\n\nEntity recognition\nRecurrent neural network\nClinical notes\nDeep learning\nSequence labeling\n\n\n# Abstract\n \n## Background \n  \nEntity recognition is one of the most primary steps for text analysis and has long attracted considerable attention from researchers. In the clinical domain, various types of entities, such as clinical entities and protected health information (PHI), widely exist in clinical texts. Recognizing these entities has become a hot topic in clinical natural language processing (NLP), and a large number of traditional machine learning methods, such as support vector machine and conditional random field, have been deployed to recognize entities from clinical texts in the past few years. In recent years, recurrent neural network (RNN), one of deep learning methods that has shown great potential on many problems including named entity recognition, also has been gradually used for entity recognition from clinical texts. \n\n\n## Methods \n  \nIn this paper, we comprehensively investigate the performance of LSTM (long-short term memory), a representative variant of RNN, on clinical entity recognition and protected health information recognition. The LSTM model consists of three layers: input layer \u2013 generates representation of each word of a sentence; LSTM layer \u2013 outputs another word representation sequence that captures the context information of each word in this sentence; Inference layer \u2013 makes tagging decisions according to the output of LSTM layer, that is, outputting a label sequence. \n\n\n## Results \n  \nExperiments conducted on corpora of the 2010, 2012 and 2014 i2b2 NLP challenges show that LSTM achieves highest micro-average F1-scores of 85.81% on the 2010 i2b2 medical concept extraction, 92.29% on the 2012 i2b2 clinical event detection, and 94.37% on the 2014 i2b2 de-identification, which is considerably competitive with other state-of-the-art systems. \n\n\n## Conclusions \n  \nLSTM that requires no hand-crafted feature has great potential on entity recognition from clinical texts. It outperforms traditional machine learning methods that suffer from fussy feature engineering. A possible future direction is how to integrate knowledge bases widely existing in the clinical domain into LSTM, which is a case of our future work. Moreover, how to use LSTM to recognize entities in specific formats is also another possible future direction. \n\n \n\n# Body\n \n## Background \n  \nWith rapid development of electronic medical record (EMR) systems, more and more EMRs are available for researches and applications. Entity recognition, one of the most primary clinical natural language processing (NLP) tasks, has attracted considerable attention. As a large number of various types of entities widely exist in clinical texts, studies on entity recognition from clinical texts cover clinical entity recognition, clinical event recognition, protected health information recognition (PHI), etc. Compared to entity recognition in the newswire domain, studies on entity recognition in the clinical domain are slower initially. \n\nThe early entity recognition systems in the clinical domain are mainly rule-based, such as MedLEE [ ], SymText/MPlus [ ,  ], MetaMap [ ], KnowledgeMap [ ], cTAKES [ ], and HiTEX [ ]. In the past several years, lots of machine learning-based clinical entity recognition systems have been proposed, may due to some publicly available corpora provided by organizers of some shared tasks, such as the Center for Informatics for Integrating Biology & the Beside (i2b2) 2009 [ ], 2010 [ \u2013 ], 2012 [ \u2013 ] and 2014 track1 [ \u2013 ] datasets, ShARe/CLEF eHealth Evaluation Lab (SHEL) 2013 dataset [ ], and SemEval (Semantic Evaluation) 2014 task 7 [ ], 2015 task 6 [ ] 2015 task 14 [ ], and 2016 task 12 [ ] datasets. The main machine learning algorithms used in these systems are those once widely used for entity recognition in the newswire domain, including support vector machine (SVM), hidden markov model (HMM), conditional random field (CRF) and structured support vector machine (SSVM), etc. Among the algorithms, CRF is the most popular one. Most state-of-the-art systems adopt CRF. For example, in the 2014 i2b2 de-identification challenge, 6 out of 10 were based on CRF, including all top 4 systems. The key to the CRF-based systems lies in a variety of features, which are time-consuming. \n\nIn recent years, deep learning, which has advantages in feature engineering, has been widely introduced into various fields, such as image processing, speech recognition and NLP, and has shown great potential. In the case of NLP, deep learning has been deployed to tackle machine translation [ ], relation extraction [ ], entity recognition [ \u2013 ], word sense disambiguation [ ], syntax parsing [ ,  ], emotion classification [ ], etc. Most related studies are limited to the newswire domain rather than other domains such as the clinical domain. \n\nIn this study, we comprehensively investigate entity recognition from clinical texts based on deep learning. Long-short term memory (LSTM), a representative variant of one type of deep learning method (i.e., recurrent neural network [ ]), is deployed to recognize clinical entities and PHI instances in clinical texts. Specifically, we investigate the effects of two different types of character-level word representations on LSTM when they are used as parts of input of LSTM, and compare LSTM with CRF and other state-of-the-art systems. Experiments conducted on corpora of the 2010, 2012 and 2014 i2b2 NLP challenges show that: 1) each type of character-level word representation is beneficial to LSTM on entity extraction from clinical texts, but it is not easy to determine which one is better. 2) LSTM achieves highest micro-average F1-scores of 85.81% on the 2010 i2b2 medical concept extraction, 92.29% on the 2012 i2b2 clinical event detection, and 94.37% on the 2014 i2b2 de-identification, which outperforms CRF by 2.12%, 1.47% and 1.79% respectively. 3) Compared with other state-of-the-art systems, the LSTM-based system is considerably competitive. \n\nThe following sections are organized as: section 2 introduces RNN in detail, experiments and results are presented in section 3, section 4 discusses the experimental results and section 5 draws conclusions. \n\n\n## Methods \n  \nEntity recognition is usually treated as a sequence labeling problem, which can be modeled by RNN. Instead of traditional RNN, we used Long short-term memory (LSTM) [ ,  ], a variant of RNN that is capable of capturing long-distance dependencies of context and avoiding gradient varnishing or exploding [ ,  ], for entity recognition from clinical texts. The overview architecture of the LSTM used in our study is shown in Fig.\u00a0 , which consists of the following three layers: 1) input layer - generates representation of each word of a sentence using dictionary lookup, which includes two parts: token-level representation (denoted by grey squares) and character-level representation (denoted by blank squares); 2) LSTM layer \u2013 takes the word representation sequence of the sentence as input and returns another sequence that represents context information of the input at every position; 3) Inference layer \u2013 makes tagging decisions according to the output of the LSTM layer, that is, outputting a label sequence. Before introducing each the three layers one-by-one in detail, we present the LSTM unit first as it is used in both input layer and LSTM layer.   \nOverview architecture of our LSTM \n  \n\n### LSTM unit \n  \nA LSTM unit is composed of three multiplicative gates: an input gate, a forget gate and an output gate, which control the proportion of input information transferred to a memory cell, the proportion of historical information from the previous state to forget, and the proportion of output information to pass on to the next step respectively. Fig.\u00a0  gives the basic structure of an LSTM unit at step   t   that takes   x  ,   h   and   c   as input and produces   h   and   c   via the following formulas:   \nStructure of an LSTM unit \n  \n\nwhere   \u03c3   is the element-wise sigmoid function, \u2609is the element-wise product,   i  ,   f   and   o   are the input, forget, and output gates,   c   is the cell vector,   W  ,   W  ,   W  ,   W   (with subscripts:   x  ,   h   and   c  ) are the weight matrices for input   x  , hidden state   h   and memory cell   c   respectively, and   b  ,   b  ,   b   and   b   denote the bias vectors. \n\n\n### Input layer \n  \nThe representation of a word is generated from the following two aspects: token-level and character-level, which capture context information and morphological information of the word respectively. The token-level representation is usually pre-trained by neural language models, such as continuous bag-of-words (CBOW) and skip-gram [ ], on a large unlabeled data. To generate character-level representation, we can use a bidirectional LSTM, which can capture both past and future contexts of words, or a convolutional neural network (CNN) to model the character sequences of words (see Fig.\u00a0 ). In the bidirectional LSTM (see Fig.\u00a0 ), the last two output vectors of the forward and backward LSTMs (rectangles in grey) are concatenated into the character-level representation of the word (i.e., pain). In the CNN (see Fig.\u00a0 , where chess boards are paddings), the sequence of character embeddings are convoluted with filters and further pooled to generate the character-level representation of the word (i.e., pain). For detailed information about CNN, please refer to [ ].   \nCharacter-level representation generation models.   a   Bidirectional LSTM.   b   CNN \n  \n\n\n### LSTM layer \n  \nA bidirectional LSTM is used to generate context representation at every position. Given a sentence   s\u2009=\u2009w   w   \u2026w   with each word   w   (1\u2009\u2264\u2009  t  \u2009\u2264\u2009  n  ) represented by   x   (i.e., concatenation of token-level and character-level representations of the word), the bidirectional LSTM takes a sequence of word representations   x\u2009=\u2009x   x   \u2026x   as input and produces a sequence of context representations   h\u2009=\u2009h   h   \u2026h  , where   h  \u2009=\u2009[  h  ,   h  ]  (1\u2009\u2264\u2009  t  \u2009\u2264\u2009  n  ) is a concatenation of outputs of both forward and backward LSTMs. \n\n\n### Inference layer \n  \nConditional random field (CRF) is employed to predict a label sequence from a sequence of context representations. Given a training set   D\u2009=  \u2009{(  x  ,   y  )|   i  \u2009=\u20091,\u2026,  m  } (  y   is a label sequence like \u201c\u2026 O B-problem I-problem O \u2026\u201d for clinical entity recognition), all parameters of CRF (  \u03b8  ) are estimated by maximizing the following log-likelihood function over   D   (only 1  order is considered here): \n\nwhere \n\n Y  (  x  ) denotes the set of possible label sequences for   x  . \n\nThe goal of inference at test phase is to search the label sequence   y*   with the highest conditional probability: \n\nEquation\u00a0  and equation\u00a0  can be solved efficiently by dynamic programing and the Viterbi algorithm respectively. \n\nIt is clear that if interactions between successive labels are not considered, the inference layer will be simplified into a softmax output layer to classify each token individually. \n\n\n\n## Results \n  \nIn order to investigate the performance of LSTM on entity recognition from clinical texts, we start with two baseline systems: 1) a CRF-based system using rich features (denoted by CRF); 2) a LSTM-based system only using token-level word representations in the input layer (denoted by LSTM-BASELINE), then compare them with the LSTM-based systems using token-level word representations and two different types of character-level word representations. Moreover, we also compare the LSTM-based systems with other state-of-the-art systems. Three benchmark datasets from three clinical NLP challenges: i2b2 (the Center for Informatics for Integrating Biology & the Beside) 2010, 2012 and 2014 are used to evaluate the performance of all systems. Both 2010 and 2012 i2b2 NLP challenges have a subtask of clinical entity recognition, and the 2014 i2b2 NLP challenge have a subtask of PHI recognition. \n\n### Datasets and evaluation \n  \nThree types of clinical entities, namely problem, test and treatment, require to be recognized in the 2010 i2b2 NLP challenge, while six types of clinical entities, namely problem, test, treatment, department, evidential and occurrence, in the 2012 i2b2 NLP challenge. In the 2014 i2b2 NLP challenge, seven types of PHI need to be recognized. The detailed statistics of the entity recognition datasets of the three challenges are listed in Table\u00a0 , where \u201c2010\u201d, \u201c2012\u201d and \u201c2014\u201d denote the i2b2 NLP challenges in corresponding years, and \u201c#*\u201d denotes the number of \u2018*\u2019.   \nStatistics of entity recognition datasets used in our study \n  \n\nThe performances of all systems are measured by micro-averaged precision (P), recall (R) and F1-score (F) under different criteria, which are calculated by the official evaluation tools provided by the organizers of the challenges. A brief introduction of the evaluation criteria for the three entity recognition tasks is presented in Table\u00a0 , where the key criteria are marked with \u201c*\u201d.   \nEvaluation criteria for the three entity recognition tasks \n  \n*represent\ufeffs the primary evaluation criterion for each task \n  \n\n\n### Experimental settings \n  \nBefore training LSTM, we use the following two simple rules to split raw texts into sentences and tokenize the sentences:   \nSentence split: separate sentences using \u2018\\n\u2019, \u2018.\u2019, \u2018?\u2019 and \u2018!\u2019. \n  \nTokenization: split sentences into tokens by blank characters at first, and then separate those tokens composed of more than two types of characters (letters, digitals and other characters) into smaller parts that only contains only one type of characters. For example, \u201c4/16/91CPT Code:\u201d is split into \u201c4/16/91CPT\u201d and \u201cCode:\u201d at first, and then further separated into \u20184\u2019, \u2018/\u2019, \u201c16\u201d, \u2018/\u2019, \u201c91\u201d, \u201cCPT\u201d, \u201cCode\u201d and \u2018:\u2019. \n  \n\nIn this study, we use \u201cBIOES\u201d (B-beginning of an entity, I-insider an entity, O-outsider an entity, E-end of an entity, S-a single-token entity) to represent entities, and follow previous studies [ \u2013 ] to use the stochastic gradient descent (SGD) algorithm for parameter estimation with hyperparameters as shown in Table\u00a0 . The token-level word representations are pre-trained by word2vec [ ] on a large-scale unlabeled dataset from MEDLINE and Wikipedia, and the character representations are randomly initialized from a uniform distribution ranging in [-1, 1]. Both token-level word representations and character representations are fine-tuned during training. We adopt CRFsuite [ ] as an implement of CRF, and the features used in the CRF-based system includes bag-of-words, part-of-speech, combinations of words and POS tags, word shapes, affixes, orthographical features, sentence information, section information, general NER information, and dictionary features. All model parameters are optimized by 10-fold cross validation on training datasets.   \nHyperparameters chosen for all our experiments \n  \n\n\n### Experimental results \n  \nLSTM only using token-level word representations as input (i.e., LSTM-BASELINE) achieves F1-scores of 85.36% and 92.58% under \u201cexact\u201d and \u201cinexact\u201d criteria on the 2010 i2b2 challenge test set, F1-scores of 92.20% and 87.74% under \u201cspan\u201d and \u201ctype\u201d criteria on the 2012 i2b2 challenge test set, and F1-scores of 93.30% and 96.05% under \u201cexact\u201d and \u201ctoken\u201d criteria on the 2014 i2b2 challenge test set, as shown in Table\u00a0 , much higher than CRF. The key performance measure differences between LSTM-BASELINE and CRF on the three test sets are 1.67%, 1.38% and 0.72%, respectively.   \nPerformances of LSTM and CRF-based models for the three tasks (F1-score %) \n  \n\nWhen one type of character-level word representations (i.e., character-level word representations generated by LSTM or CNN, denoted by char-LSTM and char-CNN respectively in Table\u00a0 ) is added in the input layer as shown in Fig.\u00a0 , the performance of LSTM is slightly improved, LSTM considering char-LSTM (i.e., LSTM\u2009+\u2009char-LSTM) achieves a little better performance on the 2010 and 2012 i2b2 NLP challenge test sets, while the LSTM considering char-CNN (i.e., LSTM\u2009+\u2009char-CNN) achieves a little better performance on the 2014 i2b2 NLP challenge. No remarkable sign shows which character-level word representation is better. When both two types of character-level word representations are added, the performance of LSTM is not further improved. The highest F1-scores of LSTM are 85.81% and 92.91% under \u201cexact\u201d and \u201cinexact\u201d criteria on the 2010 i2b2 challenge test set, 92.29% and 86.94% under \u201cspan\u201d and \u201ctype\u201d criteria on the 2012 i2b2 challenge test set, and 94.37% and 96.67% under \u201cexact\u201d and \u201ctoken\u201d criteria on the 2014 i2b2 challenge test set. \n\nMoreover, we also compare \u201cLSTM\u2009+\u2009char-LSTM\u201d with other state-of-art systems including the best systems of the three challenges and the best up-to-date systems on the same corpora (as shown in Table\u00a0 , where the starred systems are the best systems of the corresponding challenges). \u201cLSTM\u2009+\u2009char-LSTM\u201d significantly outperforms the best systems of the three challenges. On the 2010 i2b2 NLP challenge corpus, \u201cLSTM\u2009+\u2009char-LSTM\u201d achieves almost the same F1-score as the current best system (85.81% vs 85.82%), which is a SSVM-based system using rich hand-crafted features, under \u201cexact\u201d criterion. On other two i2b2 NLP challenge corpora, \u201cLSTM\u2009+\u2009char-LSTM\u201d outperforms the current best systems.   \nComparison of the performances of various systems on the three tasks (%) \n  \n\n\n\n## Discussion \n  \nIn this study, we investigate the performance of LSTM on entity recognition from clinical texts. The LSTM-based systems achieves highest F1-scores of 85.81% under \u201cexact\u201d criterion on the 2010 i2b2 challenge test set, 92.29% under \u201cspan\u201d criterion on the 2012 i2b2 challenge test set, and 94.37% under \u201cexact\u201d criterion on the 2014 i2b2 challenge test set, which are competitive with other state-of-the-art systems. The major advantage of the LSTM-based system is that it does not rely on a large number of hand-crafted features any more. Similar to previous studies in the newswire domain, LSTM shows great potential on entity recognition in the clinical domain, outperforming most traditional state-of-the-art methods that suffer from fussy feature engineering such as CRF. \n\nExperiments shown in Table\u00a0  demonstrate that any one type of the two character-level word representations is beneficial to entity recognition from clinical texts. The reason may lie in that both the two types of character-level word representations have ability to capture some morphological information of each word such as suffixes and prefixes, which cannot be captured by the token-level word representation that relies on word context. Then, when any one of the character-level word representations is added into the input layer of LSTM, errors like \u201cTest\u201d event \u201cURINE\u201d missed in \u201c2014-11-29 05:11\u00a0PM URINE\u201d and hospital \u201cFPC\u201d correctly identified in \u201c\u2026 have a PCP at FPC \u2026\u201d but missed in \u201c\u2026 Dr. Harry Tolliver, FPC cardiology unit \u2026\u201d are fixed. \n\nAlthough the LSTM-based system shows better overall performance than almost all state-of-the-art systems mentioned in this study, but it does not show better performance on all types of entities. For example, the best system on the 2012 i2b2 challenge corpus (i.e., Xu et al. (2013) [ ]) achieves better \u201cspan\u201d F1-score than the LSTM-based system on \u201cTest\u201d events (94.16% vs 93.69%). The best system on the 2014 i2b2 challenge corpus (i.e., Yang et al. (2015) [ ]) achieves better \u201cexact\u201d F1-score than LSTM-based system on \u201cID\u201d instances (92.71% vs 91.94%). There are two main reasons: 1) the current LSTM-based system does not use knowledge bases widely existing in the clinical domain, but the other state-of-the-art systems take full advantages of them; 2) although the character-level word representation has ability to capture some morphological information of each word, it cannot cover morphological information of specific words such as fixed size digitals. Therefore, there are two possible directions for further improvement in our opinion: 1) How to integrate widely existing knowledge bases into the input of LSTM; 2) How to use LSTM to recognize entities in specific formats. We will try them in the future. \n\nIn recent months, a few studies on deep learning for entity recognition from clinical text are also proposed. For example, Abhyuday et al. proposed two RNN-based models for medical event detection on their own annotated dataset, one of which recognizes medical event detection as a classification problem and the other one as a sequence labeling problem [ ,  ]. Both the two RNN-based models adopt traditional RNN, which is not as good as LSTM, and only take token-level word representation as their input. Franck et al. deployed a similar RNN model for the de-identification task on the 2014 i2b2 NLP challenge corpus and the MIMIC dataset [ ]. According to the experimental results reported in this study and the similar studies, we may conclude that our LSTM outperforms theirs. For example, the F1-score of the RNN model proposed by Franck et al. on the 2014 i2b2 dataset, as reported, is 97.85% under the binary HIPAA token criterion (only evaluating the HIPAA-defined PHI instances under \u201ctoken\u201d criterion). Under the same evaluation criterion, the corresponding F1-score of \u201cLSTM\u2009+\u2009char-LSTM\u201d is 98.05% on i2b2-2014 dataset. The results demonstrate that our LSTM outperforms RNN proposed by Franck et al [ ]. Therefore, the results reported in this study can be a new benchmark system based on deep learning methods. \n\n\n## Conclusions \n  \nIn this study, we comprehensively investigate the performance of recurrent neural network (i.e., LSTM) on clinical entity recognition and protected health information (PHI) recognition. Experiments on the 2010, 2012 and 2014 i2b2 NLP challenge corpora prove that 1) LSTM outperforms CRF; 2) By introducing two types of character-level word representations into the input layer of LSTM, LSTM is further improved; 3) the final LSTM-based system is competitive with other state-of-the-art systems. Furthermore, we also point out two possible directions for further improvement. \n\n \n", "metadata": {"pmcid": 5506598, "text_md5": "f3d156d6f5a53272b6fb37e8968591fb", "field_positions": {"authors": [0, 108], "journal": [109, 133], "publication_year": [135, 139], "title": [150, 217], "keywords": [231, 322], "abstract": [335, 2618], "body": [2627, 22458]}, "batch": 1, "pmid": 28699566, "doi": "10.1186/s12911-017-0468-7", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5506598", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5506598"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5506598\">5506598</a>", "list_title": "PMC5506598  Entity recognition from clinical texts via recurrent neural network"}
{"text": "Landolsi, Mohamed Yassine and Hlaoua, Lobna and Ben\u00a0Romdhane, Lotfi\nKnowl Inf Syst, 2022\n\n# Title\n\nInformation extraction from electronic medical documents: state of the art and future research directions\n\n# Keywords\n\nElectronic medical records\nInformation extraction\nMedical named entities recognition\nMedical relation extraction\nSection detection\n\n\n# Abstract\n \nIn the medical field, a doctor must have a comprehensive knowledge by reading and writing narrative documents, and he is responsible for every decision he takes for patients. Unfortunately, it is very tiring to read all necessary information about drugs, diseases and patients due to the large amount of documents that are increasing every day. Consequently, so many medical errors can happen and even kill people. Likewise, there is such an important field that can handle this problem, which is the information extraction. There are several important tasks in this field to extract the important and desired information from unstructured text written in natural language. The main principal tasks are named entity recognition and relation extraction since they can structure the text by extracting the relevant information. However, in order to treat the narrative text we should use natural language processing techniques to extract useful information and features. In our paper, we introduce and discuss the several techniques and solutions used in these tasks. Furthermore, we outline the challenges in information extraction from medical documents. In our knowledge, this is the most comprehensive survey in the literature with an experimental analysis and a suggestion for some uncovered directions. \n \n\n# Body\n \n## General introduction \n  \nFor centuries, physicians play an important role in ensuring good health. Indeed, a physician must be well trained and must be able to manage the disease and patient information in order to find the right treatment and make the right decision. In medicine, different types of information are used for treatment and can be found in narrative documents written by humans. For example, to make a medical prescription, the patient record and medication manufacturer are used [ ]. Thanks to the development of information technologies and Hospital Information System (HIS), medical information is digitized into electronic records named Electronic Medical Record (EMR) [ ] or Electronic Health Record (EHR). Digital records could be stored, managed, transmitted and reproduced efficiently. The widespread adoption of HIS has contributed to billions of records [ ], and they are recognized as valuable resources for large-scale analysis. \n\nSimilarly, the number of diseases and medications is gradually increasing. In medical prescription, for example, the doctor must know all the indications and contraindications to prescribe a drug. Also, he has to take a lot of time to read a whole unstructured narrative medical leaflet especially for new doctors [ ,  ,  ]. Today, medical records are available on the Internet even for any person. Nevertheless, the physician must manage the large amount of information that is written in natural language and must select the important information from the narrative documents. Many medical researchers are overwhelmed in the huge amount of medical data in their studies [ ]. Indeed, the main source of errors in medicine is related to drug prescriptions. In 2006, there were 3900 prescription errors in Germany [ ]. For 18 years, the Institute of Medicine in the United States reported that there were 237 million medication errors per year in England that were costly for the country [ ]. Also, it reported that there were between 1700 and 22303 deaths per year due to adverse drug reactions. In addition, there are more than 234 million cases and 4 million deaths caused by coronavirus disease (COVID-19) [ ]. Likewise, the information of this virus\u2019s symptoms need to be analyzed by efficient tools in order to inform risk assessment, prevention and treatment strategy development and outcome estimation. \n\nAll these facts make the use of these available electronic documents more than a necessity. Hence, extracting useful information will be greatly helpful. Unfortunately, this process is not trivial mainly due to the huge number of documents, and consequently requires models able to deal with big data. This is hampered by the unstructured (or semi-structured) nature of such documents. In order to make Information Extraction (IE) feasible and efficient, structuring these documents in a more abstract form that is easily readable by machines/algorithms becomes a fundamental step. The main technologies of IE are the recognition of named entities and the extraction of relations between these entities. Entity recognition involves recognizing references to different types of entities such as medical problems, tests, allergies, risks, adverse events, drugs and treatments. In addition, detecting different sections in a document which can improve IE tasks by providing more context. Generally, medical text mining and IE help in medical decision and disease risk prediction. \n\nIn fact, several reviews related to IE in medical field have been published. Meystre et\u00a0al.[ ] have focused on research about IE from clinical narrative from 1995 to 2008. However, they have not discussed the research on IE from biomedical literature. Liu et\u00a0al. [ ] have introduced a novel IE paradigm Open IE (OpenIE) which begins to attract great attention in Biomedical IE (BioIE). Biomedical OpenIE (Bio-OpenIE) aims to extract tuples with any relation types with no, or little, supervision. Their review focuses mainly on recent advances in deep learning-based approaches such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). However, they have focused only on the main deep learning techniques in Bio-OpenIE. Wang et\u00a0al. [ ] have presented researches on clinical IE applications from 2009 to 2016 for a discussion in terms of publication venues, data sources, clinical IE tools, methods, applications in the areas of disease and drug-related studies, and clinical workflow optimizations. Also, they have gained a more concrete understanding of underlying reasons for the gap between clinical studies using EHR and studies using clinical IE. However, their research is made before deep learning was really adopted as mainstream in the informatics community. Sun et\u00a0al. [ ] focuses on the process of medical document processing and analyzes the key techniques involved include Named-Entity Recognition (NER) and Relation Extraction (RE) for IE. They make an in-depth study on the text mining applications, the open challenges and research issues for future work. Pomares-Quimbaya et\u00a0al. [ ] have reported the results of a systematic review concerning section identification in narrative medical documents. It was the first understanding review which focuses on this concept, its existing methods and its growing contribution on the IE tasks. Hahn and Oleynik [ ] have discussed the contributions of recent publications from 2017 to 2020 in medical IE and foreshadow future directions of research. They have focused on the methodological paradigm shift from standard machine learning techniques to deep learning. Also, they have selected only two the diseases and drugs semantic classes and the relation between them. Nasar et\u00a0al. [ ] have focused specifically on NER and RE with major focus on advances via deep learning approaches. They have presented recent trends in domain of IE along with open research areas. \n\nIn our paper, we have made a comprehensive and up-to-date review about the medical IE domain. We have discussed different techniques for each task in this area such as NER, RE and section identification. Also, we have presented some issues related to the nature of entities with current solutions. Furthermore, we have discussed the data used for IE such as the nature of medical data in general and information about useful resources and datasets used in many published studies. Also, we have made an experimental analysis about the current methods based on their results. We also present the current state and propose some future directions in this field. To the best of our knowledge, this is the most comprehensive survey to be presented in the literature. \n\nOur paper is organized as follow: Sect.   is a discussion about the medical data and its problematics; Sect.   presents the general classic data processing steps to treat structured data before passing to the unstructured data; Sect.   shows the text mining procedure that uses an unstructured text in order to show the position of the IE step in this process; Sect.   classify and discusses the methods of IE tasks according to the techniques they use; Sect.   presents specifically the section detection task and classifies several methods according to the used techniques; Sect.   presents some applications used to discover new knowledge which is useful for some medical tasks; Sect.   presents useful data and benchmark datasets in IE; Sect.   shows some shared tasks in the field; Sect.   is employed for experimental studies; Sect.   is a general discussion of the current state and its limits; and in the Conclusion, we suggest some research directions to be further developed. \n\n\n## Problem setting \n  \nIn fact, there are different sources of medical information, such as daily activities, Internet and clinical staff [ ]. The rapid development of hospital information technology has resulted in a rapid accumulation of medical data, and a significant amount of this data is in the form of free text written by the author [ ]. Note that it is very likely to have long and maybe useless parts in a narrative content. A clinical narrative is a report-style free-text that is found in the medical document, used for clinical documentation. It is a rich source of information for medical research and analysis, and this source of data is needed to make health care decisions. There are several examples of clinical narratives that vary between full-fledged documents or clinical notes: Discharge Summaries, Radiology Reports, Emergency Reports, Pathology Reports, Urology Reports, Letter of Communication, History or Family History, Physical Exam, Medical Dictation, Admission Notes, Nursing Notes, Progress Notes, Operative or Procedure Notes and Clinic Visit Notes. Thus, there are several sources for these narratives: analytical repositories [ ,  ], EMR/EHR systems [ ,  ], speech recognition or dictation systems [ ,  ], external sources provided by competitions [ ,  ], or other open data sources [ ]. EMR or EHR are digital representations of medical information. These records are popularized thanks to the development of information technologies and the HIS. They allow medical staff to record digital information, such as texts, symbols, diagrams, graphs, data, etc. Thus, they allow medical institutions to record the patient\u2019s condition, such as diagnostic information, procedures performed and treatment results. These records are therefore sources of clinical information such as demographic data, diagnostic history, medications, laboratory test results and vital signs. Digital records could be stored, managed, transmitted and reproduced efficiently. Widespread adoption of HIS has contributed to billions of records [ ], and they are recognized as valuable resources for large-scale analysis. \n\nMany medical researchers are overwhelmed by the huge amount of medical data in their studies, because it is difficult for them to discover the hidden knowledge lying in the massive amount of medical texts [ ]. Indeed, there are three types of data [ ], structured data contains basic information such as medications taken, allergies and vital signs. This data requires traditional pre-processing technologies that include data cleansing, integration, transformation and reduction. As well, semi-structured or unstructured data contains more health information and requires more complex and difficult processing methods such as text mining. Semi-structured data usually has a flow chart format, similar to Resource Description Files (RDF), and includes names, values and timestamps. Unstructured data includes narrative data and stores a lot of valuable medical information, but it lacks common structural frameworks. The processing complexity of this data is increased by grammatical misuse, misspellings, local dialects, and semantic ambiguities. \n\nElectronic drug prescription helps to control the prescription and monitors the consumption of drugs. To make the appropriate treatment decision, the patient\u2019s EHR and medication leaflets are used [ ]. The medication leaflet has information to avoid interactions with other medications, diseases, allergies or the patient\u2019s conditions [ ]. For example, SmPC is a legal document approved by the European Medicines Agency, used to represent the package insert, and it is also available in electronic form. According to physicians, the most preferred SmPC section titles are: \u201c4.3 Contraindications\u201d, \u201c4.1 Therapeutic indications\u201d, \u201c4.2 Posology and method of administration\u201d, \u201c4.8 Undesirable effects\u201d, \u201c5.3 Preclinical safety data\u201d, \u201c4.5 Interaction with other medicinal products and other interactions\u201d, \u201c5.2 Pharmacokinetic properties\u201d, \u201c4.4 Special warnings and precautions of use\u201d and \u201c5.1 Pharmacodynamic properties\u201d. Sorted from most to least preferred [ ]. Also, the most important sections of the medical record for establishing the appropriate treatment are as follows: Contraindications, Therapeutic indications and Dosage [ ]. \n\nIndeed, the main source of errors in medicine is related to the prescription of drugs, where the number of diseases and drugs is gradually increasing, and the doctor must know all the indications and contraindications to prescribe a drug. For this reason, there are several problems especially for new doctors, as it needs a lot of time to read all the unstructured instructions [ ,  ,  ]. Also, it is necessary to find out about new diseases that require effective treatments. Also, have to find out what new treatments or drugs the doctors need to access. So, doctors may prescribe the wrong treatments or drugs. In 2006, there were 3900 prescribing errors in Germany [ ]. For the last 18 years, the U.S. Institute of Medicine has reported that there are 237 million medication errors per year in England, also with costs to the country. Also, it reported that there were between 1700 and 22303 deaths per year due to adverse drug reactions. Current solutions to avoid these problems are the creation of written procedures, improved training for health care professionals, automation of support or research operations, quality control in medicine, improving communication between physicians and encouraging cooperation between medical departments [ ]. It is a great challenge to effectively use text in medical documents [ ]. Automatic document analysis is affected by ambiguity, format diversity, brevity, sloppy writing, redundancy, complex longitudinal information [ ]. Also, obstacles to data mining are diversity, incompleteness, redundancy, and privacy.   \nThe general steps of the EMR data processing \n  \n\n\n## General steps of data pre-processing \n  \nBefore we pass to the medical unstructured text processing, which is the object of this survey, we introduce the classic data processing [ ,  ,  ] in general which was initially aimed for structured data and is adaptable for unstructured texts. The general procedure for processing medical data consists of five steps as outlined in Fig.  . The first step is the collection of the data by the government or professional medical institutes [ ,  ,  ]. The second step is pre-processing which transforms the unstructured data into a useful format. In the third step, a data mining method is used considering the nature of the dataset, such as classification, clustering, association rules, regression, etc. The fourth step consists of some tests to understand the performance of the model. The last step is the knowledge application which is the goal of the data processing and also its driving force. Knowledge translation is more involved in medical management and arrangement of processing programs. In this step, the extracted patterns and knowledge must be analyzed and improved. A knowledge model is relatively better when it contains interactive iterations and requires continuous corrective feedback. \n\nThe classic EMR data pre-processing step ensures that the data is accurate, complete, and consistent, and protects privacy, so as not to affect data mining. In particular for medical data, pre-processing methods should be chosen reasonably. Pre-processing facilitates the analysis of complex EMR data. In addition, high quality data is more likely to yield high quality results. Noting that the workload of the pre-processing step is over 60%. The classical pre-processing of the data consists of five steps: data cleaning, integration, data reduction, data transformation and privacy protection. \n\n### Data cleaning \n  \nThe first step is data cleaning where different operations can be performed. The incomplete data filling operation is used when some data attributes are lost due to manual errors and system failure. Usually, missing data that have a significant influence on the processing will be ignored. Alternatively, if the dataset is relatively small, the defaults can be filled in manually, but this is time-consuming and expensive. In addition, defaults can be filled in by averaging the attribute values when the data distribution is uniform and when the cost budget is not significant. Also, machine learning models such as regression, formal Bayesian methods and decision tree induction can be used to determine the optimal value. But in extreme cases, they can show a relatively large deviation. Another operation for cleaning is noise processing which consists of correcting illegal values in the data source. There are several methods such as binning methods to examine the values around the data in order to smooth these values, regression methods to change the value of noise by adjusting to the value of the attribute and outlier analysis by making groups for similar attributes. The last operation is the correction of data inconsistency which consists in avoiding inconsistency in different sources or homologous data by analyzing the correlation between the data. All these cleaning operations can also be performed by recovering data from other sources. \n\n\n### Integration \n  \nThe second step is integration, which can improve the speed and accuracy of data mining. It consists of dealing with heterogeneous data, and its redundancy. In fact, there are different forms of redundancy such as attributes that can derived or inferred from others or same medical records that can be come from multiple sources. Thus, redundant attributes should be cleaned up and the duplicate records should be purged out or consolidated and merged if they have complementing information. Most redundant data can be detected by correlation analysis. For nominal data, the chi-square test is used for detection. \n\n\n### Data reduction \n  \nThe third step is data reduction, where a large amount of data is added every day, and reducing the size of the dataset lead for convenient and efficient data mining. There are different reduction techniques. Dimension reduction is relatively popular and easy to achieve with better effect. It makes the projection to a smaller dataset using Principal Component Analysis and Wavelet Transform methods. Thus, it selects subset of attributes by detecting and removing irrelevant, weakly correlated or redundant attributes or dimensions. There are also the reduction in the quantity and compression of data. \n\n\n### Data transformation \n  \nThe fourth step is data transformation, which involves converting the dataset into a unified form suitable for data mining. This way, data analysis can be more efficient through directional and targeted data aggregation. This step consists of smoothing out noise, aggregating the data, and normalizing the data to avoid dependency of data attributes on units of measurement. Normalization into a smaller common space [ ] can be achieved by different methods such as Min-Max, ZeroMean and Fractional Scale. It is more beneficial for neural networks and also for classification algorithms based on distance measures. \n\n\n### Privacy protection \n  \nThe last step is privacy protection, where sensitive information about the patient\u2019s private life can cause a serious problem when obtained by the offenders. This step has attracted a lot of attention with the popularity of Software Defined Networking (SDN) technology that facilitates network management [ ,  ,  ]. The two main methods of this stage are the use of data protection and access control protocols, where the technical issues are encryption, privacy anonymity processing [ ] and access control. \n\n\n\n## Challenges of processing EMR \n  \nIn this section, we describe the text mining procedure and its application to the medical data and especially the EMR together with its challenges in the medical field. In fact, text mining is similar to classical data processing where the difference between them is that data mining in classical data processing is aimed for the analysis of structured data, but text mining is used for the analysis of unstructured texts. Text mining plays a major role in the health sector for the prediction of diseases, it allows to examine the data and join them into useful information. Its main objective is the prediction or the description of a medical information. Medical text mining extracts hidden knowledge from unstructured medical text [ ], and its general procedure consists of four steps. First, it retrieves information to obtain the desired texts. Then, it extracts pre-defined information from these texts, this step is similar to pre-processing. Then, it explores the knowledge and extracts new knowledge. At the end, it applies the inferred facts to practice. The main strategy used for medical text mining is to convert the texts into structured computer-readable data using IE and Natural Language Processing (NLP) technologies. NLP technology has become a popular application of text classification and clustering, information retrieval and filtering, IE and question/answer (Q & A) system, machine translation, and new information detection. The application of NLP technology to clinical medicine has become a common topic of interest in academia and medicine. NLP for medical research started in 1960, but its research lags behind other fields because of the lack of data, corpus labeling costs too much and mismatches between research and development with the current needs of medical institutions [ ]. Also, text structuring helps medical researchers to use EMR text in an easy way to examine the clinical knowledge that resides in the text [ ]. The processed text data can furthermore be easily used by computer-aided analysis according to specific needs such as providing effective treatment to patients, creating more structured information, and extracting and structuring important information [ ]. \n\nMedical text mining technology has several applications in the medical field that can be summarized in a few general applications. This technology can be applied for medical decision support and disease risk prediction. Decision support [ ,  ,  ] is especially beneficial for physicians who have less clinical experience and can also advise medical experts on the treatment plan of symptoms. Risk prediction helps physicians judge the possibility of disease deterioration or improvement, and it also reduces costs for patients. In addition, this technology is applied for mobile health [ ], networked medical treatment and personalized health care. Mobile health and networked medical treatment facilitate the consultation of doctor\u2019s advice and make the capture of physical quality more accurate. Personalized health care develops treatment plans and nursing methods based on the actual situation of the patient. There are other applications that are prediction of disease evolution and detection of drug reactions. They help to quickly discover the medical trajectory of the disease over time, and detect adverse drug events in a cost-effective manner [ ]. \n\nGenerally, the challenges of text mining in the medical field include the lack of sufficient annotated public corpus and this is an obstacle especially for Chinese EMR studies, where English corpus studies are more mature and systematic [ ]. Another challenge is the lack of personal and knowledge base dictionaries, where there is little useful content of medical dictionaries that is not sufficient, and the quality of dictionaries requires increased evaluation and certification of specialized institutions. In addition, privacy protection is another challenge especially as the transmission of EMR data will become more and more frequent and the market demand needs a more manageable data protection system. There is also the challenge of reasonable selection of processing tools, where a designed method may have poor performance in the biomedical field. Finally, a larger scale and more complex structure of EMR makes text mining difficult to process but more socially and economically beneficial. \n\n\n## Information extraction \n  \nInformation extraction is a step in text mining that is similar to pre-processing in the classical data processing procedure. This step is mainly based on automatic NLP and machine learning [ ]. The main technologies of EMR IE are NER and RE. Indeed, there are three main steps for the clinical IE that start with the extraction of medical problems, tests and treatments from discharge summaries and progress notes. Then, the classification of the assertions made about the medical problems. At the end, the classification of relationships between medical concepts [ ]. In addition, the main tasks in the recognition of named entities are the identification of clinical events and temporal expressions. In addition, the main task in the biomedical literature is the recognition of bio-entity names [ ]. \n\n### Named entity recognition \n  \nNER was introduced in 1995, its general role is to identify types of names and symbols [ ], but its role in the medical domain is to identify medical entities that are important for treatment, such as disease names, symptoms and drug names. It is a task of IE and it consists of recognizing references to different types of entities such as medical problems, tests, allergies, risks, adverse events, drugs and treatments. Indeed, the complexity of natural language increases with negated expressions [ ], co-references [ ], misspellings [ ,  ], different language structures [ ], detection of acronyms or abbreviations, expansion and disambiguation [ ], anaphoric relations [ ], etc. There are two main steps for this task which are the identification of the entity boundary followed by the determination of the entity class. The metrics used for evaluating NER methods are precision, recall, and f1-score, that can be computed based on token level or entity level. For entity level, there are two methods, partial matching and exact matching [ ]. These methods are affected by the physician\u2019s writing style, different writing forms of medical terms, ambiguity in term abbreviations, and compound or modified medical terms especially in Chinese. NER methods are classified into three main classes: rule-based; dictionary-based; and machine learning models. Hereafter, we review briefly state-of-the-art approaches within each category. \n\n#### Rule-based methods \n  \nActually, hand-coded rules give a better result than machine learning methods. However, these methods are valid only in specific datasets [ ] and require manual construction of rules by the assistance of a medical expert. \n\nXu et\u00a0al. [ ] have proposed an unsupervised method to detect boundaries and classify medical entities mentioned in a medical Chinese text, and link mentions to their entities. Initially, the method exploits the part-of-speech and dependency relations, and maps the text to concepts in offline and online lexical resources to detect mentions of medical entities. Then, it classifies these mentions into categories and gives a Word2vec representation for each category. Next, the approach selects candidate entities from a knowledge base that are most similar to medical entity mentions based on the characters. In addition, candidates that are not similar to the category representation will be removed. Finally, the method calculates the similarity between the mention and its candidates according to the common characters, the popularity of the entity and the similarity between the words in the context that have a dependency relationship with the mention and the words in the description of each candidate. In addition, semantic correlation knowledge is added by computing the character similarity between the linking entity descriptions of the context mentions and the description of each candidate. Thus, the target entity is the candidate with best similarity. As an advantage, the method is unsupervised and generalizable. Also, it can recognize nested entities and better cover medical entities. In addition, it outperforms the state-of-the-art methods in terms of performance. Furthermore, the efficiency of online detection to solve the limitation problem of the dictionaries is well proved. However, the linking approach lacks semantic analysis. In addition, the detection may obtain inexact entities in the boundary detection step, and the filtering of non-medical terms may lose medical entities. \n\nAlex et\u00a0al. [ ] have used hand-crafted rules and lexicons made by experts for NER from tokenized and POS tagged medical text. This method can recognize named entities reliably and accurately using brain imaging reports from Edinburgh Stroke Study (ESS) data and perform very well on the new data. In fact, ESS was the first set to be annotated by experts in this domain. Furthermore, this method can outperform machine learning approaches. However, hand-crafted rules are costly and time-consuming, especially when adapted to new and different data. \n\nZhao et\u00a0al. [ ] propose a weakly supervised method where they manually prepare some seeding rules and automatically extract all possible rules from unlabeled text for each of the six rule types, and connect them in a graph using cosine similarity. Note that the rule is represented by the average contextual embedding of its matched candidate entities. Then, propagate the labeling confidence from seeding rules in the graph to obtain new rules. These new rules are applied on the text to obtain a label matrix in order to estimate noisy labels using Linked Hidden Markov Model (LinkedHMM) generative model. Finally, Bidirectional Long Short-Term Memory-Conditional Random Field (BiLSTM-CRF) discriminative model is trained with noisy labels using Bidirectional Encoder Representations from Transformers (BERT) [ ] as token embedding to label tokens by their entity types. As advantage, high-quality new labeling rules can be automatically learned from only a few manually constructed rules and an unlabeled text. In addition, with a limited number of manual rules, a substantially better performance can be achieved. Furthermore, they defined six useful types of rules for entity recognition. Moreover, the performance is improved using a graph neural network with a new class distance-based loss function to maximize the distance between positive and negative rules. However, some rules are overlapped and are not helpful to improve the recognition. Also, some rules learned from training data can\u2019t be applied on testing data due to the mismatch between these datasets. \n\n\n#### Dictionary-based methods \n  \nThese methods are widely used in large-scale annotation and indexing of clinical medical texts. The most common techniques of this approach are Fuzzy Dictionary Matching and Post-processing. However, these methods require manual construction of dictionaries with the assistance of a medical expert and it\u2019s difficult to cover several variants of medical terminology with a single dictionary. \n\nQuimbaya et\u00a0al. [ ] have proposed a combined approach by preprocessing the text of electronic health records to apply exact and fuzzy NER techniques by the help of a knowledge base. In addition, a lemmatized recognition is applied after lemmatizing the target text and the knowledge base. Then, the overlapped entities recognized by these three techniques are combined to decide the final result. As advantage, the use of the Fuzzy Gazetteer match approach can find more instances of the dictionary concepts and even mistyped instances. Furthermore, the combination of these techniques improve the recall. However, this method does not take into consideration the context and surrounding words that appear near a candidate named entity. \n\nThe work of Nayel and ShashrekhaH. [ ] consists in exploiting a dictionary information with the representation vector of the word and its characters to predict the named entity by a Bidirectional Long Short-Term Memory (BiLSTM) model. For this purpose, the authors used a Skip-gram model to build the word representation with. Also, they gave an initial representation vector for each character to train a BiLSTM to generate orthographic features from the characters of a word. Noting that all numbers are replaced by \u201cNUM\u201d and characters are changed to lowercase. In addition, the method adds dictionary information by using a merged disease vocabulary (MEDIC) as a dictionary. Thus, it represents dictionary information for each word in a binary vector to indicate whether a word is an abbreviation or synonym of a disease or is part of a multi-word disease name. Then, all these 3 types of features were passed to a BiLSTM model to do the learning. Thus, the contextual representation generated by this model is passed to a Conditional Random Field (CRF) layer to classify and select the most appropriate feature to annotate the input word from a sequence of words. As an advantage, Skip-gram learning improves the semantic and syntactic representation of words especially with a huge biomedical and generic text corpus. Also, the character representation significantly increases the performance. Moreover, the addition of dictionary information improves the result according to f1-score. \n\nSun and Bhatia [ ] is based on sequence tagging by fine-tuning   model [ ] on Medical Information Mart for Intensive Care III (MIMIC-III) dataset [ ] as a NER tagger, and train a gazetteer tagger (w/NER tagger) on clinical datasets. They have prepared a dictionary of medical conditions and drugs from the Unified Medical Language System (UMLS) [ ] meta-thesaurus. Thus, NER tagger is trained using word and character BERT embedding, while gazetteer tagger is trained separately using gazetteer embedding, where their outputs before their Softmax layers are merged to return the entity type of the word such as medication, treatment, etc. As advantage, the fusion of the separate taggers lead to a better interpretability and flexibility. Also, its worth noting that even without the gazetteer tagger during inference, the NER tagger can preserve the gains. Furthermore, the fusion model is data efficient, interpretable and able to improve NER systems and to easily adapt to new entities mentions in gazetteers. In addition, this model can benefit from different clinical NER datasets. Also, the use of name knowledge from gazetteer leads to an improvement. Furthermore, this method is effective on handling non-stationary gazetteers and limited data. However, this method can be improved by extending to structured knowledge to further improve NER systems and for more interpretability. \n\n\n#### Machine learning-based methods \n  \nFor this approach, different models are used such as Hidden Markov Model (HMM), Support Vector Machine (SVM), CRF [ ] and maximum entropy. The CRF model is the most popular and allows the incorporation of various features, which is appropriate for sequence annotation. Although the rule-based methods are widely used and are high performing, based on recent shared tasks, machine learning methods tend to perform best [ ]. However, the algorithms and features largely affect the performance of the model, and these methods require standard annotations of the training data. \n\nLei et\u00a0al. [ ] have tested multiple features such as bag-of-characters, word segmentation, Part of Speech (PoS), and section information by training multiple machine learning models on manually annotated Chinese clinical documents to predict named entities such as clinical problems, procedures, laboratory test, and medications. They conclude that Structural SVM (SSVM) and CRF sequence-labeling models outperforms others, where SSVM is the highest. In addition, most features are gainful for the Chinese NER systems even with limited improvements. Moreover, the fusion of word segmentation and section information lead for the highest performance, and they complement each other. Also, they found that domain knowledge is important for Chinese word segmentation. However, the NER on English clinical text is more difficult than the Chinese because it contains many more entities mentions and the boundaries of its entities are harder to detect. Furthermore, the most errors occurs in long entities where they are not often completely detected. In addition, the training set can\u2019t cover all concepts and some errors caused by unseen samples. \n\nThe work of Song et al [ ] consists in evaluating learning models for NER in the biomedical domain. For this, the authors use a model based on CRF, and another one based on RNN. Also, different word representations are tested with these models, such as Word2vec, Global Vector (Glove) and Canonical Correlation Analysis (CCA). In addition, these models have been compared with other state-of-the-art methods. In this work, a dataset is annotated by biomedical categories such as protein, DNA, RNA, cell type and cell line. Thus, the models train on the annotations in order to predict the correct categories for the named entities. Indeed, the results show that the CRF model with Word2vec features outperforms all other models. As an advantage, word features are automatically built thanks to unsupervised learning by Word2vec which exploits the juxtaposition of words to extract their context. Thus, feature construction does not require manual annotation, dictionary, domain knowledge or other external resources. In addition, CRF can consider the context and neighbors of the entered word. However, CRF still needs manual annotation for learning. Moreover, the models proposed in this work are not optimized enough. \n\nLi et al. [ ] have improved a deep learning model based on the Deep Belief Network (DBN) to predict the named entity of a word using its PoS with its Word2vec feature vector. Indeed, using the PoS of the word leads to the best performance. Thus, this method is beneficial for NER. Moreover, Word2vec vectors capture useful semantic information. Moreover, with the improvement, DBN outperforms state-of-the-art methods. But, this requires manual annotation of the training data. Also, the content of the corpus knowledge is not rich enough. \n\nGhiasvand and Kate [ ] use Decision Forest classifier to find named entities and their boundaries, while seed terms from UMLS are used for an unsupervised annotation. They have used 3 words before and after the target named entity which are presented by PoS, lemmatize form, stemming form and UMLS semantic types as features. Initially, training samples are collected using exact matched unambiguous terms from UMLS. Then, new samples are gathered by applying back the trained method on the corpus for self-training. To select samples for the model, they extract noun phrases that have at least one medical word which is included in any UMLS term. Then, another step is added to learn if a word can expand the boundary of its named entity using the same classifier and features with UMLS. As advantage, this method does not require any manual annotations. In addition, this method can determine the correct boundaries for the detected named entities. Moreover, it performs better than unsupervised methods and competitive to supervised methods. However, using data from different sources may reduce the performance. Furthermore, the automatically obtained noun phrases are not always perfect and can lead to errors. \n\nThe method of Chirila et\u00a0al. [ ] consists in extracting the most important information, which are named entities, from the sections of a semi-structured drug package insert. For this purpose, the authors trained the Stanford NER Tagger model on leaflets to predict word entities using distributional similarity based features in addition to other word features. According to the results, the accuracy with drugs of the same type is the highest. But, the method is tested only for the \u201cTherapeutic indications\u201d section and for the Roman language. \n\nDeng et\u00a0al. [ ] combine BiLSTM with CRF to be trained on crawled and manually annotated Chinese TCM patents\u2019 abstract texts using character embedding, in order to recognize entity types such as herb names, disease names, symptoms and therapeutic effects. Firstly, each character embedding vector is an input of a BiLSTM time step. A pre-trained embedding matrix is used to represent each character one-hot vector and is fine-tuned during the back-propagation, in order to extract sentence features. Finally, CRF layer learns the potential relationship between sequences and returns the optimal labeling sequence. As advantages, this method can learn semantic information in the context without feature engineering. Mainly, the use of characters instead of words leads to better performance, while the characters contain a lot of linguistic information and are able to mostly avoid errors caused by poor segmentation. Furthermore, BiLSTM can easily learn about contextual relationships in the text to provide more comprehensive contextual information. Likewise, the CRF layer complements the BiLSTM by optimizing the recognition comprehensively from the sentence level. However, the used data scale affects the learning model and cannot well support its requirements. Also, this method is restricted by the entity labeling granularity where some entities are nested within other entities. \n\nZhou et\u00a0al. [ ] have pre-trained two deep contextualized language models on clinical corpus from the PubMed Central (PMC): Clinical Embeddings from Language Model (C-ELMo) for word-level features and C-FLAIR clinical contextual string embeddings for character-level features. Then, each of the two embeddings is concatenated with Glove embedding and passed to BiLSTM-CRF model to extract entity types. As advantage, the models gain dramatic improvements compared to domain-generic language models and static word embeddings. In addition, these models can support different applications. Results show that C-Flair can handle entities that do not appear in word-level vocabulary. As well, C-ELMo can better capture the relationship between the word-level contextual features. Also, word-level models may be more robust than purely character-level models. However, it is hard for the two models to correctly recognize complex phrase-level entities. \n\n\n\n### Relation extraction \n  \nIn RE technology, the relationship represents the correlation between two named entities appearing in the same sentences. Indeed, understanding the semantic relations between entities is required for many applications in IE such as semantic knowledge bases construction to infer the relationships between entities, and development of question answering systems for text summarization or concepts taxonomy construction. Semantic relations can be classified into two families according to their types, paradigmatic relations and syntagmatic relations [ ]. Mainly, paradigmatic relations are operating on concepts of the same class. Usually, concepts are organized as a tree, where these relations are represented hierarchically by vertical links. These relations include the relation of synonymy, antonymy and hypernymy. Likewise, syntagmatic relations rely between two or more medical entities. They represent semantic links in an expression and rely between multiple linguistic units. They can be found by analyzing the syntactic forms in text and by a predicate. According to Uzuner et\u00a0al. [ ], we can also categorize these relations depending on the type of relationships, such as disease relationships, disease-medical examination relationships, and disease-treatment relationships. Generally, the cause-effect or causal RE has received ongoing attention in many medical fields [ ] which can assist doctors, by supporting the construction of a knowledge graph, to quickly find causality and customize treatment plans. There are many examples of causal relations such as diseases-cause-symptoms, diseases-bring-complications and treatments-improve-conditions. The most commonly used techniques in this task are rule-based methods and machine learning-based methods [ ]. \n\n#### Machine learning-based methods \n  \nUsually, this approach uses supplement NLP tools [ ,  ,  ] to generate high-level features for a large quantity of examples which are fed to the machine learning model for a classification task. These methods are the most widely used while the biomedical data have an increasing growth. In fact, approaches based on this technique extract useful information from syntactic structures rather than applying manually constructed patterns [ ]. Indeed, deep learning methods in particular can deal with the feature sparsity problem by transforming features into low-dimensional dense vectors. Thus, deep learning models have exhibited superior performances compared to the traditional machine learning-based and rule-based models [ ,  ]. The most used machine learning models are Long Short-Term Memory (LSTM), CRF, Graph Convolutional Network (GCN) and SVM. However, the quality and the quantity of data have a big impact on this task while data must have sufficient examples. Usually, a manually annotated dataset by medical expert is needed. \n\nTran and Kavuluru [ ] propose a novel distant supervision approach to extract medical treatment predication relations in PubMed abstracts by training a BiLSTM model after leveraging MeSH sub-headings and preparing training sentences with entities using NLM\u2019s MetaMap [ ] and UMLS Semantic Network. Indeed, this method uses a variant of BiLSTM with a modified noise-resistant loss function, where the input is word embeddings and learnable position vectors. As advantage, the position vectors can enhance the RE performance. Furthermore, the automatically generated training data is of reasonable quality without the costs of human involvement. In addition, MeSH sub-headings are precisely utilized while it leads for a better filtering of treatment and drug entities. However, this method focuses only on treatment predications. Furthermore, this method may have difficulties dealing with trivially negative cases as it is not trained on them. Also, linguistic phrasing is understandably difficult when there is a weak connecting word. Moreover, false negatives may occur when a unique concept entity is mentioned several times in the same sentence. \n\nShi et\u00a0al. [ ] train an end-to-end deep learning method to identify people\u2019s pandemic concerns, and extract \u201cco-occurrence\u201d and \u201ccause-effect\u201d relations between them in tweets. The authors consider 8 types of concerns which are finance, government, disease, medicine, person, location, food, and date and time. This method uses BiLSTM-CRF to detect concern entities combined by Bidirectional GCN (BiGCN) model to extract relations, where a hidden state of BiLSTM is shared with BiGCN. Accordingly, each tweet is represented by sequential features using BERT embeddings and regional features using Concern Graph (CG) module. In the CG, each node represents a concern associated by its score and type. The concern score is calculated by sentiment polarity and retweet count of the tweet. To represent the regional features, 3 vectors merge for each concern word to represent PoS and syntactic dependency relation, concern score and type, and relation features. For that, an automated deep learning-based framework [ ] was used to detect and construct a concern knowledge graph in order to get the concern types and relations. Likewise, the same framework is used to annotate the training set. As advantage, the state sharing enhance the influences from concerns to improve the performance of the RE. Note that this relation can reveal people\u2019s thoughts behind the expressed concerns or identify the cause of public concerns. Furthermore, the regional features from CG improve the concern identification effectiveness and lead to a high noise-tolerance. In addition to contextual information, this method captures specific features of entities by a designated CG to perform better on tweets. However, this method is not the best for high-quality and manually annotated datasets. \n\nThe work of Yang et\u00a0al. [ ] consists of developing a series of RE models based on 3 transformer architectures, namely BERT [ ], RoBERTa [ ], and XLNet [ ] to identify relation like \u201cDrug-Adverse events\u201d and \u201cDrug-Reason\u201d. This method uses already annotated entities to select candidate entity pairs for classification. Some rules are used as a strategy to generate these candidates, where the 2 entities must be a valid combination according to the annotation guidelines. Thus, the strategy selects only 2 entities in a same sentence or in 2 consecutive sentences as a candidate entity pair. Another strategy is to use the same rules but by using cross-sentence distance to select the number of consecutive sentences. Hence, it applies a separate model for each group of candidates which have different distance value. The results show that clinical pre-trained transformers consistently achieved better performance. In addition, XLNet and RoBERTa achieved the best performance for 2 different datasets. Furthermore, binary classification strategy consistently outperformed the multi-class classification strategy. Moreover, adding positional information to entities as features is critical to learn useful representations. However, there is no significant difference between the 2 generating candidates strategies, but there is still lack of an efficient method to solve both missing samples and sample distribution bias issues. Furthermore, there is a quite small number of training examples for some relation categories which significantly lead to a training difficulty and decent performance. Indeed, this work only focused on RE task, while this task is highly dependent on the NER result. \n\nThe work of Tran et al. [ ] is aimed to acquire knowledge from COVID-19 scientific papers. For that, an enormous number of relations between entities are extracted by combining several methods. ReVerb [ ] is based on verb-based relation phrases. OLLIE [ ] extracts relations mediated by verb, nouns, adjectives, and more. ClausIE [ ] is a clause-based approach. Relink [ ] extracts relations from connected phrases. OpenIE [ ] finds the maximally simple relations after breaking a long sentence into short and coherent clauses. Thus, the extracted relations are tagged by biomedical entity types recognized by SciSpacy models [ ] trained on different corpus. Finally, the extracted relations are clustered and scored for their informativeness over the corpus to construct the retrieval system. As advantage, higher extraction coverage could be obtained by combining several methods. In addition, various specialized entity information can be obtained by covering different sets of biomedical entities. Also, knowledge can be rapid and efficient across a large number of scientific papers. However, some wrong results are caused by the complexity of the biomedical text where there are long sentences, and conjunctions and nested clauses are commonly used. \n\n\n#### Rule-based methods \n  \nFor rule-based systems, the commonly used strategies include dependency parsing [ ], concept co-occurrence detection [ ] and pattern matching [ ]. Usually, rules are defined manually by domain experts [ ] or even automatically generated by using machine learning techniques [ ] on annotated data. The co-occurrence based methods are the most straightforward techniques. The key idea is when there are 2 entities mentioned together with higher frequency, there is a stronger chance to be related together. In fact, rule-based methods rely on a set of patterns, procedures or heuristic algorithms in order to directly identify candidate relations. Additionally, this technique can use supplement knowledge resources and apply some matching and mapping techniques. Indeed, sentence structure analysis can be used to explore patterns while it improves the performance and the extraction of implicit causal relations. However, preparing rules and supplement resources is labor-intensive and needs a domain expert for manual effort. Moreover, this approach severely restricts the generalizability and portability of RE for other types of data. Also, some difficult cases can be found in sentences and phrases which prevent rules to be correctly applied. \n\nBen Abdessalem\u00a0Karaa et\u00a0al. [ ] extract relations between drug and disease entities such as \u201ccure\u201d, \u201cno cure\u201d, \u201cprevent\u201d, \u201cside effect\u201d, and \u201cother relations\u201d. This method use a combination of features for each sentence by the help of UMLS meta-thesaurus to provide semantic annotations by configuring MetaMap system in order to extract concepts and semantic types. By combination of NLP technique and UMLS knowledge, many features can be extracted such as frequency, lexical, morphological, syntactic and semantic features. Thus, these features are passed to an SVM model to predict the relation associated with the sentence using MEDLINE 2001 [ ] as a standard training set. As advantage, this method can extract correct and adequate features while they are relevant to discover interesting relationships between concepts. Furthermore, it outperforms other methods for all types of relations especially in terms of f1-score and for the \u201ccure\u201d relation. Note that this performs better in a multidimensional context and is suitable for semantic relations in natural language texts. However, the lack of training data for \u201cno cure\u201d relation leads to low performance for all comparison methods. In addition, this method requires an annotated training set. \n\nKim et\u00a0al. [ ] propose a sequence labeling hybrid method to recognize family members and observations entities in EHR text notes, and extract relations between them in addition to living status. A rule based system is used to select family member entities by matching relevant noun terms by the help of PoS. Then, a number of BiLSTM models trained on dependency-based embeddings [ ] as static embedding and Embeddings from Language Models (ELMo) [ ] as context-dependent embedding. In addition, MetaMap [ ] maps semantic types from UMLS and aligns them with entities to choose relevant ones. the family members and observations recognized by these BiLSTM models are ranked and have been voted based on models\u2019 f1-scores. The heuristic rules are used to normalize the family member entities by a simple dictionary-based mapping, and determine family side by looking at cue words considering the degree of relatives. Thus, two Online Gradient Descent (OGD) [ ] models are trained on lexical features based on the identified entities to determine living status and observations associated with family members. Hence, alive and healthy scores are assigned for living status phrases using cue words. Likewise, negation attribute is assigned to observations using ConText algorithm [ ] with customized trigger terms. As advantage, voting ensemble of BiLSTM models contributes in terms of diversity to achieve better performance, and provides efficient and convenient integration of individual LSTM models which are not deterministic. In addition, this method is substantially benefited from a combination of 2 datasets. Integrating heuristics and advanced IE models lead to high level of performance. The performance is improved especially on RE and benefited by the large training set and the pre-trained embeddings. However, choosing the voting ensemble threshold can achieve best performance for one task but not the highest accuracy for other task. Also, some positive relations which rely on 2 entities in different sentences can be missed by using a carriage return character to filter examples. \n\n\n\n### Section detection \n  \nGenerally, section detection can improve IE methods. By definition, a section is a segment of text that groups together consecutive clauses, sentences or phrases. It shares the description of a patient dimension, patient interaction or clinical outcome. In fact, unstructured text has sections defined by the author. There are two types of sections which are explicit sections that have titles, and implicit sections that do not have titles [ ]. In addition, the section has a level, where it can be a section or a sub-section. The problem is that the author is free and even the precise defined templates are often ignored, which leads to less uniform titles and finds many sections without titles [ ]. In addition, there are different types of documents due to the lack of a standardized typology. \n\nSection detection aims to improve the performance of clinical retrieval tasks that deal with natural language such as entity recognition [ ], abbreviation resolution [ ], cohort retrieval [ ] and temporal RE [ ]. Section titles and orders in a prospectus may differ from one source to another. For this, standardizing the sections can help avoid this problem [ ]. There are several applications of section detection. Generally, it consists in giving more context to the task by knowing the specific position of a concept. Section detection is used for information retrieval to support cohort selection and identification of patients with risk factors. In addition, some tasks can be improved like co-NER and reference resolution by adding the section as a feature. Another application is that this detection is useful for distinguishing sensitive terms in de-identification. Also, section detection methods can improve tasks that consider the order of events by identifying temporal sections. In addition, this detection can be applied for document quality assessment. Furthermore, these methods can select supporting educational resources by extracting relevant concepts. The main challenges of section detection are the production of a benchmark dataset, the integration of texts from different institutions, the selection of the types of measures to be evaluated, the adaptation of different natural languages and the integration of the most robust methods into high-impact clinical tasks. \n\nAccording to an analysis of 39 section detection methods [ ], there are several methods that adapt pre-built methods for this task. In addition, almost all section detection methods have custom dictionaries created by the authors, and most of them use a controlled terminology named UMLs Meta-thesaurus which is the most common. Also, precision and recall are very high in these methods, but recall is always lower. In addition, almost all methods detect sections that have titles, and half of them can detect sections without titles. Also, more than half of the methods do not detect sub-sections. Concerning the type of narration, most of the methods work on Discharge Summaries in the first class, maybe because of the predefined internal structure or because of the fact that this type of document is frequently transferred. Thus, Radiology Reports are in the second place. There are a few methods that deal with other than the English language. Generally, when the genre of the text, the medical specialty or the institution imposes more restrictions, the methods may have better results. \n\nThe methods of section detection can be classified according to the technique used into rule-based methods, machine learning-based methods and hybrid methods. \n\n#### Rule-based methods \n  \nIn this approach, there are three types of rules which are exact match, regular expressions and probabilistic rules. This approach requires dictionaries as additional information. The dictionaries can be title flats which are lists containing possible terms used as titles, hierarchical title dictionaries, or synonym and word variant dictionaries containing modifications or abbreviations of standard titles. In addition, the methods in this approach can handle different features such as the most frequent concepts/terms in each section, probabilities assigned to headings or formatting features that represent the shape of the section e.g., number of words, capitalization, a question mark, enumeration pattern, size, etc. But let us keep in mind that feature values may vary depending on the corpus, e.g., the size of the section. Most methods use non-probabilistic rules and require titled sections. With exact matching, recall decreases when the level of standardization of headings is low. Regular expressions can search and match the expected text even with variants. Indeed, the minority of methods is based on probabilistic rules. These rules complement exact matching and regular expressions. They are based on text features to give very high results. These rules are more advanced to detect even unannotated sections. Most methods are based on this approach. \n\nBeel et al. [ ] have proved that style information, specifically font size, is very useful for detecting titles in scientific PDF documents in many cases. The authors used a tool to extract formatting style information from a PDF file such as font size and text position. Then, they used a simple heuristic rule to select the three largest font sizes on the first page. Thus, identifying the texts that have these sizes as titles. In fact, this method outperformed an approach based on SVM, which uses only text, in accuracy and even in runtime. Moreover, this technique is independent of the text language because it only considers the font size. However, this method depends on the font size and requires the existence of formatting information. \n\nThe approach of Edinger et al. [ ] is to identify sections in medical documents and use them in queries for information retrieval. To do this, the authors prepared a list of variations of all section titles for each document type. Variations in terminology, punctuation and spelling were selected to identify the most common section titles using a set of documents for each type. To identify titles in a document, a simple exact search is applied to find them by their variations. Then, the headings are annotated and the document text has been segmented according to these headings. As an advantage, the use of sections in the query instead of searching the whole document increased the accuracy of the search. Thus, this method can avoid retrieval of irrelevant documents. However, it has a smaller recall where the other method can retrieve more relevant documents. In addition, the exact search is accurate enough for section detection. \n\nLup\u015fe and Stoicu-Tivadar [ ] have proposed a method that supports prescribing by extracting and structuring information from medical records. The principle of this approach is to detect sections of the text and unify their titles using regular expressions and a set of section titles. Then, it removes empty words and applies the Stemming algorithm on the sections to root the words without touching medical terms. Thus, this method can suggest drugs that match the patient\u2019s disease, are not contraindicated and do not conflict with other diseases, treatments or allergies of the patient. Indeed, this approach reduces medical errors in drug prescriptions and structures the necessary drug information. However, there are some medical terms that are still modified by Stemming. Also, this method is tested only with the Roman language. \n\nZhang et al. [ ] have tried to effectively use temporal information in the text of electronic medical documents to structure them and help medical researchers to examine clinical knowledge and to facilitate computer-aided analysis. This method is based on rules to perform a few successive steps. These steps consist of correcting pronunciation errors, dividing texts according to grammatical rules, describing medical facts and events and finalizing by processing temporal expressions. However, these texts have little temporal information. In addition, the method gives the same weighting for different speech words. \n\n\n#### Machine learning-based methods \n  \nThis approach uses learning models, where the most popular are CRF, SVM and Viterbi. These methods also require training and testing datasets. These data can be created manually, with rules, with an automated method or with a combination of active learning and remote supervision. The size of the datasets used in these methods analyzed is between 60 and 25842 texts. There are also methods that use the same data for training and evaluation, and can be called cross-validation. This approach also can handle different types of features for this approach such as syntactic features that represent the structure of the sentence, grammatical roles and PoS. Semantic features represent the meaning of words and terms in a sentence. Contextual features are relative or absolute features of a line or section in the text such as the position of the section in the document and layout features. Lexical features are at the word level that can be extracted directly, e.g., the entire word, its prefix, suffix, capitalization, its type (e.g., number or stop word), among others. Indeed, the predictive value of the features had a great positive impact. These methods are not easily adaptable to other contexts. Size variation impacts performance and adaptability to new data. There is a lack of large datasets for learning and evaluation. Also, the construction of training data is time consuming. Non-standard abbreviations in titles and inappropriate phrases in sections result in incorrect classifications. The use of very specific terms for subheadings results in accuracy degradation. \n\nThe method of Haug et al. [ ] consists in annotating each section in a medical document by its main concept. This approach is based on Tree Augmented Naive Bayesian Networks (TAN BN) to associate topics with sections as their semantic features. For this purpose, this method was trained using features generated by extracting N-grams from the text of section titles, in combination with the document type. In fact, the identification of the section topic improves the accuracy and avoids errors when extracting a specific information. Thus, this task can reduce the natural language processing effort and prepares the document for more targeted IE. However, n-grams have limitations with complex and large documents. In addition, this Bayesian model does not consider the consistent sequencing of section topics. \n\nThe method of Del\u00e9ger and N\u00e9v\u00e9ol [ ] classifies each line in French clinical documents into its specific high-level sections such as header, content and footer. Thus, a statistical CRF model is trained based on some information about the line taking into account the first token in surrounding lines, first two tokens in the current line, the first token is in uppercase, relative position of the line, number of tokens, presence of preceding empty lines, digits and e-mail addresses. As advantage, the performance is very high especially for content and header lines. It is well noted that the headers and footers are very present in the document and should be identified to focus on the core medical content. However, the granularity level of sections is very high while there are more useful sections within the content that are not identified. \n\nLohr et al. [ ] have trained a logistic regression model on a manually annotated German clinical discharge summaries, short summaries and transfer letters to automatically identify sections using BoW statistics as features for each sentence. As advantage, this method achieves promising results in terms of f1-score. Furthermore, these authors have chosen a set of feasible and relevant categories for annotation. In addition, a sentence was chosen as an annotation unit while it has an appropriate granularity. However, the method does not perform well for categories that barely appear in the corpus. \n\nLup\u015fe and Stoicu-Tivadar [ ] have made a method that consists of homogenizing the sections of drug package inserts by standardizing the section names. At first, the method collects all section names from all drug package inserts, and prepares unique and common reference names that represent different kinds of sections. Then, machine learning is used to find the appropriate reference for each section name. Through this method, access to drug information has been improved for better processing. Moreover, this technique can be used in clinical decision applications to provide the necessary data to physicians. Thus, it helps especially new young doctors or those who start a new specialty. Neural network leads to highest results in the extraction of relevant information and outperforms cosine similarity according to the f1-score metric. Moreover, this model can be generalized to any language or domain. However, this model is appropriate only for records where sections are defined by headings. \n\nThe method of Chirila et\u00a0al. [ ] consists in supporting the prescription of drugs by structuring and categorizing the text into sections. For this, a machine learning model was trained to associate each part of the text with its appropriate section. According to the results of this method, the accuracy of the CNN based model is more superior especially with uniform name sections. Moreover, this method was applied on the Roman language where there is no dataset in this language with fully structured information. However, the execution time of CNN increases significantly when compared with a model based on Naive Bayesian classification. Moreover, this method is applied only on the Roman language. \n\nGoenaga et\u00a0al. [ ] have tested rules and machine learning based methods for section identification on Spanish Electronic Discharge Summaries. They have found that machine learning-based method gives the best results. This method is based on transfer learning using FLAIR model [ ] and generates character embeddings for sequence of tokens to annotate them by a BiLSTM-CRF model. Indeed, the rules have the lower results especially when an incorrectly marked section affects the surrounding sections even by carefully designing rules which is a time-consuming process. In contrast, the FLAIR method can identify sections even with variations or where headings are absent while it can learn from the headings and the vocabulary inside the sections. Also, training the method on data with more variability is useful to keep obtaining higher efficiency on different types of data. However, the results degrade when testing the trained model on different data and the degradation may be drastic in some cases. Furthermore, errors can be caused by high variability with the lack of training. In addition, implicit and mixed sections are the cause of several errors. \n\nNair et\u00a0al. [ ] have proposed a method to classify the sentences of i2b2 2010 clinical notes into different major SOAP sections using BiLSTM model with the fusion of Glove, Cui2Vec and ClinicalBERT embeddings. As advantage, the contextual embeddings and the transfer learning provide an efficient solution to this task. Also, the authors have found that 500 sentences per section is a sufficient starting point to achieve a high performance. However, they have considered only 4 sections and ignore other sections and sub-sections. Moreover, they have not considered the context-sensitivity of clinical sentences. \n\n\n#### Hybrid methods \n  \nThese methods use rules often for the construction of training data. These methods are weaker than rule-based methods, more ambitious than rule-based methods in dealing with different types of documents, and better than Machine Learning methods. \n\nJancsary et al. [ ] have trained CRF to recognize (sub)sections in report dictations giving lexical, syntactic categories, BoW, semantic type and relative position features for each word. The training data is constructed by aligning the corrected and formatted medical reports with the text from automatic speech recognition while the annotations are generated by mapping (sub)headings to the (sub)section labels using regular heading grammar. As advantage, this method can detect various structural elements even without explicit dictated clues. Furthermore, it can automatically assign meaningful types for (sub)sections even in the absence of headings. In addition, it is still effective under ideal conditions and can deal with the errors of real-life dictation. However, the manual correction is required to solve the errors of the automatically generated annotations which impact the segmentation results. \n\nApostolova et al. [ ] have constructed a training set by hand-crafted rules to train SVM to classify each medical report sentence into a semantic section using multiples sentence features such as orthography, boundary, cosine vector distance to sections and exact header matching. As advantage, a high-confidence training set is created automatically. Also, the classification of semantically related sections is significantly improved by boundary and formatting features. Furthermore, the segmentation problem could be solved when the NLP techniques are applied. Moreover, using SVM classifier outperforms a rules-based approach. However, it is hard to classify a section when its sentences are often interleaved with other sections. \n\nNi et al. [ ] have classified medical document sections into pre-defined section types. These authors applied two advanced machine learning techniques: One is based on supervised learning and the other on unsupervised learning. For the supervised technique, a heuristic model pre-trained on old annotated documents is used to select a number of new candidate documents that will be annotated by people and will be used for learning. For the unsupervised technique, a mapping method was used to find and annotate sequences of words, which represent section titles, by their corresponding section types using a knowledge base. A maximum entropy Markov model was used for section classification. The chosen model is faster in learning and allows richer features. In fact, the techniques used can reduce the cost of annotation and allow a quick adaptation on new documents for section classification. In addition, both techniques can achieve high accuracy. However, the supervised technique requires more annotation cost than the other technique. In addition, the performance of the unsupervised technique is highly dependent on the quality of the knowledge base. \n\nDai et al. [ ] have proposed a token-based sequential labeling method with the CRF model for section heading recognition using a set of word features such as affix, orthographic, lexicon, semantic and especially the layout features. To construct training data, they have employed section heading strings from terminology to make candidate annotations. Then, three experts are used to manually correct the annotations of top most section headings. As advantage, this was the first work which treats section detection as a token-based sequential labeling task and outperforms sentence-based formulation and dictionary-based approaches. This method has an integrated solution which avoids the development of heuristics rules to isolate heading from content. Also, layout features improve the results and can recognize section headings that are not appearing in the training set. However, it is difficult to recognize rare or nonstandard topmost section headings. In addition, subsections are not taken into consideration. Furthermore, some section headings are not the topmost in some records. Also, the absence of layout information can decrease the recall. \n\nSadoughi et al. [ ] have applied section detection on clinical dictations in real time. They used automatic speech recognition to transform the speech into plain text. Also, a unidirectional LSTM model, which tracks short and long term dependencies, is run on the text to annotate its section boundaries using Word2vec vectors to represent the input words. To do this, regular expressions were applied on a set of reports to annotate the headings. Each time, a post-processing task is applied on each section to transform the text into a written report. As advantage, the post-processing task can become faster with the processing of a complete section each time, instead of re-executing after each dictated word. Thus, the post-processing of the previous section happens in parallel with the dictation of the current section without disturbing the user. Moreover, the post-processor can benefit from the full context of the section during the transformation. Thus, real-time section detection ensures that the medical report is directly usable for other processes after dictation. However, the detection of a section depends only on the words dictated so far without seeing the whole document. This prevents it from exploiting all the information in the document to provide a better quality result. \n\n\n\n\n## Methods dealt with issues related to the nature of entities \n  \nIn this section, we highlight some solutions proposed by the state-of-the-art methods to solve problems related by the nature of the medical NER. Thus, we have cited and classified some methods into four categories based on the main problems: Ambiguity, Boundary detection, Name variation and Composed entities. Table   shows some of the methods used to deal with these various problems.   \nSome methods that addresses the issues related to the nature of named entities. The abbreviation \"NI\" in this table means Not Included \n  \n\n### Ambiguity \n  \nThe ambiguity is when a medical named entity can belong to more than one class depending on the context. Thus, for some entities, we should explore the preceding words and the position of the entity in the text in order to know its meaning. Generally, that is the most important problem in entity recognition while most studies focus on how to provide more context to recognize the entity. As a special case, the abbreviations are likely to be ambiguous. Moreover, it is hard to determine the expansion of an abbreviation with a very small number of characters. Thus, an abbreviation mostly may have different meanings depending on the context. As common solutions, works try to enrich context information by word or character embedding, knowledge base and word position in the text such as section, surrounding words, PoS, etc. Also, they try to capture the contextual dependence and relation. Lei et\u00a0al. [ ] have reached a higher performance by merging the word segmentation and the section information. Ghiasvand and Kate [ ] have benefited from UMLS which provide a lot of entity terms which are declared as unambiguous. Thus, they do an exact matching for these terms to annotate a maximum number of unambiguous entities to partially solve the ambiguity. Xu et\u00a0al. [ ] have benefited from more context to solve the ambiguity problem by using categories representation by Word2vec, PoS, dependency relation and semantic correlation knowledge. However, the method may miss some medical entities in the non-medical terms filtering step. The method of Zhou et\u00a0al. [ ] can solve the ambiguity problem by making two types of embeddings for more context, which are C-ELMo for word-level features and C-Flair for character-level. Likewise, the relationship between word-level contextual features can be captured by the C-ELMo model. However, this method fails to detect the boundary of complex phrase-level entities. Deng et\u00a0al. [ ] have used character embedding with BiLSTM-CRF to avoid feature engineering by learning the semantic information in the context. Thus, BiLSTM can provide more comprehensive contextual information and easily learn about contextual dependencies. Moreover, the CRF optimizes the result from the sentence level. However, this method is restricted by the entity labeling granularity where we can find some nested entities. The method of Zhao et\u00a0al. [ ] avoids the ambiguity as it automatically propagates some seed rules based on lexical or contextual clues which are strong indicators of entity recognition. In addition, the authors have fine-tuned a pre-trained contextual embedding model BERT in the biomedical domain. Also, they used a pre-trained contextual embedding model ELMO to give an average embedding for each rule to estimate the semantic relatedness between rules. Li et\u00a0al. [ ] have tested Word2vec by the help of BiLSTM to improve the results by capturing the contextual information. Indeed, BERT embedding alone is more effective than Word2vec and ELMo and it does not need BiLSTM since it has already captured the contextual information. The method of Sui et\u00a0al. [ ] is based on the interactions among the words, entity triggers and the whole sentence semantics to recognize the entity from its context. \n\n\n### Boundary detection \n  \nA method can recognize a part of a named entity and fail to determine the exact words which construct that entity. Thus, the method can miss some words from the full named entity, or may add surrounding words that do not belong to this entity. Indeed, searching for the presence of named entities and their approximate positions is easier than finding entities with their exact beginning and ending positions. Thus, these positions are named by the named entity boundary, where the boundary detection is known as an important challenge for medical named recognition methods. The most popular solutions to this problem is the extraction of noun phrases while the most of named entities are noun phrases or overlapped with noun phrases [ ]. However, the noun phrase extraction is not always perfect and there are still some rare entities that do not belong to noun phrases. Also, the sequence tagging with the IOB annotation format, especially by BiLSTM-CRF model, can learn well how to determine the entity limit. However, this annotation is sensitive for nested entities. Lei et\u00a0al. [ ] use a Chinese medical dictionary as a knowledge source for word segmentation. Indeed, most errors appear in long entities. Xu et\u00a0al. [ ] have extracted medical native noun phrases in a boundary detection step. In addition, they have exploited a knowledge-driven method to detect boundary, by mapping text to concepts in offline and online lexical resources. Thus, the recognition performance is significantly improved. However, this method may still obtain inexact entities which show some decline in precision and recall. Ghiasvand and Kate [ ] have trained a classifier by the medical terms found in UMLS to learn how to expand the boundary of words. Thus, the classifier is applied to all noun phrases in which the detected entity occurs in order to select the entity with the highest score. However, automatically obtaining noun phrases can make mistakes. Also, sometimes we may find named entities that are not noun phrases. In order to avoid incorrect identification for the entity boundary, Deng et\u00a0al. [ ] have ensured the integrity and accuracy of the named entity by the bidirectional storage of text information. In addition, the IOB labeling method is introduced. Also, they have used character-level embedding which can avoid poor segmentation. However, the phenomenon of nesting entities leads to unclear definition of boundary and results in poor accuracy. Zhao et\u00a0al. [ ] have extracted all noun phrases from each sentence as candidate entity mentions based on a set of PoS patterns. The method of Li et\u00a0al. [ ] apply a relation classification on each pair of candidate entity fragments to determine if it is a discontinuous entity or not. \n\n\n### Name variation \n  \nThe same named entity can be written in different forms by adding and deleting some characters, changing the order of its component words or changing some words by synonyms. Also, we may have typos on a narrative text written by humans. Thus, an exact searching for named entities cannot cover all the forms of named entities. As common solutions, the preprocessing step is often used to transform words on unified form. Thus, we need some techniques that are able to recognize the named entity in any form in the text. Also, considering the surrounding words or characters may be useful to determinate the named entity. Another solution is to use the character embedding. The method of Quimbaya et\u00a0al. [ ] can solve the variation of named entities using exact, fuzzy and lemmatized matching by a knowledge base. However, it cannot take into consideration the context and the words surrounding the named entity. In the work of Ghiasvand and Kate [ ], the lemmatize and stemming form of the words surrounding the entity in addition to other features are fed to a decision tree based classifier. Thus, that can tackle the variability problem. The method of Deng et\u00a0al. [ ] is based on the character embedding which can avoid the variability problem while it is not restricted by a vocabulary of words. Zhou et\u00a0al. [ ] can solve the variability by using character embedding to handle out-of-vocabulary words. Zhao et\u00a0al. [ ] can avoid the name variation problem where they define different types of rules which considers the lexical, contextual and syntax information based on the clues to find entities. However, some rules may not be applicable due to the mismatch between the training set and a different dataset. Li et\u00a0al. [ ] have tested the ELMo character-level embedding which is able to represent out-of-vocabulary words. However, the characters can\u2019t capture the whole meaning of words and should be merged with word-level embedding. Sui et\u00a0al. [ ] have added entity triggers to help the model recognizing the entity by the surrounding cue words. However, this method requires manual effort by annotators to annotate a large group of words to prepare entity triggers. \n\n\n### Composed entity \n  \nThe named entity can be composed by multiple words and may be a long phrase. Hence, the different named entities are able to overlap with each other. Consequently, we may find a nested named entity which is included in another one and belong to a different class. Thus, the granularity level should be considered to recognize all the named entities that can be found in one longer named entity. To solve this problem, some works extracts all possible noun phrases including the nested ones. But automatically extracting noun phrase is still not perfect. Also, few named entities are not overlapping with noun phrases. Xu et\u00a0al. [ ] can handle nested entities by identifying entity candidates based on the dependency relationships between words. Thus, medical native noun phrases, such as single nouns and maximum noun phrases, are extracted. The method of Ghiasvand and Kate [ ] is able to detect nested entities by obtaining all noun phrases, with nested ones, using a full parsing. The method of Li et\u00a0al. [ ] enumerates and represents all possible text spans [ ] to recognize the overlapped entities. Thus, a relation classification is applied to judge whether a pair of entity fragments to be overlapping or succession. Sui et\u00a0al. [ ] have proposed a cost-effective and efficient trigger-based graph neural network to cast the problem into a graph node classification task. \n\n\n\n## Knowledge discovery methods \n  \nIn this section, we cite examples of studies that applicate text mining methods to discover new knowledge in medical field. Thus, this knowledge can be used to make medical decisions or to gather useful information and conclusions for some medical tasks. Indeed, knowledge discovery is the final and the most important step in medical data processing. This step consists in discovering a needed knowledge from the extracted information which directly supports the medical staff. \n\nThe method of Sudeshna et al. [ ] is able to predict the probability of having heart disease by using the supervised SVM algorithm to classify the data. This method takes particular symptoms, given by the patient, and the patient\u2019s health record. Based on this data, this method can suggest diseases, treatments, medications and dietary habits for the doctor. The latter can confirm and send the information to the patient. As advantage, this system is reliable, easy to understand and adaptive. Also, the disease is automatically analyzed more efficiently and is easily identified. In addition, this method can identify the best medication for the disease. \n\nAich et al. [ ] have proposed a method to analyze abstracts of articles related to Parkinson\u2019s disease to automatically find the relationship between walking and Parkinson\u2019s disease. This method is based on text mining. It does a simple pre-processing on the text. Then, it provides a graphical visualization to indicate the word frequencies through a Word Cloud. Also, it categorizes the similar terms using a hierarchical classification and K-means based on a distance calculation between words. Indeed, this approach has a great potential to classify a text into different groups. However, this method can be improved by providing more articles to be analyzed or by exploring other classification approaches. \n\nThe review of Al-Dafas et al. [ ] have shown that applying algorithms of data mining techniques on patients\u2019 health data is able to help doctors to make the right decision at the right time. Thus, these techniques can detect the cancer disease early without surgical intervention. As advantage, they can reduce treatment costs and medical errors in diagnosing the disease. The authors suggest to adapt machine learning-based data mining in the medical diagnosis process.   \nBenchmark datasets used in IE. \n  \n\n\n## Benchmark datasets and supplement resources \n  \nIn this section, we cite the most important benchmark datasets used in the IE task which are manually annotated and considered as gold standard. Most of these datasets are used in shared tasks and used to train and evaluate IE state-of-the-art methods. Table   shows some details about these datasets. Generally, the annotation focus on information related to diseases, medicament and chemical entities. Many datasets are constructed from PubMed articles and especially the abstracts because they are easier to collect. Other datasets are obtained from discharge summaries and clinical reports which are de-identified to hide personal information. However, the available datasets are in textual form while the medical documents are originally available on PDF form. Thus, useful information is not available such as formatting style, which is important to define the document structure. Indeed, there is some recent datasets Cohen et al. [ ], Islamaj et al. [ ] obtained from full-text article which can provide richer information rather than just an abstract. \n\nAlso, many methods use supplement resources such as dictionaries and ontologies. The most popular resources that used in IE are UMLS [ ] meta-thesaurus and Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) [ ] ontology. The UMLS meta-thesaurus in its 2021 update can cover 16543671 terms in different languages (11755677 in English) which are associated with 4536653 medical concepts. This large biomedical thesaurus is collected from different sources such as SNOMED-CT, MEDLINE, MeSH, NCBI, etc. Also, it provides 127 semantic types of the concepts such as Disease/Syndrome, Clinical drug, Therapeutic or preventive procedure, laboratory or test result, etc. Furthermore, it provides 54 semantic relations between these semantic types, such as Process of, Result of, Property of, Part of, Associated with, Complicates, Affects, Causes, etc. \n\nThe SNOMED-CT ontology covers more than 1314668 clinical terms with their description and they are associated with different concepts. There are 19 top-level concepts which are body structure, finding, event, environment / location, linkage concept, observable entity, organism, product, physical force, physical object, procedure, qualifier value, record artifact, situation, social context, special concept, specimen, staging scale and substance. In addition, it provides more than 3092749 relation mentions of 122 types of relations such as \u201cis a\u201d, \u201cfinding site\u201d, \u201cassociated morphology\u201d, \u201cmethod\u201d, \u201ccausative agent\u201d, etc.   \nRule-based NER methods \n    \nDictionary-based NER methods \n    \nMachine learning-based NER methods \n  \n\n  \nMachine learning-based RE methods \n    \nRule-based RE methods \n  \n\n\n## Experimental analysis \n  \nIn this section, we have collected results of some methods for each IE task such as NER, RE and section detection. Thus, we have organized these methods according to the used technique, dataset and features. Hence, we can make an experimental analysis according to their available results. As a look at all the results in general, we find that most methods use the machine learning technique. For the NER, the best result is achieved by a machine learning-based method, while a rule-based method gets the best result for RE and a hybrid method is the best for section detection. \n\nNamed Entity Recognition: Tables  ,   and   show the results of rule-based, dictionary-based and machine learning-based named entity recognition methods, respectively. The A, P, R and F symbols refer to Precision, Recall and f1-score, respectively. The best f1-score (96.05%) was for the method of Popovski et\u00a0al. [ ] which uses rules based on computational linguistics and semantic information to describe food concepts in order to annotate entities in 1000 food recipes. This method is aimed to extract food entities which can be useful in medical field such as food safety. For disease and symptoms and some other named entity types, the method of Deng et\u00a0al. [ ] , based on machine learning and tested on 20% characters from 1600 TCM patents\u2019 abstracts, gets the best result in terms of f1-score (94.48%). This method is based on fine-tuning character embeddings with the use of BiLSTM-CRF model. For the i2b2-2010 benchmark dataset, a model based on machine learning gets the best result in terms of f1-score (87.45%), which uses LSTM-CRF model with contextualized and GloVe embeddings. Thus, machine learning-based model using context-specific embedding with the model based on LSTM and CRF can get better results. In fact, the unsupervised models [ ,  ], predictably, get the worst results but it was acceptable (51.06% and 59.43% f1-score), and it is worth avoiding manual effort. \n\nRelation Extraction: Tables   and   show the results of machine learning-based and rule-based RE methods, respectively. The P, R and F symbols refer to Precision, Recall and f1-score, respectively.. The best method [ ] gets the best f1-score (96.06%) which have used rules to generate candidates and classify them based on clinical BERT model. It was trained and tested on n2c2 2018 benchmark dataset and thus it is the best method applied on this dataset which is annotated by relations linking concepts to their medication. In fact, this method can be considered as hybrid-based method while it uses the rules with machine learning. The best machine learning-based methods [ ,  ] have achieved 94% f1-score and have trained and evaluated on n2c2 2018 benchmark dataset to link concepts to their medication. These methods have used a fine-tuned BERT as embedding. However, BERT embedding is not suitable neither for i2b2 2010 dataset, which is annotated by relations between medical problem, treatment and test entities, nor for n2c2 2019 dataset which is annotated by relations between family members, observations and living status. The methods [ ,  ] applied on these datasets with fine-tuned BERT have almost the worst results. Indeed, there is machine learning based method [ ] applied on the text of tweets collected from Twitter social media. As expected, it gets the worst result in terms of f1-score (56.7%) while it is a very difficult task to apply a medical IE on the social media text. This type of text is less structured compared to professional documents and articles while the writer of tweets is often not an expert on medicine and does not pay attention to correct writing. Thus, getting 54% of precision and 63% of recall and 56.7% of f1-score in this type of text is an appropriate result. \n\nSection Detection: Tables  ,   and   show the results of rule-based, machine learning-based and hybrid section detection methods, respectively. The A, P, R and F symbols refer to Accuracy, Precision, Recall and F1-score, respectively. In fact, the section identification methods are not well experimented while this task needs a more standardized evaluation method such as a benchmark dataset. Moreover, some methods use different segmentation and granularity level which make comparing state-of-the-art so complicated. However, according to the results, the best method according to the accuracy metric is a rule-based method [ ] which has used a simple regular expression to match titles from a set of section titles and is tested on 3 datasets. The maximum accuracy score (90%) was achieved using 1630 prospectuses. This method outperforms a machine learning based method in terms of accuracy, and this is the only available accuracy result we have. However, when a larger dataset is used the result may decrease significantly. Also, it may differ from a dataset to another while the regular expressions may be inappropriate for other types of documents. For example, the method applied on 3814 prospectuses gets 88% of accuracy while the same method applied on 3002 prospectuses from another source get 66% of accuracy. The best method in terms of f1-score (94.19%) is a hybrid method [ ] which uses a terminology to make a semi-supervised annotation of the training set and train a CRF model on layout, affix, orthographic, lexicon and semantic features for sentence tagging. This method is trained and evaluated on a dataset which is already considered as a benchmark for another different IE task and contains patient records such as discharge summaries and procedural notes, etc. Also, this method has the best result in terms of f1-score among the methods applied on discharge summaries. In second place, a method based on machine learning has achieved 93.03% f1-score which uses BiLSTM-CRF with FLAIR character embedding fine-tuning on reports of hospital discharges. Thus, the use of CRF model can lead to the best results in section detection task.   \nRule-based section detection methods \n    \nMachine learning-based section detection methods \n  \n\n  \nHybrid section detection methods \n  \n\n\n## Literature review \n  \nIn the last years, we can see that the machine learning methods are well studied in the IE, where many models are tested with many types of features. Generally, CRF is the most popular model which is a sequence of tagging model while the problems are mostly defined as tokens labeling especially in the named recognition task. Recent works have showed that the fusion between BiLSTM and CRF give better result [ ,  ,  ], because BiLSTM can consider the order from double directions which makes it able to well understand a sequence of features. Hence, CRF can perform an accurate labeling using the features provided by BiLSTM. \n\nHowever, the manual effort problem is one of the most important challenges in this field. Many methods are destined to search an automatized or semi-automatized techniques to obtain a high quality annotated dataset due to the lack of training data [ ,  ,  ]. In addition, there are different fields and different types of data, and the medical field is evolving day after day, which makes one annotated training set not enough to be used permanently. A manually annotating dataset need much effort and is time consuming, and that is why we need to automatize this step to enable the model to easily adapt new types of data. Comparing to rules, a machine learning model can learn implicit and deep rules using automatically generated features which make possible to explore and predict the best output in many new cases, whereas the rules usually only deal with pre-defined cases although they can be smooth to exploit the context and knowledge. In fact, rules are more precise when they can be exactly matched on the data, but cannot discover new knowledge beyond the defined rules. Also, rules fail in many cases, especially for variable data. \n\nIndeed, some methods try to automatize the construction of rules. An important recent study [ ] uses graph propagation method to find new rules giving few seeding manual rules that are easily constructed by experts. Thus, even rules can be automatically generated. In addition, the graph based techniques can be exploited, while they are able to treat the relationships inside data. The synonyms and antonyms relations, contextual similarity and many other relation types between words and phrases are very important and can be well exploited using a graph technique. In addition, even the social media can be very critical area for this task, where the data is more noised and contains many grammatical mistakes, and especially false information by which people can be influenced. Some recent works are aimed for social media [ ,  ], for example, Shi et\u00a0al. [ ] have used machine learning models by the help of Concern Graph to apply entity recognition on pandemic concern entities in Twitter. Recently, the social media have played an important role during the COVID-19 pandemic period and it is crucial to automatically understand and supervise a lot of people\u2019s interactions. Indeed, social media is a rich source of information which mostly contains unstructured and confused textual and other multimedia data. Thus, some studies are applied to extract information from that data in order to perform some tasks such as associating tags to posts [ ], identifying relevant information [ ], etc. It is worth noting that the graph of users\u2019 relations such as following relations can be exploited too to enhance the medical IE. Thus, some important tasks such as community detection and influence identification [ ,  ,  ,  ] can be combined with medical IE tasks in social media. \n\nTalking about supplement resources, is good to construct a big resource which tries to cover all things, but it cannot be. Some studies try to automatically update these resources by an online phase, for example, by using a search engine [ ]. Indeed, the most used additional resources are UMLS [ ] and SNOMED-CT [ ] that can cover a lot of concepts, languages, semantic types and much information about terms, which make it possible to even annotate a raw text or extract more information using some matching techniques. Some studies tried to find a smooth technique to use the information inside dictionary in order to overcome the limited coverage. For example, new relationships can be learned using the synonym relationships provided by the SNOMED-CT ontology using CNN model [ ]. Also, we can make a gazetteer tagger using the power of machine learning which learns features from a dictionary to provide an output which improves another machine learning model [ ]. Indeed, the relation between machine learning and dictionary based methods is well studied, while using machine learning can make the use of a dictionary more robust to override the limits of the resource. Thus, these studies try to find a better way to exploit the knowledge of ontologies. Furthermore, the knowledge inside the dictionary can be used as additional feature for machine learning models [ ] and it gives a useful information about the context. \n\nTalking about features, most recent works focus on distributed representation for words as well for characters such as BERT and Word2vec, which can well extract context information for machine learning methods. Furthermore, the models which generate these embeddings can be used in different manners, where they can be pre-trained from other sources, trained during the whole model training or fine-tuned. In particular pre-trained and fine-tuned language models such as BERT have achieved state-of-the-art performance on many natural language processing tasks. For example, the work of Yang et\u00a0al. [ ] have showed that clinical pre-trained transformers achieve better performance for RE. In addition, other types of features can be beneficial such as knowledge and syntactic features. Indeed, there is some information that is not well exploited although that has showed a good potential to improve the IE task. Generally, this task can be improved by giving more context. Indeed, the study of Lei et\u00a0al. [ ] have proved that section information is able to improve the entity recognition task, but it is not well exploited in this field. In addition, Tran and Kavuluru[ ] have used sub-headings to improve the RE. Furthermore, the formatting style of the document can be very important, while it have proved its ability to detect section titles in the study of Beel et al. [ ] using only font size information. A medical document is generally created on PDF format which provides more useful information than a raw text. However, all available benchmark datasets are provided only as an annotated raw text. Furthermore, most datasets which contain medical articles are available only with annotated titles and abstracts. Recently, some researches [ ,  ] are trying to construct new annotated datasets especially for NER with full-text articles which frequently contain more detailed information. These articles are usually collected from PMC [ ]. \n\nConcerning the named entities, most works focus on the disease entity type, even most datasets [ ,  ,  ] and the widely used meta-thesaurus [ ] provide annotations and information about this type of entity. Thus, extract information about disease in medical texts is a very important task in the medical field. Indeed, the named entity boundary problem is another challenge [ ,  ,  ], while solving this problem lead to obtain a better result especially according to the exact matching metric. However, this metric always give a very low score comparing to partial matching metric. Generally, this problem is related to the noun phrase chunking which is adopted by most works. \n\nA lot of studies in the medical IE are destined for the Chinese language. Although English language is more suitable for this task, the Chinese researchers are trying to improve this task for them while they are very interested by the evolution in medical field generally. However, the Chinese language in medical field is more difficult compared to English especially on the segmentation while it has complicated syntax rules, and on the lack of Chinese data. Hence, many studies are destined to deal with this language problems [ ,  ,  ]. The segmentation is generally used to provide samples and extract features from them. It can be performed on word, phrase, sentence and section level. Indeed, Deng et\u00a0al. [ ] found that making features for a sequence of characters is more suitable especially for Chinese language. \n\n\n## Conclusion and Future Research Directions \n  \nThe IE in the medical field is very interesting especially to find information about diseases. Generally, this task provides more knowledge, supports persons to find relevant information and helps doctors to release the best decision, for example, to choose the right treatment, make the appropriate drug prescription or discover causes and effects of some diseases. A large amount of unstructured medical textual information is terrible to be manually analyzed by doctors while is considered as a heavy treasure of information. \n\nIn our survey, we conclude that the rule-based and hybrid methods are generally the promising techniques for IE where they have shown the best results. However, the rules depend highly on a specific domain. Thus, it is difficult to adapt these methods on new type of data since a manual effort by domain experts is needed. Thus, generating rules that are constructed dynamically to adapt the data type is a promising direction. Most methods focus on the combination of CRF and BiLSTM models which is very beneficial for sequence-tagging tasks. However, the CRF model is usually used for flat NER, which is not appropriate for nested and discontinuous named entities. Domain-specific embeddings, especially which are provided by BERT model, are used by many methods and give better results. There is a lack of the medical data where these data contain more privacy compared to other fields. The manual annotation is also a big problem for machine learning and many recent methods are going to automate it. Thus, it is a challenging issue to provide a high quality data for training and other supplement data which can cover the new medical terms, several types of data, all possible cases, multiple languages, different types of labels, etc. Also, all datasets are available in textual format while the formatting style is very important and should be more exploited. Indeed, it adds a very useful information on the text which is especially used to understand the structure of the document and even the meaning of words. \n\nIndeed, section detection is a challenging issue and showed a positive impact on the performance of several IE tasks. Mainly, the position of a concept in a document can provide more contextual information. However, this task is not well covered by the research work and methods. Thus, benchmark datasets should be constructed for it. Another issue is combining rule-based, dictionary-based and deep-learning approaches to well benefit from them in one hybrid method. Many ideas have been proposed and can be exploited further. Indeed, we can use rules to prepare data for machine learning or we can use machine learning to generate rules. Besides, we can ameliorate the dictionary matching by machine learning or we can annotate a lot of data by a dictionary to train a machine learning model. Also, we can use the rules by constructing regular expressions to perform a dictionary matching or we can use dictionary as a supplement resource to support the rules. In addition, we can make features by using dictionary and rules based techniques. \n\nSome important directions about IE in medical documents are not well covered by our analysis. Thus, the research themes in these directions can be further developed since they have provided promising results. Indeed, the difficulty of handling multi-language documents is an interesting issue which makes it hard to adapt the model for data with different languages. Hence, it is good if we can benefit from data of multiple languages. For example, we can use the transfer learning in order to benefit from the knowledge learned from a pre-trained model. Consequently, the model will be able to adapt the knowledge on another language rather than training the model from scratch on one language. Thus, we can combine the learned knowledge from more than one dataset of different languages. Generally, adapting a method on different languages can solve the lack of data and benefit from the knowledge gathered from different data sources. In addition, while most of researches are aimed for English and some for Chinese language, the other languages have a poor chance to be well treated. Therefore, unifying multiple languages in one method would be a really important achievement. \n\nIn fact, document summarization is another major and very challenging issue. This task can depend on the NER while it consists in making a text summarization which contains only the most important information. Hence, we can easily recognize a relevant and a very reduced readable part of text in a document instead of reading a whole text. Thus, the useless part of text can be eliminated even for other tasks of IE. Due to the diversity and the growing quantity of medical information, persons need to quickly assimilate and determine the content of a medical document. Thus, document summarization helps persons to quickly determine the main points of a document. However, this research field has not yet reached maturity, while variety of challenges still needs to be overcome such as handling large scale data, providing sufficient annotated data, etc. \n\nAnother important issue and possible research direction, which has been discovered especially during the COVID-19 pandemic, is about analyzing the propagated medical information in social media. The content on social media is very different from documents especially when the information is written by normal users and not medical experts. Thus, the natural language processing will be much more difficult while we can find unstructured texts and many typos. As well, we can easily find a big number of users influenced by false medical information which represents a critical problem. Likewise, the social media environment is rich of useful information more than just a document. Therefore, we can easily benefit from the reactions on the post, the owner profile, contacts, etc. Thus, IE in social media is very challenging more than in medical documents and is needed to detect the spread of false information and understand the people\u2019s interactions with medical information. \n\n \n", "metadata": {"pmcid": 9640816, "text_md5": "dec05120ba700234dc83244b34f296ba", "field_positions": {"authors": [0, 67], "journal": [68, 82], "publication_year": [84, 88], "title": [99, 204], "keywords": [218, 349], "abstract": [362, 1673], "body": [1682, 114320]}, "batch": 1, "pmid": 36405956, "doi": "10.1007/s10115-022-01779-1", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9640816", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9640816"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9640816\">9640816</a>", "list_title": "PMC9640816  Information extraction from electronic medical documents: state of the art and future research directions"}
{"text": "Li, Luqi and Zhao, Jie and Hou, Li and Zhai, Yunkai and Shi, Jinming and Cui, Fangfang\nBMC Med Inform Decis Mak, 2019\n\n# Title\n\nAn attention-based deep learning model for clinical named entity recognition of Chinese electronic medical records\n\n# Keywords\n\nNamed entity recognition\nAttention mechanism\nChinese electronic medical records\n\n\n# Abstract\n \n## Background \n  \nClinical named entity recognition (CNER) is important for medical information mining and establishment of high-quality knowledge map. Due to the different text features from natural language and a large number of professional and uncommon clinical terms in Chinese electronic medical records (EMRs), there are still many difficulties in clinical named entity recognition of Chinese EMRs. It is of great importance to eliminate semantic interference and improve the ability of autonomous learning of internal features of the model under the small training corpus. \n\n\n## Methods \n  \nFrom the perspective of deep learning, we integrated the attention mechanism into neural network, and proposed an improved clinical named entity recognition method for Chinese electronic medical records called BiLSTM-Att-CRF, which could capture more useful information of the context and avoid the problem of missing information caused by long-distance factors. In addition, medical dictionaries and part-of-speech (POS) features were also introduced to improve the performance of the model. \n\n\n## Results \n  \nBased on China Conference on Knowledge Graph and Semantic Computing (CCKS) 2017 and 2018 Chinese EMRs corpus, our BiLSTM-Att-CRF model finally achieved better performance than other widely-used models without additional features(F1-measure of 85.4% in CCKS 2018, F1-measure of 90.29% in CCKS 2017), and achieved the best performance with POS and dictionary features (F1-measure of 86.11% in CCKS 2018, F1-measure of 90.48% in CCKS 2017). In particular, the BiLSTM-Att-CRF model had significant effect on the improvement of Recall. \n\n\n## Conclusions \n  \nOur work preliminarily confirmed the validity of attention mechanism in discovering key information and mining text features, which might provide useful ideas for future research in clinical named entity recognition of Chinese electronic medical records. In the future, we will explore the deeper application of attention mechanism in neural network. \n\n \n\n# Body\n \n## Background \n  \nElectronic medical records (EMRs) contain rich health data and important clinical evidence, which are helpful to support clinical decision-making and disease monitoring [ ]. But the large number of unstructured clinical texts limit the large-scale knowledge discovery and application of electronic medical records [ ]. It is urgent to explore the auto-information extraction methods to transform unstructured texts into structured data that are easy to understand and use for computers. \n\nAs a key step in natural language processing (NLP), clinical named entity recognition (CNER) has been a popular research topic on extracting all kinds of meaningful information in unstructured clinical text. Early studies focused on designing characteristic templates with the help of linguistic knowledge and professional dictionaries [ ]. With the publication and application of large-scale electronic medical record corpus [ ], the method of named entity recognition based on statistical learning has been widely used and proven to achieve good performance in many studies [ ,  ]. But traditional machine learning methods depend on large tagged corpus and effective feature engineering. In order to reduce the dependence on linguistic knowledge and complicated feature engineering, deep learning with muti-layer neural network structure has become the most popular method for clinical entity recognition [ \u2013 ]. \n\nThe clinical text features of Chinese electronic medical records pose many challenges to named entity recognition task. Firstly, as shown in the examples of Table\u00a0 , the clinical texts are more objective than the common natural language, and the logic of semantic is relatively concentrated. Therefore, it is difficult to reuse the common domain language model in the named entity recognition of electronic medical records. Secondly, there are so many professional and uncommon clinical terms used to describe different patients\u2019 situations. And many terms are expressed in the different form of English abbreviations or another names, such as \u201c\u76f4\u80a0\u4f4e\u4f4d\u524d\u5207\u9664\u672f (low anterior resection)\u201d is also expressed as \u201cDixon\u672f (Dixon operation)\u201d in the same clinical text. Different expressions of clinical terms make it difficult to use Chinese medical dictionary for effective entity recognition. In addition, the use of Chinese word is more flexible, sometimes we can\u2019t judge whether the word is a named entity in the context, and even if it is a named entity, it may belong to different types in different context. For example, \u201c\u53cc\u4fa7\u8f93\u5375\u7ba1\u5207\u9664\u672f (bilateral salpingectomy)\u201d should be recognized as an \u201cOperation\u201d entity as a whole, but \u201c\u53cc\u4fa7\u8f93\u5375\u7ba1 (Bilateral tubal)\u201dshould be recognized as an \u201cAnatomical\u201d entity when it appears alone. In this case, the method of clinical named entity recognition needs strong learning ability to capture the key context-critical information, which can improve the recognition performance both\u00a0on entity type and entity\u00a0boundary.\n   \nExample of Chinese EMRs \n  \n\nIn most previous studies, Chinese word embedding and traditional domain features were used to improve the performance of deep learning model in CNER tasks. In the China Conference on Knowledge Graph and Semantic Computing (CCKS) 2018 CNER challenge [ ], the team from Alibaba Health Information Technology used cw2vec method to construct word embedding for the first time in the field of medical texts [ ], they explored the features contained in strokes and radicals of Chinese characters and finally won the first place with an F1-measure of 89.13% [ ]. Zhang et al. added 50-dimensional word embedding, 50-dimensional dictionary embedding and stroke embedding trained by cw2vec tool as additional features to the training of neural network models and finally achieved good experimental results (F1-measure of 88.64%) [ ]. Although these methods are effective, they also spend a lot of time on selecting features and training word embedding. \n\nAttention mechanism was initially used in the field of image recognition to emphasize the different influence of different input data on output data [ ]. It is a selective mechanism for allocating information processing capabilities, which can selectively focus on some important information and ignore other information received at the same time. In recent two years, it has been widely used in sequential learning tasks of NLP and achieved excellent performance [ ,  ]. Sui proposed an Encoder-Decoder-CRF model with Attention mechanism to solve the problem of redundancy or missing of context information in Chinese NER tasks, results showed the model with attention mechanism performed better in the recognition of organization and place names than Bi-LSTM-CRF model, thus it could be more flexible to obtain appropriate context information for entities [ ]. Ma et al. proposed an attention-based BLSTM-CRF model for new energy vehicle patent terminology extraction and corrected the results using a dictionary-based and rule-based method. The accuracy of this model reached more than 86% [ ]. Luo et al. proposed a novel attention-based BiLSTM-CRF approach for document-level chemical NER to solve the tagging inconsistency problem, finally achieved the state-of-the-art performances on the BioCreative IV CHEMDNER corpus and the BioCreative V chemical-disease relation (CDR) corpus [ ]. \n\nOver all, the attention mechanism can make up for the shortcomings of the semantic representation in the traditional encoder-decoder model, and it is easier to capture the long-distance interdependent features in the sentence [ ]. Therefore, aiming to improve the performance of clinical named entity recognition in Chinese EMRs, we proposed a deep learning model named BiLSTM-Att-CRF that combined bidirectional long-short time memory network with attention mechanism. \n\n\n## Methods \n  \n### Data preprocessing \n  \nThe processing flow of our method is shown in Fig.\u00a0 . Firstly, some preprocessing steps including data annotation, sentence splitting and character segmentation were performed. In order to preserve the meaning of the special punctuation marks in the clinical text, e.g. \u201cCERBB-2(2+)\u201d, and considering the centralized semantic logic of clinical texts, we split the input sentences by commas and periods. Unlike English, Chinese clinical texts don\u2019t have space as the boundary mark of words, and there are many different combinations of characters. In order to avoid the entity boundary recognition errors caused by word segmentation, we took characters as the input of the model.\n   \nThe processing flow of our method \n  \n\nRecently, known as distributed feature representation, word embedding has been widely used in the field of natural language processing, especially for deep learning methods. Compared with the one-hot representation based on the bag-of-words (BOW) and n-gram method, word embedding is low dimensional and dense,it can automatically measure the semantic similarity of words from a large unlabeled corpus [ ]. In recent years, several tools such as word2vec [ ] and GloVe [ ] have been widely used in the field of NLP. Consistent with character-based input, we employed the character embedding as the basic feature. To achieve a high-quality pre-trained character embedding, a total of 3605 medical records were collected from the CCKS 2017 [ ] and CCKS 2018 CNER challenge task [ ], they were all used to train 200-dimensional character embedding by the word2vec tool. \n\nClinical texts usually have relatively fixed syntax and common expressions, therefore traditional linguistic and domain features based on dictionaries and grammatical structures can effectively improve the performance of CNER [ ]. So additional features including dictionary and part-of-speech were introduced into this study. Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) is the most comprehensive clinical terminology covering 19 clinical categories. We built our dictionaries based on the terms from three categories (\u201cBody structure\u201d, \u201cProcedure\u201d, \u201cPharmaceutical/biologic product\u201d) related to our NER task. We also collected the dictionaries from Sogou to supplement more commonly used medical terms to our dictionaries, including \u201cGeneric list of commonly prescribed drugs\u201d, \u201cSurgical classification code (ICD-9-CM3)\u201cand \u201cHuman anatomy\u201d [ ]. After removing duplicates and irrelevant words, finally three dictionaries (named Dic_anatomy, Dic_drug, and Dic_operation) were built. The details of each dictionary are shown in Table\u00a0 . Bi-direction maximum matching method was used to capture longest possible match, and each character in the match was encoded in \u201cB/I/O/E/S+ dictionary name\u201d tagging scheme. Furthermore, the character-level part of speech (POS) tags were generated by Jieba segmentation system [ ]. At last, the lookup table was used to output 100-dimensional additional feature embedding.\n   \nDetails of custom dictionaries \n  \n\n\n### BiLSTM-Att-CRF model \n  \nWe proposed an improved deep learning model named BiLSTM-Att-CRF which applied the attention mechanism to the basic BiLSTM-CRF model. The framework of BiLSTM-Att-CRF model is illustrated in Fig.\u00a0 .\n   \nThe structure of BiLSTM-Att-CRF model \n  \n\nAs a sequence-labeling task, the model determined a sequence of labels with the largest joint probability for the sequence of input tokens and a predefined set of labels. In this paper, we utilized the \u201cBIESO\u201d (Begin, Inter, End, Single, Other) tagging scheme to represent the position of the tokens within the entities. The first layer of model is embedding layer, by looking up the pre-trained character embedding table, the input sentence is represented as a sequence of vectors X\u2009=\u2009(X ,\u2009X ,\u2009X \u2026,\u2009X ), where n was the length of the sentence. Next, the vectors are given as the input to the BiLSTM layer. \n\nLSTM (Long Short-Term Memory) is a special form of traditional recurrent neural network (RNN). By introducing memory cell, LSTM network can effectively utilize more information and solve the gradient vanishing or exploding problems of RNN [ ]. The hidden layer of LSTM network is composed of specially constructed memory cells. Each memory cell learns the context characteristics of input text under the synergistic action of loop connection cell, input gate, output gate and forget gate. Two separate hidden states are always used to capture both past (forward) and future (backward) context information. In our BiLSTM (Bidirectional Long Short-Term Memory) layer, a forward LSTM computed a representation   the sequence from left and another backward LSTM computed a representation of   in reverse. For every word,   was represented by concatenating two distinct context representations. \n\nDifferent from the traditional BiLSTM-CRF model put the out of BiLSTM layer as the final feature directly, we added an attention layer on the top of the BiLSTM layer to capture more contextual feature in the sentence. Value   \u03b1   in the attention matrix is the attention weigh which computed by comparing the current target word representation x  with other word x  in the same sentence:\n \n\nHere, e  is usually a single-layer or multi-layer perceptron, which is called alignment function, the values will be larger with the increase of similarity. Where the weigh   W   needs to be learned in the process of training.\n \n\nThen a global vector   c   is computed as a weighted sum of each BiLSTM output   h  :\n \n\nNext, we concatenated the global vector   c   and the BiLSTM output   h   into   a vector z  to represent each word, the vector is fed to a tanh function to produce the output of attention layer.\n \n\nThen, through the Dense layer, the decision probability of s  mapping to the annotated result is expressed as.\n \n\nFinally, instead of modeling tagging decisions independently, the CRF layer is added to decode the best tag path in all possible tag paths. In the CRF layer, the current input is predicted by the past input and the state to which the input belongs, the score by moving from state i to state j is represented by the probability transfer matrix T , the element P  of the matrix is the score of the j  tag of the i  word in the sentence. The maximum likelihood estimation is used as the loss function, and the Viterbi algorithm is used to compute optimal tag sequences for inference. The calculation formula of the output state sequence Y\u2009=\u2009(y ,\u2009y ,\u2009y \u2026,\u2009y ) is as:\n \n\n\n\n## Result \n  \n### Dataset \n  \nIn this study, Two Chinese EMRs datasets released by CCKS CNER challenge were used to train our model [ ]. The distribution of entities in two datasets is shown in Table\u00a0 , we will mainly discuss the results on the CCKS 2018 dataset in this study. A total of 1000 records from CCKS 2018 are officially divided into 600 training data and 400 test data, and five categories of entity are pre-defined: (1) Anatomical Part (AP), the functional structural unit of body, such as \u201c\u8179\u90e8 (abdomen)\u201d; (2) Symptom Description (SD), patient\u2019s abnormal experience or feeling that needs to be combined with the anatomical part, such as \u201c\u4e0d\u9002 (uncomfortable)\u201d; (3) Independent Symptom (IS), patient\u2019s abnormal experience and feeling that can be independently output, such as \u201c\u5455\u5410 (emesis)\u201d; (4) Drug, a chemical substance used in the treatment of disease to enhance physical or mental well-being, such as \u201c\u963f\u83ab\u897f\u6797 (Amoxil)\u201d; (5) Operation, the medical treatment of injuries or diseases, such as \u201c\u76f4\u80a0\u764c\u6839\u6cbb\u672f (colorectal tumor surgery)\u201d. Another 400 records were derived from CCKS 2017 CNER challenge which focus on the named entity recognition of \u201cDisease\u201d, \u201cSymptom\u201d, \u201cTreatment\u201d, \u201cTest\u201d and \u201cAnatomical Part\u201d. Each entity in our datasets is annotated as {entity, start position, end position, entity type}, the example is\u00a0shown in Table\u00a0 .\n   \nDistribution of entities in two datasets \n    \nExample of the manually annotated records \n  \n\n\n### Experimental settings \n  \nOur deep learning models were implemented using open-source library Tensorflow and Keras for Python 3.6, Table\u00a0  shows the adopted hyper-parameters in our study. CRF++(0.58) tool for python was used to train our basic CRF model. We fixed the content window size at 5 and built 33 Unigram templates to extracting context character.\n   \nHyper-parameters of deep learning models \n  \n\n\n### Evaluation metrics \n  \nAccording to the evaluation metrics provided by CCKS 2018 CNER task organizer [ ], \u201cStrict\u201d metrics was defined as a correct match that the ground truth and the mention shared same mention, same boundaries (start position, end position) and same entity type. Precision (P), Recall (R), and F1-measure were used in our experiments to evaluate the recognition performance under the \u201cStrict\u201d metrics. \n\n\n### Evaluation results \n  \nIn order to verify the effectiveness of the attention-based deep learning model in the CNER task of Chinese electronic medical record, the basic CRF model and BiLSTM-CRF model which had achieved good performance in previous studies were selected as comparative experiments. In this chapter, we compared the recognition performance of the basic models on different types of entities, and analyzed the influence of additional features. \n\n#### Performance comparison of BiLSTM-Att-CRF model and basic models \n  \nBy comparing the results of three basic models shown in Table\u00a0 , our BiLSTM-Att-CRF model achieves better performance than BiLSTM-CRF model and CRF model in two datasets. Without adding other external resources and additional features, two deep learning models are more effective than the traditional CRF model. They can not only learn the similarity between input characters by the pre-trained character embedding, but also capture more context information through the units in LSTM layer. But in the CRF model, limited context information can be learned within the fixed window. Moreover, attention layer added in the BiLSTM-Att-CRF model can learn the structure of sentences directly and capture the relationships between two tokens regardless of their distance. The contribution of attention mechanism will be discussed in details in the next section.\n   \nPerformance comparison of BiLSTM-Att-CRF model and basic models \n  \nThe bold values denote the highest values \n  \n\nAs shown in Fig.\u00a0 , we analyzed entity recognition performance on five types in CCKS 2018 dataset. The BiLSTM-Att-CRF model achieves better performance on most types of entities, but it is a little worse on \u201cSymptom Description\u201d entities than CRF model. The limitation of dataset is one possible reason. There are a lot of \u201cSymptom Description\u201d entities with inconsistent labels in the training set, for example, \u201c\u4e0d\u9002 (uncomfortable)\u201d is annotated as \u201cIndependent Symptom\u201d in the context of \u201c\u8fdb\u98df\u4e0d\u9002 (eating was uncomfortable)\u201d, it is\u00a0also annotated as \u201cSymptom Description\u201d in the context of \u201c\u4e0a\u8179\u80c0\u75db\u4e0d\u9002 (abdominal was pain and uncomfortable)\u201d, but it is not annotated as any type in \u201c\u65e0\u5176\u4ed6\u4e0d\u9002\u201d(no other uncomfortable). Furthermore, the semantically related information of \u201cSymptom Description\u201d entities usually have longer distance from the entities, and sometimes they might not be in one sentence split by commas. Therefore they cannot be learned by our sentence-level attention layer, such as the sentence of \u201c\u4e3b\u8bc914\u5929\u524d\u60a3\u8005\u51fa\u73b0\u4e2d\u4e0b\u8179\u90e8\u95f7\u75db\u4e0d\u9002|\u95f4\u6b47\u6027\u75db|\u7ffb\u8eab\u5411\u53f3\u4fa7\u65f6\u75bc\u75db\u6709\u6240\u7f13\u89e3 (The patient complained of mid-lower abdominal pain and discomfort 14 days ago| intermittent pain| when turning over to the right, the pain was relieved)\u201d . The attention-based model could not capture the key information \u201c\u4e2d\u4e0b\u8179\u90e8 (mid-lower abdominal)\u201d, so \u201c\u95f4\u6b47\u6027\u75db (intermittent pain)\u201d and \u201c\u75bc\u75db (pain)\u201d were incorrectly recognized as \u201cIndependent Symptom\u201d . The recognition performance on \u201cDrug\u201d entities was also not as expected without adding the dictionary feature. Except for the small size of the training samples, we speculated that the large number of obscure words and abbreviations in \u201cDrug\u201d entities may interfere with the recognition effect, for example, \u201c\u8c03\u6574\u65b9\u6848TXT+DDP*3(Adjustment scheme TXT+DDP*3)\u201d. Moreover, there are many unprecedented \u201cDrug\u201d entities in the test set, so the ability of BiLSTM-Att-CRF model to discover and recognize new words needs to be improved by combining large medical knowledge base.\n   \nRecognition performance of five entity types in CCKS 2018 dataset \n  \n\n\n#### The effect of additional features \n  \nWe also analyzed the effect of two additional features (POS, dictionary) on the performance of our models. As shown in Table\u00a0 , the BiLSTM-Att-CRF model achieves the best performance by adding additional features (F1-measure of 86.11% in CCKS 2018 and 90.48% in CCKS 2018). The P and R value of CRF model are greatly improved by adding additional features, it proves that the traditional linguistic and domain features can help to improve the performance of the statistical learning model. Particularly, dictionary feature contributed more than POS. However, the two neural network models have limited improvement after adding additional features, and the results are even worse than baseline models when add POS only. Although the custom dictionary was used in POS tagging process, there were still many boundary errors of nested clinical terms, such as \u201c\u884c/v\u5e7f/a\u6cdb/a\u6027/a\u5b50/n\u5bab/n\u5207/v\u9664/v +/x\u53cc/m\u4fa7/v\u8f93/n\u5375/n\u7ba1/n\u5375/n\u5de2/n\u5207/v\u9664/v\u672f/n\u201d. In the previous study, Cai et al. proposed a Reduced-POS tagging method to improve the accuracy of Chinese entity boundary detection [ ]. In the future, more effective features pre-trained by deep learning model should be chosen to enhance the recognition performance of the neural network model. \n\n\n\n\n## Discussion \n  \n### Performance of BiLSTM-Att-CRF model in improving Recall \n  \nHigher Recall means that the model can memorize more details related to entities and classify more unrecognized or misjudged entities into correct entity types. Table\u00a0  shows the Recall among three baseline models without additional features in CCKS 2018 dataset, we can see that Recall is improved significantly in our BiLSTM-Att-CRF model, especially in types of \u201cOperation\u201d and \u201cDrug\u201d entities which always difficult to recognize. Compared to other types of entities, \u201cOperation\u201d entities usually have long length and contain nested structures, which easily cause boundary errors of recognition. But there is also a good point, some fixed keywords are always used around \u201cOperation\u201d entities, e.g. \u201c\u884c (undergo)\u201d, \u201c\u672f (surgery)\u201d. As the example of recognition performance shown in Table\u00a0 , all three models can recognize the entity when the keyword is closed to it in the context, but CRF model can\u2019t capture context information of a little bit long-distance. Better than the CRF model, LSTM can solve hard long-time lag problems with the gating mechanism, but later words are more dominant than earlier words, which leads to recognition difficulty on long sentences. Therefore, the attention mechanism performs better when the keyword is long away or the length of entity is too long.\n   \nRecall of Bilstm-Att-CRF model and other basic models \n  \nThe bold values denote the highest values \n    \nExample of recognition performance of Bilstm-Att-CRF model and other basic models \n  \nThe * denote the correct recognition.   OP   Operation,   AP   Anatomical Part \n  \n\n\n### Performance of different attention widths \n  \nNot all problems need long-term or globally dependent attention mechanism, many problems only rely on local features [ ]. As introducted in Background, the semantic logic of clinical text is relatively concentrated and related semantic information are basically concentrated in one short sentence. In order to explore the influence of different attention widths on named entitity recognition of Chinese NERs, we added different attention widths in the training process based on the Bilstm-Att-CRF model. As shown in Fig.\u00a0 , the attention width   r   means that the current word is only associated with the   r   words before and after it, so attention only be computed between the (2  r  \u2009+\u20091) words, \u201clong\u201d means the input sentence of model was split only by period (\u201c\u3002\u201d, length: Ave\u2009=48, Max\u2009=\u2009455), and \u201cshort\u201d means the input sentence was split by comma and period (\u201c,\u201d & \u201c\u3002\u201d, length: Ave\u2009=\u200915, Max\u2009=\u2009176). The results show that F1-measure improves with the increase of attention width regadless of whether long or short input, but when attention width increases to a large value, the F1-measure improves slowly. And the F1-measure of short input performe better than that of long input. So, we found that the meaning of each short sentence in the Chinese clinical text is relatively independent, and the key information does not need to be learned from too long distance, local feature learning with proper attention width could achieve good performance in the clinical entity recognition of Chinese EMRs.\n   \nRecognition performance of different attention widths \n  \n\n\n\n## Conclusion \n  \nIn this paper, we integrated the attention mechanism into neural network, and proposed an improved clinical named entity recognition method for Chinese electronic medical records called BiLSTM-Att-CRF. Recognition Performance showed that this model could capture more useful information of context and avoided the problem of missing information caused by long-distance factors, especially performed in Recall improvement. In the CNER task of CCKS 2018 and CCKS 2017, our BiLSTM-Att-CRF model finally achieved better performance than other widely-used models without additional features(F1-measure of 85.4% in CCKS 2018, F1-measure of 90.29% in CCKS 2017), and achieved the best performance with POS and dictionary features (F1-measure of 86.11% in CCKS 2018, F1-measure of 90.48% in CCKS 2017), which also verified the validity of additional features. Moreover, the influence of different attention widths in BiLSTM-Att-CRF model was also discussed, which indicated that choosing appropriate attention width to focus on local feature learning can achieve good performance. Overall, this paper preliminarily confirmed the effectiveness of attention mechanism in the field of clinical named entity recognition, which proved some useful ideas for future research in this field. \n\nHowever, there are still some shortcomings in our study. The BiLSTM-Att-CRF model has insufficient ability to recognize new words, and the selection and generation of pre-trained feature embedding should be more careful. In order to achieve better recognition performance, our future research will explore the deeper application of attention mechanism in neural network, such as the \u201cBERT\u201d model proposed by Google AI Language team [ ]. \n\n \n", "metadata": {"pmcid": 6894110, "text_md5": "c9b481a182c048fc3e7d71e89c457318", "field_positions": {"authors": [0, 86], "journal": [87, 111], "publication_year": [113, 117], "title": [128, 242], "keywords": [256, 336], "abstract": [349, 2368], "body": [2377, 26854]}, "batch": 1, "pmid": 31801540, "doi": "10.1186/s12911-019-0933-6", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6894110", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6894110"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6894110\">6894110</a>", "list_title": "PMC6894110  An attention-based deep learning model for clinical named entity recognition of Chinese electronic medical records"}
{"text": "Venugopal, Vineeth and Sahoo, Sourav and Zaki, Mohd and Agarwal, Manish and Gosvami, Nitya Nand and Krishnan, N. M. Anoop\nPatterns (N Y), 2021\n\n# Title\n\nLooking through glass: Knowledge discovery from materials science literature using natural language processing\n\n# Keywords\n\nnatural language processing\nartificial intelligence\nglass science\nmaterials science\nknowledge discovery\n\n\n# Abstract\n  Summary  \nMost of the knowledge in materials science literature is in the form of unstructured data such as text and images. Here, we present a framework employing natural language processing, which automates text and image comprehension and precision knowledge extraction from inorganic glasses\u2019 literature. The abstracts are automatically categorized using latent Dirichlet allocation (LDA) to classify and search semantically linked publications. Similarly, a comprehensive summary of images and plots is presented using the caption cluster plot (CCP), providing direct access to images buried in the papers. Finally, we combine the LDA and CCP with chemical elements to present an elemental map, a topical and image-wise distribution of elements occurring in the literature. Overall, the framework presented here can be a generic and powerful tool to extract and disseminate material-specific information on composition\u2013structure\u2013processing\u2013property dataspaces, allowing insights into fundamental problems relevant to the materials science community and accelerated materials discovery. \n   Highlights  \n  \nNatural language processing is used for information extraction from research papers \n  \nCaption cluster plots are used for exploring figure captions across the entire corpus \n  \nElemental maps are used to identify the chemical elements reported in a study \n  \nA framework to extract domain-specific queries from the literature \n  \n   The bigger picture  \nMost knowledge generated through scientific enquiry in materials domain is presented in the form of unstructured data. Among the available sources such as online websites, digital data, and publications, peer-reviewed journals serve as the undisputed source of reliable information regarding materials synthesis, characterization, and properties. Despite the availability of large data, only a limited fraction is compiled in the form of machine-readable databases, most of which are manually curated. Here, applying natural language processing on a large corpus of journal publications on inorganic glasses, we present a framework of information extraction from text and images, which answers queries related to synthesis and characterization techniques, and even chemical elements used. The scalable approach presented here can be applied to other domains for efficient information retrieval from scientific literature. \n  \nThis work demonstrates the use of natural language processing tools to extract information from a large corpus of scientific literature in the field of inorganic glasses. The scalable approach presented here combines information from the text and images to answer domain-specific queries, thereby enabling accelerated materials discovery. \n \n\n# Body\n \n## Introduction \n  \nThe overwhelmingly large amount of knowledge generated through scientific enquiry is mostly stored as unstructured data in the form of texts and images. These range from expository archives, such as books, journals, and dissertations, to condensed representations, such as handbooks and manuals. Materials science, being a highly interdisciplinary area, commands a large repository of scientific publications. However, only a limited fraction of this knowledge is collected and curated in the form of structured data, for example, a database of composition\u2013structure\u2013property relationships. The information on material science is increasingly siloed and simply too large for the efficient utilization of any one individual or group. Just as in all other branches of science, the materials community is afflicted by the curse of knowledge incommensurate with the available information.  Thus, the accessibility to the vast majority of knowledge in the literature is limited, as it (1) is time-consuming to manually read and analyze texts and images, and (2) requires a domain expert to understand, interpret, and summarize the information. \n\nRecent advancements in natural language processing (NLP) provide a promising solution to this problem through the automation of text comprehension, querying, and knowledge extraction from scientific texts. NLP has been applied extensively to scientific literature specifically in biological sciences for more than 2 decades. ,  ,   A biomedical-specific language model, namely, BioBERT,  which is used extensively for biomedical text mining, stands as a testimony to the advances and contributions of NLP in biological sciences. In contrast, the applications of NLP to materials science remain sparse. ,  ,   Similar to biological sciences, the study of materials present some unique challenges to the direct application of NLP to mine text data due to the domain-specific jargons and lack of uniform conventions in scientific writing.  Despite these challenges, recent studies have shown that NLP can indeed be used to address some open challenges in materials science such as novel materials discovery,  unraveling synthesis pathways,  and extracting composition\u2013property databases. \n\nCole et\u00a0al ,  ,   have demonstrated the automated generation of databases for magnetic  and battery materials  using ChemDataExtractor,  which has also been used in predicting phase diagrams.  Olivetti et\u00a0al  have used NLP together with artificial neural networks to predict synthesis parameters of inorganic oxides  and in extracting the properties of zeolites  and cementitious materials.  Jain et\u00a0al  have demonstrated the use of word vectors in converting semantic queries to vector algebra and extended the method to the prediction of thermoelectrics. Ceder et\u00a0al  have shown the extraction of automated synthesis recipes of inorganic oxides through a semi-supervised approach. Recently, Matscholar  has been introduced as a comprehensive material science search and discovery engine that is able to automatically identify materials, properties, characterization methods, phase descriptors, synthesis methods, and applications from a given text through a custom-built named entity recognition (NER) system. These developments suggest that artificial intelligence approaches using NLP can be a promising route to condense and represent knowledge in materials science leading to novel materials discovery and development. \n\nVery few studies have, however, focused on extracting information related to images and plots in literature.  The adage, \u201ca picture is worth a thousand words,\u201d is even more relevant to scientific literature, as images hold the most crucial information related to scientific hypothesis and theories.  Till date, there has been no framework that allows direct search or compilation of images presented in scientific literature. Further, the images of a manuscript should be read in conjunction with the text to understand the context. While many of the applications of NLP in material science have focused on extraction and processing of textual information, no effort has been made thus far to connect this textual information with the images and plots to allow knowledge dissemination in a holistic manner. \n\nHere, we demonstrate a comprehensive NLP framework that extracts information from a large corpus of text and images to provide highly specific, nuanced, and automated exploration of materials science literature. Specifically, we analyze the texts and images from approximately 100,000 research articles in the area of glasses, an archetypical disordered material. Glasses are one of the most common and widely used among engineering materials with uses spanning architectural, functional, and biomedical applications.  Recently, machine learning (ML) approaches have been used to develop predictive models for optical, electronic, and mechanical properties of glasses. ,  ,  ,  ,  ,  ,  ,  ,  ,  ,   Several recent works have shared composition\u2013property databases along with the trained ML models.  For instance, the software package, Python for Glass Genomics (PyGGi), has a large composition\u2013property database, ML models for predicting nine key properties and an optimization framework for targeted glass discovery.  These models, however, have relied on existing databases for their training and analysis,  and hence have been restricted to parameter predictions through regression models. It is well known that the properties of glasses, a nonequilibrium state, are not just a function of composition, but are also fundamentally influenced by the processing history and testing conditions. ,  ,  \n\nThrough a combination of NLP algorithms, chemical entity extraction protocols, and visualization tools, we show that the answers to very specific questions on glass literature can be answered. These include material/property specific questions as well as broader community issues, such as the following:   \nWhat are the common microstructural characterizations for glasses? \n  \nAre more of the papers published in glass science theoretical as opposed to experimental? \n  \nWhere is americium used in glasses? \n  \nWhat chemical elements have been used in LEDs? \n  \nAre there photoluminescence studies of bioactive glasses that contain Fluorine? \n  \nCan we find papers on optical glasses that have been manufactured using solid state synthesis? \n  \n\nOverall, the generic framework developed here allow highly specific exploration of scientific literature using the abstracts, text, and image captions of publications. \n\n\n## Results \n  \n### Topic modeling \n  \nTo demonstrate the proposed approach, we downloaded more than 600,000 research articles, full texts, and images related to the keyword \u201coxide glasses\u201d and \u201cmaterials science\u201d using the CrossRef metadata query API  and the Elsevier Science Direct API.  Following this, supervised learning was performed on the abstracts of the manuscripts to filter them (see   for details). Abstracts are the most information-dense organ of a scientific paper, containing information on the material under study, property being explored, and characterization/synthesis methods being used in service of the investigation. As such, they are unlikely to contain spurious information or refer to materials or properties not mentioned in the text. This specificity makes an abstract the most useful part of a text, and it is therefore not surprising that many NLP studies on materials have only taken paper abstracts as the input.  Based on the supervised learning, approximately 100,000 research articles were classified as relevant to the topic \u201cglass,\u201d with precision, accuracy, and recall of 92%, 86%, and 67%, respectively, on the test set. Note that the model with the highest recall was selected to include as many glass-related articles as possible. Although not an exhaustive list, the total number of articles downloaded are in the same range as the number of texts identified in other comprehensive literature surveys on glass. \n\nAn unsupervised NLP algorithm called the latent Dirichlet\u00a0allocation  (LDA) was used to automatically classify the corpus into 15 \u201ctopics,\u201d where each topic is defined by the set of words that have the highest probability of occurrence within the topic. LDA allows a rapid and efficient organization of the text corpus with minimal human supervision\u2014a capability provided by no other automated tool available today. The categories generated by LDA are visualized in the LDA plot in  A. Each abstract in the corpus is vectorized using Term Frequency\u2013Inverse Document Frequency  (TFIDF), which maps each document in the corpus to a unique vector in a higher dimensional space. T-distributed stochastic neighbor embedding (t-SNE) projects these vectors to a 2-dimensional (2D) plane such that vectors with the highest cosine similarity group together. The color of a pixel is determined by the topic number assigned to it by LDA.   \nLDA plot of the abstracts \n\n(A) The LDA plot presents the clusters of vectorized abstracts colored based on topics identified by LDA. \n\n(B) The number of abstracts on each topic as identified by LDA. \n\n(C) The descriptive label assigned to LDA topics by a human expert. \n  \n\nIt is seen immediately from  A that the points with similar color are grouped together. This suggests that the TFIDF vectorization followed by t-SNE clustering is able to group the abstracts with similar topics, as identified by LDA. The LDA plot is a graphical representation of the entire field of glass literature and succinctly summarizes the details mentioned earlier. Note that the descriptive label is assigned to these topics by a human expert that maps the automatically generated lexical probability distribution to established categories in glass literature. For example, the words with the highest probability of occurrence in Topic 11 are \u201cer,\u201d \u201cyb,\u201d \u201cemission,\u201d \u201cdoped,\u201d \u201cluminescence,\u201d \u201cnd,\u201d and \u201ctm.\u201d Analysis of these high-frequency words by human experts suggests that the topic is related to the luminescence of glasses doped with rare earth ions, and hence it is labeled as \u201cRare Earth glasses.\u201d The distribution of abstracts into the identified topics is shown schematically in the histogram in  B. The descriptive labels for all the other topics are similarly identified and listed in  C. \n\nA mere visual inspection confirms that Topic 5\u2013thin films\u2013is the single largest group followed by topics 13 and 6. The most common set of articles related to \u201cThin films\u201d includes both the luminescence properties of oxides such as ZnO on glass substrates, as well as the studies of transparent glasses on a thin film geometry. This is followed by \u201cglass ceramics,\u201d \u201cmethodological studies\u201d of glasses (including both theoretical and modeling studies), and \u201cnonlinear optical properties of glasses.\u201d In general, the list is found to be comprehensive covering all facets of glass literature in terms of applications, properties, and ingredients including bioactive glasses, dielectric glasses, nanomaterials, and chemical and electromagnetic properties. Other categories, such as mechanical and failure studies of glasses, are seen to be subsumed within these broad topics. Note that a more detailed classification can be further performed to obtain the subtopics by performing LDA recursively on each of the topics separately (see   and  ). \n\n\n### Caption cluster plots \n  \nThe corpus also contains a collection of 106,238 figures and their captions. It is well known that the information content of scientific articles is expressed mostly through graphics. These images and their corresponding captions, therefore, provide a technical summary of the documents that they belong to. The caption cluster plot  (CCP) shown in  A is a graphical representation of the information contained in all the captions, grouped by their semantic similarity using NLP. The captions are tokenized and vectorized using TFIDF and t-SNE, as explained earlier for the LDA plot. The pixels are colored based on categorical keywords identified by a human expert as explained in the methods section. Finally, these labels are positioned at the median (x,y) positions of the corresponding pixels, with the size of the label proportional to the number of images in that category.   \nCCP \n\n(A) The CCP presents the clusters of vectorized captions colored by preselected keywords. \n\n(B) The labels with the highest number of counts in the caption database. \n\n(C) A grayscale image of the plot showing the four ontological axes. \n  \n\nB shows the distribution of captions with respect to the topics for each image type identified in the CCP. We observe that \u201cAnneal,\u201d \u201cX-ray Diffraction\u201d (XRD), \u201cCrystal,\u201d \u201cEmission,\u201d and \u201cCrystallization temp\u201d (Tc) are the most common types of images in glass literature, underlining the importance of thermal synthesis routines, microstructure, and optical characterization methods in glass science. Captions representing similar types of images are found to cluster next to each other. For example, microstructure measurements such as scanning electron microscopy (SEM), XRD, atomic force microscopy (AFM), energy dispersive X-ray spectroscopy (EDX), and transmission electron microscopy (TEM) all cluster at the lower half of the CCP while optical properties, such as emission, absorption, fluorescence, and luminescence, are found at the very top. The use of TFIDF as the vectorization algorithm ensures that labels, and hence the captions representing images, are found to group together organically, based on their semantic similarity. The CCP of   immediately informs us the answer to the first question raised in the introduction: \u201cwhat are the most common characterization methods for glasses?\u201d The answer is seen to be XRD and SEM. Also, most of the larger tags in the image are experimental methods, as opposed to modeling techniques like finite element methods or molecular dynamics that are too small to be seen unaided. This in turn, informs the second question: most of the papers in glass literature are clearly experimental. \n\nInterestingly, the CCP is found to have four distinct axes that capture complementary information on glass literature. These are the Optical, Mechanical, Microstructural and Thermodynamic axes, as shown in  C. Optical axis are represented by images related to \u201cemission,\u201d \u201cluminescence,\u201d and \u201cfluorescence,\u201d to name a few. Images that study mechanical properties, such as \u201ccrack,\u201d \u201cstress-strain,\u201d \u201cstrength,\u201d and \u201ccompressibility\u201d are found closest to the Mechanical axis, while thermodynamic properties, such as glass transition temperature, activation energy, specific heat, differential scanning calorimetry, and differential thermal analysis (DTA), to name a few, lie along the Thermodynamic axis. Finally, the images related to \u201cXRD,\u201d \u201cEDX,\u201d \u201canneal,\u201d and \u201cCrystal\u201d fall along the Microstructural axis. This observation can be formalized by computing the Euclidean distance between caption labels and the four axes. The thermodynamic properties are closest to the Thermodynamic axis, while the optical properties are most proximate to Optical axis, etc. This observation is a direct result of using the t-SNE algorithm based on the cosine similarity of caption vectors, which ensures that the Euclidean distance between pixels is a measure of the semantic similarity of the underlying captions. \n\nCertain captions do not belong to any of the axes but are found to be equally spaced from two or more of them. For example, \u201cFracture\u201d and \u201cInterface\u201d are located at nearly equal distances from the Mechanical axis and Microstructural axis. These terms represent microstructural features that are critical determiners of mechanical properties. The captions likely contain terms that relate equally to both axes categories, justifying their position in the CCP. The position of cluster labels is an indicator of their relative co-occurrence frequency, thereby showing that \u201canneal\u201d is strongly related to \u201cCrystal\u201d (and variations thereof including \u201ccrystalline\u201d and \u201ccrystal structure\u201d), while \u201cPLE\u201d is more related to \u201cabsorption\u201d than to \u201cemission.\u201d At the same time, the proximity of \u201cbioactive\u201d to the Mechanical axis suggests that many experiments on bioactive materials pertain to the measurement of strength and hardness thereby bringing the captions into semantic convergence. Overall, we observe that the CCP provides invaluable insights into the contents of these figures, which when combined with a predefined ontology solve many of the problems stated earlier. In particular, the analysis of captions through CCP provides a visual tool that rapidly summarizes the entire field of glass literature, allowing a user to quickly comprehend the trends, themes, and common characterization methods in the community. \n\n\n### Elemental maps \n  \nNext, the Python library, ChemDataExtractor, was used to automatically extract chemical species\u2014including names of compounds, chemical formulae, and symbols\u2014from the abstracts. The chemicals that occur with the highest frequency in the database are shown in  . The chemical names and symbols were standardized following which chemical elements present in each compound were separately identified. This creates a binary marker for each element such that if the element is mentioned in the abstract, the marker assumes the value 1 and 0 otherwise. The LDA plot is redrawn such that only if the abstract contains the marker for an element is the corresponding pixel colored. Similarly, if a caption is drawn from a text wherein the element is contained in the abstract, the pixel is marked with a color in the CCP. The results are the elemental maps given in  , where the images at the top are elements mapped to the CCP and the images at the bottom are maps of LDA. The elemental maps provide a direct visual representation of the distribution of elements in glass literature. The juxtaposition of these maps with the caption plot and the LDA provides a highly specific graphic tool to analyze the intersection of selected chemistries with a topic in glass science or a specific property/characterization technique.   \nElemental maps \n\nThe elemental maps of (A) Si, (C) Ca and Er, and (E) In and Am superimposed on the CCP showing the presence of the respective elements in the abstracts of the manuscripts from which the images are taken. The elemental maps of (B) Si, (D) Ca and Er, and (F) In and Am superimposed on the LDA plot showing the abstracts where the elements have been identified to be present. \n  \n\nAs a validation of this concept, it is seen easily from  A and 3B that silicon is abundantly distributed among all topics, properties, and characterization methods in glass literature. A similar result can be seen for oxygen (see   and   of  ). This is hardly a surprise, as silicate glasses are one of the most common family of inorganic glasses. The map for calcium and erbium in  C and 3D is more illustrative. Calcium is found to be less uniformly distributed than silicon, with high concentration in the clusters identified as bioactive by LDA. This is a representation of the fact that calcium is one of the major constituents of bioactive glasses. The overlapping data for erbium in  C and 3D show that this element is mostly present in the LDA topic \u201crare earth glasses\u201d and in the caption clusters \u201cemission,\u201d \u201cPLE,\u201d \u201cband structure,\u201d and \u201cenergy diagrams.\u201d The elemental map, therefore, provides a visual diagram of the presence of an element in glass literature. Rare earth elements such as erbium, dysprosium, and ytterbium are used largely for LEDs and laser applications, which is confirmed by the region of the LDA plot that is highlighted by these elements (\u201cRare earth glasses\u201d and \u201cLEDs and Phosphors\u201d), as well as by the image categories where they predominate. \n\nThe third question in the introduction \u201cWhere is americium used in glasses\u201d is answered in  E and 3F. Only 25 articles were found with americium mentioned in the abstract. They are seen to fall over regions identified as \u201cglass ceramic\u201d and \u201cglass irradiation studies\u201d from the LDA plot. Upon inspection, it is indeed found that most of these abstracts relate to studies of nuclear exposure and radiation on glass ceramics, demonstrating a practical use of the concepts developed so far in answering a question that is otherwise not reachable through any other approach today. Similarly, indium is found to be distributed in  E and 3F mostly in the section identified as \u201cThin films\u201d relating to the large body of work that has been carried out on the transparent conducting indium tin oxide electrodes on glass substrates and glass ceramics. The elemental maps for all the 120 known elements overlapping with the LDA and CCPs are presented in section 2 of   (see  ). \n\n\n\n## Discussion \n  \nThe LDA and CCPs provide a highly specific, detailed, and succinct graphical summary of the available corpus of glass literature. The knowledge extraction pipeline for obtaining these plots has been shown in   (see  ). They provide answers to some of the questions raised in the introduction. For example, the CCP visually conveys the fact that the most studied aspects of glasses in literature relate to their annealing behavior and microstructure, and that studies on optical emission are slightly more common than that of absorption. The database generated through the CCP provides a useful source of specialized images\u2014say SEM microstructural images or AFM images of glasses\u2014which can then be used for learning images through artificial intelligence and ML algorithms such as convolutional neural networks. These can be used to answer some of the pressing problems in the field today such as identifying the causes of fracture or dissolution by linking to other structure\u2014processing parameters. \n\nSimilarly, the LDA plot provides insights on the broad themes within glass ontology into which the text is divided. It is seen immediately through visual inspection that the amount of work done on bioactive glasses is less than that on glass ceramics or that irradiation studies on glasses occupy the bottom of glass hierarchy in terms of the sheer number of publications. The LDA plot is a topological map of glass literature, where each text is assigned a unique position next to other works of similar nature, content, and theme. This provides a way to efficiently search for publications that are similar to a given paper\u2014once the position of the publication in the vector space is determined, the nearest neighbors are by default the ones that are the most semantically similar. There are currently no other tools, even in established scientific databases and search engines, that allow the detailed exploration, analysis, and easy visual analysis offered by the CCPs and the LDA plots. At the same time, combining the caption cluster, LDA, and elemental plots results in the creation of a tool that can query, explore, and analyze the information content in glass literature with unprecedented detail and specificity. \n\nAn example toward such an attempt for knowledge extraction and dissemination combining CCP, LDA, and elemental map is presented in  . The LDA plot identifies abstracts in the database that belong to the category of \u201cbioactive glasses.\u201d The elemental maps allow the selection of only those abstracts among these that have been marked with the presence of fluorine and chlorine. This is shown in  B where the red pixels corresponding to the F and Cl containing abstracts are found to overlap with the green pixels corresponding to abstracts on bioactive glasses. The region of the greatest overlap is seen to be the three islands on the LDA plot that are marked as bioactive glasses, thereby confirming that among all fields of glass science, F and Cl most commonly find their application within this topic. This in itself is a remarkable capability\u2014the identification of only those scientific publications subject to the dual constraints of topical category and chemistry. It bears repeating that there is no other method currently available that can do this.   \nKnowledge extraction combining CCP, LDA, and elemental maps \n\n(A) The bioactive glass cluster is identified from the LDA plot. \n\n(B) The abstracts that contain F and Cl are marked on the plot in red. The area of overlap are abstracts on bioactive glasses that contain F and Cl. \n\n(C) The SEM, EDX, XRD, and PLE images from journal articles belonging only to this parameter space are marked by colored pixels in the CCP. Inset images (i\u2013viii) are arbitrarily selected examples of these images as identified by the plot. ,  ,  ,  ,  \n  \n\nAdding the CCP to this information allows even deeper exploration of literature by extracting a processing, characterization, or property image from this parameter space. For example,  C shows the SEM, EDX, XRD, and PLE captions of figures from abstracts that contain F and Cl on the topic of bioactive glasses. Arbitrarily selected images from this parameter space are displayed for reference in the inset of  C (i\u2013viii). The captions of these images confirm that the figures do correspond to the selected image type, while their abstracts span a broad topic range include appetites, glass microparticles, mesospheres, and bioactive scaffolds\u2014all of which relate broadly to the subset of bioactive glasses under consideration. This, in turn, answers the fifth question: yes indeed, there are photoluminescence studies of bioactive glasses that contain fluorine. Thus, this method allows the rapid exploration of scientific data to access extremely nuanced and specific information sets. Such a method might be very useful for a researcher who wishes to access the microstructure of chloride or fluoride glasses without conducting an extensive literature survey\u2014the only alternative available today. Elemental tagging adds a chemical marker to images for computer vision tasks, as in the training of predictive algorithms that link microstructure or property to composition. \n\nThe methods of systematic scientific exploration of literature that have been developed so far can be generalized by querying the abstract database for arbitrary search terms.   presents an example where the application strings \u201coptical glass\u201d and \u201cLED,\u201d as well as the synthesis string \u201csolid state\u201d has been used to identify all the abstracts in the document space that contain the respective strings. This allows us to categorize the abstracts based on specific applications, synthesis methods, characterizations, or properties. The  A shows the LDA plot. In  B, all the abstracts relating to optical glasses are identified from corpus and are visually represented as green pixels in the LDA plot, where they are seen to overlap with the topics identified as \u201cLEDs and Phosphors,\u201d \u201cspectroscopic studies,\u201d and \u201crare earth glasses.\u201d The solution for the final question in the introduction is seen in  C, which shows all the abstracts that contain the search string \u201csolid state synthesis,\u201d which is a common method for fabricating oxides and oxide glasses. The overlap of the two sets of complementary information are the set of abstracts on optical glasses synthesized through solid state synthesis. This analysis can be carried further by combining these data with the CCP, through which specific image types such as the DTA or XRD of optical glasses made through solid state synthesis can be extracted. While the level of specificity offered by this approach is in itself useful to a researcher who wishes to learn more about the solid state synthesis of optical glasses, the method allows the use of any number of search strings\u2014allowing for a detailed multidimensional extraction of information from literature.   \nKnowledge extraction framework \n\n(A) The LDA plot. \n\n(B) Abstracts with \u201coptical glasses\u201d in the text. \n\n(C) Abstracts with \u201csolid state synthesis\u201d in the text overlapping with optical glasses. \n\n(D) Abstracts with \u201cLED\u201d in the text. \n\n(E) Abstracts with rare earths. \n    \nKnowledge extraction pipeline summary \n\nFirst, we search the CrossRef database for a search query, followed by downloading the papers from Elsevier Science Direct database. To obtain relevant research papers, a supervised classification algorithm is used. After obtaining the articles, we do topic modeling and make CCPs followed by elemental maps. \n  \n\nSimilarly, the green points in  D represent abstracts with the string \u201cLED\u201d in it. By mapping these data to the elemental maps, the presence of abstracts with specific chemical compositions can be marked, such as in  E, which highlights the elements europium, cerium, and samarium. These are therefore LEDs that contain any of these elements or any combination of them; this, in turn, answers the fourth question raised in the introduction. This data allow the user to read through these abstracts, and only these abstracts, to learn more about the subject. The user is then able to look through microstructural, luminescence, or thermodynamic data that are linked to this dataset through the CCP. \n\nAltogether, we demonstrate that the application of NLP to glass literature enables the curation and selection of data sources sorted by specific applications, properties, characterization methods, and chemistries. In turn, this allows for custom composition\u2013processing\u2013property databases to be compiled automatically, and in linking together parts of the information space in a way that has so far not been possible. \n\nWhen used together, text vectorization, LDA, and elemental maps solve many of the challenging questions impeding the accelerated discovery of glasses, some of which are stated in the introduction. We demonstrate this in the paper through six questions that were raised in the introduction and subsequently solved in the results and discussion sections. NLP tools combined with curated glass databases accurately show that most of the published work explored in the analysis is experimental rather than theoretical and that XRD, SEM, etc. are the most common characterization methods for glasses. They also provide solutions to highly specific queries, such as identifying papers on optical glasses that are manufactured using solid state sintering, glass compositions that are both bioactive and fluorine-containing, chemicals used in LED glasses, etc. Many of these questions currently have no other means of being answered, as demonstrated by identifying all the glasses that contain americium. \n\nThe techniques and tools developed in this article can be easily extended to other topics. We illustrate this with two topical examples: metallic glasses and magnesium alloys (see   and  ). The relevant literature for each topic is identified and the NLP pipeline is applied to extract the CCP, LDA plots, and elemental maps for both topics. The CCP for magnesium alloys show that \u201cdissolution\u201d is a major topic of study in the field, underscoring the interest of the scientific community in assessing the chemical stability of these alloys. Similarly, the elemental maps for metallic glasses provide a quantities measure of the importance of various metals in glass forming. By comparing the maps of copper and iron, for example, we are able to see that they are used together in many applications, while iron is preferable over copper for other applications. \n\nHowever, the major contribution of the present study is that it opens a way to ask deeper and broader questions of topical literature, that can ultimately enrich the field by solving outstanding problems\u2013old and new. Thus, our study establishes the baseline for extracting images from scientific literature related to specific keywords and topics. Further, we have developed a web-based platform making the knowledge accessible to wider community through PyGGi Pictionary as part of the Python for Glass Genomics Package (PyGGi, see:  ). \n\nAt this point, it is worth mentioning some of the limitations in the present approach.   \nClassification of abstracts as glass and nonglass has a relatively low recall. This leads to the missing of several abstracts belonging to the topic in the final classified set. \n  \nThe number of topics that can be identified using LDA is quite small at present. Further, LDA being an unsupervised approach requires an expert to understand the topic from the keywords and manually label them. \n  \nThe keyword list associated with the CCP is developed by domain experts. \n  \n\nThese problems can be tackled by algorithms that could possibly identify a larger number of categories and automatically label them. In particular, the development of a glass-specific ontology, a curated image repository, and an NER tool tailored for this community can go a long way in enhancing the methods demonstrated for the first time in this paper. Further, by combining these tools with a natural language model, such as a \u201cGlassBert,\u201d can open the field to many more advances in artificial intelligence and ML by making available rapid, efficient, and tailored composition\u2013processing\u2013property databases. The scalable methods demonstrated in this study are broadly applicable to any scientific field and consequently is of universal relevance in accelerating the scientific enquiry. \n\n\n## Experimental procedures \n  \n### Resource availability \n  \n#### Lead contact \n  \nFurther information and requests related to this study should be directed to and will be fulfilled by the lead contact, N. M. Anoop Krishnan ( ). \n\n\n#### Materials availability \n  \nThis study did not generate any new unique materials. \n\n\n#### Data and code availability \n  \nAll the data and codes used in the present work are available at:  . \n\n\n\n### Methods \n  \nThe workflow for information extraction is summarized in  . The CrossRef metadata API was used to query existing literature databases using keywords specific to the glass community. These include (1) descriptive application identifiers, such as \u201cchalcogenide glasses,\u201d \u201cbioactive glasses,\u201d \u201claser glasses,\u201d \u201coptical glasses,\u201d etc.; (2) property/processing terms, such as \u201cglass transition temperature,\u201d \u201coxide glasses,\u201d \u201coptical luminescence,\u201d etc.; and (3) conjugated keywords such as \u201cglass mechanical properties,\u201d \u201cglass-dissolution,\u201d \u201cglass AND fracture,\u201d etc. This query returned an initial list of more than 6 million DOIs out of which the full texts of 600,000 articles were downloaded using the Elsevier Science Direct API. A custom XML parser was written to extract specific sections of the article including the metadata, abstract, images, image captions, and individual sections identified by their headings. \n\nTo understand the distribution of topics in the downloaded database, an NLP algorithm, the LDA, was used to identify the number of distinct \u201ctopics\u201d in the corpus where a topic is defined as the set of words with the highest probability of occurrence in a document belonging to the topic. The LDA plot for a given sub-topic is presented in the   (see   and  ). It is seen that while some topics are relevant to the scientific literature on glass, many are from the intersection of glasses with peripheral topics such as women's health, economy, and environment. In fact, one topic only includes non-English articles identified by the French vowels \u201cun,\u201d \u201ces,\u201d \u201cle,\u201d etc. Note that the present work focuses only on the articles presented in the English language. However, a similar approach can be extended to other language literature as well. The full texts of articles belonging to topics of little relevance to the materials science literature on glasses were removed, along with editorial notes, commentaries, book reviews, retractions, and conference proceedings. The use of LDA is thus demonstrated to greatly aid the curation of topic-specific text databases, a nontrivial effort by any other means. \n\nThe remaining database was further refined by the use of a ML classifier model that performed a binary classification of the article abstracts into \u201crelevant\u201d and \u201cnonrelevant.\u201d For this, 3,060 randomly selected abstracts were manually tagged as being \u201cglass\u201d and \u201cnot-glass.\u201d Using the Python SciKit library, a logistic regression classifier model was found to have an accuracy of 86% and a recall of 67% on the test set for classification task. This model was found to outperform other text classification algorithms such as naive Bayes and Random Forests. The details of the model performance are given in the   ( ). Finally, the binary classifier categorized 94,207 articles as being relevant to the material science study of glasses. This list contained articles published from 1 August 1997 to 10 June 2020. All further natural language processing\u2013driven analyses were done only on this text corpus. The search query and the corresponding DOIs are shared in the GitHub repository: \n\n. \n\n\n### Caption cluster plot \n  \nThe final corpus contains a total of 106,238 figures and their captions. With the help of the Stanford NLTK  package, the caption texts were tokenized after removal of punctuations, numerals, and stop words. These tokens, which include words of the English language, chemical symbols, and abbreviations, form the corpus dictionary of size (N) which determines the number of dimensions of the vector space for the caption cluster plot. Each caption is mapped to a unique vector in this vector space by calculating the TFIDF of every word in the caption. TFIDF is a statistical count that reflects the relevance of a word in a document and is a common vectorization technique for text mining and information retrieval. For a term \u201ct\u201d appearing in \u201cd\u201d documents within a collection D: where \n\nThe cosine distance of all the captions from each other is calculated and used as the metric for t-SNE, which projects the vectors into a 2D plane so that vectors with the highest cosine similarities group together. Finally, each caption is assigned a unique label corresponding to the type of image it represents, such as SEM, XRD, TEM, Luminescence, Fracture, etc., through rule-based string search. The color of the pixel in the t-SNE plot is determined by its label. The TFIDF vectorization and the cosine metric ensure that the geometric distance between pixels on the 2D plot correlate with the semantic similarity of captions, and therefore that identical images cluster together. \n\n\n### Latent Dirichlet Allocation \n  \nLDA  is one of the most commonly used unsupervised topic modeling approaches. LDA classifies the documents present in a corpus into different topics based on the frequency distribution of the words occurring in the document. The specific steps in the LDA algorithm are as follows:   \nAssume there are articles belonging to   k   topics in the glass corpus. \n  \nRandomly assign a topic   k   to each word   w   in all the documents. \n  \nCompute the probabilities of a word   w   belonging to topic   k   in document   d   as p(  k  |  d  ) and probability of a document belonging to topic   k   due to the word   w   as p(  w  |  t  ). \n  \nUpdate the p(  w  |  t  ) as p(  t  |  d  )\u00d7p(  w  |  t  ). \n  \nRepeat steps 1 to 4. \n  \n\nThe 94,207 abstracts are further categorized using LDA. The optimum number of topics was found by the coherence plot, which was found to converge after 15 topics. After 500 passes, the LDA algorithm identified the topics listed in  C. Similar to the CCP, each abstract was tokenized, vectorized, and plotted in 2D using t-SNE. The coloring of the pixels is based on the topic number identified by the LDA. Once again, the pixels are seen to cluster strongly based on color, indicating that abstracts with similar lexical content have been grouped together by the algorithm. All the hyper parameters for t-SNE, TFIDF, and LDA are provided in section 1 of the  . \n\n\n### Extraction of chemical species \n  \nChemDataExtractor was used to identify and extract the individual chemical species from every abstract. This includes individual chemical elements and compounds identified by their symbols, names, and chemical formulae. A custom Python script is used to extract the individual chemical elements from the most frequent of these compounds. \n\n\n### Elemental maps \n  \nIf an element X is identified as being present in an abstract, the pixel corresponding to the abstract in the LDA plot is given a different color. This allows the mapping of elemental compositions as identified in the step above to the information content present in the LDA plot. Similarly, the elemental information and captions are correlated by merging the CCP and the LDA plot. A caption pixel is given a different color if the abstract of the text that the caption belongs to contains the element. \n\n\n \n", "metadata": {"pmcid": 8276010, "text_md5": "907dc06229438d12f4fba3070b08971e", "field_positions": {"authors": [0, 121], "journal": [122, 136], "publication_year": [138, 142], "title": [153, 263], "keywords": [277, 381], "abstract": [394, 3129], "body": [3138, 43856]}, "batch": 1, "pmid": 34286304, "doi": "10.1016/j.patter.2021.100290", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8276010", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8276010"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8276010\">8276010</a>", "list_title": "PMC8276010  Looking through glass: Knowledge discovery from materials science literature using natural language processing"}
{"text": "Garc\u00eda del Valle, Eduardo P. and Lagunes Garc\u00eda, Gerardo and Prieto Santamar\u00eda, Luc\u00eda and Zanin, Massimiliano and Menasalvas Ruiz, Ernestina and Rodr\u00edguez-Gonz\u00e1lez, Alejandro\nSci Rep, 2021\n\n# Title\n\nLeveraging network analysis to evaluate biomedical named entity recognition tools\n\n# Keywords\n\nDiseases\nBioinformatics\nScientific data\nMedical research\nClassification and taxonomy\nLiterature mining\nNetwork topology\n\n\n# Abstract\n \nThe ever-growing availability of biomedical text sources has resulted in a boost in clinical studies based on their exploitation. Biomedical named-entity recognition (bio-NER) techniques have evolved remarkably in recent years and their application in research is increasingly successful. Still, the disparity of tools and the limited available validation resources are barriers preventing a wider diffusion, especially within clinical practice. We here propose the use of omics data and network analysis as an alternative for the assessment of bio-NER tools. Specifically, our method introduces quality criteria based on edge overlap and community detection. The application of these criteria to four bio-NER solutions yielded comparable results to strategies based on annotated corpora, without suffering from their limitations. Our approach can constitute a guide both for the selection of the best bio-NER tool given a specific task, and for the creation and validation of novel approaches. \n \n\n# Body\n \n## Introduction \n  \nHuge volumes of digital textual content are generated every day in biomedical research and practice, including scientific papers, electronic medical records (EMRs), and physician notes. These sources contain information about new discoveries and new insights, providing valuable knowledge for medical applications such as disease\u2013disease relationships or drug repositioning. However, medical texts consist mainly of unstructured, free-form textual content that requires manual curation and analysis performed by domain experts . Since the manual curation and management of such large corpora are infeasible, over the last decades biomedical researchers have relied on natural language processing (NLP) methods and techniques to facilitate their use. Biomedical named entity recognition (bio-NER) is a form of NLP that identifies and categorizes biomedical terms in unstructured biomedical documents. Gene, protein, drug or disease are some common named entity classes considered in biomedical domain . In recent years, bio-NER systems have been successfully used in a diverse set of applications such as bio-medical literature mining , customer care, community websites or personal information management . \n\nNotwithstanding these achievements, the application of NER in the clinical domain still presents many challenges. Compared to the general NLP domain, determining the right boundaries of clinical named entities is a difficult task, since they are often multi-token terms with nested structures that include other entities inside them. In addition, the biomedical literature does not follow strict naming conventions. Instead, there are usually several ways to mention the same named entity and the use of symbols, digits and abbreviations is very common. This variability makes it difficult for matching-based unsupervised methods to work well in the clinical domain . As a result, early bio-NER systems such as cTAKES  or MetaMap , which worked by matching text phrases with handcrafted dictionaries and rules, have been replaced or combined with supervised methods that learn to extract and categorize clinical terms from existing data. Thus, machine learning and hybrid based solutions like CLAMP  and Bio-BERT  have achieved state-of-the-art results in the field of bio-NER, although they heavily rely on annotated datasets to train and validate their models. \n\nOver the last decade, several annotated corpora have been developed, including both manually annotated (known as gold standards) and automated or semi-automated annotated collections (silver standards) . These corpora contain texts, extracted mainly from scientific articles and medical records, and their corresponding annotated named entities (e.g., diseases, body parts, treatments) . Still, their availability is limited due to two main factors. First, annotating corpora manually is laborious and expensive, particularly so in the clinical domain in which medical expertise is required. Second, the access and exploitation of the source texts is often restricted by licensing terms and data privacy regulations, such as the Health Insurance Portability and Accountability Act (HIPAA) . As a consequence, the available datasets are old (for instance, NCBI was last revised in August 27, 2013), require registration (as is the case of i2b2 dataset, now housed in the Department of Biomedical Informatics at Harvard Medical School) and/or force to obtain a human subject training certificate (e.g., for ShARe/CLEF, currently hosted by the MIT Lab for Computational Physiology). \n\nAs an alternative to the use of annotated datasets in the development of bio-NER tools, in this study we present a method based on the exploitation of omics data and network analysis. On the one hand, the increasing availability of omics data, such as genomic, proteomic, transcriptomic or metabolomic, resulting from improvements in the acquisition of molecular biology, represents an unprecedented resource for clinical researchers. Big data originating from biology are complemented with chemical and pharmacological data published by laboratories and regulatory agencies . On the other hand, the emerging field of network medicine offers the tools of network science for interconnecting these data and discovering new insight about how diseases operate at the molecular level and how they are related to each other. Major projects such as DisGeNET  and Hetionet  have exploited this approach to obtain vast complex networks that enable researchers to formulate novel hypothesis on drug therapeutic action and drug adverse effects, and predict disease gene associations, among other applications . \n\nPrevious studies have built phenotypic disease networks out of the named entities extracted from medical texts using bio-NER tools, and compared them with omics-based networks . The results showed a very significant overlap between both types of networks, proving that shared terms (symptoms) indicate shared genes and proteins, for instance. Additionally, it was observed that disease networks obtained from medical texts tended to form clear, highly interconnected communities, which coincided significantly with the disease categories of classifications systems such as the disease ontology (DO) and the medical subject headings (MeSH) . Given these precedents, our hypothesis is that the accuracy of a bio-NER tool can be measured by building a disease network from the extracted entities and calculating both its overlapping with omics networks and the coincidence of its communities with the categories of disease classification systems. \n\nTo test our hypothesis, we selected four bio-NER tools based on unsupervised (MetaMap  and MetaMap Lite ), supervised (CLAMP ) and hybrid (BERN ) methods. First, we used each tool to extract medical terms from a dataset of Wikipedia and Mayo Clinic disease articles, and obtained their associated phenotypic disease networks by computing the similarity of the terms vector extracted for each disease. Second, we used the same approach to build omics disease networks from public available data sources (see Supplementary Table  ) and analyzed their overlapping with each phenotypic network. Third, we applied network analysis techniques to obtain the disease communities of the phenotypic networks and evaluated their coincidence with the top-level categories in MeSH, DO and International Classification of Diseases (ICD-10-CM). Finally, we compared the results to find the best performing tool and contrasted the outcome with classical evaluation approaches. Figure\u00a0  illustrates the experimental design, which is thoroughly described in the \u201c  section.   \nExperimental Design. (  a  ) First, data are extracted from textual and omics sources; (  b  ) next, networks are generated from the extracted data, and their main characteristics are analysed and compared; (  c  ) finally, network-based criteria are applied to evaluate the accuracy of the bio-NER tool, and the results are compared with existing evaluations based on annotated corpora; (  d  ) same method is applied to DISNET\u2019s bio-NER system; and (  e  ) the reference set is extended with pharmacologic data. \n  \n\nOur study confirmed that the tools with highest accuracy when evaluated with annotated corpora generally rank first according to our method. In other words, we proved that our method performs similarly to strategies based on annotated corpora, without suffering from their limitations. We also demonstrated both the extensibility of this approach, by including the comparison with disease\u2013disease networks obtained from pharmacological data, and its application to the evaluation of an alternative bio-NER tool. \n\n\n## Results \n  \n### Characterization of disease networks \n  \nTable   lists the main characteristics of both the phenotypic disease networks generated from the terms extracted with the bio-NER tools, and the reference disease networks obtained from genomic, proteomic and pharmacological data, as described in the \u201c \u201d section. Figure\u00a0  provides a visual representation of the results. In the case of phenotypic networks, while they present a similar number of nodes (ranging from 5054 to 6042), there is a significant variation in the number of edges (12,499 for BERN versus 595,110 for MetaMap Lite). While this implies that the tools are capable of extracting terms for approximately the same number of diseases, we found that the number of terms extracted per disease (and therefore, the connections between them) differs. For example, for   Larsen Syndrome  , BERN extracts 20 terms, compared to 42 for MetaMap. The density values, which range between 0.008 and 0.033, reflect this disparity and coincide with those of other phenotypic disease networks obtained from medical text mining . For their part, the reference networks have a lower number of nodes, covering in the best case only 39.38% of the total diseases in the Wikipedia and Mayo Clinic article dataset (see \u201c \u201d section), compared to a maximum of 84.01% for the phenotypic networks. This indicates a concentration of omics data on a limited set of diseases, while textual data cover a broader set. The density of the reference networks is also lower, with values around 0.005. Previous studies confirmed the low density of biological networks, arguing that they are generally sparsely connected, since this confers an evolutionary advantage for preserving robustness .   \nCharacteristics of the extracted networks. \n  \nCalculations of the transitivity, including the results of the normality tests, are available in the Supplementary Materials (see Supplementary Table  ). \n    \nBio-NER tools used in the study. MetaMap, MetaMap Lite and CLAMP provide configurable assertion detection (i.e., negation), hence the two performance values in the i2b2 2010 dataset. \n    \nComparison of network characteristics. (  a  ) Location of the analysed networks in the normalized transitivity versus modularity plane. The size and the color of the bubbles represent the density and assortativity of the networks, respectively; (  b  ) log\u2013log plot of the degree CCDF of the networks. \n  \n\nAs shown in Fig.\u00a0 a, the modularity of the reference networks is greater than in the networks obtained from texts. This denotes a greater tendency of omics networks to form communities, although the range of values obtained in the phenotypic networks (around 0.5) can also be considered as relatively high. Among them, the network associated with BERN presents the highest modularity. In contrast, the transitivity values of the phenotypic networks are generally higher than in the reference networks (see Supplementary Table   for more details). This suggests that, even though phenotypic networks have less tendency to cluster in communities, their communities are more densely connected internally, compared to biological networks. In the literature, networks with a 0.3 transitivity are considered highly transitive . Our results show that the network associated with MetaMap Lite with negation detection presents the highest transitivity. Figure\u00a0 b displays the log\u2013log plot of the degree complementary cumulative distribution function (CCDF) of the networks. For the phenotypic networks, their CCDFs show a less abrupt fall than those of the reference networks, especially genomics and proteomics. This indicates that the maximum degree in phenotypic networks is much higher than in biological networks, which is due to a greater interconnection of diseases through their symptoms, than through their associated genes or proteins. In other words, symptom-based connections are less specific than those based on genes or proteins . Our results also show that phenotypic networks tend to be assortative, meaning that disease hubs tend to connect with each other. This property is also observed in social networks, for example . In contrast, proteomic and genomic networks have low or negative assortativity, since their nodes tend to link to nodes with fewer interaction partners rather than to other hubs. Protein interaction networks and neural networks are documented examples of disassortative networks . This confirms the greater specificity of biological bonds compared to phenotypic ones, previously observed with the degree distribution. \n\nThe pharmacological network, added in this study as an example of extension of the reference networks, presents mixed characteristics. On the one hand, its density, modularity and transitivity are similar to those of the omics networks. On the other hand, its topology (degree distribution and assortativity) is closer to phenotypic networks. This reflects that pharmacology is derived from both phenotypic and biological disease knowledge. \n\n\n### Overlap of phenotypic and biological networks \n  \nSupplementary Table   lists the number of common nodes (diseases) and edges between each phenotypic network associated with a bio-NER tool, and the reference networks, as well as the z-scores obtained when comparing the values with those expected at random, and the   p   values corresponding to the Shapiro\u2013Wilk test (see \u201c \u201d section). Phenotypic networks share a similar number of nodes with reference networks. For example, the network associated with MetaMap Lite has 1506 nodes in common with the genomic network (87.30%), compared to 1470 (85.22%) of CLAMP and 1487 (86.21%) of BERN. This result was expected since, as presented in the previous section, the networks obtained from bio-NER tools have a similar number of nodes. In the same way, given that they have an uneven number of links, it was also expected that the number of overlapping links would be different, as reflected in the results. Thus, while MetaMap Lite shares 759 links with the genomic network (9.25%), MetaMap only shares 437 (5.32%). In all cases, the z-score, which indicates the significance of this overlap with respect to the random case, is higher for the phenotypic network associated with BERN. CLAMP performs second best, followed by MetaMap Lite and MetaMap. Only in the case of MetaMap Lite, the network obtained with negation detection presents a clearly superior performance than without this function. \n\nSupplementary Table   contains the results for the overlap of the phenotypic networks with all the reference networks simultaneously. In this case the number of shared nodes and links is drastically reduced. Only around 340 diseases in phenotypic networks are present in the genomic, proteomic and pharmacological networks, and the number of overlapping edges ranges from 18 to 33. The z-score confirms the ranking obtained when using the reference networks separately, which suggests that the type of reference network used to measure the overlap with the phenotypic networks has little influence. Taking into account this result and that the size of the combined network would limit the validation of bio-NER to a reduced set of diseases, we discarded this test in favor of the overlapping with individual omics networks. \n\n\n### Coincidence of communities in phenotypic network with disease categories \n  \nSupplementary Table   shows the number of communities obtained with the Louvain method for each phenotypic network (see \u201c \u201d section), as well as their ratio of coincidence with the top-level categories in MeSH, DO and ICD-10-CM, the z-scores computed by comparing the values with those obtained for random networks, and corresponding p-values of the Shapiro\u2013Wilk test. The evaluation of bio-NER tools assessed with this method is generally consistent across disease classification systems, and also with that obtained by measuring the edge overlap with the reference networks. \n\nFigure\u00a0  shows the percentage of diseases classified in the 10 largest top-level categories of DO (Fig.\u00a0 a), MeSH (Fig.\u00a0 b) and ICD-10-CM (Fig.\u00a0 c), for the best performer (BERN) and worst performer (MetaMap), as a result of the previous analysis. For reference, it also displays the actual percentage of diseases that belong to those categories in each classification system. E.g., out of a total of 2501 diseases in the dataset mapped to a DO concept, 402 (16.07%) have the category DO 863 (diseases of the nervous system). We observe that the communities of the phenotypic networks present a similar degree of coincidence for the equivalent categories in the different classification systems. In the case of BERN, we find greater coincidences in the categories MeSH C04, DO 162 and ICD-10-CM C00-D49 (neoplasms/cancer); MeSH C05, DO 17 and ICD-10-CM M00-M99 (diseases of musculoskeletal system); and MeSH C14, DO 1287 and ICD-10-CM I00-I99 (cardiovascular diseases/diseases of circulatory system). For its part, MetaMap presents greater coincidences in MeSH C10, DO 863 and ICD-10-CM G00-G99 (diseases of the nervous system). This suggests that bio-NER tools are capable of extracting terms, and ultimately relationships between diseases, consistently with classifications of diseases, as described in the literature .   \nCoincidence of network communities with disease categories. The bar plots show the proportion of diseases associated with the 10 largest first-level categories in the DO (  a  ), ICD-10-CM (  b  ) and MeSH (  c  ) classification systems, compared with the proportion obtained for the best performer (BERN) and worst performer (MetaMap). \n  \n\n\n### Comparison with gold-standard based evaluation \n  \nThe spider web chart in Fig.\u00a0 a summarizes visually the results of the tests described in the previous sections. The network associated with BERN performs best both in the overlap with the reference networks and in the coincidence of its communities with the disease categories. Overall, the two CLAMP variations (with and without negation detection) have the second-best performance. Only in the overlap with the pharmacological network, the results of MetaMap Lite (with and without negation) are similar to those of CLAMP. MetaMap obtains comparatively the worst results, except in the coincidence of communities with MeSH categories, where MetaMap Lite performs worse.   \nEvaluation of the bio-NER accuracy according to the proposed model. (  a  ) Results of the network overlapping and community coincidence tests and (  b  ) normalized average results for the two tests, compared with the normalized average F-1 score of the bio-NER tools obtained from gold-standard based evaluations. \n  \n\nIn order to compare the global results of both tests, Fig.\u00a0 b represents their normalized mean values. According to our proposed evaluation of bio-NER tools, the better the results in the tests (that is, the further up and right in the chart), the greater the accuracy of the tool. To validate our approach, we contrasted our evaluation results with those obtained through traditional methods based on annotated corpora. In Fig.\u00a0 b, the area of the bubble represents the normalized mean F-1 value of the tool (see Table 2). We observe that there is a notable correlation between the position and the size of the plots, with BERN outperforming the other tools, CLAMP ranking second, followed by MetaMap Lite and MetaMap. \n\nOur assessment coincides even for the variations within the same tool. Both MetaMap and MetaMap Lite perform better when negation detection is enabled. Only for CLAMP, we observed a difference with respect to the F-1-based ranking. Its accuracy is higher with negation detection, according to the evaluation performed with the i2b2 dataset (the only data available for this case), but our method gives a slightly greater accuracy to the variation without detection. \n\n\n### Application to DISNET \n  \nTo test the application of our evaluation method to an alternative bio-NER tool, we performed the same tests with DISNET's text extraction system, which is built on top of MetaMap with an additional dictionary-based validation of terms. The tables and figures in the previous sections include the results for this tool. According to our evaluation, the accuracy of DISNET's bio-NER is higher than that of MetaMap alone. This was expected, since the validation system eliminates false positives caused by the ambiguity of the terms detected by MetaMap . DISNET has an accuracy comparable to that of MetaMap Lite, but noticeably worse than solutions based on more advanced NER methods such as CLAMP or BERN. \n\n\n\n## Discussion \n  \nIn this study, we hypothesize that the increasingly available omics data can be used in combination with network analysis to evaluate bio-NER tools, as an alternative to traditional methods based on annotated corpora. To demonstrate our hypothesis, we first built a dataset of medical texts associated with diseases from public textual sources and used 4 bio-NER tools with known F-1 value to extract their clinical terms. Next, by computing the pairwise similarity between diseases based on the extracted terms, we generated the disease-disease phenotypic network corresponding to each tool. Additionally, we collected publicly available data on disease-gene and disease-protein associations to build reference omics networks, following the same method. The analysis of the networks, illustrated in Fig.\u00a0 , shows that their characteristics coincide with those of other networks generated in a similar way, confirming the validity of our process up to this point. \n\nIn a first test, we measured the overlapping of the phenotypic network of each bio-NER tool with the omics networks. In a second test, we evaluated the coincidence of the communities of the phenotypic networks with the top-level categories of various classification systems. The obtained results show that a better performance of the bio-NER tool in the network overlapping and community coincidence tests is associated with a greater precision of the tool when it is evaluated using gold-standards. Therefore, as proposed in our hypothesis, a metric composed of the results of both network-based tests can replace the F-1 obtained through validation with annotated corpora, as illustrated in Fig.\u00a0 b. \n\nSince annotated datasets are generally scarce, limited access and outdated, our method offers researchers an alternative based on more abundant, accessible and updated omics data. Furthermore, our approach allows other sources to easily be incorporated, as we demonstrated when using disease-drug associations. However, our solution has some limitations. First, although it makes it possible to clearly differentiate the accuracy of two different tools, it is less precise when comparing variations within the same tool, as we observed in the case of CLAMP with and without negation detection. Second, using this method requires disease-associated text sets, such as the Wikipedia and Mayo Clinic articles used in the study. Clinical texts such as EMRs, where several disorders might be discussed simultaneously, are not suitable. Last, our method only measures the accuracy of bio-NER tools, without evaluating other important aspects such as their speed or their usability. \n\nTo improve the precision of our method, in the case of the overlap with reference networks, we propose the exploitation of new sources (e.g., transcriptomics, metabolomics, epigenomics) to build a more complete set of reference networks. Regarding the coincidence of the communities with the disease categories, on the one hand it is necessary to evaluate whether alternative community detection methods offer better results. And on the other hand, we recommend studying the different hierarchical levels of the classification systems, in order to find the most appropriate level for this test. Finally, by extending the study to more bio-NER tools with known accuracy (e.g., from NER challenges in this area), it should be possible to determine which reference networks or classification systems in particular offer results closer to the reference ones, and favor their use to improve the efficiency of our method. \n\n\n## Methods \n  \n### Experimental design \n  \nThe goal of our research is to provide an alternative to the use of annotated corpora for the evaluation of bio-NER tools. Based on the previous work presented in the introduction, our hypothesis is that the accuracy of a bio-NER tool can be assessed through the analysis of the disease network generated from the extracted terms, including its overlap with omics networks and the coincidence of its communities with the categories of disease classification systems. \n\nFigure\u00a0  describes the experimental design to demonstrate our hypothesis. We first used several bio-NER tools to extract disease-term pairs from a dataset of medical articles, and mined omics sources to obtain disease-gene and disease-protein pairs (Fig.\u00a0 a). Next, we built the phenotypic and reference disease\u2013disease networks out of the disease-term pairs and disease-omics pairs, respectively, and analysed their characteristics (Fig.\u00a0 b). Finally, we evaluated the overlap between the phenotypic and omics networks as well as the coincidence of the phenotypic network communities with different disease categorizations, and contrasted the results with the bio-NER tool evaluations obtained with annotated datasets (Fig.\u00a0 c). Additionally, we demonstrated the applicability of our method to the assessment of an alternative bio-NER (Fig.\u00a0 d) and its extensibility by expanding the set of reference networks with pharmacological data (Fig.\u00a0 e). \n\n\n### Bio-NER tools \n  \nIn our study, we used four bio-NER tools: MetaMap , MetaMap Lite , CLAMP  and BERN . We selected these tools based on three aspects: (1) they are publicly available; (2) they use different bio-NER approaches (rule-based, dictionary-based, ML and hybrid); and (3) their accuracy has been evaluated against different gold standards. These criteria ensure the reproducibility, generalizability and evaluability (respectively) of our method. Table 2 shows a brief description for each tool and its performance evaluated against the i2b2 2010 , SemEval 2014  and NCBI  datasets. For more detailed information on the tools, including the version and configuration used in the study, see Supplementary Table  . \n\n\n### Disease\u2013disease networks from text datasets \n  \nFor the extraction of medical terms through the bio-NER tools, we used a dataset consisting of excerpts of 7500 Wikipedia articles and 620 Mayo Clinic articles, obtained between 2019 and 2020 as part of the DISNET project . Each article is associated with a single disease, and there may be more than one article for the same disease. As a whole, the dataset contains texts for 7192 diseases, with a total of 3,330,001 words and an average of 463.01 words per disease (standard deviation\u2009=\u200956.57). We used the Crosswalk Vocabulary API of the Unified Medical Language System (UMLS) to map the diseases by their identifiers in different terminologies . See Supplementary Table   for more details. \n\nWe processed the dataset with each bio-NER tool and extracted the named entities associated with every disease. Next, we computed the pairwise similarities between diseases expressed as vectors of the extracted terms, using the Jaccard distance . Finally, we built the disease-disease networks, in which two nodes (diseases) are connected with an edge weighted by the similarity of their extracted terms. To limit the size of the networks, only pairs with a similarity above the 95th percentile were considered. For the tools that support negation detection (MetaMap, MetaMap Lite and CLAMP), we obtained two networks, with and without this option. \n\n\n### Disease\u2013disease networks from biological sources \n  \nData of gene-disease associations were obtained from DisGeNET . For its part, the implications of proteins in diseases were extracted from Uniprot . In order to demonstrate the aggregation of new sources to our system, we also incorporated data of disease-drug associations extracted from the Stanford Network Analysis Project . Again, we used UMLS to cross-map the disease identifiers in the different sources. As in the case of text-extracted terms, we built the genomic, proteomic and pharmacological disease\u2013disease weighted networks from the pairwise similarity of their genes, proteins and drugs, respectively. Due to the greater specificity of omics data, the number of obtained pairs was much lower than with the text terms, so they were not filtered. See Supplementary Table   for more details. \n\n\n### Network characterization \n  \nAs a previous step to the application of our method in the evaluation of the bio-NER tools, we performed an analysis of the characteristics of their associated networks using the   NetworkX   Python library . \n\nFirst, we measured three dimensions of the network structure: density, modularity, and transitivity. The network density is defined as the number of existing relationships relative to the possible number. For its part, the modularity measures the degree to which the network tends to segregate into relatively independent groups. It is computed as the fraction of the edges that fall within the groups, minus the expected fraction if edges were distributed at random. Biological networks have a significantly higher modularity compared to random networks, which proves their modular nature . However, it has been shown that modularity suffers a resolution limit and, therefore, it is unable to detect small communities. On the other hand, the transitivity of a network is the relative proportion of triangles among all connected triads it contains. It can be interpreted as the probability of finding a direct connection between two nodes having a common neighbor. In general, high transitivity allows obtaining a community structure. However, high transitivity is not a prerequisite to the existence of a strong community structure . \n\nNext, we obtained data on the network topology, including the degree distribution and assortativity. The degree distribution P(k) of a network is the probability that a randomly chosen node has k connections (or neighbours). In most complex networks (including biological networks), the degree distribution is highly asymmetric due to the presence of a small number of highly connected nodes (hubs) . To compare the degree distributions of the networks, we computed the complementary cumulative distribution function (CCDF), also known as tail distribution . If the resulting plot of one distribution falls above the other, we may conclude that the upper one has a heavier tail (i.e., decays slower) than the lower. The assortativity is another measure related to the network topology, and indicates the preference for a network's nodes to attach to others that are similar in some way. Thus, a network is called assortative (i.e., its assortativity ranges from 0 to 1) if the vertices with higher degree have the tendency to connect with other vertices that also have high degree of connectivity. If the vertices with higher degree have the tendency to connect with other vertices with low degree, then the network is called disassortative (i.e., the assortativity is between 0 and \u2212\u00a01). \n\nFinally, we compared the results obtained for the phenotypic and reference disease-disease networks with each other and with the existing literature. \n\n\n### Network overlapping \n  \nFor each bio-NER tool, we obtained the edges shared between its associated disease network and the reference networks, using   NetworkX  . Next, we compared the number of observed overlapping edges to what would be expected with random networks. The Statistical Analysis section describes the statistical methods used in more detail. \n\n\n### Community detection \n  \nAs explained in the introduction, several studies have reported significant overlaps between communities in phenotypic networks and disease categories . To replicate this analysis, we first obtained the disease categories of first hierarchical level from the MeSH, ICD-10-CM, and DO classification systems. MeSH descriptors were downloaded from the NLM site. Only categories of type C (Diseases) and F03 (Mental Disorders) were considered. The ICD-10-CM code descriptions were downloaded from the website of the Centers for Medicare and Medicaid Services, and concepts of the DO were obtained from the code repository of the project. Finally, UMLS and DO mappings were used to associate the categories with the diseases in the networks (see Supplementary Table  ). \n\nTo detect the communities in the disease networks, we used Louvain's method, which optimizes modularity as the algorithm progresses . First, for each disease network associated with a bio-NER tool, we obtained the best partition using the   Community   library from the   Python-Louvain   Python package . Then, for each community obtained, we computed its associated disease category in each classification system (i.e., the most frequent among its diseases) and the proportion of community members that belonged to that category. The result indicated the ratio of coincidence of the network communities with the disease categories. Finally, as in the case of network overlaps, we compared the value obtained with that expected at random (see Statistical Analysis). \n\n\n### Comparison with DISNET extraction tool (TVP) \n  \nThe DISNET database integrates phenotypic and genetic-biological characteristics of diseases and information on drugs from several expert-curated sources and unstructured textual sources . Phenotypic data is extracted from Wikipedia, PubMed, and Mayo Clinic texts, using MetaMap and a validation system called term validation process (TVP). The TVP aims to eliminate false positives detected by MetaMap and increase the precision of the results. It could be thought of as a dictionary-based extension to MetaMap. Evaluating this extraction mechanism against an annotated dataset shows a performance improvement over MetaMap alone . In order to demonstrate the application of our approach to a new bio-NER tool, we used the DISNET extraction system to obtain the terms of our dataset and performed the same analyses as for the rest of the tools. \n\n\n### Statistical analysis \n  \nTo evaluate the statistical significance of the network transitivity, the overlap of the phenotypic and reference layers, and the coincidence of the network communities with the disease categories, we obtained for each bio-NER tool a network with the same number of randomly connected nodes and performed the same analysis. We repeated the randomization process 1000 times and recorded the results to obtain a distribution that served as a null model. We verified the normality of this distribution through the Shapiro\u2013Wilk test (i.e.,   p   value\u2009>\u20090.05 implies that it is normal). Finally, we calculated the z-scores of the results observed in the original networks with respect to the null model. A higher magnitude of the z-score (either positive or negative) indicates a greater statistical significance of the result. When a better comparability of the z-scores was needed, we used min\u2013max normalization to scale their range in [0, 1]. The   p   values of the Shapiro\u2013Wilk normality tests and the z-scores are included in the Supplementary Materials. \n\n\n\n## Supplementary Information \n  \n\n\n\n\n \n", "metadata": {"pmcid": 8242017, "text_md5": "8ab4b7c40221878da26f54895039b2ea", "field_positions": {"authors": [0, 174], "journal": [175, 182], "publication_year": [184, 188], "title": [199, 280], "keywords": [294, 414], "abstract": [427, 1426], "body": [1435, 36608]}, "batch": 1, "pmid": 34188248, "doi": "10.1038/s41598-021-93018-w", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8242017", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8242017"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8242017\">8242017</a>", "list_title": "PMC8242017  Leveraging network analysis to evaluate biomedical named entity recognition tools"}
{"text": "Dekker, Niels and Kuhn, Tobias and van Erp, Marieke\nPeerJ Comput Sci, 2019\n\n# Title\n\nEvaluating named entity recognition tools for extracting social networks from novels\n\n# Keywords\n\nSocial networks\nNamed entity recognition\nEvaluation\nDigital humanities\nClassic and modern literature\nCultural AI\n\n\n# Abstract\n \nThe analysis of literary works has experienced a surge in computer-assisted processing. To obtain insights into the community structures and social interactions portrayed in novels, the creation of social networks from novels has gained popularity. Many methods rely on identifying named entities and relations for the construction of these networks, but many of these tools are not specifically created for the literary domain. Furthermore, many of the studies on information extraction from literature typically focus on 19th and early 20th century source material. Because of this, it is unclear if these techniques are as suitable to modern-day literature as they are to those older novels. We present a study in which we evaluate natural language processing tools for the automatic extraction of social networks from novels as well as their network structure. We find that there are no significant differences between old and modern novels but that both are subject to a large amount of variance. Furthermore, we identify several issues that complicate named entity recognition in our set of novels and we present methods to remedy these. We see this work as a step in creating more culturally-aware AI systems. \n \n\n# Body\n \n## Introduction \n  \nThe characters and their relations can be seen as the backbone of any story, and explicitly creating and analysing a network from these relationships can provide insights into the community structures and social interactions portrayed in novels ( ). Quantitative approaches to social network analysis to examine the overall structure of these social ties, are borrowed from modern sociology and have found their way into many other research fields such as computer science, history, and literary studies ( ).  ,  ,  , and   have all proposed methods for automatic social network extraction from literary sources. The most commonly used approach for extracting such networks, is to first identify characters in the novel through Named Entity Recognition (NER) and then identifying relationships between the characters through for example measuring how often two or more characters are mentioned in the same sentence or paragraph. \n\nMany studies use off-the-shelf named entity recognisers, which are not necessarily optimised for the literary domain and do not take into account the surrounding cultural context. Furthermore, to the best of our knowledge, such studies focus on social network extraction from 19th and early 20th century novels (which we refer to as   classic novels  ).  Typically, these classic novels are obtained from Project Gutenberg ( ), where such public domain books are available for free. While beneficial for the accessibility and reproducibility of the studies in question, more recent novels may not imitate these classic novels with respect to structure or style. It is therefore possible that classic novels have social networks that have a structure that is very different from more recent literature. They might differ, for example, in their overall number of characters, in the typical number of social ties any given character has, in the presence or absence of densely connected clusters, or in how closely connected any two characters are on average. Moreover, changes along dimensions such as writing style, vocabulary, and sentence length could prove to be either beneficial or detrimental to the performance of natural language processing techniques. This may lead to different results even if the actual network structures remained the same.   did compare 18th and 19th century novels on the number of characters that appear in the story, but found no significant difference between the two. Furthermore, an exploration of extracted networks can also be used to assess the quality of the extracted information and investigate the structure of the expression of social ties in a novel. \n\nThus far, we have not found any studies that explore how NER tools perform on a diverse corpus of fiction literature. In this study, we evaluate four different tools on a set of classic novels which have been used for network extraction and analyses in prior work, as well as more recent fiction literature (henceforth referred to as   modern novels  ). We need such an evaluation to assess the robustness of these tools to variation in language over time ( ) and across literary genres. Comparing social networks extracted from corpora consisting of classic and modern novels may give us some insights into what characteristics of literary text may aid or hinder automatic social network extraction and provide indications of cultural change. \n\nAs previous work ( ) has included works from different genres, in this work we decided to focus on the fantasy/science fiction domain to smooth potential genre differences in our modern books. In our evaluation, we devote extra attention to the comparison between classic and modern fantasy/science fiction in our corpus. \n\nWe define the following research questions:\n   \nTo what extent are off-the-shelf NER tools suitable for identifying fictional characters in novels? \n  \nWhich differences or similarities can be discovered between social networks extracted for different novels? \n  \n\nTo answer our first research question, we first evaluate four named entity recognisers on 20 classic and 20 modern fantasy/science fiction novels. In each of these novels, the first chapter is manually annotated with named entities and coreference relations. The named entity recognisers we evaluate are: (1) BookNLP ( ;  ) which is specifically tailored to identify and cluster literary characters, and has been used to extract entities from a corpus of 15,099 English novels. At the time of writing, this tool was cited 80 times. (2) Stanford NER version 3.8.0 ( ), one of the most popular named entity recognisers in the NLP research community, cited 2,648 times at the time of writing. (3) Illinois Named Entity Tagger version 3.0.23 ( ), a computationally efficient tagger that uses a combination of machine learning, gazetteers,  and additional features extracted from unlabelled data. At the time of writing, the system was downloaded over 10,000 times. Our last system (4) is IXA-Pipe-NERC version 1.1.1 ( ), a competitive classifier that employs unlabelled data via clustering and gazetteers that outperformed other state-of-the-art NER tools on their within and out-domain evaluations. \n\nTo answer the second research question, we use the recognised named entities to create a co-occurrence network for each novel. Network analysis measures are then employed to compare the extracted networks from the classic and modern novels to investigate whether the networks from the different sets of novels exhibit major differences. \n\nThe contributions of this paper are: (1) a comparison and an analysis of four NER on 20 classic and 20 modern novels; (2) a comparison and an analysis of social network analysis measures on networks automatically extracted from 20 classic and 20 modern novels; (3) experiments and recommendations for boosting performance on recognising entities in novels; and (4) an annotated gold standard dataset with entities and coreferences of 20 classic and 20 modern novels. \n\nThe remainder of this paper is organised as follows. We first discuss related work in the section \u2018Related Work\u2019. Next, we describe our approach and methods in the section \u2018Materials and Data Preparation\u2019. We present our evaluation of four different NER systems on 20 classic and 20 modern novels in the section \u2018Named Entity Recognition Experiments and Results\u2019, followed by the creation and analysis of social networks in the section \u2018Network Analysis\u2019. We discuss issues that we encountered in the identification of fictional characters and showcase some methods to boost performance in the section \u2018Discussion and Performance Boosting Options\u2019. We conclude by suggesting directions for future work in the section \u2018Conclusion and Future Work\u2019. \n\nThe code for all experiments as well as annotated data can be found at  . \n\n\n## Related Work \n  \nAs mentioned in the section \u2018Introduction\u2019, we have not found any other studies that compared the performances of social network extraction on classic and modern novels; or compared the structures of these networks. This section therefore focuses on the techniques used on classic literature. In first part of this section, we will describe how other studies extract and cluster characters. In the second part, we outline what different choices can be made for the creation of a network, and motivate our choices for this study. \n\n### Named entity recognition \n  \nThe first and foremost challenge in creating a social network of literary characters is identifying the characters. NER is often used to identify passages in text that identify things by a name. Furthermore, identified passages are often also classified into various categories such as   person  ,   location  , and   organisation  . Typically, this approach is also used to identify miscellaneous numerical mentions such as dates, times, monetary values, and percentages. \n\n,  ,  , and   all use the Stanford NER tagger ( ) to identify characters in literary fiction. On a collection of Sherlock Holmes novels, these studies perform Named entity recognition,   F  -scores between: 45 and 54.   propose that the main difficulty with this collection is the multitude of minor characters, a problem which we expect to be also present in our collections of classic and modern novels. \n\nA big difference between the news domain (for which most language technology tools have been created) and the literary domain, is that names do not have to follow the same \u2018rules\u2019 as names in the real world. This topic is explored in the Namescape project by   namescape ( ). In this project, one million tokens taken from 550 Dutch novels were manually annotated. A distinction between first and last names was made in order to test whether different name parts are used with different effects. A named entity recogniser was trained specifically for this corpus by namescape-clin, obtaining an   F   score of 93.60 for persons. The corpus contains fragments of novels written between the 17th and 20th century, but as the corpus and tools are not available, we cannot investigate its depth or compare it directly to our work. Other approaches attempt to use the identification of locations and physical proximity to improve the creation of a social network ( ). \n\n\n### Coreference resolution \n  \nOne difficulty of character detection is the variety of aliases one character might go by, or; coreference resolution. For example, George Martin\u2019s   Tyrion Lannister  , might alternatively be mentioned as   Ser Tyrion Lannister, Lord Tyrion, Tyrion, The Imp   or   The Halfman  . In the vast majority of cases, it is desirable to collapse those character references into one character entity. However, in some cases, retaining some distinction between character references can be useful: we provide an example of this in subsection \u2018Network Exploration\u2019. \n\nTwo distinct approaches attempt to address this difficulty, (1) omit parts of a multi- word name, or (2) compile a list of aliases. The former approach leaves out honorifics such as the   Ser   and   Lord   in the above example in order to cluster the names of one character. To automate this clustering step, some work has been done by   and  . While useful, the former approach alone provides no solace for the matching of the last two example aliases; where no part of the character\u2019s name is present. The latter approach thus suggests to manually compile a list of aliases for each character with the aid of external resources or annotators. This method is utilised by   and  . In namescape-clin, wikification (i.e. attempting to match recognised names to Wikipedia resources) is used. Obviously this is most useful for characters that are famous enough to have a Wikipedia page. The authors state in their error analysis  , Section 3.2) that titles that are most likely from the fantasy domain are most difficult to resolve, which already hints at some differences between names in different genres. \n\n\n### Anaphora resolution \n  \nTo identify as many character references as possible, it is important to take into account that not all references to a character actually mention the character\u2019s name. In fact,   show that 74% of character references come in the form of a pronouns such as   he, him, his, she, her,   and   hers   in a collection of 15,099 English novels. To capture these references, the anaphoric pronoun is typically matched to its antecedent by using the linear word distance between the two, and by matching the gender of anaphora to that of the antecedent. The linear word distance can be, for example, the number of words between the pronoun and the nearest characters. For unusual names, as often found in science fiction and fantasy, identification of the gender may be problematic. \n\n\n### Network creation \n  \nFor a social network of literary characters, characters are represented by the nodes, whereas the edges indicate to some interaction or relationship. While the definition of a character is uniformly accepted in the literature, the definition of an interaction varies per approach. In previous research, two main approaches can be identified to define such an edge. On the one hand,   conversational networks   are used in approaches by  ,  , and  . This approach focuses on the identification of speakers and listeners, and connecting each speaker and listener to the quoted piece of dialogue they utter or receive. On the other hand,   co-occurrence networks   (as used by   and  ) are created by connecting characters if they occur in the same body of text. While conversational networks can provide a good view of who speaks directly to whom,   argue that   \u2018...much of the interaction in novels is done off-dialogue through the description of the narrator or indirect interactions\u2019   (p. 34). What value to assign to the edges depends on the end-goal of the study. For example,   assign a negative or positive sentiment score to the edges between each character-pair in order to ultimately predict the protagonist and antagonist of the text.   used weighted edges to indicate how often two characters interact. \n\n\n### Network analysis \n  \nSocial network analysis draws upon network theory for its network analysis measures ( ). The application of these measures to networks extracted from literature has been demonstrated insightful in assessing the relationships of characters in for example \u2018Alice in Wonderland\u2019 ( ) and \u2018Beowulf\u2019, the \u2018Iliad\u2019 and \u2018T\u00e1in B\u00f3 Cuailnge\u2019 (\u2018The Cattle Raid of Cooley\u2019, an Irish epic) ( ). Network analysis can also play a role in authorship attribution ( ,  ) and characterising a novel ( ). \n\n\n\n## Materials and Data Preparation \n  \nFor the study presented here, we are interested in the recognition and identification of persons mentioned in classic and modern novels for the construction of the social network of these fictitious characters. We use off-the-shelf state-of-the-art entity recognition tools in an automatic pipeline without manually created alias lists or similar techniques. For the network construction, we follow   and apply their co-occurrence approach for the generation of the social network links with weighted edges that indicate how often two characters are mentioned together. We leave the consideration of negative weights and sentiments for future work. Before we will explain the details of the used entity recognition tools, how they compare for the given task, and how their results can be used to build and analyse the respective social networks, we explain first the details of our selected corpus, how we preprocessed the data, and how we collected the annotations for the evaluation. \n\n### Corpus selection \n  \nOur dataset consists of 40 novels\u201420 classic and 20 modern novels\u2014the specifics of which are presented in   in the Appendix. Any selection of sources is bound to be unrepresentative in terms of some characteristics but we have attempted to balance breadth and depth in our dataset. Furthermore, we have based ourselves on selections made by other researchers for the classics and compilations by others for the modern books. \n\nFor the classic set, the selection was based on Guardian\u2019s Top 100 all-time classic novels ( ). Wherever possible, we selected books that were (1) analysed in related work (as mentioned in the subsection \u2018Coreference Resolution\u2019) and (2) available through Project Gutenberg ( ). \n\nFor the modern set, the books were selected by reference to a list compiled by BestFantasyBooksCom ( , last retrieved: 30 October 2017). For our final selection of these novels, we deliberately made some adjustments to get a wider selection. That is, some of the books in this list are part of a series. If we were to include all the books of the upvoted series, our list would consist of only four different series. We therefore chose to include only the first book of each of such series. As the newer books are unavailable on Gutenberg, these were purchased online. These digital texts are generally provided in .epub or .mobi format. In order to reliably convert these files into plain text format, we used Calibre ( ), a free and open-source e-book conversion tool. This conversion was mostly without any hurdles, but some issues were encountered in terms of encoding, as is discussed in the next section. Due to copyright restrictions, we cannot share this full dataset but our gold standard annotations of the first chapter of each are provided on this project\u2019s GitHub page. The ISBN numbers of the editions used in our study can be found in   the Appendix. \n\n\n### Data preprocessing \n  \nTo ensure that all the harvested text files were ready for processing, we firstly ensured that the encoding for all the documents was the same, in order to avoid issues down the line. In addition, all information that is not directly relevant to the story of the novel was stripped. Even while peripheral information in some books\u2014such as appendices or glossaries\u2014can provide useful information about character relationships, we decided to focus on the story content and thus discard this information. Where applicable, the following peripheral information was manually removed: (1) reviews by fellow writers, (2) dedications or acknowledgements, (3) publishing information, (4) table of contents, (5) chapter headings and page numbers, and (6) appendices and/or glossaries. \n\nDuring this clean-up phase, we encountered some encoding issues that came with the conversion to plain text files. Especially in the modern novels, some novels used inconsistent or odd quotation marks. This issue was addressed by replacing the inconsistent quotation marks with neutral quotations that are identical in form, regardless of whether if it is used as opening or closing quotation mark. \n\n\n### Annotation \n  \nBecause of limitations in time and scope, we only annotated approximately one chapter of each novel. In this subsection, we describe the annotation process. \n\n#### Annotation data \n  \nTo evaluate the performance for each novel, a gold standard was created manually. Two annotators (not the authors of this article) were asked to evaluate 10 books from each category. For each document, approximately one chapter was annotated with entity co-occurrences. Because the length of the first chapter fluctuated between 84 and 1,442 sentences, we selected an average of 300 sentences for each book that was close to a chapter-boundary. For example, for   Alice in Wonderland  , the third chapter ended on the 315th sentence, so the first three chapters were extracted for annotation. While not perfect, we attempted to strike a balance between comparable annotation lengths for each book, without cutting off mid-chapter. \n\n\n#### Annotation instructions \n  \nFor each document, the annotators were asked to annotate each sentence for the occurrence of characters. That is, for each sentence, identify all the characters in it. To describe this process, an example containing a single sentence from   A Game of Thrones   is included in  . The   id   of the sentence is later used to match the annotated sentence to its system-generated counterpart for performance evaluation. The   focus sentence   is the sentence that corresponds to this   id  , and is the sentence for which the annotator is supposed to identify all characters. As context, the annotators are provided with the   preceding   and   subsequent   sentences. In this example, the contextual sentences could be used to resolve the   \u2018him\u2019   in the   focus sentence   to   \u2018Bran\u2019  . To indicate how many persons are present, the annotators were asked to fill in the corresponding number (  #  ) of people\u2014with a maximum of 10 characters per sentence. Depending on this number of people identified, subsequent fields became available to the annotator to fill in the character names. \n   Annotation example.      \nTo speed up the annotation, an initial list of characters was created by applying the BookNLP pipeline to each novel. The annotators were instructed to map the characters in the text to the provided list to the best of their ability. If the annotator assessed that a person appears in a sentence, but is unsure of this character\u2019s identity, the annotators would mark this character as   default  . In addition, the annotators were encouraged to add characters, should they be certain that this character does not appear in the pre-compiled list, but occurs in the text nonetheless. Such characters were given a specific tag to ensure that we could retrieve them later for analysis. Lastly, if the annotator is under the impression that two characters in the list refer to the same person, the annotators were instructed to pick one and stick to that. Lastly, the annotators were provided with the peripheral annotation instructions found in  . \n   Annotation instructions.        \nWhile this identification process did include anaphora resolution of singular pronouns\u2014such as resolving   \u2018him\u2019   to   \u2018Bran\u2019  \u2014the annotators were instructed to ignore plural pronoun references. Plural pronoun resolution remains a difficult topic in the creation of social networks, as family members may sometimes be mentioned individually, and sometimes their family as a whole. Identifying group membership, and modelling that in the social network structure is not covered by any of the tools we include in our analysis or the related work referenced in the section \u2018Related Work\u2019 and therefore left to future work. \n\n\n\n\n## Named Entity Recognition Experiments and Results \n  \nWe evaluate the performance of four different NER systems on the annotated novels: BookNLP ( ), Stanford NER ( ), Illinois Tagger ( ), and IXA-Pipe-NERC ( ). The BookNLP pipeline uses the 2014-01-04 release of Stanford NER tagger ( ) internally with the seven-class ontonotes model. As there have been several releases, and we focus on entities of type Person, we also evaluate the 2017-06-09 Stanford NER four-class CoNLL model. \n\nThe results of the different NER systems are presented in   for the classic novels, and   for the modern novels. All results are computed using the evaluation script used in the CoNLL 2002 and 2003 NER campaigns using the phrase-based evaluation setup ( , last retrieved: 30 October 2017). The systems are evaluated according to micro-averaged precision, recall and   F   measure. Precision is the percentage of named entities found by the system that were correct. Recall is the percentage of named entities present in the text that are retrieved by the system. The   F   measure is the harmonic mean of the precision and recall scores. In a phrase-based evaluation setup, the system only scores a point if the complete entity is correctly identified, thus if in a named entity consisting of multiple tokens only two out of three tokens are correctly identified, the system does not obtain any points. \n   Precision (P), Recall (R), and   F  -scores of different NER systems on classic novels.           Precision (P), Recall (R), and   F   scores of different NER systems on modern novels.        \nThe BookNLP and IXA-Pipe-NERC systems require that part of speech tagging is performed prior to NER, we use the modules included in the respective systems for this. For Stanford NER and Illinois NE Tagger plain text is offered to the NER systems. \n\nAs the standard deviations on the bottom rows of   and   indicate, the results on the different books vary greatly. However, the different NER systems generally do perform similarly on the same novels, indicating that difficulties in recognising named entities in particular books is a characteristic of the novels rather than the systems. An exception is   Brave New World   on which BookNLP performs quite well, but the others underperform. Upon inspection, we find that the annotated chapter of this book contains only five different characters among which \u2018The Director\u2019 which occurs 19 times. This entity is consistently missed by the systems resulting in a high penalty. Furthermore, the \u2018Mr.\u2019 in \u2018Mr. Foster\u2019 (occurring 31 times) is often not recognised as in some NE models titles are excluded. A token-based evaluation of Illinois NE Tagger on this novel for example yields a   F  -score of 51.91. The same issue is at hand with   Dr. Jekyll and Mr. Hyde   and   Dracula  . Although the main NER module in BookNLP is driven by Stanford NER, we suspect that additional domain adaptations in this package account for this performance difference. \n\nWhen comparing the   F  -scores of the 1st person novels to the 3rd person novels in   and  , we find that the 1st person novels perform significantly worse than their 3rd person counterparts, at   p   < 0.01. These findings are in line with the findings of  . \n\nIn the section \u2018Discussion and Performance Boosting Options\u2019, we delve further into particular difficulties that fiction presents NER with and showcase solutions that do not require retraining the entity models. \n\nAs the BookNLP pipeline in the majority of the cases outperforms the other systems and includes coreference resolution and character clustering, we further utilise this system to create our networks. The results of the BookNLP pipeline including the coreference and clustering are presented in  . One of the main differences in that table is that if popular entities are not recognised by the system they are penalised heavier because the coreferent mentions are also not recognised and linked to the correct entities. This results in scores that are generally somewhat lower, but the task that is measured is also more complex. \n\n\n## Network Analysis \n  \nIn this section, we explain how the networks were created using the recognised named entities (subsection \u2018Network Construction\u2019), followed by an explanation of network analysis measures that we applied to compare the networks (subsection \u2018Network Features\u2019). We discuss the results of the analysis (subsection \u2018Results of Network Analysis\u2019), as well as present an exploration of the network of one novel in particular to illustrate how a visualisation of a network can highlight particular characteristics of the interactions in the selected novel (subsection \u2018Network Exploration\u2019). \n\n### Network construction \n  \nAs explained in the section \u2018Related Work\u2019, we opt for the co-occurrence rather than the conversational method for finding the edges of our networks. The body of text that is used to define a co-occurrence differs per approach. Whereas   define such a relation if characters are mentioned in the same sentence,   use a paragraph for the same definition. We consider the delineation of what constitutes a paragraph to be too vague for the purpose of this study. While paragraphs are arguably better at conveying who interacts with whom, simply because of their increased length, it also brings forth an extra complexity in terms of their definition. Traditionally, paragraphs would be separated from another by means of a newline followed by an indented first line of the next paragraph. While this format holds for a part of our collection, it is not uniform. Other paragraph formats simply add vertical white space, or depend solely on the content ( ). Especially because the text files in our approach originate from different online sources\u2014each with their own accepted format\u2014we decided that the added ambiguity should be avoided. For this study, we therefore define that a co-occurrence relationship between two characters exists if they are mentioned in the same sentence. For a co-occurrence of more than two characters, we follow  . That is, a multi-way co-occurrence between four characters is broken down into six bilateral co-occurrences. \n\nFor the construction of each social network, the co-occurrences are translated to nodes for characters and edges for relationships between the characters. We thus create a static, undirected and weighted graph. For the weight of each edge, we follow  . That is, each edge is assigned a weight depending on the number of interactions between two characters. For the construction of the network, we used NetworkX ( ) and Gephi ( ) to visualise the networks. To ground the network analysis to be presented below, we gathered some overall statistics of the network creation process shown in   on page 23. As mentioned in the subsection \u2018Annotation\u2019, if the annotator decided that a character was definitely present, but unable to assert which character, the occurrence was marked as   default  . The fraction of defaults represents what portion of all identified characters was marked with   default  . The fraction of unidentified characters represents the percentage of characters that were not retrieved by the system, but had to be added by the annotators. Next, we present some overall statistics such as sentence length, the average number of persons in a sentence, and the average fraction of sentences that mention a person. Lastly, we kept track of the total number of annotated sentences, the total number of unique characters and character mentions. The only difference that could be identified between classes is the average sentence length, which was significant at   p   < 0.01. The sentences in classic books are significantly longer than in modern novels, suggesting that there is indeed some difference in writing style. However, other than that, none of the other measures differ significantly. This is useful information, as it helps support that the novels used in either class are comparable, despite their age-gap. \n\n\n### Network features \n  \nWe analyse the following eight network features:   \n Average degree   is the mean degree of all the nodes in the network. The degree of a node is defined as the number of other nodes the node is connected to. If the degree of a node is zero, the node is connected to no other nodes. The degree of a node in a social network is thus is measure of its social \u2018activity\u2019 ( ). A high value\u2014for example, in   Ulysses  \u2014indicates that the characters interact with many different other characters. Contrarily, a low value\u2014for example, in   1984  \u2014indicates that the characters only interact with a small number of other characters. \n  \n Average Weighted Degree   is fairly similar to the average degree, but especially in the sense of social networks, a distinction must be made. It differs in the sense that the weighted degree takes into account the weight of each of the connecting edges. Whereas a character in our social network could have a high degree\u2014indicating a high level of social activity\u2014if the weights of all those connected edges are relatively small, this suggests only superficial contact. Conversely, while the degree of a character could be low\u2014for example, the character is only connected to two other characters\u2014the two edges could have very large weights, indicating a deep social connection between the characters.   underlines the importance of this distinction in his work on scientific collaborations. To continue the examples of   Ulysses   and   1984  ; while their average degrees are vastly different (with Ulysses being the highest of its class and 1984 the lowest), their average   weighted   degrees are comparable. \n  \n Average Path Length   is the mean of all the possible shortest paths between each node in the network; also known as the geodesic distance. If there is no path connecting two nodes, this distance is infinite and the two nodes are part of different graph components (see item 7, Connected Components). The shortest path between two nodes can be found by using Dijkstra\u2019s algorithm ( ). The path length is typically an indication of how efficiently information is relayed through the network. A network with a low path length would indicate that the people in the network can reach each other through a relatively small number of steps. \n  \n Network Diameter   is the longest possible distance between two nodes in the network. It is in essence the longest, shortest path that can be found between any two nodes in the network, and is indicative of the linear size of the network ( ). \n  \n Graph density   is the fraction of edges compared to the total number of possible edges. It thus indicates how complete the network is, where completeness would constitute all nodes being directly connected by an edge. This is often used in social network analysis to represent how closely the participants of the network are connected ( ). \n  \n Modularity   is used to represent community structure. The modularity of a network is   \u2018...the number of edges falling within groups minus the expected number in an equivalent network with edges placed at random\u2019   ( ). Newman shows modularity can be used as an optimisation metric to approximate the number of community structures found in the network. To identify the community structures, we used the Louvain algorithm ( ). The identification of community structures in graph is useful, because the nodes in the same community are more likely to have other properties in common ( ). It would therefore be interesting to see if differences can be observed between the prevalence of communities between the classic and modern novels. \n  \n Connected components   are the number of distinct graph compartments. That is, a graph component is a subgraph in which any two vertices are connected to each other by paths, and which is connected to no additional vertices in the supergraph. In other words, it is not possible to traverse from one component to another. In most social communities, one \u2018giant component\u2019 can typically be identified, which contains the majority of all vertices ( ). A higher number of connected components would indicate a higher number of isolated communities. This is different from modularity in the sense that components are more strict. If only a single edge goes out from a subgraph to the supergraph, it is no longer considered a separate component. Modularity attempts to identify those communities that are basically \u2018almost\u2019 separate components. \n  \n Average clustering coefficient   is the mean of all clustering coefficients. The clustering coefficient of a node can perhaps best be described as \u2018all-my-neighbours-know-each-other\u2019. Social networks with a high clustering coefficient (and low average path length) may exhibit   small world   ( ) properties ( ). The small world phenomenon was originally described by Stanley Milgram in his perennial work on social networks ( ). \n  \n\n\n### Results of network analysis \n  \nTo answer our second research question, we compared the network features presented in the subsection \u2018Network Features\u2019 for the social networks of the two different sets of novels.   on page 25 shows the results. The most striking feature of these results is the wide variance across social networks on all these network measures for both the classic and the modern novels. The size of these network ranges from just 10 nodes to networks more than 50 times as large. The network size alone can also explain at least a large part of the differences in graph density, diameter, and average path length, but also average degree and clustering coefficient show wide variation. \n\nWhile we can observe large variation overall, there is no clear difference between the two classes, that is, between classic and modern novels. None of the evaluated network features differ significantly between these classes. Graph density is the feature that comes closest to being significant (  p   = 0.09), with our classic novels on average exhibiting denser networks than the modern ones. \n\nIn order to better interpret these values, and in order to find out whether this variance in network features is by itself a characteristic property of social networks exposed in novels, or whether this is true for social networks in general, we need a point for comparison. For that purpose, we compare our network results to metrics that have been reported for other social network in the literature.   shows 10 such networks for comparison, including three small networks on karate club members, football players, and email users ( ), three medium-sized networks of mathematicians, a larger group of email users, and actors ( ), and four large networks of online platforms ( ). \n   Comparison to other social networks.        \nWe can see that social networks reported elsewhere exhibit a wide variation as well, showing (unsurprisingly) an even much wider range for the network size, with the reported online social networks reaching millions of nodes. Our networks from novels are on the lower end of the size range, with the smallest ones being smaller than the smallest network of our comparison set (Karate). This directly explains why the path lengths are also on the lower end of the range, but with a considerable overlap. With respect to the average degree, our novel networks are covered by the range given by these comparison networks, with even the outliers of our dataset being less extreme than the most extreme cases of the comparison networks. The same holds for the clustering coefficient, except for the outlier for a very small network with a clustering coefficient of 0 (Alice in Wonderland). In summary, we can say that social networks from novels appear to be no different than social networks in general in showing a high variation in basically all network features across different networks. While networks differ much individually, there is no significant fundamental difference between classic and modern novels. \n\n\n### Network exploration \n  \nIn addition to the formal analysis above, we show here a more informal exploration of one of the networks in order to give a more intuitive explanation of our results. For that purpose, we selected the largest network of the modern novels, which is   A Game of Thrones  . A visualisation of that network is shown in  . We see that it is a quite dense network with many connections (it has the highest average degree of all modern novels; see  ) and a complex structure. Despite this complexity, the relationship between the main characters of this novel can easily be identified from this visualisation, and one can clearly identify social clusters. Such informal visual explorations should then of course be substantiated with formal analyses, that is, by ranking the edges of the network by their weights and by applying a clustering algorithm in the case of the two given examples. As the readers of this novel might have already spotted,   Dany   resides in a completely different part of the world in this novel, which explains her distance from rest of the network. Moreover, in   A Game of Thrones  , this character does not at any point physically interact with any of the characters in the larger cluster. This highlights a caveat of the use of co-occurrence networks over conversational networks. The character   Dany   does not truly interact with the characters of this main cluster, but is rather name- dropped in conversations between characters in that cluster. Her character \u2018co-occurs\u2019 with the characters that drop her name and edges are created to represent that. \n   Social network of G.R.R. Martin\u2019s   A Game of Thrones.     \nTo stick with the example of   Dany  , we can also identify two seemingly separate characters,   Dany   and   Daenerys Targaryen   in  . These names actually refer to the same entity. As mentioned in the section \u2018Related Work\u2019, this issue may be addressed by creating a list of aliases for each character. Some online sources exist that can help expedite this process, but we would argue these sources are not applicable to our modern novels. Whereas 19th century novels typically have characters with more traditional names such as   Elizabeth Bennet  , modern fantasy novels have unconventional names such as   Daenerys Targaryan  . External sources such as on metaCPAN  can help to connect   Elizabeth   to nicknames such as   Lizzy  , but there are no sources that can do this for   Daenerys   and   Dany  . Even if there was such a source, the question remains whether if it is desirable to collapse those characters. Especially in   A Game of Thrones  , the mentions of   Dany   and   Daenerys Targaryen   occur in entirely different contexts. Whereas references to   Dany   occur in an environment that is largely friendly towards her; her formal name of   Daenerys Targaryen   is mostly used by her enemies (in her absence). Rather than simply collapsing the two characters as one, it might be useful to be able to retain that distinction. This is a design choice that will depend on the type of research question one wants to answer by analysing the social networks. \n\n\n\n## Discussion and Performance Boosting Options \n  \nIn analysing the output of the different NER systems, we found that some types of characters were particularly difficult to recognise. Firstly, we found a number of unidentified names that are so called word names (i.e. terms that also occur in dictionaries, for example to denote nouns such as   Grace   or   Rebel  ). We suspected that this might hinder the NER, which is why we collected all such names in our corpus in   on page 21, and highlighted such word names with a \u2020. This table shows that approximately 50% of all unidentified names in our entire corpus consist at least partially of a word name, which underpins that this issue is potentially widely spread. In order to verify this, we replaced all potentially problematic names in the source material by generic English names. We made sure not to add names that were already assigned to other characters in the novel, and we ensured that these names were not also regular nouns. An example of these changed character names can be found in  , which shows all names affected for   The Black Company  . \n   Unidentified names in   The Black Company   replaced by generic English names.      \nSecondly, we noticed that persons with special characters in their names can prove difficult to retrieve. For example, names such as   d\u2019Artagnan   in   The Three Musketeers   or   Shai\u2019Tan   in   The Wheel of Time   were hard to recognise for the systems. To test this, we replaced all names in our corpus such as   d\u2019Artagnan   or   Shai\u2019Tan   with   Dartagnan   and   Shaitan  . By applying these transformations to our corpus, we found that the performances could be improved, uncovering some of the issues that plague NER. As can be observed in  , not all of the novels were affected by these transformations. Out of the 40 novels used in this study, we were able to improve the performance for 14. While the issue of the apostrophed affix was not as recurrent in our corpus as the real-word names, its impact on performance is troublesome nonetheless. Clearly, two novels are more affected by these transformations than the others, namely:   The Black Company   and the   The Three Musketeers  . To further sketch these issues, we delve a bit deeper into these two specific novels. \n   Effect of transformations on all affected classic and modern novels in   F  -score in using the BookNLP pipeline (includes co-reference resolution).    \nThese name transformations show that the real-word names and names with special characters were indeed problematic and put forth a problem for future studies to tackle. As illustrated by  , the aforementioned issues are also present in the classic novels typically used by related works (such as   The Three Musketeers  ). This begs the question of the scope of these problems. To the best of our knowledge, similar works have not identified this issue to affect their performances, but we have shown that with a relatively simple workaround, the performance can be drastically improved. It would thus be interesting to evaluate how much these studies suffer from the same issue. Lastly, as manually replacing names is clearly far from ideal, we would like to encourage future work to find a more robust approach to resolve this issue. \n\n### The Black Company \n  \nThis fantasy novel describes the dealings of an elite mercenary unit\u2014  The Black Company  \u2014and its members, all of which go by code names such as the ones in  . With a preliminary   F  -score of 06.85 (see  ),   The Black Company   did not do very well. We found this book had the highest percentage of unidentified characters of our collection. Out of the 14 characters found by our annotators, only five were identified by the pipeline. Interestingly enough, eight out of the nine unidentified characters in this novel have names that correspond to regular nouns. By applying our name transformation alone, the   F  -score rose from 06.85 to the highest in our collection to 90.00. \n\n\n### The Three Musketeers \n  \nThis classic piece recounts the adventures of a young man named   d\u2019Artagnan  , after he leaves home to join the Musketeers of the Guard. With an   F  -score of 13.91 (see  ),   The Three Musketeers   performs the second worst of our corpus, and the worst in its class. By simply replacing names such as   d\u2019Artagnan   with   Dartagnan   the   F  -score rose from 13.91 to 53, suggesting that the apostrophed name was indeed the main issue. To visualise this, we have included figures of both   The Three Musketeer   networks\u2014before and after the fix\u2014in   and  . As can be observed in  , the main character of the novel is hardly represented in this network, which is not indicative of the actual story. The importance of resolving the issue of apostrophed named is made clear in  , where the main character is properly represented. \n   Social network of   The Three Musketeers   without adjustment for apostrophed names.       Social network of   The Three Musketeers   with adjustment for apostrophed names.    \n\n\n## Conclusion and Future Work \n  \nIn this study, we set out to close a gap in the literature when it comes to the evaluation of NER for the creation of social networks from fiction literature. In our exploration of related work, we found no other studies that attempt to compare networks from classic and modern fiction. To fill this gap, we attempted to answer the following two research questions:\n   \nTo what extent are off-the-shelf NER tools suitable for identifying fictional characters in novels? \n  \nWhich differences or similarities can be discovered between social networks extracted for different novels? \n  \n\nTo answer our primary research question, we evaluated four state-of-the-art NER systems on 20 classic and 20 modern science fiction/fantasy novels. In our study, we found no significant difference in performance of the named entity recognisers on classic novels and modern novels. We did find that novels written in 3rd person perspective perform significantly better than those written in 1st person, which is in line with findings in related studies. In addition, we observed a large amount of variance within each class, even despite our limitation for the modern novels to the fantasy/science fiction genre. We also identified some recurring problems that hindered NER. We delved deeper into two such problematic novels, and find two main issues that overarch both classes. Firstly, we found that word names such as   Mercy   are more difficult to identify to the systems. We showed that replacing problematic word names by generic placeholders can increase performance on affected novels. Secondly, we found that apostrophed names such as   d\u2019Artagnan   also prove difficult to automatically identify. With fairly simple methods that capture some cultural background knowledge, we circumvented the above two issues to drastically increase the performance of the used pipeline. To the best of our knowledge, none of the related studies discussed in the section \u2018Related Work\u2019 acknowledge the presence of these issues. We would thus like to encourage future work to evaluate the impact of these two issues on existing studies, and call to develop a more robust approach to tackle them in future studies. \n\nTo answer our secondary research question, we created social networks for each of the novels in our collection and calculated several networks features with which we compared the two classes. As with the NER experiments, no major differences were found between the classic and modern novels. Again, we found that the distribution of network measures within a class was subject to high variance, which holds for our collection of both classic and modern novels. We therefore recommend that future work focuses on determining particular characteristics that can influence these analyses first and then perform a comparative analysis between subsets to see if this similarity between classes holds when the variance is reduced. Future studies could therefore attempt to compare classic and modern novels in the same genre or narration type (e.g. first-person vs third-person perspective). Lastly, different types of networks that for example collapse characters that occur under different names (cf. Dany and Daenerys) as well as dealing with plural pronouns and group membership (e.g. characters sometimes mentioned individually and sometimes as part of a group) are currently unsolved problems for language technology and knowledge representation. These issues point to a strong need for more culturally-aware artificial intelligence. \n\n\n## Appendix: Additional Statistics \n     Characters that were not identified by the system, supplied by the annotators.           Classic and modern novels included in this study.           Overall statistics for classic and modern novels in our corpus.           Results of the complete BookNLP pipeline: Named entity recognition (Stanford NER), Character name clustering (e.g. \u2018Tom\u2019, \u2018Tom Sawyer\u2019, \u2018Mr. Sawyer\u2019, \u2018Thomas Sawyer\u2019 \u2192 TOM_SAWYER) and Pronominal coreference resolution.           Social network measures for classic and modern novels.        \n \n", "metadata": {"pmcid": 7924459, "text_md5": "583301a6eb69860eabf861fbf828602a", "field_positions": {"authors": [0, 51], "journal": [52, 68], "publication_year": [70, 74], "title": [85, 169], "keywords": [183, 296], "abstract": [309, 1530], "body": [1539, 51309]}, "batch": 1, "pmid": 33816842, "doi": "10.7717/peerj-cs.189", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7924459", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7924459"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7924459\">7924459</a>", "list_title": "PMC7924459  Evaluating named entity recognition tools for extracting social networks from novels"}
{"text": "Cho, Hyejin and Lee, Hyunju\nBMC Bioinformatics, 2019\n\n# Title\n\nBiomedical named entity recognition using deep neural networks with contextual information\n\n# Keywords\n\nText mining\nNamed entity recognition\nNeural networks\nLong short-term memory\nContextual information\n\n\n# Abstract\n \n## Background \n  \nIn biomedical text mining, named entity recognition (NER) is an important task used to extract information from biomedical articles. Previously proposed methods for NER are dictionary- or rule-based methods and machine learning approaches. However, these traditional approaches are heavily reliant on large-scale dictionaries, target-specific rules, or well-constructed corpora. These methods to NER have been superseded by the deep learning-based approach that is independent of hand-crafted features. However, although such methods of NER employ additional conditional random fields (CRF) to capture important correlations between neighboring labels, they often do not incorporate all the contextual information from text into the deep learning layers. \n\n\n## Results \n  \nWe propose herein an NER system for biomedical entities by incorporating n-grams with bi-directional long short-term memory (BiLSTM) and CRF; this system is referred to as a contextual long short-term memory networks with CRF (CLSTM). We assess the CLSTM model on three corpora: the disease corpus of the National Center for Biotechnology Information (NCBI), the BioCreative II Gene Mention corpus (GM), and the BioCreative V Chemical Disease Relation corpus (CDR). Our framework was compared with several deep learning approaches, such as BiLSTM, BiLSTM with CRF, GRAM-CNN, and BERT. On the NCBI corpus, our model recorded an F-score of 85.68% for the NER of diseases, showing an improvement of 1.50% over previous methods. Moreover, although BERT used transfer learning by incorporating more than 2.5 billion words, our system showed similar performance with BERT with an F-scores of 81.44% for gene NER on the GM corpus and a outperformed F-score of 86.44% for the NER of chemicals and diseases on the CDR corpus. We conclude that our method significantly improves performance on biomedical NER tasks. \n\n\n## Conclusion \n  \nThe proposed approach is robust in recognizing biological entities in text. \n\n \n\n# Body\n \n## Background \n  \nWith the increasing number of biomedical articles and resources, searching for and extracting valuable information has become challenging [ ]. Researchers consider multiple information sources and transform unstructured text data into refined knowledge to facilitate research productivity [ ,  ]. However, manual annotation and feature generation by biomedical experts are inefficient because they involve a complex process and require expensive and time-consuming labor [ ]. Therefore, efficient and accurate natural language processing (NLP) techniques are becoming increasingly important for use in computational data analysis, and advanced text mining techniques are necessary to automatically analyze the biomedical literature and extract useful information from texts [ \u2013 ]. \n\nFor extracting valuable information, such as relationships among objects, the identification of significant terms from texts is important. Meaningful terms or phrases in a domain, which can be distinguished from similar objects, are called named entities, and named entity recognition (NER) is one of the important tasks for automatically identifying these named entities in text and classifying them into pre-defined entity types [ ,  ]. NER should be performed prior to tasks, such as relation extraction, because annotated mentions play an important role in research on text mining. In the biological domain, a fundamental task of biomedical NLP is the recognition of named entities, such as genes, diseases, chemicals, and drug names, from texts. However, biomedical NER is a particularly complex task because biological entities (i) continually increase with new discoveries, (ii) have large numbers of synonyms, (iii) are often referred to using abbreviations, (iv) are described by long phrases, and (v) are mixtures of letters, symbols, and punctuation [ ,  ]. Several approaches have been proposed to solve these problems [ ]. \n\nMost early methods for biomedical NER relied on dictionary- or rule-based approaches. NER systems using a dictionary-based method extract named entities in pre-defined dictionaries that consist of large collections of names for each entity type. Another NER system, using the rule-based approach, recognizes named entities by means of several rules that are manually defined based on their textual patterns [ ,  ,  ]. The majority of these traditional approaches have shown significant improvements in terms of coverage and robustness, but rely heavily on a set of words in well-defined dictionaries and hand-crafted rules. Moreover, although relatively well-constructed dictionaries are available for common biological entities, such as disease and gene names, dictionaries for many other biological entities are not comprehensive or adequate [ ]. In the case of rule-based methods, pre-defined patterns also depend on the specific textual properties of an entity class. In other words, entity-specific dictionaries and patterns require time-consuming processes and expert knowledge [ ,  ]. \n\nTo address the shortcomings of past approaches, traditional NER methods have been replaced by supervised machine learning methods, including hidden Markov models, maximum entropy Markov models, conditional random fields (CRFs), and the support vector machine [ \u2013 ]. Furthermore, machine learning methods are often combined with various others to yield hybrid approaches that are more accurate [ ,  ]. Although most machine learning approaches have led to significant improvements in NER, and despite several general-purpose NER tools based on machine learning methods being available, they are still limited in terms of reliance on hand-crafted features and human labor for feature engineering [ \u2013 ]. \n\nDeep learning approaches using a large number of unstructured data items have lately drawn research interest and have been applied to NLP problems with considerable success. For NER tasks in the biomedical domain, a domain-independent method based on deep learning and statistical word embeddings, such as the bi-directional long short-term memory network (BiLSTM) with CRF and GRAM-CNN, has been shown to outperform state-of-the-art entity-specific NER tools such as a disease-specific NER tool DNorm and a chemical-specific NER tool ChemSpot [ ,  ,  \u2013 ]. Recently, Devlin et al. proposed a new architecture named BERT [ ] for NLP. BERT (Bi-directional Encoder Representations from Transformers) is a deep bi-directional pre-trained self-attention model by the Transformer [ ] and uses more than 2.5 billion words for pre-training the model and obtains new state-of-the-art results on various NLP tasks, including NER. \n\nFor machine learning, contextual information has already been demonstrated to lead to significant improvements [ ]. Context representations usually define a collection of neighboring word embeddings in a window around the target word or an average of these window-based embeddings [ ]. We propose herein an NER system designed to more explicitly deal with contextual information in text. The architecture of our system focuses on capturing important local contexts based on n-gram characters and word embeddings via BiLSTM and CRF. The performance of our model, Contextual LSTM with CRF (CLSTM), is evaluated using three biomedical corpora and various assessment methods. \n\n\n## Results \n  \n### Data sources \n  \n#### Corpora \n  \nWe used three kinds of corpora to train and test the NER models, where each contained manual annotations for one or more entity types. The corpora were the National Center for Biotechnology Information (NCBI) disease corpus for disease names [ ], the BioCreative II Gene Mention (GM) corpus for gene names [ ], and the BioCreative V Chemicals Disease Relationship (CDR) corpus for both disease and chemical names [ ]. The corpora consist of a training set, a development set, and a test set, which were respectively used to construct the models, determine the optimal parameters for models, and evaluate the models. Table\u00a0  lists the sizes of the corpora. We represented a sequence of labels in the IOB format (inside, outside, beginning), indicating that each token was at the beginning of an entity as a B-label, inside an entity as I-label, or outside it as an O-label. In this case, the labels simultaneously incorporated the type of named entity, such as disease or chemical, with the position of the token within the entity.\n   \nStatistics of the NCBI, GM, and CDR corpora \n  \n\n NCBI   We used the NCBI corpus for the disease NER task. The NCBI disease corpus is the gold standard of disease name recognition. It is a manually annotated resource for biomedical text created and curated by a team of 14 annotators. It consists of 793 PubMed abstracts and 6892 disease mentions, with 790 unique disease concepts mapped to MeSH and OMIM identifiers. \n\n GM   We used the GM corpus for the gene NER task. The second BioCreative challenge was held in 2006 and consisted of three tasks: gene mention, gene normalization, and protein\u2013protein interaction. The entire corpus consisted of 20,000 sentences and a set of gene mentions and their alternative annotations judged by human annotators. This corpus did not contain a development set; hence, we randomly divided the training set into two parts to create the development corpus. \n\n CDR   We used the CDR corpus for the disease and chemical NER task. The BioCreative V challenge was organized for CDR tasks based on disease named entity recognition (DNER) and chemically induced disease (CID) relation extraction tasks. It is composed of 1500 articles with 4409 annotated chemical names, 5818 disease names, and 3116 CID relations. This corpus has become a valuable resource for research on text mining. \n\n\n#### Parameters \n  \nPretrained word embeddings are beneficial over random initializations in several NER tasks. Pyysalo et al. [ ] trained the embedding model using approximately 23 million PubMed abstracts and nearly 700,000 PubMed Central full-text articles. We initialized our word representation using those trained by Pyysalo et al. We used 200 embedding dimensions with the skip-gram model at a window size of five [ ]. These embeddings were fine-tuned during training. In experiments for BiLSTM, BiLSTM-CRF, and CLSTM, we used default values from Lample   et al   [ ], except for three hyperparameters: (i) the tag scheme, which we set to the IOB scheme instead of IOBES; (ii) the number of dimensions of token embeddings and the size of the token LSTM hidden layer, which we set to 200 instead of 100; and (iii) pretrained embeddings, which we set to our embeddings instead of being none. For GRAM-CNN and BERT, we trained each model with its own default parameters. \n\n\n\n### Evaluation \n  \nFor comparative evaluation, we used BiLSTM without the CRF layer, BiLSTM-CRF [ ], GRAM-CNN [ ], and BERT [ ]. For the comparison, we trained our CLSTM models on each corpus with one of three training options, called word-level model, character-level model, and word+char model. In the word- and character-level CLSTM models, window sizes at the word level or at the character level are needed, respectively. For the word+char CLSTM model, window sizes both at the character and word levels are required. To obtain the proper window size of each model, we used development sets. Odd numbers of window sizes, such as 3, 5, and 7, were used as candidate sizes to have equal context information for the left and right sides of the target word. Using the development sets, for the word-level CLSTM model, we decided on a window size of 5 for all three corpora. Similarly, for character-level CLSTM models, we decided on window sizes as 3, 5, and 7 for NCBI, GM, and CDR corpora, respectively. The word+char CLSTM model for NCBI used a window size of 5 for both the word and character levels, and the optional values for GM and CDR were set to 3 for both the word and character levels. \n\nUsing the test sets, we compared all methods in terms of precision, recall, and F-score. We performed strict matching at the IOB token level and strict and partial matching at the level of mention to compute these values. We counted the true positives (  TP  ), false positives (  FP  ), and false negatives (  FN  ). The evaluation was based on measures of precision (  p  ), recall (  r  ), their harmonic average, and the F-score (  f  ), as follows:\n \n\nTable\u00a0  shows the prediction performances over all corpora in terms of precision, recall, and F-score using three evaluation methods (i.e. the strict matching, the partial matching, and the IOB tag matching). The first four rows in each table show the performance of other methods, while the last three rows show the results of the CLSTM models. F-scores of CLSTM that outperformed the comparative methods are marked in bold.\n   \nComparison of performance for comparative methods on the NCBI, GM, and CDR corpora using strict and partial matching and IOB tag matching \n  \n\n Strict matching   When start and end boundaries and the type of a predicted mention and those of a gold standard mention are identical, it is considered correct prediction. This evaluation criterion evaluates tag units as one result that recognizes mentions from the B-tag to its end. On the NCBI corpus, our model with word+char levels attained an F-score of 85.68%, which is a 1.5% improvement over the previous methods. Among the previous methods, GRAM-CNN achieved the best F-score of 84.18%. Moreover, word-level CLSTM and character-level CLSTM also obtained results (85.31% and 84.72%, respectively) better than those of the comparative approaches. On the GM corpus for the gene NER, our CLSTM yielded an F-score of 81.44%. Although BERT improved the F-score by 0.21% compared with CLSTM (81.65% vs. 81.44%, respectively), this difference was slight considering that BERT incorporates other huge datasets as well as the GM corpus. On the CDR corpus, the word+char levels CLSTM model had an F-score of 86.44% for chemicals and disease NER. As for the CDR corpus, when we assessed the results using the strict matching, all results of the CLSTM with word and character and word+char levels outperformed those of the previous method (86.36%, 85.92% and 86.44%, respectively). \n\n Partial matching   When start and end boundaries of a predicted mention and those of a gold standard mention are overlapping, and types of the prediction and the gold standard are the same, and it is considered correct prediction. When this evaluation criterion was used, all models yielded F-scores higher than those obtained using other evaluation criteria (i.e., strict and IOB tag matchings). Although our model recorded a slightly inferior performance to GRAM-CNN on the NCBI corpus, our NER model achieved the best F-scores for GM and CDR corpora. Among previous methods, BiLSTM-CRF and GRAM-CNN achieved the best F-score on the GM and CDR corpora, respectively. \n\n IOB tag matching   We further assessed the performance of our method on the three corpora at the level of tokens. For each IOB tag, the agreement between prediction and the gold standard tag is assessed. This procedure involves comparing the results of the gold standard tags with those of the predicted tags at the token level. This evaluation depends on the lengths of the mentions. On the NCBI corpus, our model with word-level layers attained an F-score of 89.10%, which shows a 1.45% improvement over the previous methods. Among the previous methods, GRAM-CNN achieved the best F-score (87.65%). Moreover, all results of the CLSTM (88.34% and 88.84% for the character-level CLSTM and word+char levels CLSTM, respectively) outperformed other approaches. Similar to the strict matching, the BERT model on the GM corpus improved the F-score compared with the proposed model. On the CDR corpus, the word+char levels CLSTM model represents a maximum F-score of 86.83%, which improves the F-score by 0.02% compared with the previous method (86.83% vs. 86.81%). From Table\u00a0 , all results of the CLSTM on the GM and CDR corpus outperformed those of the previous methods, except for BERT. \n\n\n### Model robustness \n  \nCharacter vectors were randomly initialized for every character, and word vectors that do not have an embedding in the lookup table were mapped to a UNK embedding before being entered into the model [ ]. Therefore, the performance of our models might depend on the random initialization of weights. Thus, we independently trained the CLSTM model five times and analyzed the results by applying strict matching to estimate the robustness of our models with respect to initialization. \n\nTable\u00a0  shows the performance comparison between our CLSTM model for all five trials and the other methods on the NCBI, GM, and CDR corpora. For each method and corpus, we used optimal hyperparameters obtained from development sets. In the NCBI corpus, although the best score of the other methods yielded an F-score of 84.18%, our model achieved the best F-score of 85.68% and the worst F-score of 85.02%. Thus, the worst performance of the CLSTM model was better than that of GRAM-CNN with a difference of 0.84%. Despite recording a slightly inferior performance compared with BERT on the GM corpora, our NER model was better than all other comparative models. In the CDR corpus, our models also outperformed all other methods, which were similar with the performance on the NCBI corpus. Therefore, the results confirm the superiority of our model, regardless of the randomness of initialization.\n   \nComparison between a series of CLSTM (contextual long short-term memory networks [LSTMs] with conditional random fields [CRF]) experiments and the comparative methods on the NCBI, GM, and CDR corpora using strict matching \n  \n\n\n\n## Discussion \n  \n### Error analysis \n  \nWe analyzed error cases on the test corpora and classified them into several cases as follows:\n   \nThe entity boundary is not clear due to adjective phrases: For example, our model annotated \u201cfemale breast cancer\u201d and \u201cidiopathic hemolytic uremic syndrome\u201d as disease entities. However, disease mentions in the NCBI test set were \u201cbreast cancer\u201d and \u201chemolytic uremic syndrome\u201d, respectively. On the other hand, although disease mentions in the NCBI test set were \u201cnon-inherited breast carcinomas\u201d, \u201csporadic T-cell leukaemia\u201d, and \u201cdominantly inherited neurodegeneration\u201d, our model predicted \u201cbreast carcinomas\u201d, \u201cT-cell leukaemia\u201d, and \u201cneurodegeneration\u201d, respectively. \n  \nElliptical coordinated compound noun phrases are used: This case is a kind of coordinate structures, where two or more words of the same type are combined into a larger phrase with the same semantic relation [ , ]. For example, names such as \u201cpineal tumours and retinal tumours\u201d and \u201ccolorectal adenomas and/or colorectal carcinoma\u201d are often described in biomedical abstracts as \u201cpineal and retinal tumours\u201d and \u201ccolorectal adenomas and/or carcinoma\u201d to avoid word repetition. Moreover, they were annotated as a single entity in the NCBI test set. For these cases, our model predicted their entity boundaries as \u201ctumours\u201d in the first example, and \u201ccolorectal adenomas\u201d and \u201ccarcinoma\u201d, respectively, in the second example. \n  \nEntity contains brackets: This case often happens when an entity name and its acronym appear together with brackets. For example, \u201c62-kDa protein (p62)\u201d, which contains a gene name, its acronym \u201cp62\u201d and brackets, was annotated as a single gene mention in the GM test set. However, CLSTM separately predicted gene mentions as \u201c62-kDa protein\u201d and \u201cp62\u201d without brackets. \n  \nDifferent entity types are predicted: When an entity type is nested in another entity type, the different entity type was predicted. This is more likely to happen when multiple entity types are predicted at the same time. For example, \u201cserotonin syndrome\u201d in the CDR test set was annotated as a disease mention. However, our model predicted \u201cserotonin\u201d as a chemical entity. Another example is that although \u201chepatitis B surface antigen\u201d was annotated as a chemical type, our model predicted \u201chepatitis B\u201d as a disease type. \n  \nThe corpus annotation inconsistency: The same disease was annotated differently in the same corpus. For example, \u201ctype I autosomal dominant cerebellar ataxia\u201d was annotated as a disease mention in the NCBI corpus (PubMed ID: 7573040). However, in \u201cEye movement abnormalities correlate with genotype in autosomal dominant cerebellar ataxia type I (PubMed ID: 9506545),\u201d only \u201ccerebellar ataxia type I\u201d was annotated as a disease mention, and did not include \u201cautosomal dominant.\u201d In the latter case (PubMed ID: 9506545), our model predicted \u201cautosomal dominant cerebellar ataxia type I.\u201d \n  \n\nThe above analysis shows that some NER errors occurred due to various forms of entity mentions, and usually occurred in entity boundaries. For example, when we mannally examined false positives on the NCBI corpus, we found that 35.3% and 9.3% of NER errors were due to entity boundaries and elliptical coordination errors, respectively. Thus, it is important to develop the NER model to resolve these ambiguities. \n\n\n### Cross-corpus evaluation \n  \nWe performed cross-corpus evaluation between the NCBI and the CDR corpora. We tested the disease entities in the CDR corpus using the model trained on the NCBI disease corpus, and also tested mentions in the NCBI disease corpus using the model trained on the CDR corpus. \n\nTable\u00a0  shows that our CLSTM model had a higher F-score than those of other models except BERT. Although the precision of the CLSTM model was higher than that of BERT, BERT had higher recall values and F-scores. The high recall values may be because BERT has already been pre-trained with huge volumes of data from general datasets. Thus, although the guidelines for constructing two corpora of disease mentions (NCBI and CDR corpora) are different in terms of determining disease mentions, BERT can have a high recall value. For constructing each corpus, the authors of NCBI used the 2012 version of MEDIC, which integrated both OMIM and MeSH disease terms. On the other hand, the authors of CDR used the 2015 version of MeSH terms and annotated disease mentions with a \u2018-1\u2019 identifier (ID), even if the ID mapping for disease mentions is not possible. For example, although \u201cpain\u201d and \u201cnecrosis\u201d in the CDR corpus were treated as disease mentions with \u201cD010146\u201d and \u201cD009336\u201d, respectively, these words were not annotated in the NCBI corpus. To examine the difference between two corpora, we counted disease mentions annotated in the NCBI test data, but not annotated as disease mentions in the CDR training data despite appearing in the sentences of the CDR corpus, and vice versa. We found 19 such mentions out of 960 mentions in the NCBI corpus, and 83 out of 10,875 mentions in the CDR corpus. Although our model correctly predicted 4 and 7 mentions in each corpus, BERT correctly predicted 11 and 41 mentions for NCBI and CDR corpus, respectively. It implies that the CLSTM model is more likely to reflect characteristics of the training data than BERT. Thus, even though our model may have lower recall values than BERT, it demonstrated higher precision. Note that as each corpus had different optimal window sizes from development sets, we tried several window sizes in Table\u00a0 .\n   \nComparison of the performance of cross-corpus evaluation for comparative methods using strict matching \n  \nTest the disease entities in the NCBI corpus using the model trained on the CDR corpus \n\nTest the disease entities in the CDR corpus using the model trained on the NCBI corpus \n\nThe number in parentheses represents the window size at the character level. \n\nThe numbers in parentheses represent the window sizes at the word and character level, respectively \n  \n\n\n### Computational time \n  \nWe measured computational time for CLSTM and for the comparative models. We ran all models in a hexa-core workstation using an i7-5930K CPU and a Titan Xp GPU with 12G memory and set a default training epoch of 100 on each dataset. Table\u00a0  shows the training time of each model on three datasets. Overall, the execution time was determined in proportion to the size of the data. The fastest method was BERT, followed by BiLSTM, and GRAM-CNN had the longest training time. We observed that the character-level CLSTM model had relatively faster training time than other CLSTM models because the character embedding dimension of CLSTM was smaller than the word and word+char embedding dimensions of CLSTM. However, the CLSTM model required 20% longer training times than the BiLSTM-based models. The reason for the superior speed of BERT compared with the other methods is that BERT is a fine-tuning system and does not require the training of a deep neural network from scratch. However, the original pre-training of BERT took 4 days [ ].\n   \nComparison of training time between CLSTM (contextual long short-term memory networks [LSTMs] with conditional random fields [CRF])and comparative methods for the NCBI, GM, and CDR corpora \n  \n\n\n\n## Conclusions \n  \nIn this study, we investigated neural architectures with contextual information for biomedical named entity recognition based on various corpora and word embeddings. The experimental results show that our system outperforms several other NER approaches and exhibits similar performance to the transfer learning approach. The results of this study will help to make biomedical text mining more accurate and more robust irrespective of the entity type. \n\n\n## Methods \n  \n### CLSTM \n  \nThis section provides a brief description of the architecture of our CLSTM model. We provide details of the model from scratch. \n\n#### LSTM \n  \nRecurrent neural networks (RNNs) are specially designed to process sequential data. They represent connections between previously occurring hidden states and a given hidden state, and thus reflect the network\u2019s historical information. While the RNN is a simple and powerful model in theory, it cannot capture long-term dependencies because of problems of vanishing and exploding gradients, where the gradients may exponentially decline and grow over long sequences [  \u2013  ]. \n\nLong short-term memory networks (LSTMs) [ ] are variants of the RNN applied to a memory cell to learn long-term dependencies. An LSTM unit is composed of three gates: an input gate, a forget gate, and an output gate. These gates control the amount of information for the network to remember and forget for the next time step. \n\nIn sequence-labeling tasks like NER, determining the contexts in sentences, where both past and future contexts are useful, is important. However, standard LSTMs can use only previous contexts without future information. Graves et al. [ ] introduced a BiLSTM model, the basic idea of which is to describe each sequence in the forward and reverse directions to two separate layers. Two hidden states,   and  , are then concatenated to represent the final output. For an input sentence (  x  ,  x  ,\u2026,  x  ) containing   n   words, an LSTM computes a left representation   of the given sentence at every word   t  . Similarly, a representation of the right context   can be achieved from the same sequence in reverse. As a result, BiLSTM yields the representation of a word by concatenating the outputs of its left and right contexts,   [ , , ]. \n\n\n#### CRF \n  \nNER can be considered a sequence-labeling problem, which means that words in a given sentence are tokens to be assigned proper labels. For sequence-labeling tasks, considering correlations represented by the best joint probability between adjacent labels and the entire sequence of labels is beneficial. Therefore, we jointly decode label sequences using a CRF layer instead of independently modeling tagging decisions [ , , , ]. \n\nFormally, we use   x  =(  x  ,  x  ,\u2026,  x  ) to represent an input sequence, where   x   is the input vector of the   i  -th word, and   y  =(  y  ,  y  ,\u2026,  y  ) represents a sequence of predicted labels for input   x  . All components   y   of   y   are assumed to range over a set   L  (  x  ), which is a possible labeling sequence for   x  . The global feature of CRF,   F  (  y  ,  x  ), is the summation of CRF\u2019s local feature vector   f  (  y  ,  x  ,  i  ) for input sequence   x   and label sequence   y  , where   i   ranges over input positions. The probabilistic model for the CRF defines a conditional probability   p  (  y  |  x  ,  \u03bb  ) over all possible sequences of labels   y  , given   x   and weight vector   \u03bb   in the following form:\n \n\nwhere   is a normalization factor. \n\n\n#### N-gram \n  \nIn linguistics, an n-gram is a sub-sequence of   n   contiguous items extracted from a given text. Although the items can be of various types, such as characters and words in text as well as base pairs of DNA sequences and amino acids of protein sequences, we consider herein only the text data of natural language processing. Character-level n-grams represent n-character slices of a word, while word-level n-grams represent n-word slices of a sentence. For example, word-level bi-grams (  n  =2) in the phrase \u201cbiomedical named entity recognition\u201d are \u201cbiomedical named,\u201d \u201cnamed entity,\u201d and \u201centity recognition.\u201d Similarly, character-level tri-grams (  n  =3) in the word \u201cdisease\u201d are \u201cdis,\u201d \u201cise,\u201d \u201csea,\u201d \u201ceas,\u201d and \u201case.\u201d N-gram models are robust at statistically modeling language and at natural text processing without relying on language-specific resources [ , ]. \n\n\n#### CLSTM \n  \nTo utilize contextual information in several NLP tasks, neural network-based algorithms that incorporate a large amount of unlabeled data [  \u2013  ] and neural network models such as BiLSTM that capture contextual information in an input text have been developed. In this study, to utilize more contextual information contained in sentences, we introduce the contextual long short-term memory networks with the CRF (CLSTM) model, which maximizes benefits of BiLSTM-CRF [ ] and n-gram models for contextual information. While BiLSTM represents a certain target word or a character using an input vector of itself, CLSTM represents it by concatenating input vectors of its neighbors and itself. Figure\u00a0  shows the architecture of the CLSTM model, which has the following major components: (i) a character-embedding layer, where each character in an input text is mapped to a character embedding; (ii) a character-level CLSTM layer, where character embedding vectors are input and character embedding vectors are output with the output character vector created by concatenating its left and right character embeddings within a pre-defined window size; (iii) a word-embedding layer in which each word in an input text is mapped to a word vector composed of concatenation of pretrained word vectors and the character-level representation; (iv) a word-level CLSTM layer that uses word vectors as input and output and, in a similar manner to the character level, the output is formed by concatenating its left and right word embeddings within a pre-defined window size; and (v) a label prediction layer in which for each word in the input text, the final CRF layer predicts proper entity labels based on the sequence of probabilities.\n   \nPipeline of the CLSTM (contextual long short-term memory networks [LSTMs] with conditional random fields [CRF])model \n  \n\nFor the word-level layers, we split a sentence into words by white spaces and punctuation marks such as commas and hyphens. An input sentence   S   consisting of split words   w   is represented as   S  =[  w  ,  w  ,...,  w  ]. By representing   w  =  w  \u2295  w  \u2295...\u2295  w  , where \u2295 is the concatenation symbol, {  w  ,  w  ,...,  w  } is then used as the input of the word-level layer for the window size   d  . However, it cannot be well defined for words near the beginning and the end of the word. Therefore, we augment these embeddings to deal with the border effect [ ]. We concatenate \u230a  d  /2\u230b paddings to the beginning and the end of the input of the CLSTM layer. For example, when the window size   c   is 3, the length of the word with paddings becomes   n  +(\u230a  d  /2\u230b\u22172)=  n  +2, and a new input is given as   S  =[  w  ,  w  ,  w  ,...,  w  ,  w  ], where   w   and   w   are paddings. This summarizes the contextual information of words in the input text. Similarly, the character-level representation of each word is computed by the CLSTM layers using character embeddings. \n\nThe CLSTM memory cell at the time step   t   is implemented as follows:\n \n\n\n\n\n\n\n\n\n\nwhere   is the concatenation of character embeddings and the concatenation of word embeddings for the character-level CLSTM layer and word-level CLSTM layer, respectively;   d   is the pre-defined window size;   h   is the hidden state at time   t  ;   W   is the weight matrix;   b   is the bias vector;   \u03c3   is the sigmoid function; tanh is the hyperbolic tangent function; and the \u2299 operation denotes element-wise multiplication. We apply herein a variation of the LSTM unit to use coupled input and forget gates [ ]. \n\nFinally, the output vectors of CLSTM layers are fed to the CRF layer to jointly decode the best label sequence. For the CRF layer, we use a state transition matrix to predict the tag at any given time. We denote by   T   a transition matrix and   T   a transition score from the   i  -th tag to the   j  -th tag. For a given sentence   x  =(  x  ,  x  ,\u2026,  x  ), we denote by   P   the score matrix of the outputs of the CLSTM hidden layers. The   P   represents the score of the   j  -th tag at the result of the   i  -th word in the given sentence   x  . For a sequence of predicted labels   y  =(  y  ,  y  ,\u2026,  y  ), the sum of scores from the LSTM networks along with the transition scores gives the final score of the sentence   x   and a sequence of predictions   y  . The final score can be expressed as follows:\n \n\nwhere   y   and   y   are the start and end tags of a sentence, respectively [ ]. \n\n\n\n \n", "metadata": {"pmcid": 6935215, "text_md5": "10cfd1f74de080460a1b5cda51b03a05", "field_positions": {"authors": [0, 27], "journal": [28, 46], "publication_year": [48, 52], "title": [63, 153], "keywords": [167, 266], "abstract": [279, 2277], "body": [2286, 34225]}, "batch": 1, "pmid": 31881938, "doi": "10.1186/s12859-019-3321-4", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6935215", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6935215"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6935215\">6935215</a>", "list_title": "PMC6935215  Biomedical named entity recognition using deep neural networks with contextual information"}
{"text": "Ros\u00e1rio-Ferreira, N\u00edcia and Guimar\u00e3es, Victor and Costa, V\u00edtor S. and Moreira, Irina S.\nBMC Bioinformatics, 2021\n\n# Title\n\nSicknessMiner: a deep-learning-driven text-mining tool to abridge disease-disease associations\n\n# Keywords\n\nDisease-disease associations\nNatural language processing\nBiomedical text-mining\nDeep learning\nBlood cancers\n\n\n# Abstract\n \n## Background \n  \nBlood cancers (BCs) are responsible for over 720\u00a0K yearly deaths worldwide. Their prevalence and mortality-rate uphold the relevance of research related to BCs. Despite the availability of different resources establishing Disease-Disease Associations (DDAs), the knowledge is scattered and not accessible in a straightforward way to the scientific community. Here, we propose SicknessMiner, a biomedical Text-Mining (TM) approach towards the centralization of DDAs. Our methodology encompasses Named Entity Recognition (NER) and Named Entity Normalization (NEN) steps, and the DDAs retrieved were compared to the DisGeNET resource for qualitative and quantitative comparison. \n\n\n## Results \n  \nWe obtained the DDAs via co-mention using our SicknessMiner or gene- or variant-disease similarity on DisGeNET. SicknessMiner was able to retrieve around 92% of the DisGeNET results and nearly 15% of the SicknessMiner results were specific to our pipeline. \n\n\n## Conclusions \n  \nSicknessMiner is a valuable tool to extract disease-disease relationship from RAW input corpus. \n\n\n## Supplementary Information \n  \nThe online version contains supplementary material available at 10.1186/s12859-021-04397-w. \n\n \n\n# Body\n \n## Background \n  \nSystems biology takes a holistic view on understanding biological systems, granting researchers an exceptional opportunity to dwell on previously scattered associations. Moreover, integrating knowledge from different sources on multiple diseases facilitates the understanding of Disease-Disease Associations (DDAs). The extraction of patterns from the evidence in relation to multiple diseases can ultimately establish networks encompassing such diseases. DDAs can be of various types and can be established considering different criteria as reviewed by Al-Eliwi and co-workers, all benefiting from network analysis approaches [ ]. However, establishing DDAs using experimental data based on gene-disease associations can be tiresome, costly, and complex [ ]. Hence, several strategies were proposed like: (i) Disease Ontology (DO), which integrates concepts from a plethora of sources to define related pathologies [ ] or (ii) DisGeNET that defines DDAs as diseases that share, at least, a common gene or variant among the gene-disease associations considered in the database. DisGeNET comprises data retrieved from several approaches as expert curated resources, animal models, inferred data from Human Phenotype Ontology (HPO) and variants-related resources, and previously mentioned gene-disease associations [ ]. \n\nIn this work, our goal was to retrieve DDAs while lifting the strain on establishing or possessing prior gene-disease lists, and we concentrated herein on cancer with a particular focus on Blood Cancers (BCs). Cancer is a complex and multifactorial disease for which there is a widespread availability of related literature [ ], useful for biomarker research [ ], drug discovery [ ], biological pathways detection [ ], among others, highlighting this technique's positive contribution to a highly demanding field of study. The strength of mining the literature towards building disease networks was tapped into on multiple occasions as recently reviewed by Rodr\u00edguez-Gonz\u00e1lez [ ]. In particular, BCs or hematologic cancers, which affect the production and function of blood cells encompassing the leukemia, lymphoma and multiple myeloma families, represent a large percentage of overall detected cancers (higher than 13%) and have a mortality-rate higher than 8% in the USA [ ], with over 720\u00a0K deaths worldwide each year [ ]. \n\nBiomedical text-mining, henceforth Text-Mining (TM), has already been used as a strategy to disclose analogous associations [ ,  ]. TM allows the retrieval of informative data from latent data within biomedical literature, namely toward data-driven analysis [ ,  ]. TM has greatly benefited from the takeoff of Artificial Intelligence (AI) algorithms that can be used for Natural Language Processing (NLP), an AI subfield dedicated to bridge the gap between human language and the language of computers. TM systems are most often built as pipelines that encompass a variable number of steps depending on the general goal of the research. Nonetheless, the initial steps of Named Entity Recognition (NER) and Named Entity Normalization (NEN) are generally needed. NER processes the input text into a set of entities, whilst NEN maps an entity into a concept in a terminology [ ,  ]. State-Of-The-Art methods (SOTA), such as Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT), provide excellent results for the NER step of TM, attaining human-like performance [ ]. However, there is still a lack of NEN models that can tackle multiple categories as well as a need for combined NER and NEN strategies to be time-smart and enhance accuracy by integrating other data types such as ontologies directly upon the entity\u2019s retrieval step [ ]. \n\nHerein, we present SicknessMiner,  a leading-edge TM pipeline encompassing both NER and NEN steps. The pipeline uses SOTA models as BioBERT for NER and NormCo for NEN. SicknessMiner does not require a specific input format and, as such, has a broad application and it is easy-to-use. SicknessMiner was fine-tuned for diseases and is able to return them as entities mapped to Medical Subject Headings (MeSH) terms with top performance. Both models of SicknessMiner were trained on the NCBI Disease dataset [ ]. SicknessMiner demonstrates that our chosen DDA evaluation, ranking via disease-disease co-mention in a set of scientific publications, can be highly useful to find meaningful relations. This is a powerful finding, since this technique relies only on NER and NEN models. Such high accuracy models can be easily constructed, widening SicknessMiner application as it does not require the existence of other ontologies or knowledge bases regarding each specific domain of application. To further evaluate our DDAs results, we compared them with a well-known database, DisGeNET, which classifies DDAs based on the similarity of their gene- or variant-disease lists. Figure\u00a0  illustrates the methodological pipeline followed to attain, validate and evaluate SicknessMiner.   \nSicknessMiner pipeline: a TM approach for DDAs. SicknessMiner is a two-step pipeline integrating subsequent modules for NER and NEN. First, from the RAW text input, an entity's list is plotted according to co-mentions with more than 1 BC type. To evaluate SicknessMiner, we used the BC5CDR evaluation kit. Finally, DDAs were doubly assessed via SicknessMiner and DisGeNET and further evaluation was performed for a better understanding of key improvements obtained herein \n  \n\n\n## Results \n  \nSicknessMiner two modules, NER and NEN, were trained using a corpus of 793 PubMed abstracts with over 6.8\u00a0K disease mentions mapped to 790 unique concepts either linked to the MeSH or the Online Mendelian Inheritance in Man (OMIM) databases. The NER task consists of identifying entities of interest in a given corpus. For example, given a RAW text, the output of the NER module is a set of text spans where the entities can be found, in our particular case, known diseases. After the NER step, the retrieved text spans related to the entities serve as input to the NEN module, which maps each text span to an entry in a given ontology. If an entity is not found in the ontology, it is mapped as   unknown  . The main output of the SicknessMiner consists of a set of entities found in a given corpus, with their respective IDs corresponding to the key ontology, as well as the place in the text where the entities were found. \n\nBoth models of SicknessMiner were trained on the NCBI Disease dataset [ ]. For the NER, we started the BioBERT model with the BioBERT v1.1 pre-trained weights [ ] and fine-tuned them in the NCBI Disease train dataset using the default parameter of the system made available by Lee et al. [ ]. For the NEN, we trained the NormCo [ ] on the NCBI Disease train dataset using the default parameter of the system. \n\n### SicknessMiner evaluation \n  \nWe choose a simple but efficient method to DDA evaluation: ranking via disease-disease co-mention. To evaluate the performance of our method in the NCBI Disease test set, we used the evaluation kit  from the BC5CDR task to compute the precision, recall, and F1-score for both the NER and NEN steps. The BC5CDR evaluation kit is a program that receives a set of labelled text from the user and, by using the predictions of the model as input, computes these three performance metrics as output in order to assess the efficiency of the model developed by the user. SicknessMiner attained a precision of 0.87, recall of 0.89 and F1-score of 0.88 for the NER module; and a precision of 0.80, recall of 0.83 and F1-score of 0.81 for the NEN module given a perfect NER. When the NER results were considered, the NEN module achieved a precision of 0.72, recall of 0.79 and F1-score of 0.76. \n\n\n### DDAs retrieval \n  \nWe applied DDA retrieval to BCs as already mentioned. We queried PubMed for \u201c((leukemia[Title/Abstract]) OR (multiple myeloma[Title/Abstract])) OR (lymphoma[Title/Abstract])\u201d and retrieved over 390\u00a0K titles and abstracts. We merged all the results and submitted them to the SicknessMiner pipeline. To evaluate the results obtained by SicknessMiner, we compared them against DisGeNET with the Concept IDs for the BCs types: C0023418, C0024299, and C0026764 for leukemia, lymphoma and multiple myeloma, respectively. DisGeNET was chosen for direct performance comparison as the existing alternative models exhibit a few difficulties as these sometimes are not publicly available, do not encompassed the same group of diseases, or can even involve a complex methodological approach, which goes against our main aim of attaining an easy-to-use integrative pipeline [ ,  \u2013 ]. Whilst DisGeNET retrieved a total of 57,624 co-mentions between 22,611 unique diseases and one of the diseases of interest (Leukemia, Lymphoma or Multiple Myeloma), SicknessMiner retrieved 12,263 co-mentions between 5443 unique diseases. \n\n\n### DDAs evaluation \n  \nSicknessMiner was qualitatively and quantitatively compared against the results obtained through DisGeNET. To this end, we plotted a graph for each system, where each node in the graph corresponds to a disease and each edge connects two related diseases (Figs.\u00a0  and  ; full results tables are available as Additional file  ). In Fig.\u00a0 , every resulting node in the graph is one of the top 100 hits with more shared genes connected to 2 or 3 BCs simultaneously from DisGeNET. In Fig.\u00a0 , each graph node represents one of the top 100 hits regarding the number of co-mentions to the 3 BCs via SicknessMiner. To make some sense of the totality of results and how the two systems compare, we also performed quantitative analysis for each approach (Fig.\u00a0 ). To improve readability, we filtered the 100 most relevant nodes. For the DisGeNET, the process was straightforward, since it already provided the data related to our target diseases (Leukemia, Lymphoma or Multiple Myeloma). The set of genes of the target and related disease were sorted using the Jaccard index, and the top 100 diseases related to at least two target diseases were chosen. For SicknessMiner, we considered that two diseases were related if both of them appeared together in a paper title or abstract (i.e., they are co-mentioned). Afterwards, we ranked the co-mentions based on the number of times they were in the text set and chose the top 100 related to at least two target diseases. To attain realistic DDAs in DisGeNET, we considered a threshold for the similar genes of, at least, 20 shared genes to accept a DDA as positive. While SicknessMiner could retrieve overall minus 5% of DDAs (6455 vs 6087), 92% of the DDAs delivered by DisGeNET were also available in SicknessMiner. Also, SicknessMiner yielded close to 16% unidentified DDAs by DisGeNET, contrasting to only 8% diseases from DisGeNET that were unidentified in SicknessMiner (Fig.\u00a0 ).   \nSicknessMiner Top 100 co-mentioned entries (in this case all entries are related to the 3 BCs since the query was combined) \n    \nDisGeNET Top100 entries that are correlated with 2 or 3 BCs and share, at least, 20 genes or variants between diseases \n    \nSicknessMiner and DisGeNET comparison \n  \n\nWe also looked at the results from a biological angle since SicknessMiner\u2019s goal was to retrieve diseases from biomedical literature, hence, it is domain-specific in its model. The majority of edges detected, thus the majority of the DDAs reported, are some type of cancer. However, in the top 100 edges, SicknessMiner retrieved a higher number of DDAs/edges representing non-cancerous diseases than DisGeNET, 22% and 4%, respectively. In terms of unique non-cancerous DDAs, SicknessMiner could find 17 entities against 3 in DisGeNET. The retrieved non-cancerous DDAs are listed in Table  .   \nRetrieved non-cancerous DDAs from SicknessMiner and DisGeNET \n  \n\n\n\n## Discussion \n  \nHerein, we presented SicknessMiner, a TM pipeline encompassing our view towards a useful NLP approach for biomedical texts. Our approach highlights the usefulness of TM pipelines as integrated sources of information and as end-to-end platforms easy-accessible for beginner users. The use of individual modules in the pipeline allows us to easily replace and further optimize or introduce any enhancements independently, as new SOTA tools become available, which further improves the results of the pipeline as a whole. Furthermore, by using the BioBERT SOTA neural network module, we can take advantage of pre-trained weights that are made available, by the community, and that can be applied to different tasks beside NER (e.g. Relation Extraction and Question Answering) [ ], some of which we would like to explore in future works. Moreover, SicknessMiner benefits from using BioBERT, since transfer learning is a powerful ML/DL technique that gains from any improved weights that arise in the community. \n\nBy using only abstracts rather than full-texts, our aim was to lower the false-positive rate for non-existent DDAs that could occur from negative results published in the full-text reports. Also, this enables a reproducible effect by resorting only to publicly available data since full-text articles are often difficult to access due to the existent publication fees. Despite some authors ponting to higher performance when using full texts instead of abstracts-only TM [ ], the fact that the access to papers is not universal, justifies our choice to evaluate our results based on abstracts-only. Nonetheless, we also believe that to postulate new knowledge, one must always dedicate some human curatorship to the hypothesis at hand, which should resolve any errors brought by abstracts-only mining. \n\nWith an easy access GitHub repository ( ) and reproducible examples, SicknessMiner is a valuable tool for the NER and NEN steps of any TM pipeline for disease retrieval. Furthermore, SicknessMiner, via its co-mention analysis, is also able to successfully establish DDAs. Despite the fact that co-mention is often referred to as a lesser approach for establishing associations between mapped entities, SicknessMiner was able to recover close to 92% of DDAs from a well-established web server such as DisGeNET and still contribute with 16% of new DDAs. These pave the way for further analysis to assess new contributions to the field of BCs and related diseases. DisGeNET also includes DDAs from TM approaches and yet its performance is not improved compared to SicknessMiner. Moreover, DisGeNET needs gene or variant lists related to a disease, which can be tiresome and costly. \n\nWhen analyzing our graphs (Figs.\u00a0  and  ), the most noticeable aspect was that the majority of results obtained were cancers (for full results tables, see Supporting Information). Cancer is an intricate process that, despite our scientific advances, still lacks a complete thorough understanding. Indeed, cancer can be seen as a systemic process that possesses many layers in need of a system\u2019s biology approach [ ]. With this in mind, we focused instead on the most frequent co-mentioned or the ones with the highest Jaccard index non-cancerous diseases in the case of SicknessMiner and DisGeNET, respectively. SicknessMiner was able to retrieve nearly 6 times more non-cancerous diseases. In line with the systemic take on cancer, we also believe that the results retrieved solely by SicknessMiner as \u201canemia\u201d, \u201cfever\u201d, and \u201cthrombocytopenia\u201d are encompassed in such a way that can grant a preview in the usefulness of such approaches. The only DDA reported by DisGeNET not present in the top 100 edges from SicknessMiner was rheumatoid arthritis that was assessed as associated with both leukemia and multiple myeloma. However, SicknessMiner also retrieved AutoImmune Diseases (AID) MeSH term, for which rheumatoid arthritis in one [ ], in the top co-mentions. Finally, it is noteworthy to mention that the top 30% results retrieved from DisGeNET are some form or variation of the original query, as \u201cchildhood leukemia\u201d or \u201cadult lymphoma\u201d whereas SicknessMiner, comparatively, retrieves less of those variations of the original query, highlighting \u201cgenetic diseases, inborn\u201d, \u201cepstein-barr virus infections\u201d, and \u201cmyelodysplastic syndromes\u201d in the top 30 edges. Nonetheless, the broad scope of non-cancerous DDAs found by SicknessMiner can provide a wider, more thorough glimpse and complement research lines by providing new insights of relevant DDAs. \n\nDuring our work, the major problem regarding the NEN step, was the existence of several ontologies for diseases with the most eminent being DO [ ], MeSH [ ], and Unified Medical Language System Concept Unique Identifiers (UMLS CUI) [ ]. In fact, results comparison can be hindered by the existence of several ontologies for which a direct correspondence or mapping is not always possible. A centralized ontology or the availability of an external mapping tool would be highly beneficial for TM pipelines both by enabling comparison, but also by allowing the integration of data from different sources onto the same model. Hence, in the future, other categories will be included in our model to fully take advantage of such impressive results in the DDA category, and to pave a new way towards accessibility in the AI-OMICS-era. \n\n\n## Conclusion \n  \nAccurate TM solutions/software\u2019s are sparse and still lack the inclusion of ML/DL models. Herein, we introduced SicknessMiner, a novel tool encompassing TM SOTA methods to simultaneously perform NER and NEN. SicknessMiner shows a very high performance and can retrieve 92% of all associations fetched by DisGeNET, a respected resource for DDAs. Despite the usefulness of obtaining relationships among diseases, most available models to tackle this problem tend to encompass a layer of ontology or some type of knowledge-based methodology. Our approach, despite seeming simpler, aims to harness the already available knowledge enclosed in scientific papers and uses text-mining tools to retrieve DDAs. We believe that through the development of SicknessMiner, we were able to build a comprehensive, highly upgradeable and customizable, easy to use TM pipeline to postulate new relevant DDAs. \n\n\n## Methods \n  \n### TM approach: building sicknessminer \n  \nWe used SOTA modules as the basis for the two tasks: BioBERT [ ] for NER and NormCo [ ] for NEN. BioBERT is a Deep Neural Network (DNN) model, a biomedical domain application of the Bidirectional Encoder Representations from Transformers (BERT) model [ ]. BERT models have shown promising performances in a variety of NLP tasks, including NER. BioBERT provides different sets of weights  to initialize the neural network, but the best results require parameter training on a distribution similar to the target distribution. We fitted the BioBERT parameters on the NCBI Disease dataset.  A similar approach was performed for the NEN task using as basis the NormCo [ ], a simple, yet powerful, model that uses Recurrent Neural Networks (RNN) to map the name of the entities into the IDs in a given ontology. We started from a pre-trained set of weights, made available by the authors of NormCo,  and fine-tuning the model to the NCBI Disease Corpus [ ]. \n\nBoth NER and NEN fine-tuning were performed following the experiments described in the corresponding papers, and code provided by the authors. In particular, the NER system was implemented using TensorFlow,  an Adam optimizer with a decay rate of 0.01, and a learning rate of 5e  trained for 10 epochs. The NEN system was implemented using Torch,  with the same optimizer. We used a learning rate of 5e  and 100 epochs. After each epoch, the model was evaluated against a validation set (a set of examples distinct from the training and test sets) and stopped if no improvement of the model was achieved after 15 epochs. \n\n\n### SicknessMiner evaluation \n  \nThe evaluation of SicknessMiner was performed in the test set of the NCBI Dataset using the BC5CDR evaluation kit to compute the precision, recall and F1-score for both NER and NEN. This evaluation kit is commonly used to evaluate such tasks since it employs the widely used BC5CDR corpus for results comparison [ ]. The NEN was performed on the result of the NER module, which means that errors from NER may cascade to NEN. We opted to use the BC5CDR evaluation kit, instead of the reported evaluation result implemented by each system, to attain a consistent evaluation across all systems. The precision is given by the formula  , where   is the number of true positives and   is the number of false positives. The recall is given by the formula  , where   is the number of false negatives. Finally, the F1-score is given by the formula  . Intuitively, the precision measures the confidence of the model in predicting the true examples, whilst the recall measures the capacity of the model to detect the true examples among all data; and the F1-score is a harmonic mean between the precision and recall, used to summarize both metrics in a single number, biased toward the smaller value between the two metrics. \n\n\n### DDAs retrieval \n  \nPubMed used queries were: \u201c((leukemia[Title/Abstract]) OR (multiple myeloma[Title/Abstract])) OR (lymphoma[Title/Abstract])\u201d, giving a total of 390\u00a0K titles and abstracts (as of 21st of April 2021). SicknessMiner was compared to DisGeNET with the Concept IDs for the BCs types: C0023418, C0024299, and C0026764 for leukemia, lymphoma and multiple myeloma, respectively. Upon results collection, and since DisGeNET and SicknessMiner use different ontologies to identify diseases, an extra mapping step was performed to match results from both. DisGeNET uses the UMLS CUI and SicknessMiner uses both MeSH and OMIM identifiers. One caveat of mapping at the NEN step was related to the mismatch among the ontologies used by different sources that difficulties results\u2019 comparison and/or combination. In order to overcome this shortcoming and to easily compare SicknessMiner and DisGeNET, we converted the DisGeNET mappings, originally in UMLS CUI, to MeSH or OMIM identifiers. To increase the identification power of our approach and to include additional identifiers descriptors, further mapping was performed recursively searching the xml files desc2021, supp2021, pa2021 and qual2021 made available via FTP by NCBI ( ). All performed mappings are also available on our GitHub repository. \n\n\n### DDAs evaluation \n  \nSicknessMiner was qualitatively and quantitatively compared against the results obtained through DisGeNET by both graph representation and similarity/dissimilarity percentage calculations. The set of genes of the target and related disease were sorted using the Jaccard index, a measurement of similarity between two different sets (A and B), given by the formula  . \n\nDepending on the methodology, we considered that diseases were related in two different ways. For SicknessMiner, we considered that two diseases were positively related to each other whenever there was a co-mention amongst the paper title and/or abstract. Differently, for DisGeNET, we considered that two diseases were related whenever they shared, at least, 20 genes. Hence, relations were extracted for the entirety of the results, ranked according to the decrescent number of co-mentions or shared genes for SicknessMiner and DisGeNET, respectively, and the top 100 more frequent relations were kept for further analysis. \n\n\n\n## Supplementary Information \n  \n\n\n\n\n \n", "metadata": {"pmcid": 8491382, "text_md5": "6ff672f5e5e2ca7bde8b853a4abcf35d", "field_positions": {"authors": [0, 87], "journal": [88, 106], "publication_year": [108, 112], "title": [123, 217], "keywords": [231, 339], "abstract": [352, 1572], "body": [1581, 24979]}, "batch": 1, "pmid": 34607568, "doi": "10.1186/s12859-021-04397-w", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8491382", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8491382"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8491382\">8491382</a>", "list_title": "PMC8491382  SicknessMiner: a deep-learning-driven text-mining tool to abridge disease-disease associations"}
{"text": "Cai, Xiaoling and Dong, Shoubin and Hu, Jinlong\nBMC Med Inform Decis Mak, 2019\n\n# Title\n\nA deep learning model incorporating part of speech and self-matching attention for named entity recognition of Chinese electronic medical records\n\n# Keywords\n\nPart of speech\nChinese electronic medical records\nNamed entity recognition\n\n\n# Abstract\n \n## Background \n  \nThe Named Entity Recognition (NER) task as a key step in the extraction of health information, has encountered many challenges in Chinese Electronic Medical Records (EMRs). Firstly, the casual use of Chinese abbreviations and doctors\u2019 personal style may result in multiple expressions of the same entity, and we lack a common Chinese medical dictionary to perform accurate entity extraction. Secondly, the electronic medical record contains entities from a variety of categories of entities, and the length of those entities in different categories varies greatly, which increases the difficult in the extraction for the Chinese NER. Therefore, the entity boundary detection becomes the key to perform accurate entity extraction of Chinese EMRs, and we need to develop a model that supports multiple length entity recognition without relying on any medical dictionary. \n\n\n## Methods \n  \nIn this study, we incorporate part-of-speech (POS) information into the deep learning model to improve the accuracy of Chinese entity boundary detection. In order to avoid the wrongly POS tagging of long entities, we proposed a method called reduced POS tagging that reserves the tags of general words but not of the seemingly medical entities. The model proposed in this paper, named SM-LSTM-CRF, consists of three layers: self-matching attention layer \u2013 calculating the relevance of each character to the entire sentence; LSTM (Long Short-Term Memory) layer \u2013 capturing the context feature of each character; CRF (Conditional Random Field) layer \u2013 labeling characters based on their features and transfer rules. \n\n\n## Results \n  \nThe experimental results at a Chinese EMRs dataset show that the F1 value of SM-LSTM-CRF is increased by 2.59% compared to that of the LSTM-CRF. After adding POS feature in the model, we get an improvement of about 7.74% at F1. The reduced POS tagging reduces the false tagging on long entities, thus increases the F1 value by 2.42% and achieves an F1 score of 80.07%. \n\n\n## Conclusions \n  \nThe POS feature marked by the reduced POS tagging together with self-matching attention mechanism puts a stranglehold on entity boundaries and has a good performance in the recognition of clinical entities. \n\n \n\n# Body\n \n## Background \n  \nSince the implementation of the \u201cBasic Norms of Electronic Medical Records\u201d in China, the number of Chinese Electronic Medical Records (EMRs) has increased dramatically, and the Named Entities Recognition (NER) on EMRs has received constant research attention over recent years. NER automatically identifies entities related to the patient and produces structured data to help constructing the knowledge graph in medical domain [ \u2013 ]. \n\nElectronic medical records are usually written by doctors, sometimes using Chinese abbreviations, resulting in multiple expressions of the same entity. For example, \u201c\u8d1d\u4f10\u201d, \u201c\u8d1d\u4f10\u73e0\u5355\u6297\u201d and \u201c\u8d1d\u4f10\u5355\u6297\u201d all refer to the same drug \u201cbevacizumab\u201d. And compared with English clinical entities, the composition of Chinese clinical entities is more complicated with mixed Chinese characters, letters, numbers and punctuations, adding the difficulties in identifying the entities. More importantly, the Chinese entities are not only generally longer than that English entities, but are varied in length greatly among different categories. For example, the \u201cbody\u201d entities are short and may only be of 1\u20132 characters, such as \u201c\u5934\u201d (head), while the \u201coperation\u201d entities are longer, sometimes as long as more than 20 characters. In fact, the above problem can be classified as the boundary extraction problem of Chinese entities. The Chinese named entity recognition task can be divided into two key steps, entity category extraction and entity boundary extraction. This paper focuses on the problem of entity boundary extraction with different entity lengths, and proposes a deep learning model that combines part-of-speech information and self-matching attention mechanism for name entity recognition of Chinese electronic medical records. \n\nIn the traditional NER method based on machine learning, part-of-speech information is considered as an important feature of entity recognition [ \u2013 ]. K. Liu et al. [ ] extracted characters and part-of-speech features from Chinese clinical electronic medical records and studied their effects on the NER task. They presumed that because disease entities are mostly composed of several consecutive nouns and treatment entities often occur after verbs, the POS feature can be helpful in improving the recognition of clinical entities. W. Li et al. [ ] added the part-of-speech information into the deep belief network (DBN) and found that the improved DBN method is not only superior to the original DBN method, but also exceeds CRF method at the F value. The authors thus concluded that the model can capture semantic information after added with part-of-speech information. \n\nHowever, the POS feature is only rarely incorporated in the neural network models. One of the reasons is that the current part-of-speech tagging of electronic medical records is not accurate enough, and there are many errors in the tagging results. The error propagation leads to poor entity recognition. For example, \u201c\u90e8\u5206\u5c0f\u80a0\u5207\u9664\u672f\u201d (partial small bowel resection) is a Chinese \u201coperation\u201d entity that can be marked as two parts \u201c\u90e8\u5206\u5c0f\u80a0_n \u5207\u9664\u672f_v\u201d by POS tagging tools, and the system may only pick out \u201c\u90e8\u5206\u5c0f\u80a0\u201d (partial small intestine) as an \u201coperation\u201d entity. The above-mentioned papers [ ] solved this problem by constructing a medical dictionary, which took a lot of efforts. In this paper, we propose a reduced part-of-speech tagging method, which only labels the general vocabularies rather than the clinical entities, to enhance the labeling accuracy of long entities. \n\nIn order to extract features automatically, neural networks are applied to NER task. Most of NER deep learning models are based on LSTM-CRF and its variants [ \u2013 ]. The recent research on neural networks proposed the use of \u201cattention mechanisms\u201d that rely on weighted representations in multiple time steps along a sequence, rather than relying solely on a single representation of the entire sequence [ ]. The attention mechanism allows the model to better focus on the relevant parts of the input and has proven to greatly improve the performance of the loop network in various natural language processing tasks [ \u2013 ]. The study [ ] evaluated the self-attention mechanism empirically on the CoNLL-2003 dataset and on synthetic datasets with particularly long-term relationships between words and showed that the approach achieves a much better performance in long-term interdependent text sequences. Z. Liu et al. [ ] proposed an attention-based CNN-LSTM-CRF model for extracting Chinese clinical entities, which utilizes an attention layer to select relevant words. \n\nIn this study, we proposed a SM-LSTM-CRF model design for NER task, which is based on self-matching attention mechanism. We use part-of-speech features to introduce entity part-of-speech information and sentence semantic information. And the self-attention mechanism is added to our model to solve long-dependency problems and improve the accuracy of entity boundaries recognized. Particularly, we propose a reduced part-of-speech tagging method to reduce the markup errors of long entities. \n\n\n## Method \n  \n### Data preprocessing \n  \nAs name entities become the main source of out of vocabulary (OOV) in segmentation task, the NER methods should have the ability to overcome the potential issue of incorrectly segmented entity boundaries. A few researches showed that character-based methods outperform word-based methods for Chinese NER [ ,  ]. But there is still a problem that the words are generally used as the smallest unit to express semantics in Chinese language. Though the character-based model may avoid segmentation error, it loses part of semantic as well. On the other hand, character sequence is always longer than word sequence, increasing the difficulties with entity boundary extraction. To tackle those problems, we added the part-of-speech features to the model and concatenated it with characters as the input of model. \n\nPart-of-speech (POS) refers to the characteristics of words as the basis for classifying words, including nouns, verbs, adjectives, modal particles etc. Prior work based on rules or machine learning has shown that POS can dramatically improve NER annotation, and suggested the possibility of increasing accuracy through adding POS to the deep neural network. We add POS feature into the deep learning model for the following considerations:   \nThe POS information implies potential word segmentation, which indirectly adds semantics to the model. Although the entity is divided into multiple characters in preprocessing, those characters have the same POS tag as that entity, which making the boundary easier to determine. \n  \nMost of name entities are OOV that cannot be correctly labeled during POS tagging. While the common words can be tagged exactly, to the benefit of distinguishing the edges between the medical entities and the ordinary words. \n  \nIn different word contexts, the same character can be marked as different POS tags, which suggest the possibility of increasing the accuracy of entity category recognized. \n  \n\nWhen performing POS tagging on electronic medical records by tools, the long entities may be tagged as multiple parts, which may result in recognition errors for the long clinical entity. For example, the entity \u201c\u90e8\u5206\u5c0f\u80a0\u5207\u9664\u672f\u201d(partial resection of small bowel) is marked as two parts: \u201c\u90e8\u5206\u5c0f\u80a0_n \u5207\u9664\u672f_v\u201d, and the system may pick out \u201c\u90e8\u5206\u5c0f\u80a0\u201d (partial small intestine) as an \u201coperation\u201d entity. It is traditional that medical dictionaries are added to help identify the medical entities, but because of the complexity of building medical dictionaries, we propose a reduced part-of-speech tagging method, which construct a general dictionary instead of a medical dictionary. \n\nWe first utilize the Jieba tool [ ] to perform word segmentation on the training data, gathering the non-entity word segments to get the general vocabulary dictionary. After tagging the POS of the training and the test data, reserve the POS tags of the word segments that are included in the dictionary and classify those not as character sequences which are then given POS of \u201cs\u201d. \n\nAs shown in the Fig.\u00a0  (\u201cO\u201d represents ordinary words, \u201cB_ope\u201d the beginning characters of the \u201coperation\u201d entities and \u201cI_ope\u201d the middle characters of the \u201coperation\u201d entities.), after using the reduced POS tagging, the boundaries of the clinical entity become more distinguishable, which is indeed beneficial to the boundary extraction of the entities. Meanwhile, the tags of the context are kept, making it possible to improve the accuracy of entity category recognized by learning the POS features of the context. Lastly, the common dictionary costs far less to build than the medical dictionary, and the method excluding the need for medical expert tagging can be conveniently applied to another dataset.   \nExample for reduced POS tagging \n  \n\nFormally, denote the character vectors as   and POS vectors as  , where   l   is the vector dimension and   m   is the length of the sentence. And the input of model as   V  \u2009=\u2009{  X  ;\u2009  P  }. The data preprocessing process is shown in the Fig.\u00a0 .   \nThe data preprocessing flowchart \n  \n\n\n### SM-LSTM-CRF model \n  \nThe model proposed in this paper called SM-LSTM-CRF, which is the LSTM-CRF model based on the self-matching attention mechanism. The model consists of three layers: self-matching attention layer \u2013 calculating the relevance of each character to the entire sentence; LSTM layer \u2013 capturing the context feature of each character; CRF layer \u2013 labeling characters based on their features and transfer rules. The structure is shown in Fig.\u00a0 .   \nThe structure of SM-LSTM-CRF model \n  \n\nIn Chinese EMRs, there are some types of entities having a longer length, which need to consider a farther sequence dependency. Recently attention mechanisms are proposed in sequence labeling to overcome this problem by allowing directly the use of information from across all time steps. Self-matching attention calculates the weight dependent on the input sequence and gets the attention of each characters across the sentence. Self-matching attention performs parameter calculations through a bidirectional LSTMs network, and the input of each time step is modified to be\u00a0[  v  ,\u2009  c  ], where   v   is the input vector at time step   t   and   c   is the attention vector of   v   related to the whole sentence. \n\nWhere   c  \u2009=\u2009  att  (  V  ,\u2009  v  ) is an attention-pooling vector of the whole sentence. \n\nWe further build a similar self-matching in the reverse direction, to obtain a representation that encodes the contexts from both directions for each token. \n\nThe resulting matrices   are hidden representations of the sentence, where   m   is the length of input and   k   is hidden unit of LSTM. \n\nThe recoded matrix is inputted into the bidirectional LSTMs to learn contextual feature. Compared to a simple RNN, the LSTM is better in learning context-dependent feature around the input vector. We use a standard directional LSTMs to process the recode embedding as shown below: \n\nThe resulting matrix:  , where   k   is hidden unit of LSTM. \n\nA standard CRF layer is used on top of model and the output of LSTM layer is converted to an input by a linear function. \n\nWhere   W  \u2009\u2208\u2009  R  ,   b  \u2009\u2208\u2009  R   are parameters to be learned,   n   is the number of tag types. The input matrices of CRF is   P  , where   P   represents the non-normalized probability that the characters are mapped to the name entity labels. The transition probability matrix   A   is learned in CRF and   A   represents the transition probability when a label transfers to another label. For an input   V  , the probability of outputting the optimal tag sequence y can be defined as: \n\nDuring the model training process, the loss function is defined by: \n\nBy minimizing the loss function, the probability of the optimal labeling sequence is increased, and the label transition probability matrix is obtained during the training of the model parameters. \n\n\n\n## Results \n  \n### Dataset \n  \nThe data set includes 1000 admission records for patients, which are adopted from the Chinese EMR named entity recognition task in China Conference on Knowledge Graph and Semantic Computing in 2018 ( /). \n\nTable\u00a0  shows the number of entities and the average number of characters per entity both in training datasets and testing datasets. In the experiment, 600 of the records are used as training data, and the remaining are test data. The entities in patient admission records are mainly divided into five categories:   \nAnatomical Part: the functional structural unit made up of a variety of organizations, such as \u201c\u8179\u90e8\u201d (abdomen). \n  \nSymptom Description: refers to the patient\u2019s own experience and feeling of abnormal physiological function after the patient is sick, and needs to be combined with the anatomical part, such as \u201c\u4e0d\u9002\u201d (discomfort). \n  \nIndependent Symptoms: refers to the patient\u2019s own experience and feeling of the body\u2019s physiological function after illness, can be independently output, such as \u201c\u7729\u6655\u201d (dizziness). \n  \nDrug: a kind of chemical substances used for curing, preventing or promoting health. \n  \nOperation: refers to excision, suture and other treatments operated on the patient by a surgeon. \n    \nThe number of entities and the average number of characters per entity \n  \n\n\n### Implementation and parameters \n  \nOur neural networks are implemented on Python 2.7.13 and Tensorflow 1.8.0 library, and the POS tagging are performed by Jieba 0.39. We cut the sentences to segments by the Chinese punctuation marks such as comma and period. Table\u00a0  shows the adopted hyper-parameters about our model. A model is designed to stop training, when the CRF loss of the two epoch of training did not differ by more than 0.001.   \nHyper-parameters \n  \n\n\n### Evaluation metrics \n  \nThe evaluation metrics, namely, Precision (P), Recall (R) and F1-measure (F1) are used to evaluate the performance of the NER methods, and defined as follows: \n\nAn entity is annotated as correct when its category and boundary are fully labeled correctly. TP is the count of entities labels presenting the same labels as gold standard labels, FP is the count of recognized entities marked incorrectly in the results and FN is the count of the gold standard entities not present in the results of indicator. \n\n\n### Experimental result \n  \n#### The effect of self-matching attention mechanism \n  \nAs listed in Table\u00a0 , we firstly examine the performance of Self-matching attention mechanism by comparing SM-LSTM-CRF with LSTM-CRF, both with an input of character vectors instead of word vectors.   \nPerformance of Self-matching attention mechanism \n  \n\nThe above table shows that SM-LSTM-CRF achieves a better performance compared to the LSTM-CRF with F1 improved 2.59%. The simple LSTM-CRF model copes quite well with easier instances and performs well in short entities. But as shown in Table  , there are some longer entities such as \u201coperation\u201d entities which have an average length of 8 characters. However, the self-matching attention mechanism can detect features of input space regardless of position, which exhibits good performance for the more difficult instances and improves the prediction of long entity boundaries. \n\n\n#### The effect of part-of-speech (POS) \n  \nIn order to examine the impact of POS on NER, we add POS feature to the LSTM-CRF and SM-LSTM-CRF. In this section, we compare two different input processing, one using only the character vectors as input, the other the concatenation of the character vectors and the POS vectors. The POS is tagged by the Jieba tool. \n\nAs shown in Table\u00a0 , the performance of the LSTM-CRF and the SM-LSTM-CRF proposed in this paper both improve significantly after adding POS feature. The POS feature introduces boundary and semantic information to offset the weakness of character-based input. The LSTM-CRF model has outperformed by 8.14% and the SM-LSTM-CRF increases F1 by 7.74%, which implies that POS is beneficial to enhance the performance of the deep learning model applied for NER.   \nPerformance after adding POS information \n  \n\n\n#### Comparison the impact of different part-of-speech tagging methods \n  \nIn this section, we compare the effects of two part-of-speech tagging methods on the recognition of entities. One is tagged by the Jieba tool and the other is the reduced POS tagging proposed in this paper. To verify the generality of this preprocessing method, we performed experiments on LSTM-CRF and SM-LSTM-CRF. \n\nAs shown in Table  , the reduced POS tagging method proposed in this paper has a good performance on both two models with the F1 increased respectively by 3.48 and 2.42%. In other words, it improves the extraction accuracy rate of entity boundaries through the enhanced distinguishability of the entities.   \nComparison of the different POS tagging methods \n  \n\n\n#### Comparison with other algorithms \n  \nIn this section, the work of this paper is compared with other algorithms. The results are shown in Table\u00a0 .   \nPerformance comparison of different algorithms \n  \n\nThe N-gram-Based RNN-CRF model has a complex processing for the input representation. The author concatenated the current character with its bi-gram, tri-gram representation, which leads to a larger dimension for input vectors. While the idea of the paper is worth learning, it may not be applicable to the current data set. The character-based LSTM-CRF has an F1 value of about 67.32%. In the attention-based CNN-LSTM-CRF, the author used the Xinhua dictionary to extract the radicals of Chinese characters, which were then used to calculate the similarities between characters through the CNN layer. Compared with other algorithms, the SM-LSTM-CRF has a better performance on the current task. What\u2019s more, the processing in this paper is simple without the need for any medical dictionary. \n\n\n\n\n## Discussion \n  \nThe experimental results shown in Table   demonstrate that the self-matching attention mechanism performs well in entity recognition. We analyze its performance on the five types of entities shown in Fig.\u00a0 . It can be seen that SM-LSTM-CRF model performs better in the \u201coperation\u201d categories with F1 increased by 7.6%. There is a context model around the \u201coperation\u201d entities. Take \u201c\u884c\u2009<\u2009entity>\u76f4\u80a0\u764c\u6839\u6cbb\u672f</entity>\u201d(perform <entity>radical resection of rectal cancer</entity>) as an example: the Chinese character \u201c\u884c\u201d (perform) always appears before \u201coperation\u201d entities, and often incorrectly labeled as part of a \u201coperation\u201d entity or labeled as a \u201coperation\u201d entity itself by the LSTM-CRF model. However, this error can be effectively decreased after adding the self-matching attention mechanism. The self-matching attention mechanism enhances the correlation between the characters in the entity by calculating the relevancy between characters, thus improves the recognition accuracy rate of the entity boundaries with less errors made.   \nPerformance comparison of different models on the five types of entities \n  \n\nWe statistically analyzes the distribution of entities in the incorrect results of the two models, and get the number of entities with \u201cboundary correct, category error\u201d, \u201cboundary error, category correct\u201d and \u201cboundary error, category error\u201d as shown in Table\u00a0 . It can be seen that most of the errors are caused by inaccurate boundary while the category errors only make up a small proportion. Therefore, for the NER model with characters as input, we can improve the performance by increasing the accuracy of entity boundary. Compared with the LSTM-CRF, SM-LSTM-CRF model reduces the number of entities with wrong boundary and improves the accuracy of entity labeling. We emphasize that the self-matching attention mechanism improves the accuracy of the entity boundary recognized, especially for the entities with a long length.   \nThe distribution of entities in the incorrect results \n  \n\nIn addition, we compare the performance of three types of input used for SM-LSTM-CRF on the five categories of entities. As shown in Fig.\u00a0 , after adding the POS, the model performs well on the \u201cAnatomical Part\u201d, \u201cSymptom Description\u201d, \u201cIndependent Symptoms\u201d and \u201cDrug\u201d entities, but has a reduce of F1 value in \u201cOperation\u201d entity. As we have shown in Table  , the entities of \u201cOperation\u201d have a longer character sequence and the characters are marked with different POS tags, which make the boundary became more difficult to define. However, after using the reduced POS tagging, the problem is solved and the accuracy of the \u201cOperation\u201d entities increases a lot, which reveals the effectiveness of our method.   \nPerformance comparison of different input methods on the five types of entities \n  \n\nWe plot the length distribution of the correct results from SM-LSTM-CRF using each of the three types of input, shown in Table\u00a0 . It shows that the models involving part-of-speech tagging methods perform well in the entity length of \u201c1\u20135\u201d characters, and start to have an adverse effect when the entity length increases. After processed by the reduced POS tagging method, the part-of-speech information can simultaneously improve the recognition effects on both the long entities and the short entities, which are largely due to the boundary information of unknown clinical entities obtained through the known generic terms. It is justified to say that considering the currently poor states of medical dictionary construction and medical part-of-speech tagging, we turn to involve other information in the basic natural language processing to better mine the health information from the electronic medical records.   \nThe distribution of entities with various length in the correct results \n  \n\n\n## Conclusions \n  \nThis paper proposes a SM-LSMT-CRF model with reduced POS tagging for NER task on Chinese EMRs, which can handle the long-distance dependencies problem by implementing the Self-matching attention mechanism that helps to improve the accuracy of the long entity boundary recognition. With the reduced POS tagging method, the POS feature exhibits good performance for both short entities and long entities. Our empirical evaluation shows that our model can achieve a much better performance in Chinese EMRs than other algorithms. \n\n \n", "metadata": {"pmcid": 6454585, "text_md5": "3435ddb2376d3661073f74da09474f78", "field_positions": {"authors": [0, 47], "journal": [48, 72], "publication_year": [74, 78], "title": [89, 234], "keywords": [248, 323], "abstract": [336, 2576], "body": [2585, 24856]}, "batch": 1, "pmid": 30961622, "doi": "10.1186/s12911-019-0762-7", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6454585", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6454585"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6454585\">6454585</a>", "list_title": "PMC6454585  A deep learning model incorporating part of speech and self-matching attention for named entity recognition of Chinese electronic medical records"}
{"text": "Pezanowski, Scott and Mitra, Prasenjit and MacEachren, Alan M.\nKN J Cartogr Geogr Inf, 2022\n\n# Title\n\nExploring Descriptions of Movement Through Geovisual Analytics\n\n# Keywords\n\nGeographic movement\nGeovisual analytics\nMachine learning\nNatural language processing\nBig data analytics\n\n\n# Abstract\n \nSensemaking using automatically extracted information from text is a challenging problem. In this paper, we address a specific type of information extraction, namely extracting information related to descriptions of movement. Aggregating and understanding information related to descriptions of movement and lack of movement specified in text can lead to an improved understanding and sensemaking of movement phenomena of various types, e.g., migration of people and animals, impediments to travel due to COVID-19, etc. We present GeoMovement, a system that is based on combining machine learning and rule-based extraction of movement-related information with state-of-the-art visualization techniques. Along with the depiction of movement, our tool can extract and present a lack of movement. Very little prior work exists on automatically extracting descriptions of movement, especially negation and movement. Apart from addressing these, GeoMovement also provides a novel integrated framework for combining these extraction modules with visualization. We include two systematic case studies of GeoMovement that show how humans can derive meaningful geographic movement information. GeoMovement can complement precise movement data, e.g., obtained using sensors, or be used by itself when precise data is unavailable. \n\n## Supplementary Information \n  \nThe online version contains supplementary material available at 10.1007/s42489-022-00098-3. \n\n \n\n# Body\n \n## Introduction \n  \nAutomated methods proposed by the natural language processing and information retrieval communities often form the basic building blocks in an application. However, in this paper, we argue that such automated tools, even though they have achieved some level of maturity, are not enough for the needs of the end-users, especially for domains that require higher-level information assimilation and cognition like foraging and sensemaking over spatial information. For example,\u00a0Lai et\u00a0al. ( ) have recently used natural language processing (NLP) to understand context in the extraction and geocoding of historical floods, storms, and adaptation measures. They extend the state-of-the-art for low-level information extraction, e.g., named entity extraction and geocoding, but do not provide a holistic understanding of the story underlying these events. \n\nWe posit that our research community needs to \u201csee the forest for the trees.\u201d Sensemaking is an integral part of information processing, and tighter coupling between the lower levels (information extraction) and the higher levels (information understanding and sensemaking) can improve the state-of-the-art. Specifically, we call for the community to look more at \u201chigher-level tools and systems\u201d that enable end-users to complete tasks. Towards this goal, we study the case of extracting geospatial information from text using visual analytics (VA)\u00a0(Andrienko et\u00a0al.  ; Yuan et\u00a0al.  ) to perform tasks over the extracted data. \n\nSince text is unstructured data and the information within the text is often messy, the output from computational techniques includes associated errors.\u00a0Therefore, it is not sufficient to explore mentions of movement in text without human expertise. VA can address this issue through human-in-the-loop strategies that enable analysts to work iteratively with computational methods that extract knowledge from messy data, cope with uncertainties in computational results, and improve those results over time\u00a0(Endert et\u00a0al.  ; Robinson  ). VA is especially suitable for big, diverse, messy data that can be interpreted differently\u00a0(Tapia-McClung and Silv\u00e1n-C\u00e1rdenas  ; Angelini et\u00a0al.  ; Ninkov and Sedig  ; Snyder et\u00a0al.  ; MacEachren et\u00a0al.  ). \n\nOur research objective was to determine if computational techniques and geovisual analytics can leverage   large volumes of movement statements to enable an end-user to understand the movement described   quickly. If successful, research can then take advantage of the wealth of movement data found in written descriptions about people, wildlife, goods, and other things moving throughout our world. Text statements about movement can be used to understand what is moving, when it is moving, why it is moving, and how it is moving. \n\nFor our research, \u201cgeographic movement\u201d refers to the movement of people, animals, objects, goods, information, natural physical processes, and similar things through spaces ranging in size from multiple buildings to the whole earth. We applied computational methods to identify and extract movement statements, and present them in GeoMovement, a human-in-the-loop web-based geovisual analytics system for identification, processing, and exploration of descriptions of movement. GeoMovement involves computational (1) cleaning of the messy text, (2) predicting the statements that describe movement using a machine learning (ML) model, (3) applying Geographic Information Retrieval (GIR) techniques to identify places mentioned, and (4) predicting statements that describe restricted movement or desired movement that is not possible (hereafter referred to as \u201cimpaired movement\u201d). While there is substantial research on some of these subtasks,   integrating these techniques with VA   and demonstrating its success in our chosen domain is the main contribution of this paper. \n\nWhile some progress has happened in processing descriptions of movement in text, there is very little work on detecting and understanding descriptions of impaired movement. For example, the COVID-19 pandemic prompted us to focus on impaired movement due to the importance of disruptions and restrictions in global movement patterns of people and, perhaps equally important, the movement of goods like food and medicines. Therefore, another important contribution of this work is that we show an adaptation of an existing approach developed to detect negation in picture descriptions\u00a0(van Miltenburg et\u00a0al.  ) can successfully uncover impaired geographic movement in text documents. \n\nSpecifically, we integrated (and adapted or extended) many existing computational and VA methods to produce a system that supports information foraging related to geographic movement as reflected in text statements. GeoMovement is unique in identifying movement statements and filtering them by place and time. \n\nFigure\u00a0  shows GeoMovement\u2019s user interface. Users can search and filter based on search terms, the statements\u2019 dates, and impaired movement status. The statements originate from three sources ingested into GeoMovement to demonstrate its capabilities and utility for investigating movement:   \n398 thousand News articles from August 2019 to May 2020, \n  \n328 million Twitter tweets from February 2020 to May 2020, \n  \n15.6 thousand Scientific articles from August 2019 to November 2020. \n  Over 520 million total statements contain diverse movement patterns, things moving, geographic coverage, and temporal differences.   \nThe GeoMovement user interface:   A   Main mapping interface with hexagons selected for place aggregation.   B   Connection lines between co-occurring place mentions from an area near New York City selected.   C   Free text key term search.   D   Text data source buttons.   E   Normal movement vs. impaired movement buttons.   F   Geo-binning buttons.   G   Classification method for bins and connection lines colors and the number of classes chooser.   H   Statements that match the search.   I   Most common bi-grams that match the search.   J   Two-sided Temporal Bar Chart that shows the number of statements, by date, with a divided bar depicting normal movement on the right (green) and negated movement on the left (purple)   K  ) Time range slider \n  \n\nExisting geographic movement research has improved analysis methods\u00a0(Dodge et\u00a0al.  ; Dodge  ; Dodge et\u00a0al.  ; Dodge  ; Graser et\u00a0al.  ,  ,  ; Soares Junior et\u00a0al.  ; Huang  ) and shown how these methods can derive valuable information about human movement and wildlife movement\u00a0(Wang et\u00a0al.  ; Dodge et\u00a0al.  ; Miller et\u00a0al.  ; Zhu et\u00a0al.  ; Li et\u00a0al.  ). However, partly due to the challenges and because computational techniques to address them are relatively new, this prior research focused on geographic movement in precise movement trajectories from sensors such as GPS and mostly ignored movement described in text documents. The limited existing research to analyze movement described in text has only focused on narrow tasks like mapping route descriptions. This work fills a gap in the research to support the study of geographic scale movement by developing and demonstrating methods for analyzing movement described in text documents at a broad scale. We anticipate that our effort will spur more research to leverage this under-utilized geospatial movement information in text documents. \n\nWe present two case studies (and a third case study in a supplementary video demonstration) related to the global pandemic that demonstrates the potential of our approach and show, despite the messy data and imprecise computational predictions, a human can derive meaningful and essential information about geographic movement described in text with GeoMovement. The case studies also demonstrate that GeoMovement can support multi-scale information foraging through vast volumes of messy text statements about geographic movement. Furthermore, the space-time concept/attribute filtering methods implemented effectively narrow in on information relevant to an analyst\u2019s objectives. \n\n\n## Related Work \n  \nFirst, we review related efforts to analyze various geographic movement types described in text documents. Existing research into studying descriptions of geographic movement has primarily focused on reconstructing route descriptions. These routes (driving directions\u00a0Jaiswal et\u00a0al.  ; Drymonas and Pfoser  ), hiking and general route descriptions\u00a0(Moncla et\u00a0al.  ,  ,  ; Piotrowski et\u00a0al.  ), and historical exploration routes\u00a0(Bekele et\u00a0al.  ; Blank and Henrich  ) form a constrained subset of movement statements that simplify and thus do not address many of the challenges with a broader set of movement statements. Other research took the opposite approach of textualizing (convert routes to description) to take advantage of the benefits of text\u00a0Chu et\u00a0al.  ; Al-Dohuki et\u00a0al.  ). Additionally,\u00a0Huang et\u00a0al. ( ) showed how geovisual analytics could improve the retrieval of trajectories in a search. But, their focus on text analytics was only on users\u2019 queries of the data (which was precise sensor-based trajectories). Furthermore, in addition to addressing narrow domains, most of this research has restricted the data to a small amount of text. \n\nThe GeoCAM project\u00a0(Jaiswal et\u00a0al.  ) created an application that identified, extracted, and generated maps of route directions found on webpages providing textual (often formatted) directions to reach a location. While the GeoCAM project complements our current research, their problem is simpler since route directions are just a small subset of movement descriptions. Furthermore, route directions typically follow a semi-structured pattern that allows for simpler ML models and rules-based approaches. In a precursor to the GeoCAM project, Drymonas and Pfoser ( ) used similar techniques to map route directions. These projects encouraged future work like ours to go beyond route directions to general movement descriptions\u00a0(Klippel et\u00a0al.  ). \n\nSecond, we describe related efforts that use geovisual analytics on geographic movement described in text documents. The complicated nature of analyzing place in text documents, especially the need to represent spatial relationships best rendered visually on a map, has prompted other researchers to take a geovisual analytics approach. SensePlace was a system to analyze place mentions in text and pull information from other sources to aid analysis\u00a0(Tomaszewski et\u00a0al.  ). SensePlace2 developed and applied geovisual analytics to methods that focus on the extent to which Twitter users\u2019 tweet location compared to the places they discuss in their tweets\u00a0(MacEachren et\u00a0al.  ). Robinson et\u00a0al. ( ) performed a user study with experts that showed both the advantages, usefulness, and difficulties of such a system for crisis management. SensePlace3 extended this effort by advancing the geovisual analytics techniques and scaling the system to work on millions of tweets per month, thereby improving analysis\u00a0(Pezanowski et\u00a0al.  ). \n\nThe SMART system complements the SensePlace versions\u2019 focus by providing a visual interface enabling human analysts to explore text\u2019s spatial, temporal, and topical components\u00a0(Snyder et\u00a0al.  ; Karimzadeh et\u00a0al.  ). SMART implemented advanced geovisual analytics techniques, including a tweet classifier to filter semantically and a cluster lens to visualize keywords at a large scale. However, like SensePlace3, their system did not focus on analyzing movement. Finally, the NewsStand system\u00a0(Teitler et\u00a0al.  ; Samet et\u00a0al.  ) mapped places where news articles are written compared to the places they discuss but focuses less on geovisual analytics for analysis and more on correctly mapping the text. \n\nA few other efforts use geovisual analytics and mapping systems to analyze places mentioned in tweets\u00a0(Thom et\u00a0al.  ; Bosch et\u00a0al.  ,  ; Felmlee et\u00a0al.  ) and show the potential to take advantage of this geographic data source, albeit mainly focusing on tweets with a geocoded location that makes the challenge different. Mapping and geovisualizations have also been combined with topic analysis and network graph analysis to show similarities between cities\u00a0(Hu et\u00a0al.  ). \n\nJamonnak et\u00a0al. ( ) combined location information associated with videos and the narrations of those videos to show their locations on the map and the topics and sentiment discussed at those locations. Xu et\u00a0al. ( ) created a system to explore Yelp business reviews in areas and their change over time. Ma et\u00a0al. ( ) showed that geovisual analytics is vital to understanding critical local places that need immediate help in a disaster from 911 call transcripts and clusters of specific crime types from police reports. Although these systems successfully demonstrated geovisual analytics on text, they focused on particular topics and did not consider movement. Therefore, they could not be applied to our goal of analyzing wide-ranging types of movement described in text. Since impaired movement detection is not the primary focus of this research, we describe work related to it in Sect.\u00a0 . \n\n\n## Text Computational Processing Predictions \n  \nWe first acquired three different sets of documents consisting of 398 thousand news articles, 328 million tweets, and 15.6 thousand scientific articles. Our document sources and the keyword and time parameters used to obtain them are described in detail in Appendix\u00a0B.1. We cleaned the documents using typical text pre-processing methods and applied computational techniques to identify places in the text, predict the statements that describe movement, and predict statements that describe impaired movement. \n\n### Predicting Movement \n  \nIn Pezanowski and Mitra ( )\u2019s work, humans label the statements with a binary class as either describing geographic movement or not. Since they took this initial step and created a corpus to train a model, we can use this model to predict statements that describe movement. The prediction of this ML model is a probability value between zero (no movement) and one (movement). We set an arbitrary cut-off for GeoMovement to only show statements with a probability greater than 0.6 that the statements are about movement. This relatively loose threshold was arrived at by trial and error and is acceptable for all three sources. \n\n\n### Predicting Geographic Location \n  \nWe used the GeoTxt system to perform GIR on our text sources\u00a0(Karimzadeh et\u00a0al.  ,  ). We chose this because GeoTxt performs comparably or better than other state-of-the-art geoparsers and performs best without case sensitivity, which is common in Twitter data (one of our data sources)\u00a0(Gritta et\u00a0al.  ). A small evaluation corpus had an F1-score of 0.78 for geoparsing place names and an accuracy of 0.91 in resolving those place names correctly. Moreover, it performed even better on higher-order administrative places such as countries and states, which are common in our statements. For enterprise projects, paid commercial sources also exist from software companies such as Esri, Google, and Microsoft. However, we chose not to use these products because they do not allow for customization compared to GeoTxt, which is open-source software and allows adjustments to the software in the future. \n\n\n### Predicting Impaired Movement \n  \nThe global pandemic of 2020\u20132021 brought attention to global movement and how it spreads, and how the pandemic disrupted or prevented regular global movement. Because the pandemic highlighted the importance of analyzing disruptions to movement, we investigated potential strategies for detecting statements about impaired movement. \n\nTo detect impaired movement in our statements, we looked to adapt existing methods of negation detection in text. Negation detection strategies can potentially uncover statements about formal restrictions on movement (of the sort imposed by governments), decisions not to move taken by individuals for their safety, and impediments to movement created by limited public/commercial transport such as canceled flights due to the lack of passengers or ill crew. \n\nIn this section, first, we review existing related research on negation detection and its everyday use cases. Second, we describe how we adapted an existing approach from the literature that detects negation in picture descriptions\u00a0(van Miltenburg et\u00a0al.  ) to our challenge of detecting impaired geographic movement described in text. Third, we show how we improved upon our initial attempt to detect impaired movement using our geovisual analytics methods to analyze initial mistakes in predictions and then modify the rules specifically to detect impaired movement. \n\n#### Existing Approaches to Negation Detection \n  \nAddressing negation in text has been identified as a problem in several existing works. For example, Fialho et\u00a0al. ( ) had remarked that \u201cwhen a negation was involved in a sentence, the classifiers found more difficult to return the appropriate label\u201d in the context of negation in sentences as identified as part of discourse representation structures. Negation detection in text is vital in challenges like automated summarization of medical reports\u00a0(Vincze et\u00a0al.  ; Slater et\u00a0al.  ), summarizing picture descriptions\u00a0(van Miltenburg et\u00a0al.  ), and as a hint in identifying sarcasm\u00a0(Reyes and Rosso  ). Hiremath and Patil ( ) show that sarcasm detection depends upon detecting negative sentences in positive situations and positive sentences in negative situations. \n\nMuch of the current state-of-the-art research on negation detection was influenced by a Workshop titled   Resolving the Scope and Focus of Negation  \u00a0(Morante and Blanco  ), which also produced labeled datasets that continue to be used in training and evaluating the success of new methods. Supervised ML-based solutions such as the LSM Network\u00a0(Zhao et\u00a0al.  ) have learned negative terms while performing sentiment mining automatically from large-scale training data. The current state-of-the-art method, NegBert, is based on ML\u00a0(Khandelwal and Sawant  ; Khandelwal and Britto  ). Although this ML approach is the current state-of-the-art, the challenge in using ML approaches is the need for time-consuming labeling of large amounts of training data. NegBert was trained and tested on datasets designed explicitly for negation detection evaluation and therefore could not be used for our tangential challenge of detecting impaired movement. In the absence of training data, we show that rules-based approaches can still be used. \n\nvan Miltenburg et\u00a0al. ( ) and van Son et\u00a0al. ( ) have used rules to detect negation. They had humans annotate Flickr picture descriptions for negations and define categories of negations. This annotation exercise produced simple clues for negation, thereby allowing their rules-based approach to be effective on picture descriptions. In general, rules-based approaches have been proven to work in negation detection when the domain is relatively narrow and, like most rule-based systems, typically provides high precision but low recall. Rule-based methods can work fine in our application, where a sample of the negative sentences suffices, but having false positives can result in incorrect conclusions. \n\n\n#### Applying Negation Detection to Descriptions of Geographic Movement \n  \nDetecting impaired movement is similar to previous negation detection using specific key terms. However, what constitutes a negated word is ill-defined and varies from domain to domain and problem to problem since terms, like   canceled   or   diverted  , would not always be considered negated. But, when applied to movement, they are. This ambiguity complicates the task. We investigated if we could adapt the current methods that are focused on the negation of words (ex. She does   not   have cancer. The alarm clock did   not   have the feature I wanted.) to detect impaired geographic movement (ex. Our flight to England was   canceled  . The fruit was   stuck   in Brazil because of initial fears early in the pandemic.). \n\nWe used\u00a0van Miltenburg et\u00a0al. ( )\u2019s rules-based approach for negation detection. As discussed above, we adapted this rules-based approach, as opposed to an ML approach, to (1) avoid costly labeling of training data, (2) achieve transparency in how the results are obtained, and (3) because our needs in detecting impaired movement are relatively narrow, which the literature suggests\u00a0(van Miltenburg et\u00a0al.  ; van Son et\u00a0al.  ; Slater et\u00a0al.  ) is a good fit for a rules-based approach. \n\nvan Miltenburg et\u00a0al. ( )\u2019s method tags part-of-speech in text (such as verbs, nouns, adjectives, etc.) and then searches for a list of negation keywords, prefixes, and suffixes to detect negations. Some negation rules examples are a) exact negated words like \u201cno\u201d and \u201cnot,\u201d b) verbs that start with \u201cde,\u201d \u201cmis,\u201d \u201cdis,\u201d or c) adjectives that end with \u201cless.\u201d Overall, all rules are relatively simple and are easily understandable and reproducible. If the input sentence matches a rule, it contains negation. \n\nWe first applied van Miltenburg et\u00a0al. ( )\u2019s exact rules for negation to our statements previously predicted to describe movement (as described in Section\u00a0 ) to predict impaired movement. After 800,000 movement statements were predicted, we stopped and obtained a summary count. The model predicted about 28% of these movement statements as describing impaired movement. \n\nWe selected a stratified random sample of 50 predicted impaired movement statements and 50 predicted normal movement statements. The initial model did not do very well to predict impaired movement correctly. There were 23 true positives where the statement was correctly predicted as impaired movement compared to 27 false positives where the statement was predicted to be impaired movement, but it was normal movement. It did slightly better to correctly predict normal movement with 42 true negatives as opposed to eight false negatives. Table\u00a0  shows the confusion matrix for these predictions. These values equate to a precision of 0.46, a recall of 0.74, an F1-score of 0.57, and an accuracy of 0.65 on the stratified sample.   \nThe confusion matrix for prediction of negated movement with unmodified rules from general negation \n  \n\n\n#### Improving the Detection of Impaired Movement \n  \nTo improve our detection of impaired movement, we either added new rules or removed existing rules. Since there were more false positives than false negatives, this hints that rules should mostly be removed so that fewer statements are predicted to have impaired movement. \n\nNot only did we analyze the errors in statement predictions, but we also consulted a list online of verbs common to movement see Appendix\u00a0A and  ). First, we removed rules specific to verbs that had the prefixes \u201cde,\u201d \u201cmis,\u201d and \u201cdis.\u201d These prefixes could rarely negate the verbs we found related to movement. Second, based on an online list of adjectives related to movement (see Appendix\u00a0A and  ), we removed the adjectives\u2019 rules beginning with \u201ca\u201d and \u201cdis\u201d because they are much too broad and did not make sense with those commonly related to movement. Third, we added the specific lemmas \u201ccancel,\u201d \u201cpostpone,\u201d \u201cprevent,\u201d and \u201cavoid\u201d because they relate to impaired movement. Lemmas allow us to match many different synonyms of these words that all mean the same. \n\nThese rules modifications improved predictions on the existing stratified random sample with a precision of 0.74, a recall of 0.76, an F1-score of 0.75, and an accuracy of 0.79. Therefore, the F1-score improved from 0.57 to 0.75. \n\nSince we used the first set of 100 sampled statements to determine some of our rules, it would not be fair to rely on these improved metrics alone to conclude we improved our method. The rules may over-fit the sample. Therefore, we selected a second unseen test set of 100 statements using the same random stratified technique. On this unseen set, the original rules\u2019 prediction F1-score was 0.59, while our new rules were again much better with a 0.65 F1-score. Overall, our minor modifications to the rules produced substantial improvement. \n\nFinally, since we selected the two sets of 100 statements from new articles, as a final assessment, we sampled 100 statements from both the tweets and scientific articles using the same stratified random sampling technique. Predictions using our modified rules for impaired movement on the tweets resulted in an F1-score of 0.61 and an F1-score of 0.60 on scientific articles. Therefore, our predictions for impaired movement performed slightly less accurately for these two sources than news articles, but still respectable. This lower accuracy is likely because both sources contain language more common for that audience (i.e., slang and other informal languages in tweets and technical language in scientific articles). Based on our experience modifying the rules for news articles, we estimate that additional rule changes would also improve predictions for tweets and scientific articles. However, based on our experience and the literature, we surmise that given resources to label a large amount of training data, using an ML approach as in\u00a0Khandelwal and Sawant ( ) would likely produce more accurate predictions. In summary, we are using a rules-based approach as a proof-of-concept prediction that is important to show the potential benefits of detecting impaired movement statements and use this as an attribute to analyze geographic movement described in text. \n\n\n\n\n## Geovisual Analytics to Find Meaning in Descriptions of Movement \n  \nGeoMovement is a web-based geovisual analytics system that allows users to explore descriptions of geographic movement. GeoMovement serves as an interface between the human and the data described above and is summarized in Fig.\u00a0 . The statements\u2019 content, place mentions, date of creation, and impaired movement prediction are all searchable. The map visualizes place mentions in multiple levels of aggregate geo-bins. A geo-bin is many smaller places (e.g., cities) aggregated and displayed as one larger place that they all reside within (e.g., country). Geo-bins allow for a clear summary of location-based data. By choosing particular places of interest on the map, the user can visualize the co-occurrences of places. The individual statements view completes the overview-first + detail approach. This workflow matches the information-seeking mantra of overview-first to gain an awareness of the information and details of interest on-demand\u00a0(Shneiderman  ). This section is divided into three subsections that follow the information-seeking mantra that starts with the overviews, then options for the user to search to filter to statements of interest, then detailed views of the filtered statements. \n\nFor detailed information on GeoMovement, Appendix\u00a0B.1 describes the sources and nature of the data sources and statements. In addition, Appendix\u00a0B.2 includes technical details of the application development that allow for fast user queries on large volumes of text, thereby enabling efficient sensemaking. An important technical component of our approach is our use of Elasticsearch ( )) as the primary information storage and retrieval software for GeoMovement. Elasticsearch is a search engine that accepts many search parameters like free text and time, and returns matching results ordered by most relevant to the user\u2019s search. In addition, it can group results by attributes like place mentions in the text. Most impressively, Elasticsearch does all of this and returns results very quickly, most often in the matter of milliseconds. \n\n### Overviews \n  \nThere are two primary overview means to explore the movement descriptions. The first overview is the map (Fig.\u00a0 A) that displays the number of statements spatially aggregated by their place mentions. Since we have a large number of statements from multiple sources, GeoTxt extracted over 98,000 unique place mentions from them. Displaying this large number of places on a map using points would likely be very confusing for users. Many places would overlap, and quickly seeing overall patterns and comparing quantities between places would be difficult. This is the primary reason for aggregating the statements place mentions. Polygon bins that contain many\u00a0place mentions in the statements are colored\u00a0darker than those with fewer place mentions. Users get a clear overview of place mentions in the statements and a way to visually compare places. \n\nWe chose five shapes to spatially aggregate the number of statements by their place mentions. The geo-bins scales include continent, country, administrative 1 (the worldwide equivalent of a state in the US), and two sizes of a hexagonal pattern. Each scale allows users to explore different types of movement data, like long-range bird migrations by continent, traded goods by country or administrative level, and detailed movement through the hexagons that do not adhere to political boundaries like the spread of disease. We chose hexagons as an aggregation shape since they tessellate and will distort values less than squares\u00a0(Birch et\u00a0al.  ; Esri  ). The user interface provides an option shown in Fig.\u00a0 F, where the user can choose the aggregate level. \n\nThe bin counts are divided into classes to ease visual comparisons between bins. The user has control of the bin count classification technique and\u00a0the number of classes, as shown in Fig.\u00a0 G. The user has five choices on how to classify the aggregate counts. Jenks Natural Breaks, Equal Interval, Standard Deviation, Arithmetic Progression, and Quantile are options, and they can enter the number of classes between two to seven. These classification techniques are well-accepted statistical methods to make the data more understandable. \n\nWe used the geostats JavaScript library ( ) to calculate the class breakpoints for the chosen users\u2019 classification method. Each classification method is valuable depending on the user search and resulting data. Darker colored bins represent areas that have a larger number of place mentions within the bin. The color scheme is a sequential color scheme chosen from ColorBrewer ( )\u00a0(Brewer et\u00a0al.  ) to ensure the classes of statement counts are easily distinguishable. \n\nThe second overview in GeoMovement is a Two-sided Temporal Bar Chart. Research has shown differences in how people understand geographic movement, such as those who think more spatially than others\u00a0(Liben and Downs  ; Ishikawa  ). Based on this research, GeoMovement provides the user with\u00a0multiple views of the data. The Two-sided Temporal Bar Chart shows aggregate counts of statements grouped by the month they were published (Fig.\u00a0 J). The oldest month is at the top and the youngest month is at the bottom. The Two-sided Temporal Bar Chart allows for visualization of the amount and changes over time, both in impaired movement on the left side and normal movement on the right side. When hovering the mouse over the chart, the number of statements that match the current user search are shown in bars in the foreground, while the total number of statements in GeoMovement are shown in bars in the background. Figure   shows an example of the number of statements that match the user search in the foreground once the user has hovered the cursor over the chart. \n\n\n### Search \n  \nAfter the overviews give the user an understanding of the data, they can begin filtering it through search options. The five key ways to search the data are (1) free-text search (Fig.\u00a0 C), (2) buttons to select from the three sources for statements \u2014 news articles, tweets, and scientific articles (Fig.\u00a0 D), (3) buttons to select impaired movement statements or normal movement statements (Fig.\u00a0 E), (4) a time-range slider (Fig.\u00a0 K), (5) and clicking location(s) on the map to filter by location. Multiple features can be chosen by holding the Ctrl-key on their keyboard and selecting the next feature with a mouse-click. After any of the first four searches are performed, the geo-bins and Two-sided Temporal Bar Chart update to show counts of statements that match the searches. After the fifth search, the detailed views appear. \n\n\n### Detailed Views \n  \nThe detailed views of statements and their attributes include (a) connecting great circles drawn on the map between place mentions in the statements, and co-occurring place mentions in the same statement, (b) the five most common bi-grams for the set of statements matching the search, (c) and a table showing the actual statements that match the search. All connections between places are aggregated to show places commonly used together. The aggregate count classes adhere to the users\u2019 currently chosen map classification method and total class number in a sequential green color scheme that is color-blind friendly, also selected from ColorBrewer\u00a0(Brewer et\u00a0al.  ). \n\nIt is important to note that the connection lines are completely accurate in showing movement between the places since they are drawn solely by the places\u2019 co-occurrence in a statement. For example, the statement below has three place mentions and, therefore, three possible movement pairs: Sydney\u2013New York, Sydney\u2013London, New York\u2013London. The first two are correct concerning movement references in the statement, but the third is not correct because there is no direct movement between New York and London. One could argue that the movement could be from New York to Sydney and then next to London from Sydney, but this is not probable. However, these connection lines give the user overall patterns of interest that they can confirm through inspection of the statements.  \nBut this will be the first time a commercial flight is flown from Sydney to New York, and just the second time from Sydney to London, Qantas said\u00a0(Garber  ). \n Second, the ten most common bi-grams are shown for the set of statements that match the search and have place mentions in the chosen location bin. This provides details about the actual statements behind the overviews by showing the most common words and topics in the selected statements. Since many of the statements contain place mentions and the search location is an essential parameter for the matching statements, two-word place names are often in this bi-grams list. These place mentions may be valuable, but we also found that it was often more important to see bi-grams about topics and not necessarily places when exploring the data. Therefore, we allow users to double-click a bi-gram to remove the bi-gram from the list, and the next most common bi-gram appears, up to the 20th most common bi-gram. To re-populate the bi-grams list, the user can re-run the search. The user can also select multiple bins by holding the Ctrl-key and clicking another map bin. Users can then compare the most common bi-grams for both map bins to see differences between statements with place mentions in each bin. \n\nTo complete the overview first + detail approach to analysis, once the user found sets of statements of interest in the overviews and chose a map bin, the user can see the actual statements matching the search in a paged list. The displayed statements match all search parameters (when selecting multiple bins, statements can have place mentions in either bin). The user can scroll through the pages of statements. Each statement\u2019s published date is also shown. If there is a particular statement of interest, holding the Ctrl-key while clicking on the statement will open the original document on the Web in a new browser window. \n\nGeoMovement\u2019s tight use of modern search engine technologies and Information Retrieval (IR) allows for extremely fast human-in-the-loop sensemaking for the most relevant information on movement. Again, Appendix\u00a0B provides further details, and our supplemental video showcases a real-time live demonstration of efficient knowledge discovery using GeoMovement. \n\n\n\n## GeoMovement Assessment \n  \nFirst, we show the challenge of interpreting movement described in statements without GeoMovement by discussing summary statistics of the data. Second, we present two case studies that show how our approach can retrieve information about movement from vast quantities of statements. A third case study is included in a supplemental video. It is recorded in real-time to show that a user can quickly extract meaningful information about movement despite the challenges posed by the large quantity of messy text, ambiguity in text descriptions, and imprecise computational predictions. Third, we show how these case studies also generated future GeoMovement needs. Finally, in Appendix\u00a0C, we discuss the skill level and hardware and software requirements for GeoMovement users. \n\n### Illustrative Data Summary Statistics \n  \nTo show the value of GeoMovement, we created summary statistics of the data to clearly illustrate how it is unreasonable to think a human can analyze and understand large quantities of movement statements without such a system. We chose three keywords related to our case studies and three prominent place names: one being a country, one a state, and one a city. The number of our statements that match these keywords and place names is shown in Table\u00a0 . To relate these statistics to our first case study in Sect.\u00a0 , we filter GeoMovement\u2019s 520 million statements (36 thousand of those contain the term   smuggling  , 275 thousand of them include the term   gold,   and 201 thousand mention   India  ) and efficiently identify important gold smuggling patterns around and in India. To produce these statistics, we used our GIR extracted place names to determine the number of statements that contain each selected place name. And, to find the number of statements containing each of the keywords, we searched the statements in a Postgresql database using a full-text search ( ) so that different variations of the same word will be matched (e.g., smuggling, smuggled, smuggle).   \nSummary statistics of the number of statements that contain selected key terms and place mentions\u00a0plus the total number of statements \n  \n\nThis exercise to generate summary statistics is meant to show that without such a geovisual analytics system like ours, it would be extremely difficult, if not unfeasible, to perform geographic, temporal, and attribute sensemaking of the described movement. Although GeoMovement\u2019s 520 million statements are substantial, it is still a fraction of the accessible text available that could be included and analyzed in GeoMovement, given more development and computational resources. \n\n\n### Case Studies \n  \nTo confirm our claim that the geovisual analytics interface helps users understand and make sense from the statements and multiple computational predictions, we provide two detailed case studies below from different types of (prototypical, fictitious) potential users. The case studies presented provide evidence of usability. A third case study, given only in the video supplement (due to space limitations in the text), adds additional evidence about the flexibility and utility of GeoMovement to explore the mix of text data sources from different perspectives. \n\n#### Understanding International Crime Affecting India \n  \nJennifer Lang is a college student who wants to write a class report about different types of international crime affecting India. She opens her web browser to GeoMovement and types   smuggling   to begin her search. She notices that many statements involving smuggling are in October 2019, despite that month having fewer statements overall. She adjusts the time range slider to filter statements to that month and sees a hotspot of activity in England. She clicks the hexagon bin in England and is reminded of a significant human smuggling event in that month where many people lost their lives after being trapped in a truck that was smuggling them (Fig.\u00a0 ). Although this is a significant smuggling event from a British perspective and also highlighted in the U.S. news, she decides to look for other ways to focus on India.   \nBy filtering the search results for   smuggling   to only show statements from October 2019, she sees many statements from a significant human trafficking event in England where many people lost their lives \n  \n\nAs her next step, she chooses to aggregate place mentions by the country level and select India. Figure   shows the results, and she sees that most results related to smuggling are affecting India from the neighboring countries of Pakistan and Bangladesh, and a few more countries. In the bigrams list, she sees many references to drugs and gold being smuggled into India.   \nA search for   smuggling   shows important location sources for India and a spike of activity in October 2009 \n  \n\nNext, after reading some individual statements, she finds out more about multiple cases of gold smuggling from both outside and within India (Fig.\u00a0 ). To get a more detailed analysis of the movement, she types   gold   in the search box and chooses to aggregate place mentions by the state level. By clicking the neighboring state of Sindh, Pakistan, which has many place mentions in the statements, connections with many states in India are highlighted, including a state in southeast India.   \nA search for   gold   produces many statements about gold smuggling and possible routes \n  \n\nShe clicks this state in India and discovers that the state is Tamil Nadu, where the large city of Chennai is located. From reading a few statements, she knows that gold smuggling in Chennai is arriving at the airport and through the Chennai Express train from Mumbai. The connection with Mumbai through the train is confirmed on the map by the strong connection with Mumbai\u2019s state, Maharashtra, on the west coast, closer to Pakistan (Fig.\u00a0 ). She can now clearly visualize and report on some of the prominent drugs and gold smuggling sources into and throughout India and read more detailed descriptions about individual incidents.   \nWhile looking for more detail about gold smuggling, statements suggest a smuggling route from Mumbai to Chennai along the Chennai Express train \n  \n\n\n#### Examining the Impact of the Pandemic on Travel for Tourism \n  \nArti Reddy is a travel agent in India. She uses GeoMovement to understand the pandemic\u2019s impact on global travel and travel related to India. As of May 2020, like other countries in the World, India was dealing with a global pandemic. Since Arti had previously planned to advertise to potential customers traveling for sporting events, she chose this topic to investigate. She loads GeoMovement and enters the term   sports   in the text search box. With the geospatial hexagon geo-bins as a layer, she quickly sees a hotspot of discussion in Japan. She selects to filter statements for impaired movement. While mousing over the Two-sided Temporal Bar Chart, she sees that the impaired movement statements are more prevalent in recent months (except May, where there is less data), as seen in the Two-sided Temporal Bar Chart in Fig.\u00a0 .   \nMousing over the Two-sided Temporal Bar Chart shows the number of filtered statements for impaired movement (purple bars to the left of the axis) involving sports has been increasing since the pandemic began (bars are aggregate counts of statements by month, with the oldest month at the top). Filtered statements by the search are shown in the foreground, while all statements are shown in the background on the mouse hover \n  \n\nArti switches to the country geo-binning, chooses to view both types of movement again, and moves the timeline from July 2019 to December 2019, and Japan is still a popular location. After selecting Japan on the map, she sees in the bi-grams list that the upcoming Olympics in Japan are prominent, and on the map, she sees that there are connections between Japan with many other places in the World (Fig.\u00a0 ). There is much discussion about the potential impact of the pandemic on the Olympics. She is intrigued to see that there are connections between Japan and her home country that implies her business will be affected.   \nSports have been disrupted during the pandemic, including the Olympics, where athletes travel from around the World. Mousing over the Two-sided Temporal Bar Chart shows that the filtered statements for impaired movement have increased since the pandemic began \n  \n\nSince she is most familiar with India\u2019s geography, she changes the geo-binning to the state level for more detail and selects two states in India with much discussion (Fig.\u00a0 ). A quick look through the statements shows that the Olympics are in jeopardy, and a closer-to-home event of the under-17 women\u2019s soccer World Cup scheduled to be in India was unfortunately postponed. Teams would have come from around the World for this event. This sad news prompts her to follow up on the story by clicking the statements to view the original articles in her web browser to see if it will be rescheduled and thus if there will be a future need for travel. From reading the statements, she sees that an auto show, the AP World Indoor Sporting Championships, and the Australian Grand Prix auto racing\u2019s China leg are three events postponed in China. Cricket in Australia was also severely affected, with teams planning to come from India, England, and many other countries to compete. Major sporting leagues in the US, like the NBA basketball league, were also interrupted. The disruptions to nearby events like cricket are particularly concerning given the sport\u2019s popularity with Indians and their potential spectator travel related to her business.   \nSports in India have also been affected, including competitions where local teams compete against other teams from around the World \n  \n\nLastly, to see how widespread the pandemic\u2019s impact is on global sporting events where Indians may travel, Arti explores other countries and sees that the pandemic has significantly impacted professional soccer in Spain and other European countries. Madrid, Milan, and cities in Germany where prominent soccer clubs play all show up clearly on the map as having their games affected (Fig.\u00a0 ).   \nEurope is another example where the pandemic postponed many of their beloved soccer matches \n  \n\nThese use cases highlight the potential for information foraging and acquisition from statements describing movement. Geovisual analytics that combines multiple views of the data and overview + detail capabilities allow users to quickly identify important locations, connections between locations, time periods, and topics of interest. We showed how the map view could show critical hotspots, and the Two-sided Temporal Bar Chart can show essential time periods, prompting the user to investigate details. \n\nIn this section, our use cases show GeoMovement\u2019s current sensemaking capabilities. Moreover, the examples of computational prediction errors and suggested solutions highlight how geovisual analytics is well suited for allowing user interactions to either explicitly or implicitly improve GeoMovement\u2019s computational predictions. Improved prediction accuracy can make the sensemaking process more efficient by reducing the human effort to sift through incorrect intermediate results. \n\n\n\n### Case Studies Needs Assessment \n  \nIn addition to our case studies demonstrating the effectiveness of GeoMovement in extracting information about movement, we used them as a needs assessment to propose additional functionality. In the first case study, Jennifer identified statements from an event that she was not interested in. Although she quickly found what she was looking for, other problems may require much further analysis, and future results should not include statements deemed not relevant. Therefore, a future addition to GeoMovement can consist of a mechanism to either mark statements as completely \u201cnot relevant\u201d and not show them or as \u201cless important.\u201d If user accounts were added to GeoMovement, these preferences in statements could be stored. If certain types of statements will never be relevant to that user, their marked statements can be used to affect their future search results. Such user input can be used as feedback to the system, and in the future, statements that are very similar to those that are marked not of interest will also be filtered out; i.e., the system learns from the feedback. Elasticsearch allows for on-the-fly criteria provided in searches that promote or demote statements or exclude them, allowing for these criteria to be personalized if user accounts were added and changed for each search. \n\nIn the second case study, Arti found that impaired movement became more prevalent in recent months. It would be a fair assumption that this is because of travel restrictions from the pandemic. However, Arti may ask for more detail about \u201cHow is impaired movement changing over time?\u201d To do this, she should easily access statements by month. Currently, she would have to change the time slider filter to each month and select the impaired movement button. A straightforward way to answer this question would be to choose any bar in the Two-sided Temporal Bar Chart to update the time range and impaired movement filters. For example, clicking on the impaired movement purple bar for January 2020 would update all views to show these statements. Similarly, when Jennifer found a spike in smuggling activity in India in October 2009, she would likely ask herself why this is and want a quick way to filter to those statements. \n\nFinally, both Jennifer and Arti were interested in India. A question they both may have is: \u201cWhat other locations have similar problems as India?\u201d Currently, GeoMovement allows selection on the map of multiple locations to show bi-grams lists for both locations\u2019 statements next to each other. However, there are automated ways to give users hints on locations with similar statements. For example, Elasticsearch provides a \u201cpercolate\u201d query where, after the user already selects a location, they could choose a user interface control and click a second location. The statements from the first location can be used in the percolate query. The result would be statements like the first set, and therefore other places like that place will show. \n\nOur evaluation of the sensemaking capabilities of GeoMovement includes a statistical summary illustrating the challenge in extracting meaningful information without it, case study demonstrations, a needs assessment, and a list of user requirements. We show that despite the large volume of messy data that is at times ambiguous, GeoMovement can quickly extract meaningful information about geographic movement. \n\n\n\n## Results and Implications \n  \nAnalyzing a large volume of text describing geographic movement requires imprecise computational processing and predictions on already messy data that different people can perceive differently, resulting in some errors. We present the data in a geovisual analytics web application that follows the overview-first + detail mantra\u00a0(Shneiderman  ). Users can find exciting patterns in the overviews and areas that need further investigation. They can also search the data and eventually drill down to the actual statements and articles. Results that appear to be erroneous or merely uninteresting can be hidden so that it is easier for the analyst to focus on more helpful information. \n\nOur approach tightly integrates geovisual analytics with behind-the-scenes computational predictions for statements describing movement from most text that does not, GIR methods to extract geography for mapping and analysis, and a novel prediction for impaired movement. We show the value of GeoMovement through case studies where humans quickly learn about and visualize geographic movements. Furthermore, we demonstrate that a multi-scale system that applies a space-time-concept/attribute filtering process can effectively make sense from large volumes of messy movement statements. \n\nBecause of the challenges of utilizing messy big text data for understanding geographic movement, previous work has either focused only on movement data from precise sensors or addressed narrow domains of movement described in text. However, the need for improved methods to utilize big text data about movement is illustrated in the\u00a0work described in Sect.\u00a0 , and Hultquist and Cervone ( ), Hultquist and Cervone ( ), and Janowicz et\u00a0al. ( ), where geography in text complements sensor data or stands alone to solve real-world problems. MacEachren ( ) also encouraged such advances in his positional paper. GeoMovement (1) tightly integrates computational predictions and geovisual analytics, and (2) is built with modern web computing technologies that return results from user queries in milliseconds. These features allow users to make sense of and uncover movement patterns quickly. We show how this efficient and broad movement exploration is unfeasible without computational and visual means to assist a human. Moreover, this completes our research objective to show that modern advances in NLP and IR can leverage large volumes of movement statements to understand the movement described quickly. \n\nAlthough we stopped collecting text for GeoMovement, our techniques can be adapted in a straightforward technical way to accommodate a continually updating source given modest computational and storage requirements. In addition, all of our code is available with an open-source license to prompt future advances (Pezanowski  ). \n\nWe anticipate GeoMovement or a production version to be beneficial for scientists, journalists, or even the general public to find information about the movement of humans, wildlife, goods, disease, and much more. Advances to analyzing movement statements can provide similar knowledge gains as movement recorded by sensors with precise geospatial and temporal data. \n\n\n## Conclusions and Future Work \n  \nOur research shows that modern computational techniques can be combined with a human-in-the-loop geovisual analytics system to overcome significant challenges and identify, process, and explore large volumes of movement statements to quickly obtain an overview of movement patterns and forage for detailed information of interest. Future research should take advantage of the wealth of context information found alongside geographic movement in text documents about why, when, and how the movement is occurring that is not often present in precise GPS data. Our research methods can likely be adapted to analyze statements involving other attributes like time and more complex spatial analysis like correlation. In addition, future research should explore the integration of precise geospatial movement data with movement in text. One initial way is to link them spatially and temporally. For example, the following steps can involve connecting entities in the text to other information using the Semantic Web (Berners-Lee et\u00a0al.  ) and linked geographic data (Stadler et\u00a0al.  ; Janowicz et\u00a0al.  ). Future research needs to identify what makes movement statements different from other statements. Pezanowski et\u00a0al. ( ) took initial steps towards this goal by identifying vital characteristics of movement statements that humans use to differentiate the movement described. \n\nAlso, since we chose to illustrate the effectiveness of GeoMovement through the case studies and other assessments in Sect.\u00a0 , future work should include a more thorough evaluation of user needs, especially focusing on the additions suggested in this Section. \n\nMoreover, prior research shows that a geovisual analytics system combined with ML predictions can continually improve the accuracy of the ML predictions by having users correct machine errors\u00a0(Snyder et\u00a0al.  ; Andrienko et\u00a0al.  ). An extension of GeoMovement can make it an intermediary between computational predictions and humans. As more humans use GeoMovement, humans can iteratively correct any errors they encounter, thereby improving the predictions. As an example, in Sect.\u00a0 , we discuss how an ML model would likely be superior to a rules-based approach detecting impaired movement but requires a large amount of training data. GeoMovement can show initial predictions of normal movement and impaired movement from an ML model that used a small amount of training data. A simple tool can allow users to correct errors in the predictions. Once many users are using GeoMovement, the training data set can proliferate. This technique has also shown success in commercial production mapping systems like Google Maps, where users reach their destination, and Maps asks about driving directions\u2019 accuracy. \n\nTwo overall ways that GeoMovement can improve the computational predictions are, first, explicitly asking for feedback on any incorrect predictions and second, through implicit user actions. For example, in GeoMovement\u2019s statements view, a button can be added to explicitly mark any statements that do not describe movement. Once a sufficient number of users mark a statement to conclude that it is incorrect, the corrected statement can be added to the corpus of statements used to train the model. As an example of implicit actions, the statements\u2019 list is currently returned in order by the search engine software that roughly corresponds to how closely they match the search terms entered and how frequently those search terms appear in the statement. Skipped statements can be recorded when users page through results to find what they are looking for. If many users skip certain statements, they can be deemed less valuable and given a weighting that lets them be listed lower in the order they are returned, thereby promoting more critical statements. There are many other possibilities to obtain either explicit feedback (users correct place mentions that were assigned to the incorrect location using the GIR techniques\u00a0Karimzadeh and MacEachren  ) or implicit feedback from users (identifying important locations to highlight based on where previous users navigated to on the map) to improve GeoMovement\u2019s sensemaking ability. \n\nAdditionally, none of the existing GIR systems have ideal performance metrics. There is often a trade-off between different geoparsers\u2019 false positives and false negatives and other ways to rank geocoding results on large datasets. Future work can allow user controls in GeoMovement to choose different geoparsing back-ends. Also, the open-source GIR GeoTxt can be modified to allow GeoMovement users to set a tolerance to allow more false-positives in situations where missing relevant statements are most critical or allow fewer false-positives in situations where the key is a quick overview and having every relevant statement does not matter. Our use of geovisual analytics for GeoMovement adds substantial potential for future improved analysis of movement statements. \n\nFinally, it is important to revisit that GeoMovement only supports English. Therefore, a valuable extension to our research would be the addition of other languages. The higher-level software, programming languages, and APIs (discussed in Sect.\u00a0B) we used in our research to harvest, process, store, retrieve, and visualize data are all capable of handling many languages and character sets. However, incorporating documents in other languages into GeoMovement would require significant work on the three main computational predictions we used. First, identifying statements that describe movement would require a large amount of labeled training data in the added language. Also, for higher accuracy in predictions, language models like ELMo would need to be re-trained for that language, like the research of Che et\u00a0al. ( ) and Fares et\u00a0al. ( ). Second, our geoparsing methods to extract place mentions and geocode them to their correct location on Earth would need to be adapted (Mandl et\u00a0al.  ). Third, our negation detection techniques would also require adaption to support other languages (Morante and Blanco  ). These changes are possible but need significant work as existing research is less robust than the equivalent in English. \n\nThe World is a dynamic place, and understanding geographic-scale movements of things is essential in many domains like business, public health, and environmental science. Leveraging information about movement found in text can complement precise sensor-based movement data. Also, movement described in text is valuable since precise movement data is often unavailable because of high costs or impracticality to deploy sensors. GeoMovement and the combination of computational and visual methods it integrates are steps toward that objective. \n\n\n\n### Supplementary Information \n  \nBelow is the link to the electronic supplementary material. \n\n\n \n", "metadata": {"pmcid": 8866112, "text_md5": "5d3e7edb38b1ab0e97de4d25f4496230", "field_positions": {"authors": [0, 62], "journal": [63, 85], "publication_year": [87, 91], "title": [102, 164], "keywords": [178, 282], "abstract": [295, 1747], "body": [1756, 62516]}, "batch": 1, "pmid": 35229072, "doi": "10.1007/s42489-022-00098-3", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8866112", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8866112"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8866112\">8866112</a>", "list_title": "PMC8866112  Exploring Descriptions of Movement Through Geovisual Analytics"}
{"text": "Sung, Mujeen and Jeong, Minbyul and Choi, Yonghwa and Kim, Donghyeon and Lee, Jinhyuk and Kang, Jaewoo\nBioinformatics, 2022\n\n# Title\n\nBERN2: an advanced neural biomedical named entity recognition and normalization tool\n\n# Keywords\n\n\n\n# Abstract\n \n## \u2003 \n  \nIn biomedical natural language processing, named entity recognition (NER) and named entity normalization (NEN) are key tasks that enable the automatic extraction of biomedical entities (e.g. diseases and drugs) from the ever-growing biomedical literature. In this article, we present BERN2 (Advanced Biomedical Entity Recognition and Normalization), a tool that improves the previous neural network-based NER tool by employing a multi-task NER model and neural network-based NEN models to achieve much faster and more accurate inference. We hope that our tool can help annotate large-scale biomedical texts for various tasks such as biomedical knowledge graph construction. \n\n\n## Availability and implementation \n  \nWeb service of BERN2 is publicly available at  . We also provide local installation of BERN2 at  . \n\n\n## Supplementary information \n  \n are available at   Bioinformatics   online. \n\n \n\n# Body\n \n## 1 Introduction \n  \nBiomedical text mining is becoming increasingly important due to the constantly growing volume of biomedical texts. From these texts, biomedical entities of various types such as gene/protein or disease can be automatically annotated with named entity recognition (NER) and linked to concept unique IDs (CUIs) with named entity normalization (NEN). Many biomedical text mining tools combine NER and NEN in a single pipeline to support large-scale annotation of biomedical texts. One popular example is PubTator Central (PTC) ( ). With recent progress in biomedical language models ( ;  ), biomedical text mining tools that are more accurate than PTC have been introduced ( ) and they are often used for downstream tasks such as biomedical knowledge graph construction ( ) and biomedical search engine ( ). \n\nHowever, existing biomedical text mining tools for biomedical NER, which often come with NEN, have several limitations. First, they provide annotations for a small number of biomedical entity types (e.g. five entity types in  ). Second, they often use multiple single-type NER models to annotate entities of different types ( ), which require larger Graphics Processing Unit (GPU) memory for parallelization, but are very slow at inference when used sequentially. Lastly, many tools employ NEN models based on pre-defined rules with dictionaries ( ;  ), but they cannot cover complex morphological variations of biomedical entity mentions. For instance, a simple dictionary-matching NEN model cannot normalize \u2018oxichlorochine\u2019 into its canonical mention \u2018hydroxychloroquine\u2019 (mesh: D006886) unless the dictionary explicitly contains the mention \u2018oxichlorochine\u2019. \n\nAs shown in  , our proposed tool, BERN2 (Advanced Biomedical Entity Recognition and Normalization) addresses these challenges by (i) supporting nine biomedical entity types, which are the largest among other commonly used tools listed in the table, (ii) dramatically reducing the annotation time by using a single multi-task NER model and (iii) combining rule-based and neural network-based NEN models to improve the quality of entity normalization. We provide BERN2 as a web service with RESTful Application Programming Interface (API) and also allow users to locally install it. The usage of BERN2 is detailed in  . \n  \nComparison of different biomedical text mining tools \n    \n\n## 2 Materials and methods \n  \nBERN2 is designed to recognize and normalize nine types of biomedical entities (gene/protein, disease, drug/chemical, species, mutation, cell line, cell type, DNA and RNA). As illustrated in  , we support two input formats: plain text and PubMed ID (PMID). \n  \nAn overview of BERN2. Given plain text or a PubMed ID (PMID), BERN2 recognizes nine biomedical entity types and normalizes each concept \n  \nWhen plain text is given, a multi-task NER model of BERN2 first extracts the exact positions and types of biomedical named entities in the text (see Section 2.1 for a detailed description of the multi-task NER model). For example, for plain text \u2018\u2026 tumor growth through arginine\u2026,\u2019 our NER model locates the positions of \u2018tumor\u2019 and \u2018arginine\u2019 in the text and classifies them as disease and drug/chemical types, respectively. These named entities are then normalized into corresponding CUIs in designated dictionaries by our NEN models. We use hybrid NEN models for three entity types (gene/protein, disease and drug/chemical) to increase the number of entities being normalized correctly (see Section 2.2 for a detailed description of the hybrid NEN model). For example, our NEN model normalizes a gene/protein mention \u2018atg7\u2019 into \u2018NCBIGene: 10533\u2019 and a drug/chemical mention \u2018arginine\u2019 into \u2018mesh: D001120\u2019. \n\nWe use the plain text annotation mode described above to pre-compute the annotations of PubMed articles (abstracts only), which are stored in an external database. Hence, when PMIDs are given, BERN2 returns pre-computed annotations from its database whenever possible, which is much faster than annotating the plain text as shown in  . If an abstract of a PubMed article has not been pre-computed (i.e. not found in the database), BERN2 annotates it and stores the annotation results in the database. \n\n### 2.1 Multi-task named entity recognition \n  \nWhile BERN ( ) employs accurate NER models based on a pre-trained biomedical language model ( ), it uses multiple single-type NER models (i.e. four BioBERT models to annotate four entity types except for mutation), which requires a large amount of GPU memory for parallelization but makes the entire pipeline slow without such parallel inference. BERN2 adopts a multi-task NER model that supports efficient parallel inference for eight entity types (except for mutation). \n\nFollowing  , our multi-task NER model consists of a shared backbone model and a separate task-specific layer for each entity type. We use Bio-LM ( ), a state-of-the-art pre-trained biomedical language model, as our backbone model and use two-layer MLP with ReLU activation as a task-specific layer. Each task-specific layer outputs probabilities of whether each token is the beginning, inside or outside (BIO) of named entities. During training, we merge five training sets of all entity types. We use BC2GM ( ) for gene/protein, NCBI-disease ( ) for disease, BC4CHEMD ( ) for drug/chemical, Linnaeus ( ) for species and JNLPBA ( ) for cell line, cell type, DNA and RNA. \n\nAt inference, our NER model takes an input text and outputs predictions from all task-specific layers in parallel. In fact, the multi-task NER model in BERN2 only consumes a small amount of GPU memory that is on par with using a single pre-trained LM since having multiple task-specific layers\u2014two-layer MLP each\u2014only adds a small number of parameters. This also allows us to adopt a larger pre-trained language model on a single GPU, which often outperforms smaller models. In this work, we use the large version of Bio-LM. (We use the   RoBERTa-large-PM-M3-Voc   checkpoint from  .) Compared to BERN whose NER model has approximately 432M parameters (i.e. four LMs with 108M parameters each) and takes 600\u2009ms to annotate four entity types, our multi-task NER model in BERN2 has approximately 365M parameters (i.e. a single LM with 365M parameters) and takes 200\u2009ms to annotate eight entity types while also enjoying the expressiveness of a large pre-trained LM. Note that due to the lack of public training sets for mutations, both BERN and BERN2 use tmVar2.0 ( ) for mutation NER. We refer interested readers to   for a detailed description of the NER model used by BERN2. \n\n\n### 2.2 Hybrid named entity normalization \n  \nRule-based NEN models that are often used by biomedical text mining tools ( ) cannot handle all morphological variations of biomedical named entities. Instead,   introduce BioSyn, a neural network-based biomedical NEN model that leverages vector representations of entities to cover such variations. Specifically, BioSyn builds a dictionary embedding matrix from an entity encoder, where each row vector denotes the representation of an entity name in the dictionary. The input mention embedding is computed from the same entity encoder and BioSyn retrieves an entity name from the dictionary matrix that has the highest inner product score with the input mention embedding. Each mention is then normalized into the CUI of the retrieved entity. \n\nBERN2 first tries rule-based normalization on each named entity and only the ones that were not normalized by the rule-based models are then normalized by BioSyn. We employ this hybrid approach for three entity types (gene/protein, disease and drug/chemical) where fine-tuned BioSyn is available. (We use the checkpoints   biosyn-sapbert-bc2gn   for gene/protein,   biosyn-sapbert-bc5cdr-disease   for disease and   biosyn-sapbert-bc5cdr-chemical   for drug/chemical from  .) Other types are processed by rule-based models. With our hybrid NEN model, we safely increase the number of correctly normalized entities. For the sake of users, we also show whether each entity has been normalized by the rule-based NEN model or BioSyn in the annotation results.   provides details of our normalization model. \n\n\n\n## 3 Results \n  \n### 3.1 Named entity recognition \n  \n shows the NER performance of different biomedical text mining tools including PTC ( ),   ( ), BERN ( ) and BERN2. While BERN2 supports the largest number of entity types, it also outperforms other tools on most types except on the species type. \n  \nResults on biomedical NER benchmarks \n    \n\n### 3.2 Named entity normalization \n  \n shows the normalization accuracy of the BC2GN (gene/protein) and BC5CDR (disease and drug/chemical) test sets. Again, BERN2 that uses hybrid NEN (rule-based + BioSyn) outperforms other tools. \n  \nResults on biomedical NEN benchmarks \n    \n\n\n## 4 Conclusion \n  \nIn this article, we present BERN2, a biomedical text mining tool for accurate and efficient biomedical NER and NEN. With a multi-task NER model and hybrid NEN models, BERN2 outperforms existing biomedical text mining tools while providing annotations more efficiently. We support both web service and local installation of BERN2 for the ease of employing BERN2 in other systems.   provides example usages where BERN can be easily replaced with BERN2. \n\n\n## Funding \n  \nThis work was supported in part by National Research Foundation of Korea (NRF-2020R1A2C3010638, NRF-2014M3C9A3063541), the Ministry of Health & Welfare, Republic of Korea (HR20C0021) and the ICT Creative Consilience program (IITP-2021-0-01819) supervised by the IITP (Institute for Information & communications Technology Planning & Evaluation). \n\n Conflict of Interest  : none declared. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 9563680, "text_md5": "a9a967b98be16a006ce2ef8c1078d06d", "field_positions": {"authors": [0, 102], "journal": [103, 117], "publication_year": [119, 123], "title": [134, 218], "keywords": [232, 232], "abstract": [245, 1155], "body": [1164, 10905]}, "batch": 1, "pmid": 36053172, "doi": "10.1093/bioinformatics/btac598", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9563680", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9563680"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9563680\">9563680</a>", "list_title": "PMC9563680  BERN2: an advanced neural biomedical named entity recognition and normalization tool"}
{"text": "Han, Xu and Kim, Jung-jae and Kwoh, Chee Keong\nJ Biomed Semantics, 2016\n\n# Title\n\nActive learning for ontological event extraction incorporating named entity recognition and unknown word handling\n\n# Keywords\n\nActive learning\nBiomedical natural language processing\nInformation extraction\n\n\n# Abstract\n \n## Background \n  \nBiomedical text mining may target various kinds of valuable information embedded in the literature, but a critical obstacle to the extension of the mining targets is the cost of manual construction of labeled data, which are required for state-of-the-art supervised learning systems. Active learning is to choose the most informative documents for the supervised learning in order to reduce the amount of required manual annotations. Previous works of active learning, however, focused on the tasks of entity recognition and protein-protein interactions, but not on event extraction tasks for multiple event types. They also did not consider the evidence of event participants, which might be a clue for the presence of events in unlabeled documents. Moreover, the confidence scores of events produced by event extraction systems are not reliable for ranking documents in terms of informativity for supervised learning. We here propose a novel committee-based active learning method that supports multi-event extraction tasks and employs a new statistical method for informativity estimation instead of using the confidence scores from event extraction systems. \n\n\n## Methods \n  \nOur method is based on a committee of two systems as follows: We first employ an event extraction system to filter potential false negatives among unlabeled documents, from which the system does not extract any event. We then develop a statistical method to rank the potential false negatives of unlabeled documents 1) by using a language model that measures the probabilities of the expression of multiple events in documents and 2) by using a named entity recognition system that locates the named entities that can be event arguments (e.g. proteins). The proposed method further deals with unknown words in test data by using word similarity measures. We also apply our active learning method for the task of named entity recognition. \n\n\n## Results and conclusion \n  \nWe evaluate the proposed method against the BioNLP Shared Tasks datasets, and show that our method can achieve better performance than such previous methods as entropy and Gibbs error based methods and a conventional committee-based method. We also show that the incorporation of named entity recognition into the active learning for event extraction and the unknown word handling further improve the active learning method. In addition, the adaptation of the active learning method into named entity recognition tasks also improves the document selection for manual annotation of named entities. \n\n \n\n# Body\n \n## Background \n  \nA common framework of information extraction systems is supervised learning, which requires training data that are annotated with information to be extracted. Such training data are usually manually annotated, where the annotation process is time-consuming and expensive. On the other hand, in biomedical domain, recent research efforts on information extraction are extending from focusing on a single event type such as protein-protein interaction (PPI) [ ] and gene regulation [ ] to simultaneously targeting more complicated, multiple biological events defined in ontologies [ ], which makes the manual annotation more difficult. There is thus the need of reducing the amount of annotated data that are required for training event extraction systems. \n\nActive learning is the research topic of choosing \u2018informative\u2019 documents for manual annotation such that the would-be annotations on the documents may promote the training of supervised learning systems more effectively than the other documents [ ]. It has been studied in many natural language processing applications, such as word sense disambiguation [ ], named entity recognition [ \u2013 ], speech summarization [ ] and sentiment classification. Its existing works can be roughly classified into two approaches: uncertainty-based approach [ ] and committee-based approach [ ]. The uncertainty-based approach is to label the most uncertain samples by using an uncertainty scheme such as entropy [ ]. It has been shown, however, that the uncertainty-based approach may have worse performance than random selection [ \u2013 ]. \n\nIn the biomedical information extraction, the uncertainty-based approach of active learning has been applied to the task of extracting PPIs. For instance, [ ] proposed an uncertainty sampling-based approach of active learning, and [ ] proposed maximum uncertainty based and density based sample selection strategies. While the extraction of PPI is concerned with a single event type of PPI, however, recent biomedical event extraction tasks [ ] involve multiple event types, even hundreds of event types in the case of the Gene Regulation Ontology (GRO) task of BioNLP-ST\u201913 [ ]. \n\nThe committee-based approach, based on a committee of classifiers, selects the documents whose classifications have the greatest disagreements among the classifiers and passes them to human experts for annotation. This approach, however, has several issues in adaptation for event extraction tasks. First, event extraction (e.g. PPI extraction, gene regulation identification) is different from many other applications of active learning, which are in essence document classification tasks. Event extraction is to locate not only event keywords (e.g. bind, regulates), but also event participants (e.g. gene/protein, disease) within documents and to identify pre-defined relations between them (e.g. subject-verb-object). Thus, even if the event extraction systems produce confidence scores for its resultant events, the confidence scores do not correspond to the probability of how likely a document expresses an event type: in other words, how likely a document belongs to an event type class, which should be the goal of classifiers of the committee-based approach for event extraction. Second, previous classifiers for the committee-based approach may miss some details of events including event participants. For example, the keyword \u2018expression\u2019 may mislead a classifier to predict that the document with the keyword expresses gene expression event, although the document does not contain any gene name. \n\nOur target tasks of event extraction for active learning in this paper are those introduced in BioNLP-ST\u201913 [ ], which involve multiple, complicated event types. Currently, there is only one event extraction system available for all the tasks, called TEES [ ], and we need an additional classifier to follow the committee-based approach. \n\nWe thus propose as an additional classifier a novel statistical method for informativity estimation, which predicts how likely a text expresses any event concept of a given ontology. The method is based on a language model for co-occurrences between n-grams and event concepts. Furthermore, it independently estimates the presence of event participants in a text and the probabilities of out-of-vocabulary words and combines them with the prediction of event concept in the text. We collectively estimate the informativity of a text for all the concepts in a given ontology, similarly to the uncertainty-based approach of [ \u2013 ]. \n\nWe also present a revised committee-based approach of active learning for event extraction, which combines the statistical method with the TEES system as follows: Since the confidence scores of the TEES system are not reliable for active learning, we take TEES outputs as binary, that is, whether the system extracts any instance of a concept from a text or not. The disagreement between the TEES system and the statistical model is captured when, given a text (T) and an event concept (C), the TEES system does not extract any instance of C in T, but the probabilistic model predicts a high probability of C in T. In other words, the TEES system is used for filtering potential false positives, and the probabilistic model for ranking them. \n\nWe further adapt our active learning method and the statistical method for event concept detection to named entity recognition, including gene name recognition. We show that our method can improve active learning for named entity recognition as well, when tested against the BioCreative and CoNLL datasets. \n\n\n## Methods \n  \nWe formalize the general workflow of active learning as follows: At the start of round   t  , let   be the pool of unlabeled documents and let   be the pool of labeled documents, where   t   starts from 1. In round   t  , we select the most \u2018informative\u2019 document   x   from  , manually label it, and add it to  . If the label   y   is assigned to the document   x   by the oracle, the labeled and unlabeled document sets are updated as follows: \n \n\nSuch process is iterated until a certain stopping criteria is met, such as when   and after a pre-defined number of rounds. It also can be done in a batch mode, where a group of documents are selected at each round for the manual labeling. \n\n### Active learning method for event extraction \n  \nAs explained above, our active learning method follows the committee-based approach. As the committee, we employ two classifiers: A classifier based on an event extraction system called TEES and a statistical classifier based on language modeling (see the next section for details). The TEES [ ] is a state-of-the-art biomedical event extraction system based on support vector machine, and was the only system that participated in all the tasks of BioNLP-ST\u201913, showing the best performance in many of the tasks [ ]. The TEES system produces the confidence score of each event it extracts. However, we do not use the score for active learning because the confidence score does not indicate the probability of the event in the document. We also assume that if the TEES system extracts an event (E) from a document (D), D is not informative for E, because true positives are already not informative and because the correction (i.e. labeling) of false positives might not be useful for training event extraction systems where event descriptions are scarce, and thus there are far more negative examples than positive examples. In other words, the primary goal of our active learning method is to correct more false negatives, that is, to annotate the true events not extracted by the existing system. Figure   depicts the workflow of the proposed method.\n   \nOverview of proposed active learning method. The integration of underlying system into active learning method. For event extraction task, if the underlying event extraction system (TEES) can recognize the concept (C) in the given document (D), the D is not considered as informative \n  \n\nOur method works iteratively as follows: In round   t  , we train the TEES system and the statistical classifier based on  . We measure the informativity of each unlabeled document among   and choose the top documents as feed for manual annotation. We measure the informativity score of a document at the sentence level, that is, the average of the informativity scores of all the sentences in the document, as illustrated in ( ). \n \n\n \u03b8   indicates the current models of the TEES system and the statistical classifier at round   t  , but we will omit it for simplicity. \n\nThe informativity of a sentence (  S  ) is measured for the event concept set  , which contains all event defined in a given ontology, as expressed in ( ). The informativity score for event concept set is denoted as  . In fact, the BioNLP-ST\u201913 tasks include not only events, but also relations. A key difference between events and relations is that an event always involves an event keyword (e.g. \u2018regulates\u2019 for GeneRegulation), but a relation does not have any keyword (e.g. partOf). For simplicity, we mention only events in the paper, while our method involves both events and relations in the same way. \n \n\n#### Informativity for event concept set \n  \nThe informativity of a sentence for event concept set is calculated as the sum of the informativity scores of the sentence for all the event as follows: \n \n\nAs explained earlier, we treat a sentence as non-informative for an event if the event extraction system TEES can extract any instance of the event from the sentence. Otherwise, the informativity score is estimated as the probability of the concept given the sentence as follows: \n \n\n p  (  E  |  S  ) can be converted into ( ) using the Bayes\u2019 theorem. \n \n\nThe   P  (  E  ) is estimated using the maximum-likelihood estimation (MLE) based on the statistics of event annotations in the training data. \n\nAs for   P  (  S  |  E  ), we score the correlation between the sentence   S   and the event   E   with a real value scoring function   Z   (see below for details) and use the softmax function to represent it as a probabilistic value, shown in ( ). \n \n\nWe use two types of units to approximately represent the sentence   S  : n-grams (NG) and predicate-argument relations (PAS) produced by the Enju parser [ ]. A sentence is represented as a bag of elements of a unit, for example, a bag of all n-grams or a bag of all predicate-argument relations from the sentence. \n\n##### A. Using N-gram feature for probability estimation \n  \nIf we use the bag of n-gram model, the score   Z  (  S  :  E  ) is measured using the average of the correlation score between the n-gram (NG) contained in the sentence with the event, expressed in ( ), where len(  S  ) is the normalization factor and is calculated as the word count of sentence   S  . \n \n\nWhile the probability between the n-gram and event   p  (  N   G  |  E  ) is also calculated using a correlation score   W  (  N   G  ,  E  ) between the n-gram and the event, together with the softmax function, shown in ( ). \n \n\nThe correlation score   W  (  N   G  ,  E  ) is calculated using one of the following three methods: 1) Yates\u2019 chi-square test, 2) relative risk, and 3) odds ratio [ ]. For the calculation of the three methods, a 2 \u00d72 table is constructed for each pair of an N-gram and an event at the level of sentences, as shown in Table  . For example,   a   indicates the number of sentences that contain the N-gram   N   G   and express the event   E  .\n   \nNumbers of sentences for the calculation of correlation score between   E   and   N   G  \n  \n\nBased on the 2 \u00d72 table, the three methods of Yates\u2019 chi-square test, relative risk, and odds ratio calculate the correlation score for the pair as shown in the formulas ( ), ( ), and ( ), respectively. \n \n\n\n\n\n\n\n##### B. Using predicate-argument relation for probability estimation \n  \nSimilarly for the bag of predicate-argument relation model, the score   Z  (  S  :  E  ) is calculated with the average of the correlation scores between the event and the predicate-argument relations from the sentence, as in ( ). \n \n\n\n\n\n### Additional features of active learning \n  \nWe introduce two additional features of our active learning method: Incorporation of event participants and dealing with out-of-vocabulary words. \n\n#### Incorporation of event participants \n  \nThe absence of event participants should negatively affect the prediction of events. To reflect this observation, we utilized a gene name recognition system, called Gimli [ ], in order to recognize gene/protein names, since most of the BioNLP shared tasks involve genes and proteins (e.g. gene expression, gene regulation, phosphorylation). We incorporate the results of the Gimli system into our active learning method as follows: \n \n\n\n\n T   indicates the number of gene/protein names predicted in a sentence   S  . \n\nIn fact, the Gimli system can be replaced with other named entity recognition systems for tasks whose event participants are other than gene/protein. Since the event extraction tasks for evaluating our active learning method (i.e. BioNLP shared tasks) are mainly about gene/protein, we do not replace the Gimli system when evaluating the incorporation of event participants. When we apply our active learning method for the tasks of named entity recognition (NER), however, we will evaluate it against two NER systems (i.e. Gimli, Stanford NER system) (see for details Sections \u2018 \u2019 in Page 8, \u2018 \u2019 in Page 11, and \u2018 \u2019 in Page 19). \n\n\n#### Dealing with OOV issue with word similarity \n  \nWhen we use the n-gram features, there is Out-of-Vocabulary (OOV) issue, such that some n-grams in the test dataset may not appear in the training dataset. To tackle this issue, we adopt the word2vec system, which is an unsupervised method for representing each word as a vector in a latent semantic model and for measuring word similarity [ ], as follows: Consider an n-gram   N   G   that does not occur in the training dataset. We use word2vec to find the top-k n-grams   N   G   that are closest to   N   G  , where the word similarity score between   N   G   and each   N   G   is designated as   S   i   m  (  N   G  ,  N   G  ). We then recalculate the correlation scoring function   W  (  N   G  ,  E  ) as shown in Formula ( ). Note that since word2vec can only handle unigrams, and also since unigrams show the best performance in our experiments of parameter optimization (see the next section), we only deal with unknown unigrams in this method. The word similarity scores are trained a priori using the whole set of MEDLINE abstracts released in April 2014. \n \n\nWe denote the n-gram-based informativity of sentence calculated using the updated correlation scoring function ( ) as   I  (  S  ,  N   G  ). For example, when the correlation scoring function in ( ) is updated, the resultant informativity in ( ) is denoted as  . \n\n\n#### Linear combination of n-gram and predicate-structure relation features \n  \nWhile we choose either n-grams or predicate-argument relations as features, we also tested the linear combination of the two feature sets for our active learning method, as follows: \n \n\nTable   illustrates the calculation of informativity scores in pseudo codes.\n   \nProposed algorithm of active learning with TEES \n  \n\n\n\n### Active learning method for NER task \n  \nWe also adapt our active learning method to named entity recognition (NER), considering the ontology concepts of named entities (e.g. Gene, Disease) instead of events (e.g. PPI, gene regulation). The method for named entity recognition estimates informativity, or the likelihood of a text expressing any named entities. \n\nSimilar to Eq. ( ), the informativity estimation in the NER task is expressed in ( ). \n \n\n \u03b8   indicates the current model of a given NER system and the statistical classifier at round   t  , but we will omit it for simplicity. We evaluate our method with two NER systems of Gimli for biomedical domain and Stanford NER system for general domain (see Section \u201c \u201d for details of evaluation), one system at a time. \n\nThe informativity of a sentence for named entity set is calculated as the sum of the informativity scores of the sentence for all the named entities as follows: \n \n\nSimilar to the active learning method for event extraction, we treat a sentence as non-informative for an named entity if the NER system can recognize any instance of the named entity from the sentence. Otherwise, the informativity score is estimated as the probability of the named entity given the sentence as follows: \n \n\nThe probability   p  (  N  |  S  ) is calculated as follows: \n \n\nSimilarly to the estimation for event,   p  (  N  ) is estimated using the maximum-likelihood estimation (MLE) based on the statistics of named entities in the training data. For the calculation of   p  (  S  |  N  ), we follow similar steps as in ( ), using n-grams (i.e. Formula ( )), but not using PAS (i.e. Formula ( )). \n\n\n### Comparison with related works \n  \nIn this section, we describe the previous methods of active learning that we compare with our proposed methods for event extraction in the evaluation experiments. \n\n A. Conventional committee-based method  \n\nThe committee based active learning, based on a committee of classifiers, selects the documents whose classifications have the greatest disagreements among the classifiers and passes them to human experts for annotation, expressed as follows: \n \n\n D  (  Y  |  x  ) is the disagreements among the classifiers for a document   x   under the model   \u03b8  , and the   Y   is the whole label set. We use the summation of disagreement over the sentence   S   contained in the document   x  . \n \n\nFor each sentence, we measure the collective disagreement over the whole event concept set   defined in the ontology by using the sum of all disagreement for all event   E  . \n \n\nThe disagreement   D  (  E  |  S  ) is calculated using the absolute value of the differences of the probability produced by the classifiers, named the aforementioned informativity estimation method and the TEES event extraction system. \n \n\nThe   p  (  E  |  S  ) is the probability estimated from the TEES system, and   p  (  E  |  S  ) is from the informativity estimation using statistical method, which is calculated in Eq. ( ). Note that while   p  (  E  |  S  ) in Eq. ( ) is estimated using Eq. ( ) only for the sentences from which no   E   is recognized by the TEES, the same informativity probability in Eq. ( ) is estimated for all the sentences of unlabeled documents. \n\nHowever, as the TEES is a support vector machine (SVM) based system and do not produce probabilistic output, we use the confidence the SVM classifier has in its decision for a event prediction as follows: \n \n\n C  (  E  |  S  ) is the confidence for the classifier. \n\nThe confidence is calculated using the   difference-2   of the distance from the separating hyperplane, produced by the SVM classifier. It is shown to have best performance in active learning [ ,  ], and the calculation is expressed as follows: \n \n\nThe   d   i   s   t  (  m,S  ) is the distance of the predicted label   m   in such sentence   S  . \n\nSimilarly in adapting to the NER task, for each sentence, we measure the collective disagreement over the whole named entity concept set   by using the sum of all disagreement for all named entity   N  . \n \n\nThe disagreement   D  (  N  |  S  ) is calculated using the absolute value of the differences of the probability produced by the classifiers, named the aforementioned informativity estimation method and the NER system. \n \n\nThe   p  (  N  |  S  ) is the marginal probability provided by the Conditional Random Field (CRF) model from the NER system, and   p  (  N  |  S  ) is from the informativity estimation using statistical method. \n\n B. Entropy based active learning method  \n\nEntropy is the most common measure for uncertainty, which indicates a variable\u2019s average information content. The document selection of entropy-based methods is formalized as follows: \n \n\nThe   H  (  Y  |  x  ) is the entropy of a document   x   under the model   \u03b8   and the   Y   is the whole label set. We use the summation of entropy over the sentence   S   contained in the document   x  . \n \n\nFor each sentence   S  , we use the aforementioned bag of n-gram method, and estimate   H  (  Y  |  S  ) as the average entropy of each n-gram   N   G   in   S  , as follows: \n \n\nWe estimate the collective entropy over the whole event concept set   defined in the ontology as the summation of the entropy for all event   E  . \n \n\n H  (  E  |  N   G  ) is calculated by using the Weka package for the calculation of entropy [ ]. \n\n C. Gibbs error based active learning method  \n\nGibbs error criterion is shown to be effective for active learning [ ], which selects documents that maximize the Gibbs error, as follows: \n \n\nSimilarly to the entropy-based method implementation, we calculate the collective Gibbs error as follows: \n \n\nFor the calculation of   H  (  E  |  N   G  ), we use the conditional probability of   p  (  E  |  N   G  ), defined as follows [ ], where   p  (  E  |  N   G  ) is estimated using the proposed method as shown in ( ): \n \n\n\n\n## Results and discussion \n  \n### Datasets and employed systems \n  \nThe BioNLP shared tasks (BioNLP-ST) were organized to track the progress of information extraction in the biomedical text mining. In this paper, we used the datasets of three tasks, namely GRO\u201913 (Gene Regulation Ontology) [ ], CG\u201913 (Cancer Genetics) [ ] and GE\u201913 (Genia Event Extraction) [ ]. Each corpus was manually annotated with an underlying ontology, whose number of concepts and hierarchy are different from each other. A comparison between the datasets is given in Table  . Note that since the official test datasets for CG and GE tasks are inaccessible, we instead use parts of their training datasets as the \u2018test\u2019 datasets, and the statistics of the datasets include only those accessible documents.\n   \nSummary of task datasets used in the experiments \n  \n\nSpecifically, we employ the state-of-the-art Stanford NER [ ] system for the CoNLL-2003 [ ] dataset, and the Gimli gene name recognition system [ ] for the BioCreative II Gene Mention [ ] dataset. Note that in BioCreative task, the named entities are naturally of one class, i.e., the Gene/Protein name; while the CoNLL dataset involves four classes of named entities (i.e. Person, Organization, Location, Misc). \n\n\n### Evaluation metrics for comparison of active learning methods \n  \nTo compare the performance of the different strategies of sample selection, we plot their performance in each iteration. Since the difference between some plots is not obvious, however, we mainly use the evaluation metric of   deficiency   for comparison [ ,  ], defined as follows: \n \n\nThe   a   c   c  (  C  ) is the performance of the underlying classifier   C   at   t   round of learning iteration. AL is an active learning method, and REF is a baseline method (see below for details).   n   refers to the total number of rounds (i.e. 10). A deficiency value smaller than 1.0 means that the active learning method is superior to the baseline method, and in general, a smaller value indicates a better method. \n\n\n### Parameter optimization \n  \nWe first take a parameter optimization step to determine the most appropriate parameters for the aforementioned calculation of informativity scores. \n\n#### Correlation measure and n-gram size \n  \nAs mentioned above, we considered three correlation measures to estimate the correlation score between n-gram and event, including chi-square test, relative risk, and odds ratio. We also should determine the value of   n   for   n  -grams. To find the optimal solutions for the two tasks, we carried out a simulation of ontology concept prediction at the sentence level as follows: Given a sentence   S   and   N   ontology concepts manually annotated on the sentence, we predict the top   N   ontology concepts in   S   and compare them with the   N   manually annotated concepts, measuring the overlap between the two concepts sets. We select the best combination of co-occurrence analysis method and n-gram size for the rest of experiments in this paper. \n\nUsing 10-fold cross validation, the average prediction rate is calculated and reported in Table  . Each column corresponds to an n-gram size, and each row to one of the three co-occurrence analysis methods used for the prediction. Note that when N=2 (i.e. bi-grams), it does not include unigrams for the calculation. N=1-2 indicates the mixture of unigrams and bi-grams. This experiment is carried out using the GRO\u201913 dataset.\n   \nParameter optimization results \n  \nThe averaged concept prediction accuracy is reported. The best accuracy is highlighted in boldface \n  \n\nAs shown in Table  , for all co-occurrence analysis methods, the accuracy mostly drops as the length of N-grams increases. This may happen due to the data sparseness problem for large N-grams. We choose to use   chi-square test   and   unigrams   for the following experiments based on the results. \n\n\n#### Parameter for the incorporation of event participants \n  \nThe parameter of   \u03b4   in Eq. ( ) is to determine the significance of effects of event participants on event concept prediction. We tested our active learning method in Eq. ( ) against the GRO\u201913 dataset with the   \u03b4   values set as 0.15, 0.25 and 0.35. We summarize the performance results in terms of deficiency in Table  . We choose the   \u03b4  =0.25 for the following experiments based on the results.\n   \nParameter optimization results \n  \nThe deficiencies of active learning method using different factor against the GRO\u201913 are reported. The best deficiency is highlighted in boldface in this table and also in the tables below \n  \n\n\n#### Parameter for dealing with OOV issue \n  \nIn dealing with the OOV issue, we choose top-  k   similar words for an unknown word, as in Formula ( ). In order to choose the optimal value for   k  , we use the linear combination method in Eq. ( ) with the other parameters   \u03b1  =0.1,  \u03b2  =0.1 and   \u03b3  =0.8, and test our active learning method against the GRO\u201913 dataset, as changing the   k   value from 5 to 25. We summarize the deficiency of the active learning method using the different   k   values in Table  . As the result, we choose   k  =25 for the remaining experiments.\n   \nParameter optimization results \n  \nThe deficiencies of active learning method using different factor against the GRO\u201913 \n  \n\n\n\n### Evaluation of active learning methods for event extraction \n  \n#### Active learning methods using informativity estimation \n  \nIn the following evaluations, we show the learning curves and deficiencies of the event extraction system TEES under different sample selection strategies against the dataset of GRO\u201913, CG\u201913 and GE\u201913 task. The active learning methods use only the informativity estimation, but not the additional features such as incorporation of event participants and dealing with OOV issue, which will be discussed in the next section. \n\nWe compare the proposed active learning method with other sample selection strategies, including random selection, and entropy-based [ ], and Gibbs error [ ] based, as well as a conventional committee based active learning methods. We use the random selection as the baseline for deficiency calculation. Each experiment has ten rounds, where in each round, 10 % of the original training data are added for training the TEES system. The initial model of the TEES system before the first round is trained only on the development dataset. Note that the test data of each dataset is fixed. The followings are considered for the selection of additional 10 % training data in each round: \n   \nRandom selection: We randomly split the training data into 10 bins in advance, and during the training phase in each round, one bin is randomly chosen. We report the averaged performance of random selection for ten times (hereafter referred as RS_Average). \n  \nEntropy-based active learning: We calculate the entropy of each document based on ( ), sort documents by their entropy values and feed from documents with top values to those with bottom values as training data. (designated as AL(Entropy)) \n  \nGibbs error based active learning: We calculate the Gibbs error of each document based on ( ), sort documents by their Gibbs error values and select the documents with top values as training data. (designated as AL(GibbsError)) \n  \nProposed active learning: We evaluate the method using either unigrams (Unigram) or predicate-argument relations (PAS). The resultant method is referred as AL(Informativity_Unigram) and AL(Informativity_PAS), respectively. \n  \nConventional committee-based active learning: We evaluate the committee based method based on ( ), using the confidence score produced by TEES. We estimate the informativity using either unigrams (Unigram) or predicate-argument relations (PAS) for the proposed statistical method. The resultant method is referred as AL(Conventional Committee_Unigram) and AL(Conventional Committee_PAS), respectively. \n  \n\nWe first apply those methods to the dataset of GRO\u201913 [ ] and measure the performance change of the TEES system with the incremental feed of the training data. We summarize the deficiency for each method in Table  . The proposed active learning methods and the conventional committee-based methods achieve deficiency value of less than 1, while the entropy and Gibbs error method achieve a deficiency higher than 1, suggesting that the entropy and Gibbs error methods do not perform better than that of random selection. Particularly, the AL(Informativity_Unigram) method achieves the best deficiency of 0.760, while the corresponding conventional committee based method achieves the performance of 0.832 in AL(ConventionalCommittee_Unigram), which is an 8.65 % improvement for the informativity based method over that of conventional committee-based method. However, when using the PAS model, the AL(Informativity_PAS) achieves deficiency of 0.845, which is 1.78 % worse than that of the committee-based method, whose deficiency is 0.830. In addition, when comparing the performance of the methods using the PAS and unigram, we notice that using the unigram, the proposed informativity method shows an 10.1 % improvement over that using PAS model, yet this is not evident in the committee-based method. The results suggest that the proposed informativity method performs best when using the unigram model in the GRO\u201913 dataset. We then plot the learning curves for each method in Figs.   and  . In Fig.  , the AL(Informativity_Unigram) method is consistently performing over the other methods after 50 % of the documents are selected, which also explains the results in the comparison of deficiency values. In addition, in the comparison of average number of instances per ontological concept provided in [ ], the GRO\u201913 dataset have 13 instances per concept, while such value for GE\u201913 dataset is 82. This also suggests that in datasets such as GRO\u201913 whose document annotation may not be abundant, the active learning method using the unigram may perform better than the PAS model. However, the experiment result in the GRO\u201913 dataset indicates that the proposed informativity based active learning method with unigram model can show better performance than the conventional committee-based, the entropy based and the Gibbs error based active learning methods.\n   \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and random selection against GRO\u201913 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the GRO\u201913 task dataset. The active learning method uses the predicate-argument relation (PAS) model \n    \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and random selection against GRO\u201913 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the GRO\u201913 task dataset. The active learning method uses the unigram model \n    \nDeficiencies of sample selection methods for event extraction against the GRO\u201913, CG\u201913 and GE\u201913 datasets \n  \n\nWe then carry out a similar experiment using the CG\u201913 dataset. We summarize the deficiency for each method in the Table  . In this experiment, the Gibbs error based approach achieves the deficiency value of less than 1, while the deficiency for the entropy based method is 1.226. Comparing the PAS and unigram model, the deficiency values for PAS model are generally better than those of unigram model. For instance, in the committee-based method, the percentage of deficiency difference is 25.3 %. Similarly in the proposed informativity method, there is a 24.3 % change in the deficiency value. This may suggest that the PAS model may be more suitable for the CG\u201913 dataset. In addition, while comparing the proposed informativity method and committee-based method, the informativity method achieves better deficiency value over the committee-based method. In terms of deficiency difference, the improvements are 0.020 and 0.008, for PAS and unigram feature, respectively, which is a less obvious improvement for the informativity method. However, this also suggest that the PAS feature may be more sensitive than that of unigram in the CG\u201913 dataset. Note that one of the specialties in CG\u201913 dataset is that only a single relation type of   Equiv   is defined.   Equiv   is a symmetric and transitive binary relation to identify entity mentions as being equivalent in the sense of referring to the same real-world entity [ ]. Such relation is not evaluated in the GRO\u201913 or GE\u201913 dataset. The better performance of PAS model over unigram model may due to that the PAS model is more stable for identification of equivalent entity mentions than the unigram model. The learning curves for the active learning method are plotted in Figs.   and  .\n   \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and random selection against CG\u201913 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the CG\u201913 task dataset. The active learning method uses the predicate-argument relation (PAS) model \n    \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, random selection against CG\u201913 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the CG\u201913 task dataset. The active learning method uses the unigram model \n  \n\nWe extend the aforementioned active learning methods to the GE\u201913 dataset, and the Table   summarize the deficiency of the methods. In Table  , all methods achieve deficiency values less than the random selection. The method of Gibbs error based approach achieve the deficiency of 0.850, while the deficiency for the entropy method is 0.854. The proposed active learning methods using the unigram shows a more obvious improvement than that using PAS. For instance, in the committee-based method, there is an improvement of 40.1 % for the unigram model over the PAS model. This may suggest that, against the GE\u201913 dataset, the unigram feature is more suitable for proposed method than that of the PAS feature. We notice a more obvious improvement for the unigram model in the informativity method. Particularly, the best performing AL(Informativity_Unigram) achieve a deficiency value of 0.139. While the corresponding committee-based method achieve the deficiency of 0.263 in AL(ConventionalCommittee_Unigram). We plot the learning curves in Figs.   and  . In the Fig.  , the active learning method using unigram generally shows obvious improvement over the baseline of random selection method, yet the active learning method using PAS show less significant improvement over the baseline method. This may due to the fact that the ontology defined in GE\u201913 task is generally less complicated than that in GRO\u201913 and CG\u201913. In addition, the document annotation in the GE\u201913 dataset may be abundant, as the average number of instances per ontological concept in GE\u201913 dataset is 82, above six times more than that of GRO\u201913 dataset [ ]. Given the dataset with less complicated ontological concepts and abundant training data of document annotation, the unigram model may show obvious improvement for active learning methods.\n   \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and random selection against GE\u201913 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the GE\u201913 task dataset. The active learning method uses the predicate-argument relation (PAS) model \n    \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and random selection against GE\u201913 dataset. The learning curves for the TEES system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the GE\u201913 task dataset. The active learning method uses the unigram model \n  \n\n\n#### Active learning methods using additional features \n  \n Incorporation of event participants   We evaluate the active learning method that is incorporated with the recognition of gene/protein names for event extraction, as illustrated in Formula ( ). We show the performance of the TEES system, with active learning method that is either with or without using the gene/protein names. Such experiment is carried out using the GRO\u201913 dataset. The experiment results are plotted in Fig.   and we summarize the deficiency values in the Table  . In the Table  , the incorporation of gene/protein names shows positive effects towards the active learning method for event extraction, for both of bag of n-gram or PAS method. By using the gene/protein names, the deficiency for the active learning method using PAS is further improved from 0.845 to 0.589, which is a 30.3 % improvement. Yet in the unigram model of the informativity method, the improvement is rather less significant of 7.1 %, which may suggest that some named entities are already captured as n-grams, thus redundant.\n   \nIntegration of named entity recognition into active learning with PAS and n-grams against GRO\u201913 dataset. The learning curves for the TEES system under the proposed informativity method using predicate-argument relation (PAS) and unigram model, as well as the conventional committee (ConventionalCommittee) based active learning method as the benchmark. In contrast, each method is integrated with the output from the named entity recognition result (NE) \n    \nDeficiencies of active learning methods with and without integrating the prediction of named entities (NE) against GRO\u201913 dataset \n  \n\nIn addition, we notice similar improvement of the conventional committee-based method by incorporating the information of event participants into the part of statistical informativity estimation, from 0.830 (i.e. ConventionalCommittee_PAS) to 0.693 (i.e. ConventionalCommittee_PAS + NE), a 16.5 % improvement. However, this improvement is significantly less than that for our proposed method, which may indicate that the confidence scores of the TEES used by the conventional committee-based method hamper the effects of event participants. \n\n Dealing with OOV issue with word similarity   The n-gram model is based on the \u2018registered\u2019 n-grams that occur in the training data, which has the issue of Out-of-Vocabulary (OOV) words. We solve this by using the word2vec toolkit to find top-  k   words that are closest to a given OOV word in the test data and to use their weights to estimate the weight of the OOV word. The results of evaluating the word vector incorporation against the GRO\u201913 dataset are plotted in Fig.  , and the deficiency is summarized in Table  . Note that the experiments about OOV word handling are carried out only for events, excluding relations, observing that the relations of the BioNLP-ST\u201913 tasks are little affected by the OOV issue, since they are not associated with trigger words. By using the word similarity, the n-gram model method is further improved, as the deficiency of n-gram model goes from 0.790 to 0.769, an improvement of 2.66 %. The rather less significant improvement may suggest that such OOV issue is rather not prevalent in the GRO\u201913 dataset.\n   \nEvaluation of incorporation of the word vector method into active learning with n-grams against GRO\u201913 dataset. The word vector is applied into the active learning method to solve the out-of-vocabulary (OOV) issue that exists in the unigram model. For the unknown unigram, its score is replaced by the top-25 most similar known unigrams \n    \nDeficiencies of using word vector to solve the Out-Of-Vocabulary(OOV) issue for the unigram model \n  \n\n\n\n### Linear combination of n-gram and predicate-structure relation features \n  \nLastly, we linearly combine the proposed n-gram and predicate-structure relation features for the active learning, as expressed in Eq. ( ), and to understand which of the active learning methods proposed in this paper are more important towards the overall performance. \n\nWe use four weight combinations of (  \u03b1  =0.8,   \u03b2  =0.1,   \u03b3  =0.1), (  \u03b1  =0.1,   \u03b2  =0.8,   \u03b3  =0.1), and (  \u03b1  =0.1,   \u03b2  =0.1,   \u03b3  =0.8), as well as the equal distribution of weight (  \u03b1  =0.33,   \u03b2  =0.33,   \u03b3  =0.33). The method of AL(Informativity_PAS + NE) is used as the benchmark, as it is the best performing method in the previous experiments in the GRO\u201913 dataset. Note that the AL(Informativity_PAS + NE) corresponds to the weight combination of (  \u03b1  =0,   \u03b2  =1,   \u03b3  =1). Additionally, we also use the benchmark of only using the named entity for the active learning, i.e the weight combination of (  \u03b1  =0,   \u03b2  =0,   \u03b3  =1), to check if simply using the total number of recognized named entities be sufficient for the active learning method. \n\nThe results of comparison are plotted in Fig.  , and we summarize the deficiency values in Table  . Overall, the weight combination of (  \u03b1  =0.1,   \u03b2  =0.1,   \u03b3  =0.8) shows the best performance (deficiency 0.563). Compared to PAS or unigram-based statistics, the incorporation of event participants has the most effect on the best performance. Note, however, that the model of using only the event participants, i.e., the weight combination of (  \u03b1  =0,   \u03b2  =0,   \u03b3  =1), achieves the deficiency of 0.583, higher than the best deficiency, which indicates that the PAS or n-gram based statistics are complementary to event participants.\n   \nEvaluation of linear combination of active learning methods against GRO\u201913 dataset. The active learning modules are assigned with different weights and combined linearly. Different weight assignment strategies are compared \n    \nDeficiencies of linear combination of active learning methods \n  \n\n\n### Evaluation of active learning method for NER task \n  \nWe apply the active learning method into NER task as expressed in Eq. ( ), and follow the similar experiment design. Each sample selection method starts with the same held-out labeled development dataset for model initialization and a pool of unlabeled training dataset for selection. In each round, 10 % of the unlabeled documents in the training dataset are selected by different sample selection strategies. For evaluation, we report the performance of NER system trained with the selected training document in each round, against the same held-out test dataset following the official evaluation procedure. \n\nThe sample selection strategies are as follows: \n   \nRandom selection: We randomly split the training dataset into 10 bins in advance, one bin is randomly chosen in each round. Following 10-fold cross validation, we report the averaged performance in each round. (hereafter referred to as RS_Average) \n  \nEntropy-based active learning: The entropy of documents are calculated, and select documents by their entropy values, from the top to bottom. (designated as AL(Entropy)) \n  \nMaximum Gibbs Error based active learning: Similar to the entropy-based method, but uses the Gibbs error, as introduced in [ ]. (designated as AL(GibbsError)) \n  \nProposed active learning method using informativity scoring only: Use the aforementioned system in Eq. ( ), and selects documents based on their informativity scores. (designated as AL(Informativity)) \n  \nConventional committee-based active learning: We evaluate the committee based method based on ( ), using the confidence score produced by NER system. The resultant method is referred as AL(ConventionalCommittee). \n  \n\nWe applied these methods to the BioCreative dataset and plotted the learning curve of Gimli in Fig.  , and summarized their deficiency values in Table  . In Fig.  , the proposed active learning method show steady improvement over the other methods in most rounds. Based on the deficiency comparison in Table  , the proposed method achieved a deficiency value of 0.514, while the deficiency for the conventional committee based method is 0.684.\n   \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and random selection against BioCreative dataset. The learning curves for the Gimli system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the BioCreative task dataset \n    \nDeficiencies of sample selection methods against the BioCreative and CoNLL datasets \n  \n\nWe carried out similar experiments with the CoNLL dataset, and the learning curves are plotted in Fig.  , and the deficiencies are compared in Table  . In Fig.  , the proposed active learning method outperforms the other methods; and in terms of deficiency, the proposed method achieves 0.575 in the deficiency, a nearly 42 % improvement over the random selection. In contrast, the benchmark of Entropy and Gibbs error based approaches also are shows deficiency value of less than 1, yet their improvement over the random selection is nearly 26 % and 11 %. The deficiency for the conventional committee based method is 0.763. The experiment results in the BioCreative and CoNLL datasets indicate that the proposed informativity based method can show better performance than the conventional committee-based method, as well as the Entropy and Gibbs error based methods.\n   \nComparison of active learning with informativity based, entropy-based, Gibbs error based, and conventional committee based method, and random selection against CoNLL dataset. The learning curves for the Gimli system under active learning (AL), using the Gibbs error based method (Gibbs Error), entropy based method(Entropy), conventional committee based method (ConventionalCommittee) and the proposed informativity method (Informativity), as well as the random selection (RS), when tested against the CoNLL task dataset \n  \n\n\n\n## Conclusions \n  \nIn this study, we proposed a novel active learning method for ontological event extraction, which is more complex than the simple PPI extraction. Our method measures the collective \u2018informativity\u2019 for unlabeled documents, in terms of the potential likelihood of biological events unrecognizable for the event extraction system. We evaluated the proposed method against the BioNLP Shared Tasks datasets, and showed that our method can achieve better performance than other previous methods, including entropy and Gibbs error based methods and the conventional committee-based method. In addition, the incorporation of named entity recognition into the active learning for event extraction and the unknown word handling further improved the active learning method. Finally, we adapted the active learning method into named entity recognition tasks and showed that the method also improved the document selection for manual annotation of named entities. \n\n \n", "metadata": {"pmcid": 4849099, "text_md5": "9366adb6fe8888f67d819ae8752f92ea", "field_positions": {"authors": [0, 46], "journal": [47, 65], "publication_year": [67, 71], "title": [82, 195], "keywords": [209, 287], "abstract": [300, 2871], "body": [2880, 52630]}, "batch": 1, "pmid": 27127603, "doi": "10.1186/s13326-016-0059-z", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4849099", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4849099"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4849099\">4849099</a>", "list_title": "PMC4849099  Active learning for ontological event extraction incorporating named entity recognition and unknown word handling"}
{"text": "Batista-Navarro, Riza and Rak, Rafal and Ananiadou, Sophia\nJ Cheminform, 2015\n\n# Title\n\nOptimising chemical named entity recognition with pre-processing analytics, knowledge-rich features and heuristics\n\n# Keywords\n\nChemical named entity recognition\nText mining\nSequence labelling\nConditional random fields\nFeature engineering\nConfigurable workflows\nWorkflow optimisation\n\n\n# Abstract\n \n## Background \n  \nThe development of robust methods for chemical named entity recognition, a challenging natural language processing task, was previously hindered by the lack of publicly available, large-scale, gold standard corpora. The recent public release of a large chemical entity-annotated corpus as a resource for the CHEMDNER track of the Fourth BioCreative Challenge Evaluation (BioCreative IV) workshop greatly alleviated this problem and allowed us to develop a conditional random fields-based chemical entity recogniser. In order to optimise its performance, we introduced customisations in various aspects of our solution. These include the selection of specialised pre-processing analytics, the incorporation of chemistry knowledge-rich features in the training and application of the statistical model, and the addition of post-processing rules. \n\n\n## Results \n  \nOur evaluation shows that optimal performance is obtained when our customisations are integrated into the chemical entity recogniser. When its performance is compared with that of state-of-the-art methods, under comparable experimental settings, our solution achieves competitive advantage. We also show that our recogniser that uses a model trained on the CHEMDNER corpus is suitable for recognising names in a wide range of corpora, consistently outperforming two popular chemical NER tools. \n\n\n## Conclusion \n  \nThe contributions resulting from this work are two-fold. Firstly, we present the details of a chemical entity recognition methodology that has demonstrated performance at a competitive, if not superior, level as that of state-of-the-art methods. Secondly, the developed suite of solutions has been made publicly available as a configurable workflow in the interoperable text mining workbench Argo. This allows interested users to conveniently apply and evaluate our solutions in the context of other chemical text mining tasks. \n\n \n\n# Body\n \n## Background \n  \nIn carrying out scientific work, most researchers rely on published information in order to keep abreast of recent developments in the field, to avoid repetition of work and to guide the direction of current studies. This is especially true in the field of chemistry where endeavours such as drug discovery and development are largely driven by information screened from the copious amounts of data available. Whilst databases storing structured chemical information have proliferated in the last few years, published scientific articles, technical reports, patent documents and other forms of unstructured data remain to be the richest source of the most current information. \n\nText mining facilitates the efficient distillation of information from the plethora of scientific literature. Whilst most of the scientific text mining efforts in the last decade have focussed on the identification of biomedical entities such as genes, their products and the interactions between them, the community has recently begun to appreciate the need for automatically extracting chemical information from text. Applications in chemoinformatics, drug discovery and systems biology such as automatic database curation [ ], compound screening [ ], detection of adverse drug reactions [ ], drug repurposing [ ] and metabolic pathway curation [ ] are facilitated and informed by the outcomes of chemical text mining, a fundamental task of which is the recognition of chemical named entities. \n\nChemical named entity recognition (NER), the automatic demarcation of expressions pertaining to chemical entities within text, is considered a challenging task for a number of reasons. First, chemical names may appear in various forms, ranging from the popular and human-readable trivial and brand names to the more obscure abbreviations, molecular formulas and database identifiers, to long nomenclature-conforming expressions, e.g., International Union of Pure and Applied Chemistry (IUPAC) names and Simplified Molecular-Input Line-Entry System (SMILES) strings [ - ]. Moreover, researchers working on lead compound identification and discovery sometimes tend to report their results using their own arbitrarily assigned abbreviations, further aggravating the proliferation of chemical names. Also considered a barrier to the development of chemical named entity recognisers is the relatively small number of available supporting corpora, compared to those developed for biological, such as gene and protein, name recognition [ ]. Whilst a few notable data sets containing chemical named entity annotations have been developed, there was a lack of publicly available, wide-coverage, large-scale gold standard corpora of scientific publications. Although the SciBorg corpus [ , ] contains a substantial number of manually annotated chemical names in its 42 full-text articles, it had not been publicly available until very recently. In contrast, the large-scale CALBC corpus [ ] is publicly available, but is considered \"silver standard\" as it contains annotations resulting from the harmonisation of the outputs of five different automatic tools, rather than manual annotations. The similarly publicly available SCAI pilot corpus [ , ] contains gold standard annotations for various types of chemical names but is relatively small with only 100 MEDLINE abstracts. \n\nThis limited number of resources has influenced the means by which the state-of-the-art chemical named entity recognisers have been developed and evaluated. Built as a pipeline of several Markov model-based classifiers, the publicly available OSCAR tool [ ] was tuned to recognise the annotation types defined in the SciBorg corpus. The system was evaluated by means of three-fold cross validation on this corpus as well as on a bespoke data set of 500 annotated MEDLINE abstracts. ChemSpot [ ], another publicly available chemical named entity recogniser, is a hybrid between methods for dictionary matching and machine learning. For capturing brand names, this tool uses a lexicon-based approach for matching expressions against the Joint Chemical Dictionary [ ]. For recognising nomenclature-based expressions, however, it employs a conditional random fields (CRF) [ ] model trained on the SCAI corpus subset that contains annotations for only IUPAC names. The developers carried out a comparative evaluation of ChemSpot and OSCAR on the SCAI pilot corpus, in which the former was reported to have outperformed the latter by a margin of 10.8 percentage points. It is worth noting, however, that both of these tools have not been comparatively evaluated nor benchmarked against any large-scale, gold standard corpora. \n\nIn aiming to alleviate these issues, the Critical Assessment of Information Extraction in Biology (BioCreative) initiative organised a track in the Fourth BioCreative Challenge Evaluation workshop to encourage the text mining community to develop methods for chemical named entity recognition, and enable the benchmarking of these methods against substantial gold standard data [ ]. Known as CHEMDNER, this track publicly released a large corpus of documents containing manually annotated chemical named entities. The 10,000 MEDLINE abstracts in the CHEMDNER corpus [ ], which were grouped into disparate sets for training (3,500), development (3,500) and testing (3,000), came from various chemical subdomains including pharmacology, medicinal chemistry, pharmacy, toxicology and organic chemistry. Each annotated chemical name was labelled with one of the following mention types: systematic, trivial, family, abbreviation, formula, identifier, coordination and a catch-all category. The corpus served as the primary resource for the two CHEMDNER subtasks, namely, chemical entity mention recogniton (CEM) and chemical document indexing (CDI). Whilst the former required participating systems to return the locations of all chemical mention instances found within a given document, the latter expects a ranked listing of unique mentions without any location information. \n\nHaving participated in the CHEMDNER challenge, we have developed our own chemical named entity recogniser that obtained top-ranking performance in both the CDI (1st) and CEM (3rd) tasks. Extending that work, we describe in this paper the details of our proposed methods for optimising chemical NER performance. In the next section, we compare the performance of our methods with the state of the art and present results of our evaluation on several corpora. Furthermore, we share details on how our contributions, publicly available as a service, can be accessed and utilised by the community. The Experiments section contains a detailed discussion of our proposed methods and the experiments we have performed in order to identify the optimal solution on each of the data sets considered. We summarise the results of our work in the Conclusions section. Lastly, we provide some technical background on the techniques and evaluation metrics we have used in this study in the Methods section. \n\n\n## Results and discussion \n  \nWe developed a conditional random fields (CRF)-based method for chemical named entity recognition whose performance was optimised by (a) the selection of best-suited pre-processing components, (b) the incorporation of CRF features capturing chemistry-specific information, and (c) the application of post-processing heuristics. \n\nWe begin with describing the results from the evaluation of our method under the settings of the CHEMDNER challenge. Next, we demonstrate that our method obtains competitive performance compared to the state of the art. We then show that a statistical model trained on a large-scale, gold standard corpus such as CHEMDNER is suitable for recognising chemical names in a wider range of corpora, on which it consistently outperforms two known chemical NER tools. Finally, we describe the availability of our approach as a configurable workflow in the interoperable text mining platform Argo [ ]. Hereafter, we refer to our suite of solutions collectively as Chemical Entity Recogniser, or ChER. \n\n### Performance evaluation under the CHEMDNER challenge settings \n  \nThe first set of experiments was performed based on the specifications of the BioCreative IV CHEMDNER track [ ], which our research team participated in. The micro-averaged results on the CHEMDNER test set obtained by our solutions using specialised pre-processing analytics (i.e., Cafetiere Sentence Splitter and OSCAR4 Tokeniser) are presented in Table  . These closely approximate the results which were reported for our submissions during the official BioCreative challenge evaluation [ ], in which the variant employing knowledge-rich features and abbreviation recognition achieved the best performance in both the CEM and CDI subtasks. \n  \nPerformance of ChER under the BioCreative IV CHEMDNER track setting. \n  \nKey: Abbr. = Abbreviation recognition, Comp. = Chemical composition-based token relabelling \n  \n\n### Performance comparison against state-of-the-art methods \n  \nWe conducted a performance-wise comparison of our solution, ChER, against previously reported machine learning-based chemical NER methods, namely, that of Corbett et al. [ ], Rockt\u00e4schel et al. [ ] and Nobata et al. [ ]. To facilitate a fair comparison, we performed a series of benchmarking tasks under the same experimental settings used in their previously reported work. \n\nFollowing Corbett et al. [ ], we performed three-fold cross validation on the SciBorg corpus, taking into consideration only annotations for mentions of chemical molecules. As summarised in Table  , the F score obtained by our methods (79.66%) is slightly lower than that reported by Corbett et al. (81.20%). We cannot remark on precision and recall, however, as the authors did not report them. It is worth noting that their work became the foundation of what is now known as the OSCAR chemical NER tool. Although the software is freely available [ ], we have not been able to replicate their reported results on the SciBorg corpus as the models bundled with the downloadable release were trained on documents from the same data set. \n  \nComparative evaluation of ChER against state-of-the-art chemical name recognition methods. \n  \nOSCAR's F score was taken from the paper of Corbett et al. [ ]. \n  \nFollowing the experimental setup employed by Rockt\u00e4schel et al. [ ] in evaluating their ChemSpot tool, we trained a CRF model on the SCAI training corpus containing annotations for systematic names. Consequently, the version of ChER driven by this particular model can recognise only systematic names, and was thus evaluated only against the gold standard systematic name annotations in the SCAI pilot corpus of 100 abstracts (SCAI-100). The results shown in Table   indicate that whilst ChER and the CRF-based component of ChemSpot achieve similar recall (67.50% and 67.70%, respectively), the former obtains far more superior precision and F score (86.70% and 75.90%) over the latter (57.47% and 62.17%). We note that in conducting this comparison, we ran ChemSpot [ ] on the SCAI-100 corpus ourselves, enabling its capability to recognise multiple chemical name subtypes, in order to segregate recognised systematic names. \n\nLast in this series of evaluations is the performance-wise comparison of ChER with MetaboliNER [ ], a tool based on a CRF model that utilised the Chemical Entities of Biological Interest (ChEBI) [ ] and Human Metabolome (HMDB) [ ] databases as dictionaries. The tools were evaluated on the NaCTeM Metabolites corpus [ ] in a 10-fold cross validation manner [ ]. The obtained results, presented in Table  , indicate that MetaboliNER achieves higher precision (83.02% vs. 81.42%); however, it is outperformed by our method in terms of recall and F score (79.66% and 80.53% vs. 74.42% and 78.49%). \n  \nComparative evaluation of ChER against a state-of-the-art metabolite name recognition method. \n  \nWe surmise that our solution's superior performance over the similarly CRF-based ChemSpot and MetaboliNER tools can be explained by the richer feature set we employed in developing ChER. As described in the Experiments section below, ChER utilises a comprehensive set of character and word   n  -grams as well as orthographic features, which were then augmented with ones which capture chemical knowledge, e.g., number of chemical basic segments, dictionary and chemical symbol matches. Meanwhile, ChemSpot employs only size-two affixes, a check for leading or trailing whitespace, a quite limited set of orthographic features and bag-of-words [ ]. MetaboliNER uses a similar feature set, with the addition of word shape, part-of-speech tags and dictionary features [ ]. Based on the evaluation presented, ChER's rich feature set proved to be more informative and powerful over that of ChemSpot and MetaboliNER. \n\n\n### Performance evaluation on a variety of chemical corpora \n  \nAs stipulated earlier, one of the barriers to the development of chemical named entity recognisers was the lack of publicly available, wide-coverage, large-scale gold standard corpora. The public release of the CHEMDNER corpus directly alleviates this issue, allowing us to train our CRF model on a massive number and variety of learning examples. We argue that a model trained on the CHEMDNER corpus produces satisfactory NER performance even on documents of different types (e.g., patents, DrugBank descriptions) and from various specialised subject domains (e.g., pharmacology, metabolomics). In validating this, we utilised the CHEMDNER training and development sets to train CRF models under the various configurations detailed in the Experiments section. Taking the best performing variant, we compared its performance with that of OSCAR and ChemSpot by also running their latest versions (OSCAR4.1 and ChemSpot 2.0) on each corpus of interest. Across all five corpora we used, ChER consistently outperformed the other two NER tools, often with a noticeable margin. \n\nPresented in Table   are results of this evaluation scheme on general chemical corpora. On the SCAI-100 corpus, with all chemical name types taken into consideration, ChER achieved a good balance between precision and recall, giving an F score of 78.27% which is almost four percentage points higher than that of the second-best performing ChemSpot. An even larger margin of about 12 percentage points (also in terms of F score) was obtained by ChER over ChemSpot on the Patents corpus [ , ]. The relatively low F score on this corpus (64.75%) can be explained by the difference in document types between the corpus for model training (i.e., scientific abstracts) and evaluation (i.e., patent applications). We note that an evaluation on a third chemical corpus, SciBorg, was not carried out under this scheme. Since OSCAR was trained on the SciBorg corpus, a comparative evaluation of ChER, OSCAR and ChemSpot on this data would not have given fair results. \n  \nApplicability of ChER with the CHEMDNER model to other chemical corpora. \n  \nThe model trained on CHEMDNER data was proven suitable even for recognising mentions of drugs, which comprise a more specific chemical type (Table  ). When evaluated on each of the Drug-Drug Interaction (DDI) test [ , ] and Pharmacokinetics (PK) [ , ] corpora, more than satisfactory F scores (\u224883%) were obtained. ChemSpot's F score on the DDI test corpus trails behind by only two percentage points, but is significantly lower than ChER's on the PK corpus with a margin of almost 10 percentage points. Applying the same model to the NaCTeM Metabolites corpus, however, did not yield results as satisfactory as those on the drug corpora, with the highest F score being 73.07% (Table  ). This, nevertheless, still indicates a significant advantage over ChemSpot, whose F score is 8 percentage points behind. \n  \nApplicability of ChER with the CHEMDNER model to drug corpora. \n    \nApplicability of ChER with the CHEMDNER model to the NaCTeM Metabolites corpus. \n  \nWhilst the model obtained balanced precision and recall on the chemical corpus SCAI-100 (P = 77.85% vs. R = 78.69%), the suboptimal precision values on the DDI (P = 75.88% vs. R = 92.05%), PK (P = 79.83% vs. R = 88.34%) and Metabolites (P = 65.08% vs. R = 83.29%) corpora are noticeable. This drop in precision is to be expected, and can be explained by the differences between the annotation scopes of the training data, CHEMDNER, and of each of the latter three evaluation corpora. Both of the DDI and PK corpora contain only drug name annotations, whilst only metabolite mentions were annotated in the Metabolites corpus. Whereas the model was trained to recognise all chemical mentions, each of the DDI, PK and Metabolites corpora considers only a subset of them as correct, leading to an increase in the number of false positives. \n\n\n### Configurable chemical entity recognition workflows in Argo \n  \nIn order to facilitate the reproduction of results and further experimentation, we have made the presented named entity recognition methods available in our publicly accessible, Web-based, text mining workbench Argo [ ]. The workbench aims to bring text mining to non-technical audiences by providing a graphical user interface for building and running custom text-processing applications. Applications are built in Argo visually as block diagrams forming processing pipelines, or more generally, workflows. Individual blocks in a diagram correspond to elementary processing components that are selected by users from the available, ever-growing library of analytics. The components in the library range from simple data (de)serialisers to syntactic and semantic analytics to user-interactive components. \n\nThe proposed recogniser is available as a single component and exposes multiple configurations to choose from. Users may select one of chemical, drug or metabolite, as the model that will be used for the recognition. Additional options include the disabling of post-processing steps discussed in the next section. \n\nFigure   shows how the chemical entity recogniser component can be used in Argo workflows. Both workflows shown in the figure contain components that proved to yield the best performance on the CHEMDNER corpus. The left-hand-side workflow is set up to process PubMed articles (supplied by specifying abstract identifiers in the reader's configuration) and save the result of processing (recognised chemical names) in an RDF file. The right-hand-side workflow is a sample set up for experimenting with components available in Argo. The ultimate component in this workflow, Reference Evaluator, reports evaluation metrics based on two inputs: the reference input, which in this workflow comes directly from the CHEMDNER corpus reader and contains golden annotations, and the other branch in the workflow that attempts to reproduce the annotations in the input corpus. Users may experiment with this workflow by replacing the components (specifically the preprocessing components) with other, similar-purpose analytics available in Argo. \n  \n The chemical entity recogniser in Argo  . The proposed chemical entity recogniser is available as a processing component in the Web-based, text mining workbench Argo. The component is shown here as part of two individual workflows. The left-hand-side workflow produces an RDF file containing annotated chemicals in user-specified PubMed abstracts. The right-hand-side workflow reports effectiveness metrics for the CHEMDNER corpus. \n  \n\n\n## Experiments \n  \nThe following is a detailed description of our proposed methods and the experiments carried out to facilitate the identification of the most optimal chemical NER solutions. \n\n### Selection of pre-processing analytics \n  \nComing from a specialised domain, chemical literature exhibits unique properties, e.g., unusually long names, which are not typically encountered in documents from other subject domains. Whilst pre-processing steps to text mining have not been given much attention, we argue that the selection of suitable analytics for preprocessing chemical documents brings about a significant impact on NER performance, inspired by the findings of a prior exploratory work [ ]. This is especially relevant in our case where features employed in training our CRF models were extracted at the basic level of tokens. In this work, we focus on the two pre-processing tasks of sentence boundary detection and tokenisation. For each of these, specialised and non-specialised implementations were explored. \n\n#### Sentence splitters \n  \nIn segmenting documents into sentences, two heuristics-based tools, i.e., the LingPipe Indo-European sentence model [ ] and NaCTeM's Cafetiere sentence splitter [ ], were individually employed in our experiments. Whilst the former was tuned for documents written in general language, the latter was designed specifically for scientific text, having been enriched with specialised rules that, for instance, account for the possibility of sentences beginning with lower-case characters, as with protein names, e.g.,   p53  . \n\n\n#### Tokenisers \n  \nFor the decomposition of each sentence into tokens, we explored each of the tokenisers built into the GENIA tagger [ ] and the OSCAR4 NER tool [ ]. The former employs a statistical model trained on biomedical documents, whilst the latter applies segmentation rules specifically tuned for chemical texts. The OSCAR 4 tokeniser, for example, is capable of keeping intact long chemical names (e.g.,   4,9-Diazadodecane-1,12-diamine  ). \n\n\n\n### Model training using a chemical knowledge-rich feature set \n  \nIn building a model, we employed NERsuite, a combination of tools that include a CRF implementation [ ] and utilities for embedding custom dictionary features. The following sections describe the features we used with this tool. They include basic, weakly chemical-indicative features and chemical-specific features. \n\n#### Weakly chemical-indicative features \n  \nBy default, NERsuite extracts the character and word   n  -gram features presented in Table  . To exemplify, we provide the tokenised sentence in Table   as sample input, with   GSK214a   as the active token, i.e., the token currently under consideration. We note that the extraction of the word   n  -grams was done within a distance of two from the active token. Aside from these features, a token's symbol-level composition is also captured by means of the orthographic features listed in Table  . We have augmented this set with the following: \n  \nCharacter and word   n  -gram features extracted by NERsuite by default. \n    \nExample of a sentence tokenised and labelled with part-of-speech and chunk tags. \n    \nOrthographic features extracted by NERsuite by default. \n  \nOccurrence of Greek characters. This feature reflects an observation that several chemical names contain Greek characters, e.g.,   (S)-\u03b1,\u03b5-diaminohexonoic acid  . \n\nWord shape. The active token is transformed to a representation in which numericals are converted to the '0' characters, uppercase letters to the 'A' characters, lowercase letters to the 'a' characters and everything else to the '_' characters. Full and brief word shape variants were extracted for each token. In the former, each character in the resulting representation is retained, whereas consecutive similar character types are collapsed into one in the latter. For example, the name   10-amino-20(S)-camptothecin   would have   00_aaaaa_00_A_ _aaaaaaaaaaaa   and   0_a_0_A_a   as its full and brief word shapes, respectively. \n\n\n#### Chemical dictionary matches \n  \nRecognising that the occurrence of a token in an expert-curated dictionary indicates a high likelihood of it being a chemical name constituent, we utilised matches between token surface forms in text and entries in well-known chemical resources. Five dictionaries were compiled based on the chemical names and synonyms available in the Chemical Entities of Biological Interest (ChEBI) database [ ], DrugBank [ ], the Comparative Toxicogenomics Database (CTD) [ ], PubChem Compound [ ] and the Joint Chemical Dictionary (Jochem) [ ]. The dictionary tools available in the NERsuite package were employed in the compilation and subsequent application of these dictionaries. We configured the compiler utility to generate a compiled dictionary whose entries were normalised by the conversion of alphabetic characters to their lower-case equivalents, numericals to the '0' characters and special characters/punctuation to the '_' characters. In the matching phase, the dictionary tagging tool performs the same conversion step on input text and then captures longest possible matches between the normalised token sequences and dictionary entries. The dictionary tagging results, exemplified in Table  , were encoded in the begin-inside-outside (BIO) format. For an active token, unigrams and bigrams formed based on the BIO labels (within a distance of 2), as well as their combination with the corresponding surface forms, were generated as features. The token   starch  , for instance, would have the following as some of its CTD dictionary features: {  on:O  ,   hydroxyethyl:B  }, {  hydroxyethyl:B  ,   starch:I  } as surface form and dictionary label bigrams, and {  O  ,   B  }, {  B  ,   I  } as dictionary label bigrams. \n  \nExample of a token sequence tagged with matches against chemical dictionaries. \n  \n\n#### Chemical affix matches \n  \nMany of the chemical names, especially nomenclature-based ones, contain chemical affixes (i.e., prefixes and suffixes). We attempt to capture this property by matching tokens in text against lists of commonly used chemical prefixes and suffixes whose lengths range from two to four. Shown in Table   is a sequence of tokens matched against our compiled affix lists, which are provided in an additional file (see Additional file  ). The feature set is augmented with the resulting affix matches. \n  \nExample of a token sequence tagged with matches against our affix lists. \n  \n\n#### Number of chemical basic segments \n  \nNomenclature-based chemical expressions, e.g., systematic and semi-systematic names, are formed from combinations of chemical segments. These segments are documented in the American Chemical Society's Registry File Basic Name Segment Dictionary, which contains a total of 3,307 entries as well as a description of the procedure for decomposing a name into its basic segments [ ]. Following this algorithm, we process the surface form of each token to determine the number of constituent chemical basic segments. Table   lists the basic chemical segments found within the given expressions. We note that the number of basic segments also includes fragments which remain unmatched against the segment dictionary. For instance, only the fragments   methyl  ,   ergo   and   novi   in the name   methylergonovine   can be found by the procedure; however, the remaining fragment   ne   was also counted as a basic segment. \n  \nExamples of chemical names with corresponding basic segments. \n  \n\n#### Chemical symbol matches \n  \nIn order to account for chemical element symbols, which are not always covered by our five chosen dictionaries, we matched tokens in text against a list of symbols which occur in the periodic table of elements. This list has been provided in an additional file (see Additional file  ). \n\n\n\n### Heuristics-based post-processing \n  \nTwo CRF models were initially learned from the CHEMDNER training corpus: one with only the default features and another with our engineered ones. For each input sentence, each of the models automatically generates a label sequence in BIO format, together with the confidence values with which the labels were assigned. Upon individually applying the models on the CHEMDNER development set, we obtained the results presented in Table  . Whilst the performance boost brought about by our customised features was encouraging, the suboptimal recall prompted us to introduce post-processing steps for reducing the number of false negatives. \n  \nPerformance of models learned from the CHEMDNER training set when evaluated on the development set. \n  \nBy inspecting the distribution of false negatives according to chemical mention types (provided in Table  ), we identified the most prevalent problematic cases which we addressed with two rule-based post-processing steps. \n  \nDistribution (according to chemical subtype) of the instances incorrectly rejected by the model trained with enriched features. \n  \n#### Abbreviation recognition \n  \nTo alleviate the problem of missed abbreviations which account for about 30% of the false negatives, we introduced an abbreviation recognition step which performs the following checks given the last token   t  of a named entity   e   recognised by the CRF model: \n\n\u2022   t  is the opening parenthesis '(', \n\n\u2022   t  is the closing parenthesis ')', and \n\n\u2022   t  was recognised as a non-chemical token by the CRF model. \n\nToken   t  becomes a candidate abbreviation for   e   if all three conditions hold true. As a verification step, a procedure [ ] for checking the sequential occurrence of each character in   t  within the entity   e   is performed. Upon successful verification, all instances of token   t  within the document are relabelled as chemical tokens. In this manner, for instance, the chemical abbreviation   STMP   missed by the CRF model will be captured from the phrase, \"  ... was phosphorylated with sodium trimetaphosphate (STMP) at ambient temperature..  .\" assuming that   sodium trimetaphosphate   was recognised by the model as a chemical entity. \n\n\n#### Chemical composition-based token relabelling \n  \nAbout 42% of the false negatives correspond to missed family, trivial and systematic names, all of which typically contain chemical segments. In attempting to increase recall for these mention types, we developed a procedure that analyses tokens which were labelled by the CRF model as non-chemical with confidence values lower than a chosen threshold   t  . These tokens are of interest as the relatively low confidence values attached to them indicate their likelihood of being chemical name constituents. This likelihood was further verified by the computation of a token's chemical segment composition, given by the ratio of the number of characters comprising segments matched against the chemical basic segment dictionary to the total number of characters in the token. Sample tokens and the ratios calculated for them are provided in Table  . The procedure relabels a token of interest as chemical if its chemical segment composition is greater than a chosen threshold   t  . \n  \nSample tokens and their chemical segment composition. \n  \nDifferent combinations of the thresholds   t  and   t  were investigated to establish the most optimal values. After a few exploratory runs, we decided to restrict our search space to the range [0.91, 0.99] for   t  and to [0.5, 0.9] for   t  . These were exhaustively probed in increments of 0.01 and 0.1 for   t  and   t  , respectively, by means of evaluation on the CHEMDNER development data set. Results showed that recall is optimal with   t  = 0.96 and   t  = 0.5, and that optimal precision and F scores are obtained with   t  = 0.93 and   t  = 0.9. In the rest of the experiments presented in this paper, we used the latter threshold settings. \n\n\n\n### Evaluation \n  \nWith the proposed extensions presented above, chemical NER can be optimised according to the following five dimensions: \n\n1 Pre-processing: Sentence splitting (LingPipe Indo-European model or Cafetiere) \n\n2 Pre-processing: Tokenisation (GENIA or OSCAR4) \n\n3 Model training: Knowledge-rich features (include or exclude) \n\n4 Post-processing: Abbreviation recognition (enable or disable) \n\n5 Post-processing: Chemical composition-based token relabelling (enable or disable) \n\nOut of the possible combinations from these five dimensions, we selected 20 for each of our experiments, enabling abbreviation recognition and chemical composition-based token relabelling only when knowledge-rich features were employed in model training. This comprehensive evaluation was carried out with the utilisation of the CHEMDNER, SCAI and SciBorg corpora, as well as the following document collections: \n\nPatents [ ]. This corpus is the outcome of the collaborative effort of curators from the European Patent Office and the ChEBI project who annotated all mentions of chemical entities in 40 patent application documents [ ]. \n\nDrug-Drug Interaction (DDI) [ ]. Consisting of 233 MEDLINE abstracts and 792 textual descriptions from the DrugBank database, this corpus contains annotated drug mentions pertaining to generic names, brands, groups (e.g.,   antibiotic  ) and non-human applications (e.g.,   pesticide  ) [ ]. Released as a resource for the SemEval 2013 DDI Extraction task [ ], the corpus is divided into subsets for training and testing. \n\nPharmacokinetics (PK) [ ]. Also containing drug name annotations, this corpus is comprised of a selection of 541 MEDLINE abstracts on the topics of clinical pharmacokinetics and phamacogenetics as well as   in vitro   and   in vivo   drug-drug interactions [ ]. \n\nNaCTeM Metabolites [ ]. This document collection contains 296 MEDLINE abstracts with annotations for metabolite and enzyme names [ ]. For our evaluation, only metabolite name annotations were taken into consideration. \n\nTable   summarises the results of the best performing ChER combination in each of the experiments we conducted. For the purpose of comparison, we have also provided results obtained by our baseline, i.e., the variant of the named entity recogniser that employs non-specialised pre-processing analytics (i.e., the LingPipe Indo-European sentence model and the GENIA tokeniser) and none of the knowledge-rich features and post-processing heuristics. It can be observed that in majority of the nine sets of experiments in this table, the optimal combination for ChER incorporates the use of specialised pre-processing tools, feature set enrichment and abbreviation recognition. The lack of a unique combination yielding optimal results across all evaluation data sets can be explained by the differences of the corpora in terms of the guidelines which were adhered to during their annotation. Enabling chemical composition-based token relabelling brought about improved F scores on the SciBorg, Patents, Metabolites and DDI corpora (owing to increased recall), but resulted in lower values of F on the CHEMDNER, SCAI and PK corpora (due to decreased precision). This post-processing step, for example, captured mentions of anions which were considered as chemical names in the SciBorg and Metabolites corpora (e.g.,   silicate  ,   glutamate  ,   succinate  ) but were counted as false positives by the SCAI corpus. Similarly, some chemical umbrella terms, such as   esters   and   nucleotides  , captured by this step were treated as true positives under evaluation against the Patents corpus, but stand for false positives in the CHEMDNER, SCAI and PK corpora. Another source of discrepancy are chemical named entities which have ambiguous meanings, that this rule-based step is oblivious to.   Iron   as a metallic element, for example, was not annotated in CHEMDNER and SCAI, but is considered a drug (i.e., a vitamin) in the DDI corpus. Meanwhile, abbreviation recognition boosted ChER's performance on all corpora except for the DDI corpus, where no impact was observed due to it not having been annotated with abbreviation information. \n  \nSummary of ChER's performance under the CHEMDNER track setting (set 1), under similar experimental settings as state-of-the-art methods (sets 2-4), and when applied to various corpora (sets 5-9). \n  \nThe first row in each set corresponds to the baseline. Key: Cust. Feats. = Custom Features, Abbr. = Abbreviation recognition, Comp. = Chemical composition-based token relabelling; \u2713 = enabled, \u2717 = disabled, \u2022 = enabling or disabling makes no difference in performance. \n  \nThe results of all 20 combinations, in each of the nine experimental set-ups described in Table  , are provided in Additional file  . The impact on performance of individually selecting a particular pre-processing analytic or enabling a specific post-processing heuristic can be easily observed from this file. For example, on the CHEMDNER test data, the ChER variant that employs Cafetiere Sentence Splitter, OSCAR4 Tokeniser, knowledge-rich features and abbreviation recognition for the CEM task obtains an F score of 86.65%. Replacing OSCAR4 Tokeniser with GENIA Tokeniser, however, leads to a 6-percentage point drop in F score (80.1%). \n\n\n\n## Conclusions \n  \nThe exhaustive evaluation of our proposed tool ChER shows that in majority of cases the most optimal variant incorporates specialised pre-processing analytics (specifically, the Cafetiere sentence splitter and OSCAR4 tokeniser), knowledge-rich machine-learning features and a post-processing step for abbreviation recognition. In each experiment that we performed, comparison of the optimal combination with the baseline (i.e., the variant of the NER without any of our proposed additions) indicates noticeably better performance of the former over the latter. When compared to state-of-the-art methods, our solutions obtain competitive, if not superior, performance. \n\nChER with a statistical model learned from the training and development sets of the CHEMDNER corpus proved to achieve a satisfactory performance on a variety of corpora, regardless of document type and chemical subdomain, consistently outperforming the state of the art. \n\nAs our solutions are all accessible and usable via the Argo text mining platform, interested parties can replicate our results, if not introduce further improvements to our solution by exploring other analytics. Moreover, owing to the interoperable nature of Argo, our chemical entity recogniser, ChER, does not impose any restrictions in terms of input and output formats. It can be easily integrated as a semantic analytic in other text mining tasks such as document indexing and entity relation extraction. \n\n\n## Methods \n  \n### Sequence labelling \n  \nIn addressing the problem of named entity recognition, we employed a sequence labelling approach which involves the automatic assignment of labels to a given sequence of items, i.e., the ordered tokens in a sentence. The set of possible labels was defined by our chosen encoding scheme, the begin-inside-outside (BIO) representation. This scheme uses the labels 'B' and 'I' to indicate the beginning and continuing tokens of a chemical name, respectively, and 'O' to mark tokens which are not part of any chemical name. To transform the documents into this representation, the following pre-processing pipeline was applied on raw input text: \n\n#### Sentence splitting \n  \nText contained in each document was segmented by means of a sentence splitter. As described previously, the LingPipe Indo-European sentence model and Cafetiere sentence splitter were individually applied in this work. \n\n\n#### Tokenisation \n  \nIn segmenting each sentence into tokens, we utilised each of the GENIA and OSCAR4 tokenisers in our experiments. \n\nPart-of-speech and chunk tagging. Each resulting token is automatically lemmatised and assigned tags which correspond to its part-of-speech (POS) and enclosing chunk. This information was supplied by the GENIA Tagger [ ] which employs maximum entropy models in analysing both general and biomedical-domain documents. Shown in Table   are the lemmata, POS and chunk tags assigned to the tokens of the given sentence. \n\nOur sequence labelling approach was realised as an application of the machine learning-based conditional random fields algorithm (CRFs). Given an item sequence, a CRF model predicts the most probable label sequence based on functions capturing characteristics of the current token and its context. These functions, typically referred to as features (discussed in detail in the Experiments section), are employed in both training and prediction phases. We built our named entity recognisers on top of the NERsuite package [ ], an implementation of CRFs with a built-in extractor of features typically used in biomedical NER. \n\n\n\n### Evaluation metrics \n  \nWe reported the effectiveness of our methods with the commonly used information retrieval metrics, namely, precision (P), recall (R) and F score defined as follows: \n\n\n\nwhere   TP  ,   FP   and   FN   are the numbers of, respectively, true positive, false positive and false negative recognitions. Intuitively, precision is the fraction of recognised entities that are correct, recall is the fraction of manually annotated entities that were recognised, and F is a balanced harmonic mean between the two. F represents a more conservative metric than the arithmetic average. \n\nWe note that all of the results reported in this paper, including those of the other chemical NER tools, were obtained using the evaluation tool provided by the BioCreative organisers [ ]. The tool calculates the macro- and micro-averaged values of the aforementioned metrics. \n\n\n\n## Competing interests \n  \nThe authors declare that they have no competing interests. \n\n\n## Authors' contributions \n  \nRB carried out the development and evaluation of the proposed methods for chemical NER optimisation. RR developed the various components for making the results of this work available and usable in Argo. Both RR and RB formulated the study. SA provided guidance in the evaluation of the work and coordinated the effort. All authors read and approved the final manuscript. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 4331696, "text_md5": "86ec9c7cf19f6fb6f1e6346278844b0a", "field_positions": {"authors": [0, 58], "journal": [59, 71], "publication_year": [73, 77], "title": [88, 202], "keywords": [216, 372], "abstract": [385, 2313], "body": [2322, 44022]}, "batch": 1, "pmid": 25810777, "doi": "10.1186/1758-2946-7-S1-S6", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331696", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4331696"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331696\">4331696</a>", "list_title": "PMC4331696  Optimising chemical named entity recognition with pre-processing analytics, knowledge-rich features and heuristics"}
{"text": "Zhu, Qile and Li, Xiaolin and Conesa, Ana and Pereira, C\u00e9cile\nBioinformatics, 2017\n\n# Title\n\nGRAM-CNN: a deep learning approach with local context for named entity recognition in biomedical text\n\n# Keywords\n\n\n\n# Abstract\n \n## Motivation \n  \nBest performing named entity recognition (NER) methods for biomedical literature are based on hand-crafted features or task-specific rules, which are costly to produce and difficult to generalize to other corpora. End-to-end neural networks achieve state-of-the-art performance without hand-crafted features and task-specific knowledge in non-biomedical NER tasks. However, in the biomedical domain, using the same architecture does not yield competitive performance compared with conventional machine learning models. \n\n\n## Results \n  \nWe propose a novel end-to-end deep learning approach for biomedical NER tasks that leverages the local contexts based on n-gram character and word embeddings via Convolutional Neural Network (CNN). We call this approach GRAM-CNN. To automatically label a word, this method uses the local information around a word. Therefore, the GRAM-CNN method does not require any specific knowledge or feature engineering and can be theoretically applied to a wide range of existing NER problems. The GRAM-CNN approach was evaluated on three well-known biomedical datasets containing different BioNER entities. It obtained an F1-score of 87.26% on the Biocreative II dataset, 87.26% on the NCBI dataset and 72.57% on the JNLPBA dataset. Those results put GRAM-CNN in the lead of the biological NER methods. To the best of our knowledge, we are the first to apply CNN based structures to BioNER problems. \n\n\n## Availability and implementation \n  \nThe GRAM-CNN source code, datasets and pre-trained model are available online at:  . \n\n\n## Supplementary information \n  \n are available at   Bioinformatics   online. \n\n \n\n# Body\n \n## 1 Introduction \n  \nNamed entity recognition (NER) is one of the first steps in the processing natural language texts. This task is aimed at identifying mentions of entities (e.g. persons, organizations and locations) in documents. In the biomedical domain, BioNER aims at automatically recognizing entities such as genes, proteins, diseases and species. \n\nBioNER is considered more difficult than the general NER problem, because:\n   \nMillions of entities have been discovered, and the number is constantly increasing with the sequencing of new species. \n  \nThe same biological entity can be described in different ways ( ). \n  \nThe names of many biomedical entities are typically long (i.e. containing more than four words) ( ). \n  \nLong sequences are usual in biomedical text. \n  \n\nThere are several kinds of methods applied to extract named entities from biological texts. These include dictionary-based ( ), rule-based ( ), machine learning based ( ) and deep learning approaches ( ). \n\nDictionary-based approaches ( ) are limited by the size of the dictionary, misspellings, the use of synonyms and the constant increase of vocabulary. Rule-based approaches ( ;  ), use common naming structures or morpho-syntactic features. These methods require extensive domain knowledge in order to develop rules, which are then not easily applicable to other domains. Machine learning-based methods suppose the initial definition of the features of interest. The most effective machine learning approaches applied to the NER problem are conditional random field approaches (CRF). The performance of CRF models rely heavily on the features, for example, orthographic, morphological, linguistic-based, conjunctions and dictionary-based. Those features are generally developed by experts, implying that they are task-specific and costly to develop. In this group of methods, we can cite ABNER ( ), BANNER ( ) and Gimli ( ). \n\nDeep learning demonstrates state-of-the-art performance in many areas ( ) including speech recognition ( ), image classification ( ), image segmentation ( ), part-of-speech (POS) tagging ( ) and NER ( ;  ). Fully connected neural network is used ( ) to effectively identify entities in a newswire corpus. The application of character and word embeddings in Bi-directional Long Short-Term Memory (LSTM) ( ;  ) achieved state-of-the-art performance in several sequence-to-sequence datasets, such as CoNLL03 ( ) for NER and Penn Treebank WSJ ( ) for POS tagging. Nevertheless, deep learning methods typically require a large amount of labeled data for supervised learning and take more time and computing resources to train than the classical machine learning methods. \n\nDespite the good performance of the deep learning methods in many areas, the application of Bi-directional LSTM to the bioNER problem did not obtain as good results as conventional machine learning approaches ( ). Bi-directional LSTM uses the information contained in whole sentences. We hypothesized that long sentences could contain information unrelated with the target entities, and hence, in domains with long sentences, such as the biomedical literature, the utilization of local information rather than whole sentences may help improve precision. \n\nInspired in the inception model by ( ), we propose a novel neural network architecture to capture local information around each word in biomedical texts via Convolutional Nueral Network (CNN). To add some linguistic knowledge into our model, we also use POS tags as part of the input. This approach uses multiple n-gram features with different sizes together with its POS tag to capture each word\u2019s environment. Our method, called GRAM-CNN, is an end-to-end model requiring no task-specific resources or handcrafted features. Details on CNN can be found in subsection 2.1.1. \n\nThe GRAM-CNN approach was evaluated on three biomedical datasets, Biocreative II (BC2) ( ), NCBI disease corpus (NCBI) ( ) and the JNLPBA task ( ). It obtained an F1-score of 87.26% for BC2, 72.57% for JNLPBA and 87.26% for NCBI, always ranking among the top 2 best-performing methods. These results reveal that local information can efficiently predict the label of words and demonstrate that GRAM-CNN is a versatile approach that can theoretically be applied to wide range of BioNER tasks. \n\n\n## 2 Materials and methods \n  \nConsider the following sentence from the BC2 dataset: \u2018  STUDY DESIGN: Salivary immunoglobulin A levels of each of 20 subjects were determined on 3 occasions: first, while the subject was still smoking; second, 7\u2009days after cessation of smoking; third, on the 14th day after cessation  \u2019. In this sentence, the information about the study design (number of subjects and time points) is not relevant to understand that   Salivary Immunoglobulin A   is a protein. However, the words \u2018  levels of each of 20 subjects  \u2019, surrounding \u2018  Salivary Immunoglobulin A  \u2019, contain the term   level   that can successfully be used to tag   Salivary Immunoglobulin A   as a protein. Our approach focusses on this local context to better extract relevant information for the classification problem. We apply several convolutional kernel sizes (i.e. number of words around the target entity) to focus on the local context at multiple scales. \n\n### 2.1 The GRAM-CNN method \n  \nThe main steps of the GRAM-CNN method are as follows ( ): \n\n  \nGenerate the word, POS tag and character embeddings. \n  \nConcatenate the character embeddings of each letter of a word with its word embedding and POS tag embedding. \n  \nExtract each word\u2019s local features by GRAM-CNN with several kernel sizes as the final representation of each word (vector of CNN features) (see   for an example of kernel size 3). \n  \nApply CRF to model labels jointly based on the output of GRAM-CNN. \n  \n  \nOverall scheme of the GRAM-CNN approach. First, we apply the POS tag (diamonds on the left), word (diamonds in the middle) and character (diamonds on the right) embedding and concatenate all embeddings into a combined vector (the 3 squares). Second, we feed the concatenated vector into the GRAM-CNN to retrieve the local context information. Finally, we model the predicted labels using CRF to get the final result. In this example, we use 3 different kernels in GRAM-CNN (1, 3, 5). The \u2018B-Gene\u2019, \u2018I-Gene\u2019 and \u2018I-Gene\u2019 are labels corresponding to \u2018A3\u2019, \u2018adenosine\u2019 and \u2018Receptor\u2019, respectively, in IOB tagging scheme ( ). Further details of CNN-3 are given in  \n  \nIn this section, the GRAM-CNN architecture is described in detail following the order from inputs to outputs, layer by layer. \n\n#### 2.1.1 Embedding \n  \nWord embeddings bridge the gap between deep learning and natural language processing. Through this method, words are represented as dense vectors of real numbers, and those words that are semantically related are close in the high dimension space. \n\nWe used a pre-trained word embedding from biomedical texts proposed in 2016 ( ) that recovered 87.34% of the words of the BC2 dataset. For the POS tag embedding, randomly initialized vectors were used to represent each tag. POS tags are extracted by the NLTK toolkit ( ). During training, we fixed the word embeddings and trained POS tag embedding together with the whole system. \n\nThe word embedding method cannot give a useful representation of words that are absent from the training vocabulary (out-of-vocabulary problem). To solve this issue, character level embedding from words was applied ( ). The character embedding was implemented with a CNN ( ).\n \n  \nDiagram of the character embedding architecture. The vector of the letter   a   is in gray. In this example, CNN is applied with two different kernel sizes: 3 (a,l,k) and 4 (l, k, a,l), resulting in the production of two feature maps (middle squares). Then, a max pooling is applied to each feature map to get two representations of the word (top two squares in the figure). The length of embedding is two because in this example there is only one filter of each kernel size and two kernel sizes in total \n  \nIn this implementation, each character in a word was represented by a vector of a fixed length   d  . A word with length   l   can be represented by a matrix   M\u2009  =  \u2009R  . Each kernel   w   in CNN is a filter with shape   R  , where k is the kernel size. The kernel size is equal to the size of a convolutional window across   k   characters.\n  f   is the non-linear activation function (  tanh   in our experiments) and b is a bias vector.   Conv   is the convolution operation.   O   is the output vector of one kernel convolution with length l \u2013   k  \u2009+\u20091. Its max value is used to represent one kernel\u2019s feature. We used   k  \u2009=\u20092, 3, 4 and 40 filters for each   k  . Character embeddings were initialized randomly and trained with the whole network. \n\n\n#### 2.1.2 GRAM-CNN \n  \nGRAM-CNN is a CNN model allowing to extract local information between a target word and its neighbors ( ). The representation of an input word is a vector concatenating pre-trained word embedding and character embedding. In GRAM-CNN, all convolutional filters process the same input at the same time, and this allows the model to take advantage of multi-level feature extractions with different kernel sizes ( ). This architecture is similar to character embedding mentioned in Section 2.1.1, and the only difference is the process of feature selection. GRAM-CNN is a sequence-to-sequence network. Each output of GRAM-CNN corresponds to one input. To achieve this, only the correlated features are selected of the word, i.e. features directly computed by this word and its neighbors. In this way, we were able to reduce the noise and get a better representation of the word.\n \n  \nGRAM-CNN architecture. An example of kernel size 3 (squared) with an input of 10 words (word and character embedding concatenated) (on the left). CNN is used to extract information from these vectors. After the CNN step, a feature map is obtained (length 10\u2009\u2212\u20093\u2009+\u20091\u2009=\u20098 in this example). To extract the local information for each word, we use the correlated feature maps obtained partially with each word. A max pooling is applied over these feature maps, resulting in a vector of size one for each word (on the right). For example, the feature maps correlated with the word 3 (W3, in gray) are f1, f2, and f3 and the feature map correlated with W10 is f8 \n  \nTo extract local information and n-gram features without breaking the internal relation between embeddings, we followed ( ) using a filter with shape   (  k   is the kernel size,   w   is the length of word embedding and   c   is the length of character embedding). Suppose we are selecting a word indexed by i in a sentence from feature maps   f   convoluted by kernel size   j  .\n \n\nA max pooling is operated after getting the outputs from same kernel size   j  . The outputs of the same word are concatenated into one vector. This vector is the representation of the word. In this work, 50 filters were used for each kernel size in GRAM-CNN. \n\nOur goal is to predict a label for every word in the sentence. GRAM-CNN returns representations of each word. We then apply a two-layer fully connected network to get the final scores for each label of the word. These scores are sent to CRF to model the joint probability of words which will be described in the next subsection. \n\n\n#### 2.1.3 CRF \n  \nA simple way to label each word is to use its own features to predict the label independently. This is a fast and effective strategy when labels are not correlated. However, entity names usually consist of several words, meaning that labels do have correlations with their neighborhoods. For example, in the IOB2 annotation ( ), I-protein cannot follow B-gene or an O (outside of the entity) label. Therefore, it is beneficial to model the labels jointly. For this, a linear-chain conditional random field (CRF) ( ) is used. \n\nFor an input of a sentence containing n words, let   x   denote the input vector of the   i   word in the sentence, get  .   is the potential sequence labels of   x  .   Y  (  x  ) is the set of all possible label sequences. We used a variant of CRF, which is factored into unary potentials for single labels and binary potentials for every transition between output labels. The score was defined to be:\n \n\nThe first part of the score function is the binary score while the second is a unitary score.   is the matrix of transition scores in which   T   represents the score from tag   i   to   j  .   y   and   y   are start and end tags in the sequence that are not part of the output sequence.   P   is a matrix with dimensions   n\u2009  \u00d7  \u2009t  , where n is the number of words in the sentence and t is the number of potential tags.   corresponds to the score of   i  th word mapping to a specific tag, which is obtained from our neural network. The softmax function was used to compute the probabilities of all possible sequences.\n \n\nTo train the CRF model, the log-likelihood of the probability was maximized. To predict the sequence label, the sequence that has the largest probability was selected:\n \n\nSince the model binary potentials were the only modeled models, the Viterbi algorithm can solve the optimization problem and get an optimized result efficiently. \n\n\n\n### 2.2 Implementation details \n  \nThis section introduces how the network and parameters were trained. The GRAM-CNN approach, including the CRF layer was implemented by TensorFlow-v1.0.0 ( ) and trained with one Nvidia Titan X GPU. We use the tokenizer and POS tagger from NLTK toolkit ( ) to preprocess each passage. Except for the pre-trained word embedding, all weights including character embedding and POS tag embedding were trained together. For character embedding, we use kernel sizes 2, 3, 4 and 40 filters for each size. We found that kernel size ranging from 1 to 10 is best for BC2 and NCBI datasets, while sizes 1 to 12 worked better for the JNLPBA dataset. \n\n#### 2.2.1 Parameters initialization \n  \nWe used a pre-trained word embedding from  . For an unknown word (absent of the word embedding), the word \u2018UNK\u2019 was used to represent it, which implies that all unknown words had the same word embedding. \n\nCharacter embedding was adopted to distinguish the unknown words. In order to allow the neural network to use both word and character embedding instead of focusing on a part of it, dropout layer ( ) was applied on this concatenated vector before the vector was input to GRAM-CNN. \n\nFor character embedding, 25 dimensions were used to represent the character with a uniform sample from   ( ) where   dim   is the embedding dimension. We used 15 dimensions for POS tag embedding with the same initialization. Both character and POS tag embedding were trained with the whole network. Xavier initialization ( ) was used for all the convolutional layers and fully connect layers. All bias vectors were initialized to 0. \n\nThe kernel size decides how much local information (how many words) we take into account. Each kernel size can be seen as taking n-gram features from words. Different ranges of kernel sizes were tested to do experiments and compare to each on different datasets. \n\n\n#### 2.2.2 Optimization method \n  \nParameters in the network were optimized by stochastic gradient descent (SGD) with momentum 0.9. An initial learning rate of 0.002 with a learning decay\u2009=\u20090.95 was used. The learning rate was updated every 50 000 steps followed by  . Other sophisticated optimization algorithms such as AdaDelta ( ) and Adam ( ) were also tried. None of them meaningfully improved on simple SGD using the momentum settings specified above. \n\n\n#### 2.2.3 Tagging scheme \n  \nNER task is to assign every word in a sentence a label. A single entity may contain multiple words. IOB2 (Inside, Outside, Beginning) ( ) tagging scheme was used to tag every word in the sentence. After tokenization, every token which is a start token of a named entity is labeled as a B-label. An I-label is assigned to a token if it is inside a named entity. Other words that do not belong to any named entities are labeled as an O-label. The word \u2018label\u2019 was replaced with the type of the named entity, for example, B-gene is a beginning token for a gene entity and I-gene is inside a gene entity. \n\n\n\n### 2.3 Datasets \n  \nTo evaluate our algorithm, three biomedical NER datasets were chosen: the BioCreative II Gene Mention task (BC2) ( ), the NCBI disease corpus (NCBI) ( ) and the JNLPBA corpus ( ). \n\nBC2 is concerned with the named entity extraction of gene and gene product mentioned in the text. The training and the test data are independent and composed of 15 000 and 5000 sentences, respectively. Since BC2 does not provide a development dataset, we created it with a ratio 3:1. \n\nNCBI consists of 6892 disease mentions from 793 abstracts. Among those, 5145 are part of the training set, 787 are part of the development set, and 960 are part of the test set. \n\nThe JNLPBA dataset is a multi-entity dataset. It has protein, DNA, RNA, cell type and cell line, totaling five classes and representing a challenging scenario. It presents a training set and a test set of respectively 20 546 and 4260 sentences (51 301 and 8662 biomedical tags). The training set was divided with a ratio 3:1 in order to create the development set. \n\nTraining, validation and testing sets present in the NCBI dataset were used to evaluate these data. As the BC2 and JNLPBA datasets do not provide a validation set, their training datasets were split at a ratio 3:1 to created training and validation sets. The GRAM-CNN method was compared to the others NER methods already published and tested on the same data ( ).\n   \nDatasets used to evaluate our approach. BC2 and JNLPBA do not provide separation of training and development datasets \n    \n\n\n### 2.4 Evaluation metrics \n  \nTo evaluate the performance of the GRAM-CNN method and compare the results to other existing solutions, we used Precision, Recall and F-measure as experiment metrics:\n \n\nHere TP (True Positive) is the number of entities that are correctly identified. FP (False Positive) is the number of chunks that are mistakenly identified as an entity. FN (False Negative) is the number of entities that are not identified. In BC2 and JNLPBA, the scripts provided along with the datasets were used to evaluate the performance. Precision represents the ability of a system to predict only true items, and recall makes sure that a system can predict all true items. \n\n\n\n## 3 Results \n  \nThe described GRAM-CNN method was applied to three different datasets and six different entities. Results were compared with other deep learning methods and conventional machine learning approaches. \n\n### 3.1 BC2 \n  \nCompared to other deep learning methods ( ), GRAM-CNN increased the previous best F1-score by 6.68%. With an F1-score of 87.26%, it was also the best method among the non-ensemble methods and ranked second on the BC2 dataset. GRAM-CNN performed better than the widely used BANNER ( ), ABNER ( ), Gimli ( ) and IBM ( ). Moreover, among the top four methods, GRAM-CNN is the only one that does not require additional work. The IBM approach uses semi-supervised learning with additional data; AIIAGMT ( ) ensembles eight different CRF models with two different CRF frameworks; and Gimli is an ensemble of several models. The GRAM-CNN neural network approach, end-to-end without ensemble and gazetteers, obtained a result of high quality without any additional data.\n   \nResults of BC2 and NCBI datasets \n      \nResults of JNLPBA dataset measured in F1 score \n    \n\n\n### 3.2 NCBI \n  \nThe results obtained from the NCBI dataset are summarized in  . DNorm ( ) uses supervised semantic indexing, trained with pairwise learning to rank, then uses a CRF to return the score. TaggerOne ( ) was proposed recently, as it simultaneously performs NER and normalization and achieves an F1-score 82.9%. Deep learning methods play a leading role in the NCBI dataset. RNN with orthographic features has a result of 84.26% F1-score ( ). \n\nOur method, outperforming all other tested algorithms in recall (88.07%) and F1-score (87.26%), ranked first on this dataset. \n\n\n### 3.3 JNLPBA \n  \nJNLPBA corpus ( ) has protein, DNA, RNA, cell type and cell line totaling five different classes of entities. On this corpus, the top 3 methods presented an F1 score varying by less than 0.5% ( ). \n\nNERBio ( ), the best system on JLPBA corpus, obtained an F1-score of 72.98%. NERBio was implemented as a rule-based, post-processing approach that was designed especially for the JNLPBA task. On the BC2 dataset, NERBio only got an F1-score of 79.05%, which was under average and indicated that this solution is limited on other corpora. \n\nIn another hand, GRAM-CNN achieved an F1-score 72.57%. It outperformed most conventional machine learning systems and ranks second in the table with similar performance compared to NERBio. However, and contrary to NERBio, GRAM-CNN showed a high-performance on all the 3 tested datasets. \n\n\n### 3.4 Error analysis \n  \nWhile GRAM-CNN was the one method that showed a constant top performance across different datasets, it was not free of mis-classifications. This section provides an error analysis of our method. We focused on the JNLPBA dataset, as this is a multi-class classification problem and therefore, more challenging than the NCBI and BC2 datasets. \n\n shows several types of errors obtained with the GRAM-CNN method on the JNLPBA test set.\n   \nExamples of errors obtained with our approach applied on the JNLPBA test set \n  \n\nIn sentence A ( ), the word \u2018tumor\u2019 is described as a \u2018Cell_type\u2019 in the test set and was not predicted as a \u2018Cell_type\u2019 by GRAM_CNN. We noticed that, in the test set, the word tumor is associated with a cell type label in only 67% of the cases. This shows the difficulty for biocurators to be consistent in the annotation for nontrivial cases. This inconsistency was reflected in the predictions made by the NER methods. \n\nSentence B ( ) shows an example of misannotation of the border of the entity. In this case, the entity was recovered but the parenthesis situated just before the entity was seen as a part of the entity. A post-treatment of the protein name to ensure complete parenthesis-pairs could help solve this issue. \n\nIn sentence B, the GRAM-CNN method divided the reference to \u2018T- and B-lymphocyte\u2019 as two different entities: \u2018T-\u2019 and \u2018B-lymphocyte\u2019. In sentence C, a similar reference to entities \u2018B-cell or non-lymphoid cell lines\u2019 was considered this time as two different entities by the GRAM-CNN approach. Those differences in the annotation in the test set show a case where the annotation as one or two entities can both be considered as true and was treated inconsistently by the biocurators during their preparation of the test set. \n\nSentence C shows a case where an entity composed of several words \u2018CD4+\u2009T-cell gene Rpt-1\u2019 was partially recovered and misannotated. In this case, GRAM-CNN cut the entities in two parts and labeled the first part \u2018CD4+\u2009T-cell gene\u2019 as DNA but labeled the second part \u2018Rpt-1\u2019 as protein. \n\nSentence D presents an interesting case of false positive; \u2018lymphocytes\u2019 was recovered as a \u2018Cell_type\u2019 by GRAM-CNN but is not labeled in the test set. Since \u2018lymphocytes\u2019 is a cell type, this example is not a \u2018real\u2019 false positive of GRAM-CNN but a false negative case in the test set. \n\nAmong the randomly picked examples of errors of the GRAM-CNN method on the JNLPBA set, a fair proportion of errors corresponds to inconsistencies and errors in the test set. This could partially explain the smaller F1-score generally obtained by the NER methods on this test set compared to the results obtained from NCBI and BC2. \n\n\n\n## 4 Discussion \n  \nThe basic strategy of the GRAM-CNN is to address the BioNER problem by focussing on local information around each word rather than considering whole sequences as LSTM does. Several design choices contribute to the success of the GRAM-CNN method. First, GRAM-CNN uses a combination of word, character and POS tag embedding. The word embedding is pre-trained on biomedical text. The character embedding should provide the ability to represent new or misspelled words that are absent from the word embedding. Part-of-speech tag embedding provides the ability to take into account the grammatical information. Second, sentences in the biomedical text are typically longer than in other sources (14.53 words per sentence for CONLL-2013, 26.49 for JNKPBA). Several topics may be discussed in the same sentence and we hypothesize that local patterns can be found within these long sentences. In GRAM-CNN, the combination of multi-size CNN kernels focusing on the direct surrounding of each word reveals these local patterns, resulting in better performance than methods that merely considers the complete sentence, as is the case of LSTM. \n\nDespite the fact that the GRAM-CNN method was successfully applied to three different datasets, some particular data structures are not yet supported. Our method is suitable for mentions consisting of a word or a group of consecutive set of words, and it\u2019s also robust to misspellings. The three evaluated datasets fall into these categories. However, GRAM-CNN is not prepared to consider overlapping or disjoint mentions, neither mentions present in tables. Modifications to the IOB2 annotation ( ) would be required to allow these possibilities.   describes the type of mentions supported and not supported by GRAM-CNN. \n\nAs in all deep-learning methods, GRAM-CNN requires a significant amount of training data and is time-consuming. All the three tested datasets contain more than 5000 examples in the training set. A decrease of the quality of the assignation is expected if GRAM-CNN is trained on a smaller dataset. Training time is longer compared with conventional machine learning methods. The network converged after about 100 epochs for all three datasets. For JNLPBA and BC2 datasets, it takes about 5 and 1.5\u2009days for NCBI dataset. However, once training is finished, inference on a test set is comparatively fast, taking from 2 to 5\u2009min to complete on around 5000 sentences. \n\nFinally, it was recently shown that combining NER and normalization can improve the performance ( ). This suggests that within a multi-task architecture, combining our GRAM-CNN approach with a normalization procedure may improve performance. We anticipate that entities containing conjunctions and punctuations usual in biomedical NER will remain difficult to handle after normalization, which will still be a challenge for our and other approaches. \n\n\n## 5 Conclusion \n  \nWe hypothesized that local context information plays an important role in biomedical NER tasks. We implemented GRAM-CNN, a novel end-to-end neural network using both character embedding and word embedding for the biomedical NER tasks. This method, without using any hand-crafted features or domain knowledge, ranked among the top 2 methods on each tested dataset and achieved F1-scores of 87.26% in the BC2 dataset, 87.26% in NCBI dataset and 72.57% in JNLPBA dataset. To the best of our knowledge, this method is the first to achieve competitive performance using deep learning compared with conventional machine learning approaches in biomedical NER. \n\nBy applying the GRAM-CNN method on three different datasets, we showed that the GRAM-CNN approach is a versatile approach that can be widely applied to BioNER problems without requiring any hand-crafted features or humanly designed rules. \n\n\n## Funding \n  \nThis material is based upon work that was supported in part by the National Institute of Food and Agriculture, U.S. Department of Agriculture   (Award number 2015-70016-23029), National Science Foundation (grants ACI 1245880, ACI 1229576, CCF-1128805, CNS-1624782) and National Institutes of Health (R01GM110240). The content is solely the responsibility of the authors and does not necessarily represent the official views of the granting agency. \n\n Conflict of Interest  : none declared. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 5925775, "text_md5": "0d308dd656259bc1a8d7ac6c7ef20594", "field_positions": {"authors": [0, 61], "journal": [62, 76], "publication_year": [78, 82], "title": [93, 194], "keywords": [208, 208], "abstract": [221, 1880], "body": [1889, 29966]}, "batch": 1, "pmid": 29272325, "doi": "10.1093/bioinformatics/btx815", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5925775", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5925775"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5925775\">5925775</a>", "list_title": "PMC5925775  GRAM-CNN: a deep learning approach with local context for named entity recognition in biomedical text"}
{"text": "Tsueng, Ginger and Nanis, Steven M. and Fouquier, Jennifer and Good, Benjamin M. and Su, Andrew I.\nCitiz Sci, 2016\n\n# Title\n\nCitizen Science for Mining the Biomedical Literature\n\n# Keywords\n\ninformation extraction\ncitizen science\nmicrotask\nbiocuration\nnatural language processing\nbiomedical literature\n\n\n# Abstract\n \nBiomedical literature represents one of the largest and fastest growing collections of unstructured biomedical knowledge. Finding critical information buried in the literature can be challenging. To extract information from free-flowing text, researchers need to: 1. identify the entities in the text (named entity recognition), 2. apply a standardized vocabulary to these entities (normalization), and 3. identify how entities in the text are related to one another (relationship extraction). Researchers have primarily approached these information extraction tasks through manual expert curation and computational methods. We have previously demonstrated that named entity recognition (NER) tasks can be crowdsourced to a group of non-experts via the paid microtask platform, Amazon Mechanical Turk (AMT), and can dramatically reduce the cost and increase the throughput of biocuration efforts. However, given the size of the biomedical literature, even information extraction via paid microtask platforms is not scalable. With our web-based application Mark2Cure ( ), we demonstrate that NER tasks also can be performed by volunteer citizen scientists with high accuracy. We apply metrics from the Zooniverse Matrices of Citizen Science Success and provide the results here to serve as a basis of comparison for other citizen science projects. Further, we discuss design considerations, issues, and the application of analytics for successfully moving a crowdsourcing workflow from a paid microtask platform to a citizen science platform. To our knowledge, this study is the first application of citizen science to a natural language processing task. \n \n\n# Body\n \n## Background \n  \nBiomedical research is progressing at a rapid rate ( ). The primary mechanism for disseminating knowledge is publication in peer-reviewed journal articles. Currently more than 25 million citations are indexed in PubMed ( ), the primary bibliographic index for the life and health sciences developed and maintained by the US National Center for Biotechnology Information. PubMed is growing by over one million new articles every year. \n\nBecause of the exponential growth of the biomedical literature ( ), accessing its accumulated knowledge is a difficult problem. Journal publications are primarily in the form of free text, a format that is difficult to query and access. This problem is especially pronounced for biomedical research articles because of the imprecise way that language is used to refer to important biomedical concepts. For example, the acronym PSA has been used to refer to a number of different human genes, including \u201cprotein S (alpha)\u201d ( ), \u201caminopeptidase puromycin sensitive\u201d ( ), \u201cphosphoserine aminotransferase 1\u201d ( ), and most frequently \u201cprostate specific antigen\u201d ( ). There are also many other uses of \u201cPSA\u201d outside of the context of human genes such as \u201cPsoriatic Arthritis\u201d ( ) or \u201cPressure Sensitive Adhesive\u201d ( ). \n\nThe challenge of structuring the knowledge represented in free text is often referred to as \u201cinformation extraction\u201d, which in turn can be divided into three subtasks ( ). First, \u201cnamed entity recognition\u201d (NER) is the process of identifying the key concepts that are mentioned in the text. For example, named entities in biomedical texts might include genes, proteins, diseases, and drugs. Second, \u201cnormalization\u201d is the application of standardized vocabularies to deal with synonymous concept terms. For example, mentions of \u201cMNAR\u201d, \u201cPELP1\u201d, \u201cproline, glutamate, and leucine rich protein 1\u201d would be mapped to the same gene or protein rather than be treated as different entities. Lastly, \u201crelationship extraction\u201d is the process in which the relations between entities are characterized. \n\nCurrently, the gold standard for information extraction in biomedical research is manual review by professional scientists, a process referred to as biocuration ( ). Although an active natural language processing community is devoted to computational methods for information extraction ( ;  ;  ), the outputs of these methods are generally of insufficient quality to be widely used without subsequent expert review. \n\nPreviously, we demonstrated that crowdsourcing among non-experts could be an effective tool for NER. We used Amazon Mechanical Turk (AMT) to recruit and pay nonscientists to identify disease concepts in biomedical article abstracts ( ). As a gold standard, we compared AMT workers to professional biocurators who performed the same task ( ). We found that, following statistical aggregation, crowd annotations were of very high accuracy (F-score = 0.872, precision = 0.862, recall = 0.883), comparable to professional biocurators. \n\nAlthough this previous study demonstrated that nonexpert microtask workers are capable of performing biocuration tasks at a high level, exhaustively curating the biomedical literature using a paid system like AMT is still cost-prohibitive. Citizen science has been successfully applied to the field of biomedical research, but its application in this field has primarily focused on image processing (e.g., Eyewire, Cell Slider, Microscopy Masters), sequence alignment (e.g., Phylo), or molecular folding (e.g., Foldit, EtRNA). Citizen science also has been applied to address language problems, but these are generally focused on transcription (e.g., Smithsonian Transcription Center projects, Notes from Nature, Ancient Lives, reCAPTCHA); translation (e.g., Duolingo); or cognition (e.g., Ignore That, Investigating Word Modalities, Verb Corner). Here, we explored the use of citizen science as a scalable method to perform NER in the biomedical literature. We developed a web-based application called Mark2Cure ( ) to recruit volunteers and guide them through the same biomedical NER task that we explored in our prior AMT work ( ). \n\nIn this paper we fulfill our objective of demonstrating that citizen science can successfully be used to address big data issues in biomedical literature. Specifically, we (1) provide a brief overview on the platform we built to enable citizen scientists to do disease NER; (2) inspect our target audience by analyzing the recruitment, retention, and demographics of our participants; (3) demonstrate that citizen scientists are both willing and able to perform biocuration tasks when properly trained by assessing the performance of citizen scientists in the same disease NER task used in our AMT experiment; and (4) evaluate the success of our platform using the \u201celements of citizen science success matrix\u201d developed by ( ) and provide our results as a comparison point for other citizen science efforts. \n\n\n## Methods \n  \n### Document selection \n  \nThis experiment describes the annotation of 588 documents drawn from the training set of the NCBI Disease corpus, a collection of expert-annotated research abstracts for disease mentions ( ). Gold standard annotations for 10% of the document set (gold standard documents) were used to provide feedback and were randomly interspersed. \n\n\n### Mark2Cure design \n  \nMark2Cure was designed to provide a user-friendly interface for engaging members of the public to perform the biocuration task of NER. The goal of this experiment was to have all 588 documents annotated for the NER task by at least 15 volunteers. This threshold was chosen to allow for direct comparison with the results of the AMT experiments. This study and the subsequent survey was reviewed by and approved by the Scripps Health Institutional Review Board and placed in the \u201cexempt\u201d risk category. \n\nAt the time of the study detailed in this paper, Mark2Cure was composed of (1) a training module, (2) a feedback interface, (3) a practice module, and (4) a central \u201cdashboard\u201d that organizes volunteer work into a series of \u201cquests\u201d. Mark2Cure has always been an open source project: , and is now being developed to explore additional biocuration tasks. \n\n#### Training \n  \nIn Mark2Cure, training comprised a series of four short, interactive tutorials. Training Step 0 introduced the basic web interface for highlighting concepts in text. Training Steps 1\u20133 introduced the annotation rules distilled from the NCBI disease corpus annotation instructions ( ), gradually increased the complexity of the text, and introduced a feedback mechanism to inform the user of their performance ( ). The tutorials were designed to provide enough guidance for the participant to perform the task well, with the constraint that overly lengthy tutorials would likely discourage participants. In total there were four tutorials. \n\n\n#### Feedback \n  \nAfter a user submitted their annotations, feedback was provided by pairing the user with a partner. For visual comparison, Mark2Cure showed the user\u2019s own markings as highlights and their partner\u2019s markings as underlines ( ). A score was also calculated and shown based on the F-score (see  ) multiplied by 1000. If the document was designated a gold standard document, then the user\u2019s partner was the gold standard annotations from the Disease Corpus mentioned in document selection, personified as a single \u201cexpert user\u201d. The gold standard annotations generated by ( ) were attributed to a single \u201cexpert user\u201d to facilitate learning by providing gold standard annotations in a recognizable manner consistent with our feedback mechanism. If the document was not a gold standard and no other user had previously annotated it, then the user was not shown any feedback and was give the full allotment of 1000 points. In all other cases, the user was randomly paired with a user who had previously annotated the document. \n\n\n#### Practice \n  \nOnce users completed the tutorials, they were required to work on the practice quest consisting of four abstracts in order to unlock the remaining quests. Users completing the practice documents were always paired with the \u201cexpert user\u201d for each of these documents. \n\n\n#### Quests \n  \nFor the purpose of organizing documents into manageable units of work, the full set of 588 documents was binned into 118 quests of up to five abstracts each. In addition to the per-document point scoring system (described previously), a quest completion bonus of 5000 points was awarded upon completion of all five abstracts. For the sake of usability, users who started a quest were allowed to finish it even if the quest was subsequently completed by the community. Hence it is possible for abstracts to be completed by more than 15 different users. \n\n\n\n### Data and analysis \n  \nMark2Cure was set up with Google Analytics for site traffic analysis. During the experiment period, emails were sent weekly to the participants via Mailchimp, and Mailchimp analytics provided open and click rate information. Mark-2Cure also logged information regarding user sign ups, training, and submissions. Precision, recall, F-scores, data quality, and cost metrics were calculated as previously described ( ). A survey was sent via email to the subscribers on the Mark2Cure mailing list at the end of the experiment (379 subscribers), and 78 of the subscribers (20.5%) responded to the survey. The metrics developed by Zooniverse were loosely applied to study the success of the project, though these metrics are better suited for project suites with multiple longer-running projects. The active period for this experiment started on January 19  and ended on February 16 , 2015. An export of the data generated in this experiment (which was used for the analysis) can be found at:  \n\n\n\n## Results \n  \n### Training and retention \n  \nTraining 0 introduced the highlight mechanism underlying Mark2Cure. Training 1 introduced rules for highlighting disease mentions in a sentence-by-sentence manner. Training 2 allowed users to practice the rules they had learned while acclimating the user to longer spans of texts. Training 3 introduced the feedback and scoring mechanisms. User tests prior to the launch of this experiment indicated that all training steps could be completed in under 20 minutes, but user drop off had not been determined at that stage. \n\nBased on Mark2Cure\u2019s log files, 331 unique users completed training 1; 254 unique users completed training 2; and 234 unique users completed training 3. User drop off was highest between Training 1 and Training 2, but 92% of users that completed Training 2 went on to complete Training 3. Of the 234 unique users that completed the training, more than 90% (212) contributed annotations for at least one document. To better understand the user drop off and retention throughout the different training pages, we obtained the unique page views and average page view time for each training page using Google Analytics ( ). Problem pages within the tutorials identified with Google Analytics were confirmed by emails received from users having trouble on those pages. \n\nBeyond the training modules, Mark2Cure participants were paired with other users and given points based on their performance to encourage continuous learning/ improvement. While this was effective when users were paired with the gold standard \u201cexpert user\u201d, many users expressed frustration when paired with poor performing partners. Issues with partner pairing highlighted the need to apply sorting mechanisms or allow for \u201cexpert trailblazers\u201d like those used in Eyewire. \n\nIn addition to responding to each email received, Mark2Cure published 204 tweets, nine blog posts, and sent eight newsletters during the 28 days that this experiment was running. We estimate having 466 email interactions with the users and 221 other communications during this time yielding values of 0.282 and 0.594 for our communications and interactions metrics respectively (Table 1B). Post-survey results indicated that user interface issues were a common (but generally surmountable) obstacle to participation. \n\n\n### Recruitment, analytics, and demographics \n  \nFive months prior to the launch of this experiment, we began to blog periodically about Mark2Cure and to engage with the rare disease community on Twitter. We focused on that community because many of its members are highly motivated to read scientific literature, engaged in research, active on social media channels, and experienced with outreach. By the time this experiment was launched, we had a mailing list of 100 interested potential users and 75 followers on Twitter; however, only about 40 users signed up after the first 10 days of the experiment\u2019s launch. \n\nSessions from new users peaked the day after the article on Mark2Cure was published in the San Diego Union Tribune ( ). A second, smaller surge in sessions from new users was observed the day after a blurb on Mark2Cure was published on California Healthline; however, the total number of sessions (from both new and returning users) rivaled the total number of sessions seen from the San Diego Union Tribune article. \n\nNew users from social media peaked around Feb. 13  and 14 , as existing users were actively marking documents, tweeting about their activities, encouraging other users to do the same, and luring new users to try Mark2Cure. Over the course of this experiment, one user posted more than 50 tweets and generated Mark2Cure specific hashtags to encourage others to join the effort. New users from social media peaked again on Feb. 16  with the release of Global Gene\u2019s RareCast podcast interview featuring Mark2Cure. As with many large rare disease communities and organizations, Global Genes has a strong social media presence. \n\nSurvey results at the end of the experiment indicated that 67% of respondents learned about Mark2Cure from the newspaper, 24% learned about Mark2Cure from social media channels, while 15% learned about Mark2Cure from a friend or a google search ( ). In addition to recruiting via traditional press and social media channels, recruitment was also driven by members of the NGLY1-deficiency community. Participants from this rare disease community made a concerted effort to ensure that this experiment would be completed successfully, with the hopes that Mark2Cure could be applied to accelerate research on NGLY1-deficiency. One participant from this community approached the Missouri Military Academy (MMA) and recruited both instructors and students to participate. Over 24 participants from MMA contributed about 10% of the task completions. \n\nThe majority (65%) of survey respondents cited the \u201cdesire to help science\u201d as their motivation for participating ( ). Though not necessarily representative, the results of our survey suggest that the participants in this experiment were demographically quite different from our AMT experiments. Women were more likely to participate (or report their participation) in our survey than men ( ). On average, our participants were older than the participants from the AMT experiments ( ), in part owing to the readership/recruitment from the San Diego Union Tribune article ( ). After the publication of that article, we received many inquiries about participating in the project from citizen scientists who volunteered information about their employment status and age (particularly from retirees). In pooling the contributions from just 14 of the 212 participants (6.6%) who volunteered this information (without ever being asked), we found that seniors and/or retirees contributed at least 26% of total document annotations in this experiment. In our demographic survey, 18 respondents (25%) reported being 66 years of age or greater ( ), and 25 respondents (35%) reported being retired ( ); hence, the actual contributions from this demographic group are likely to be higher. High school student participation was underreported (3%) in our survey ( ) and does not reflect the concerted effort of students from the Missouri Military Academy. \n\n\n### Distribution and performance \n  \nOver the 28-day experiment, 212 users submitted 10278 annotated abstracts with more than half of the annotated abstracts submitted in the final week of this experiment. As with all crowdsourcing systems, the distribution of task completions among users was skewed. There were 22 users who completed over 100 documents, 92 who completed over 10, and 98 who completed 10 or fewer ( ). Roughly 80% of the document completions were submitted by 24% of the contributors, more similarly following the pareto principle than the 90\u20139-1 rule as Mark2Cure was not set up for interaction/discussion between users at this point in time. \n\nOverall, the accuracy of contributions relative to the gold standard was quite high. The average F-score per user across all of their annotations was 0.761 with a standard deviation of 0.143 ( ), which was on par with that of our previous AMT results ( ). \n\nTo assess the aggregate accuracy across all users, we computed the \u201cminimum percent agreement\u201d for each annotation, defined as the number of users marking a given annotation divided by the total number of users to process the document (usually 15), and computed accuracy statistics at multiple thresholds ( ). At 0% minimum agreement (taking the union of all user annotations), we observed precision of 0.274 and recall of 0.976. At 100% minimum agreement (the intersection of all user annotations), we observed precision of 0.992 and recall of 0.122. The maximum F-score of 0.836 was reached at 40% minimum agreement. \n\nThese accuracy results were also very similar to our AMT experiments ( ), in which precision ranged from 0.444 to 0.983 and recall varied from 0.980 to 0.321 (at a 0% and 100% agreement threshold, respectively). The maximum F-score of 0.875 was also reached at a minimum agreement threshold of 40%. For comparison, these aggregate maximum F-scores were also on par with the individual expert annotators who performed the initial phase of annotations for the disease corpus ( ). \n\nThe difference in maximal F-score could be attributed to the differences in the way the rules were presented in AMT vs Mark2Cure, or due to the difference in incentive structures and goals. In AMT, poor performers can be blocked or may self-select out of a task so they do not affect their ability to qualify for future tasks (poor performance reports can affect an AMT worker\u2019s ability to qualify for jobs). In contrast Mark2Cure contributors are never blocked and are encouraged to continue contributing, as data quality is only one of several metrics by which success is determined. Overall, the performance by citizen scientists was on par with that of AMT workers on this task, even though the demographics of the citizen scientists were quite different from those of our AMT workers. \n\nTo assess how increasing the number of contributors affected the quality of the aggregate annotations, we simulated smaller numbers of annotators per document by randomly sampling from the full dataset. We found that the greatest increase in F-score was observed when the number of annotators was increased from two (F = 0.690) to three (F = 0.783) ( ). \n\nQualitative inspection of user disagreement in the annotations revealed issues with conjunction highlighting for certain users. For example, terms like \u201ccolorectal cancer\u201d would be highlighted individually as \u201ccolorectal\u201d and \u201ccancer\u201d. It is unclear whether this type of error reflected the user\u2019s understanding of the rules or if the user had issues with the highlighting function, as difficulties were reported in highlighting conjunctions spanning more than one line. Another major source of disagreement was the inclusion of modifying terms, e.g., the inclusion of \u201cpremenopausal\u201d in \u201cpremenopausal ovarian cancer\u201d. This type of disagreement was unsurprising given the inconsistent inclusion of these modifiers in the gold standard. For example, \u201cearly-onset\u201d is a modifying term that is inconsistently included with \u201cbreast cancer\u201d in the gold standard. Some gold standard documents will have included the entire term \u201cearly-onset breast cancer\u201d, while others will have only \u201cbreast cancer\u201d. This reflects an area in the annotation rules that could be improved by biocurators in the original data set. \n\n\n### Elements of successful citizen science matrix \n  \nData quality has been the focus of much academic research on citizen science, but it is not the only measure of success. Many citizen science projects have additional goals of engaging people in science and motivating them to incorporate scientific thought, hence the process of engaging citizen scientists can in itself also be a measure of success ( ). As one of the most established metrics, data quality is the most uniformly examined. Recent efforts have been made to define, expand, and apply additional metrics of citizen science success. The Zooniverse project created a set of useful metrics that could be applied across many citizen science projects. These metrics were normalized internally so that different projects by the Zooniverse team can be compared with one another. Although these metrics can be very useful for internally evaluating different projects, the normalization used in the Zooniverse paper means that that the reported results cannot be used for comparison purposes by researchers with only one citizen science project. Hence, we calculated and reported all the Zooniverse metrics as one data point to which other citizen science projects can be compared ( ). \n\nAs this is the first publication about Mark2Cure, the performance metrics based on citations and publications such as Publication Rate, Academic Impact, and Collaboration are not meaningful. For the Completeness of Analysis metric, Mark2Cure performed well; however, this reflected more on the flexibility of the project period\u2013remaining open until all the data were collected. Hence, this metric might not be as useful for projects with flexible timelines that open and close based on the data needed and the data collected. In spite of the flexible timeline, this phase of Mark2Cure was rather short and ended just as recruitment improved, which may explain the low Sustained Engagement result. Mark2Cure scored well in Interaction and Effective Training which actually reflects difficulties users had with Mark2Cure. Much of the interaction initiated by users resulted from problems with the tutorials or interface, and by addressing these issues quickly we were able to encourage many users to complete the tutorial and make a meaningful contribution. The Distribution of Effort was higher than Zooniverse\u2019s across project average of 0.18 ( ) but similar to the Andromeda Project; this higher Distribution of Effort score may be an artifact of the short project period. \n\n\n\n## Discussion \n  \nIn the 1980s, an information scientist named Don Swanson found that several abstracts about dietary fish oil contained mentions of blood viscosity, platelet function, and vascular reactivity. Swanson also found the same terms in abstracts from a disparate body of literature surrounding Raynaud\u2019s syndrome, allowing him to uncover the relationship between Raynaud\u2019s syndrome and fish oil\u2014an undiscovered relationship even though all the information to establish the link was already publicly available ( ). This hidden knowledge was uncovered by Swanson at a time when the biomedical literature was growing at an annual rate of about 10,000\u201315,000 articles. The rate of biomedical literature publication now exceeds 1 million articles per year and represents a body of knowledge that is increasingly difficult to harness. Information extraction is a necessary step toward harnessing the undiscovered but already available knowledge; however, it encapsulates some of the most time-consuming tasks in biocuration. \n\nAs the number of professional biocurators shrinks relative to the volume of literature to be curated, alternative strategies for keeping pace need to be explored ( ). The entrance of citizen science into this domain opens up many new opportunities. Most immediately, citizen scientists can help to generate new annotated corpora for training and evaluating computational methods for information extraction ( ). Following Galaxy Zoo\u2019s example ( ), we can set up computational systems that learn to perform the current tasks of the citizen scientists. Once these methods reach acceptable levels of performance, the citizens can be directed toward other areas still in need of human input. \n\nWe were fortunate that media attention allowed us to recruit sufficient participants to complete this phase of the project; however, recruitment and sustained engagement (retention) remain an important issue for Mark2Cure. Many citizen science projects (Zooniverse, Eyewire, Foldit) have demonstrated that recruitment improves as results are produced, and it makes no sense for our small citizen science community to tackle volumes of literature too large to complete or see  . To grow at a sustainable pace, Mark2Cure is currently focused on NER of three concept types in abstracts surrounding NGLY1 deficiency\u2014the rare disease that was of interest to the greatest number of our participants during the early phase of Mark2Cure. By focusing on literature in a specific disease domain (especially one of interest to previous participants), we reward organized participation from patient/community groups to encourage recruitment and narrow the range of literature to a volume that is manageable by the citizen science community. Although we currently apply Mark2Cure to create an annotated, NGLY1-deficiency-specific corpora, we are also developing Mark2Cure toward more challenging areas of information extraction such as relationship extraction. We expect extracting relationship information in NGLY1-related literature to provide more insightful information and to be of greater utility to the NGLY1 researchers, ultimately leading to new discoveries in this field. \n\nApart from helping to develop computational methods for information extraction, citizen science participants can help at much higher levels than machines are likely to reach. A motivated community of citizen scientists can accomplish nearly any goal, including the development of their own computational methods for solving complex tasks ( ). Within the domain of biocuration, volunteers could help with challenges such as prioritizing the most interesting documents for curation, developing controlled vocabularies, annotating images in documents, creating summaries, or soliciting funding for research. The scientific possibilities are limitless. To achieve them will require the development of strong synergistic cooperative relationships. In addition to large numbers of participants, experts in information extraction who can help dramatically increase the efficiency of processes for integrating that knowledge need to cooperate with professional biomedical scientists who can turn the scattered information in the public domain into new knowledge. One caveat common to many domains of research where citizen science has been applied is the reluctance of researchers to collaborate due to \u201cdata quality concerns\u201d. We accounted for researcher cooperation when selecting the scope of our project\u2019s current efforts, and hope to demonstrate with current and future iterations of Mark2Cure how large communities of citizens whose health and well-being will ultimately benefit from their work can become fundamentally important contributors to the production of a \u201cproblem solving ecosystem\u201d ( ). \n\n\n## Conclusion \n  \nAs a citizen science project, Mark2Cure would be classified as a science-oriented virtual project ( ) subject to the issue of ensuring valid scientific results while designing for online participation/interest. We addressed data quality issues as other citizen science opportunities have, by using replication across multiple participants, having participants evaluate established control items, and using a corpus of text that already had been expertly reviewed as a benchmark ( ). As   pointed out, data quality issues in citizen science are often project design issues; hence data quality often can be improved by analyzing participant interaction and adjusting the design. By using the gold-standard NCBI disease corpus (as we did in the AMT experiments) and formulating our tutorials around the annotation rules set forth in the development of that corpus, we demonstrate that citizen scientists are willing to perform NER tasks and (in aggregate) can perform comparably with expert curators. Furthermore, we demonstrate how researchers might utilize site traffic information and logged data to improve aspects of the design to achieve quality data, and we analyze our project with accordance to Cox et al.\u2019s \u201cElements of citizen science success matrix\u201d providing a comparison point for other citizen science efforts. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 6226017, "text_md5": "c31524f5c2b17b8e46c2c6ed0fc7b8ce", "field_positions": {"authors": [0, 98], "journal": [99, 108], "publication_year": [110, 114], "title": [125, 177], "keywords": [191, 302], "abstract": [315, 1973], "body": [1982, 31045]}, "batch": 1, "pmid": 30416754, "doi": "10.5334/cstp.56", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6226017", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6226017"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6226017\">6226017</a>", "list_title": "PMC6226017  Citizen Science for Mining the Biomedical Literature"}
{"text": "\u017ditnik, Slavko and \u017ditnik, Marinka and Zupan, Bla\u017e and Bajec, Marko\nBMC Bioinformatics, 2015\n\n# Title\n\nSieve-based relation extraction of gene regulatory networks from biological literature\n\n# Keywords\n\nrelation extraction\nconditional random fields\ngene regulatory networks\nliterature mining\n\n\n# Abstract\n \n## Background \n  \nRelation extraction is an essential procedure in literature mining. It focuses on extracting semantic relations between parts of text, called mentions. Biomedical literature includes an enormous amount of textual descriptions of biological entities, their interactions and results of related experiments. To extract them in an explicit, computer readable format, these relations were at first extracted manually from databases. Manual curation was later replaced with automatic or semi-automatic tools with natural language processing capabilities. The current challenge is the development of information extraction procedures that can directly infer more complex relational structures, such as gene regulatory networks. \n\n\n## Results \n  \nWe develop a computational approach for extraction of gene regulatory networks from textual data. Our method is designed as a sieve-based system and uses linear-chain conditional random fields and rules for relation extraction. With this method we successfully extracted the sporulation gene regulation network in the bacterium   Bacillus subtilis   for the information extraction challenge at the BioNLP 2013 conference. To enable extraction of distant relations using first-order models, we transform the data into skip-mention sequences. We infer multiple models, each of which is able to extract different relationship types. Following the shared task, we conducted additional analysis using different system settings that resulted in reducing the reconstruction error of bacterial sporulation network from 0.73 to 0.68, measured as the slot error rate between the predicted and the reference network. We observe that all relation extraction sieves contribute to the predictive performance of the proposed approach. Also, features constructed by considering mention words and their prefixes and suffixes are the most important features for higher accuracy of extraction. Analysis of distances between different mention types in the text shows that our choice of transforming data into skip-mention sequences is appropriate for detecting relations between distant mentions. \n\n\n## Conclusions \n  \nLinear-chain conditional random fields, along with appropriate data transformations, can be efficiently used to extract relations. The sieve-based architecture simplifies the system as new sieves can be easily added or removed and each sieve can utilize the results of previous ones. Furthermore, sieves with conditional random fields can be trained on arbitrary text data and hence are applicable to broad range of relation extraction tasks and data domains. \n\n \n\n# Body\n \n## Background \n  \nWe are witnessing an unprecedented increase in the number of biomedical abstracts, experimental results and phenotype and gene descriptions being deposited to publicly available databases, such as NCBI's PubMed. Collectively, this content represents potential new discoveries that could be inferred with appropriately designed natural language processing approaches. Identification of topics that appear in biomedical research literature was among first computational approaches to predict associations between diseases and genes and has become indispensable to both researchers in the biomedical field and curators [ - ]. Information from publication repositories is often mined together with other data sources. Databases that store relations from integrative mining are for example the OMIM database on human genes and genetic phenotypes [ ], the GeneRIF function annotation database [ ], the Gene Ontology [ ] and clinical drug information from the DailyMed database [ ]. Biomedical mining of literature is a compelling way to identify possible candidate genes through integration of existing data. \n\nA dedicated set of computational techniques is required to infer structured relations from plain textual information stored in large literature databases [ ]. Relation extraction tools [ ] can identify semantic relations between entities found in text. Early relationship extraction systems relied mostly on manually defined rules to extract a limited number of relationship types [ ]. Later, machine learning-based methods were introduced to address the extraction task by inferring prediction models from sets of labeled relationship types [ - ]. When no labeled data were available, unsupervised systems were developed to extract relationship descriptors based on the language syntax [ ]. Current state-of-the-art systems combine both machine learning and rule-based approaches to extract relevant information from narrative summaries and represent it in a structured form [ , ]. \n\nThis paper aims at the extraction of gene regulatory networks of   Bacillus subtilis  . The reconstruction and elucidation of gene regulation networks is an important task that can change our understanding of the processes and molecular interactions within the cell [ - ]. We have developed a novel sieve-based computational methodology that builds upon conditional random fields [ ] and specialized rules to extract gene relations from unstructured text. Extracted relations are assembled into a multi-relational gene network that is informative of the type of regulation between pairs of genes and the directionality of their action. The proposed approach can consider biological literature on gene interactions from multiple data sources. The main novelty of our work here is the construction of a sequential analysis pipeline for extracting gene relations of various types from literature data (Figure  ). We demonstrate the effectiveness and applicability of our recently proposed coreference resolution system [ ]. Our system uses linear-chain conditional random fields in an innovative way and can detect distant coreferent mentions in text using a novel transformation of data into skip-mention sequences. \n  \n Architecture of the proposed sieve-based relation extraction system  . The system consists of nine sieves. The first two prepare data for processing, then six sieves try to recognize events and relations, and the last sieve cleans the extracted relations. Every input document is processed sequentially by each of the sieves and at the end a list of extracted relations is returned as a result. \n  \nWe evaluate the proposed methodology by measuring the quality of extracted gene interactions that form the well studied regulatory network of sporulation in bacteria   B. subtilis  . Sporulation is an adaptive response of bacteria to scarce nutritional resources and involves differential development of two cells [ , ]. Many regulatory genes that control sporulation or direct structural and morphological changes that accompany this phenomenon have been characterized in the last decade [ , ]. The topology of bacterial sporulation network is stable and suffers no controversy; thus, it is appropriate to serve as a reference network against which the performance of relation extraction algorithms can be compared. Our evaluation demonstrates that the proposed approach substantially surpasses the accuracy of current state-of-the-art methods that were submitted to the Gene Regulation Network (GRN) BioNLP-ST 2013 Challenge ( ). The source code of our approach is freely available [ ]. In this paper we represent a network extraction algorithm, which is an improvement on our winning submission to BioNLP 2013 [ ]. With these improvements we have been able to further reduce the prediction error from 0.73 to 0.68, measured as the slot error rate (SER). This paper substantially extends our previous work [ ]. Below, we discuss motivation for using skip-mention sequences by analyzing distributions of distances between various parts of text (i.e., mentions) that are used by specialized sieves. We further explain feature functions and rules as they are key components of the system. We analyze the number of relations extracted by each sieve. The approach described here adds a new conditional random fields (CRFs) sieve to detect direct relations between   B. subtilis   genes that are \"hidden\" as target mentions within events. To better address text from biomedicine, we use the BioLemmatizer [ ] instead of a general lemmatizer. We incorporate an additional knowledge resource -   B. subtilis   protein-protein interaction network from the STRING database [ ], which is used within the new feature function   BSubtilisPPI  . \n\nWe use the term sieve to represent a separate relationship processing component. As we may extract new relationships or delete them in each of the sieve, the term might not be well selected but we left the terminology to comply with the previously published conference paper [ ] and the coreference resolution system [ ] that inspired the architecture of our proposed system. \n\n### Related work \n  \nResearch in the field of relationship extraction focuses on extraction of binary relationships between two arguments. New systems are typically tested using social relationships in the Automatic Content Extraction (ACE) evaluation datasets [ , ], where the goal is to select pairs of arguments and assign them a relationship type. Machine learning approaches that have been used for relationship extraction include sequence classifiers, such as hidden Markov models [ ], conditional random fields [ ], maximum-entropy Markov models [ ] and binary classifiers. The latter usually employs support vector machines (SVM) [ ]. \n\nThe ACE 2004 dataset [ ] consists of two-level hierarchical relationship types. A relationship could have another relationship as an argument and a second level relationship can have only non-relationship-like arguments. Two-level relationship hierarchies could have a maximum tree height of two. Wang   et al  . [ ] proposed a system that uses a one-against-one SVM classifier to classify relationships in the ACE 2004 dataset by employing WordNet [ ]-based semantic features. The GRN BioNLP 2013 Shared Task aimed to detect three-level hierarchical relationships. These relationships are interactions that connect events or other types of interactions as arguments. In comparison to the pairwise technique [ ], we extract relationships using linear-based sequence models and manually defined rules. \n\nA relation could be written using forms in unstructured text. Machine learning techniques try to learn diverse relations by adapting models against large datasets and by exploiting informative text features. The features are instantiated by a predefined set of feature functions, which are applied on a specific dataset. A technique to overcome a low number of instances of diverse relationship forms was proposed by [ ]. They proposed lexical-syntactic feature functions based on patterns that are able to identify dependency heads. The proposed solution was evaluated against two relationship types and two languages, where they achieved promising results. In this work we define manually assigned rules to overcome the heterogeneity of the relationship representation. \n\nText used for training a relationship extraction model is most often tagged using the IOB (inside-outside-beginning) notation [ ]. In the IOB, the first occurrence of the relationship word is labeled as   B-REL  , second and later consecutive tokens, which also represent relationships are labeled as   I-REL  , and all other tokens are   O  . Part of the text that most closely identifies a known relationship between the two arguments is referred to as a relationship descriptor. Li   et al.  [ ] used a linear-chain CRF model to label such descriptors. They first changed the subject and object arguments of the accompanying relationships into a specific value (e.g., ARG-1, ARG-2). This transformation enabled them to correctly identify direction of a relationship. Moreover, they also merged all the tokens from a relationship descriptor into a single token, which enabled them to use long distance features using a linear model representation. We employ an analogous model representation, but transform a sequence of tokens in an innovative way that enables us to extract the target relationship type between the arguments and not just a relationship descriptor. Banko and Etzioni [ ] also employed linear-based classifiers for the   open   relationship extraction problem, that is, the identification of a general relationship descriptor without regard to any target relationship type. First, they analyzed specific relationship types in the text taking into account lexical and syntactic features and then they learned a CRF model against with synonym identification [ ]. Their approach is useful in scenarios where only a very limited number of relationships are known. Traditional relationship extraction methods can perform better if our goal is a high value of recall. For this reason we focus on supervised relationship extraction model. \n\nRelationship extraction methods in biomedicine have been evaluated at several shared task challenges. The LLL - Learning Language in Logic challenge on gene interaction extraction [ ] is related to the BioNLP 2013 Gene Regulatory Networks Shared Task, which includes a subset of the LLL data with some additional annotations. For the LLL task, Giuliano   et al  . [ ] used a SVM classifier and proposed a specialized local and global SVM kernel that uses neighboring words as contextual information. The local kernel was based solely on mention features, such as words, lemmas or part-of-speech (POS) tags. In contrast, the global kernel used tokens on the left side of, between and on the right side of pairs of mentions that represent candidate arguments. To identify relationships, Giuliano   et al  . processed documents that contained at least two candidate attributes and generated   example instances, where   n   was the number of all mentions in a document and   k   was the number of mentions that constituted a relationship (i.e., two). Giuliano   et al  . used their model to predict either a non-existing relationship, a subject-object relationship or an object-subject relationship. On a related note, we propose the usage of contextual features and syntactic features that depend on neighboring words. However, we predict unoriented extracted relationships and then determine their directionality, i.e., the subject and object arguments, through manually defined rules. \n\n\n### Survey of BioNLP shared tasks \n  \nThe BioNLP Shared Task challenges follow an established research-wide trend in biomedical data mining towards the specific information extraction tasks. Challenge events have been organized thus far in 2009 [ ], 2011 [ ] and 2013 [ - ], each co-located with the BioNLP workshop at the Association for Computational Linguistics (ACL) Conference. The first event triggered active research in the biomedical community on various information extraction tasks. Second shared task focused on generalizing text types and domains, and on supporting different event types. The most recent shared task took a step further and addressed the information extraction problems in semantic web, pathways, cancer-related molecular mechanisms, gene regulation networks and ontology populations. \n\nThe BioNLP 2011 Entity Relations challenge focused on the entity relationship extraction. The best performing system, called TEES [ ], used a pipeline with SVMs for the detection of entity nodes and relation prediction that was followed by post-processing routines. It predicted relationships between every two candidate mentions within a sentence. The evalution showed that the term identification step could strongly impact on the performance of the relationship extraction module. In our case, proteins and mentions of entities, these are mentions that represent genes, were identified prior to the beginning of the challenge, and thus, our work here focused on the extraction of events, relations and event modification mentions. \n\nIn this work we describe the method that we developed while participating in the BioNLP 2013 Gene Regulation Network Shared Task [ ]. We report on several refinements of our approach that were introduced after the shared task ended and that allowed us to further improve its predictive performance. The goal of the GRN task was to extract gene interactions from research abstracts and to assemble a gene network, which was informative of gene regulation. Training data contained manually labeled texts obtained from research articles that contained entity mentions, events and interactions between genes. Entities were text sequences that identified entities, such as genes, proteins or regulons. Events and relationships were defined by their type, two connected arguments (i.e., entities) and the direction between the arguments. Given a test dataset, our goal was to predict relations describing various types of gene interactions. Predicted network of extracted gene interactions was matched with the reference gene regulatory network and scored using a Slot Error Rate (SER) [ ]. The SER measures the proportion of incorrect predictions relative to the number of reference relations. \n\n\n\n## Methods \n  \nIn this section we present our proposed sieve-based system for relation extraction. We start by describing the linear-chain conditional random field (CRF) model and proceed by extending it with a novel data representation that relies on skip-mentions. We provide support for transforming data into skip-mention sequences by studying various mention distributions that are used by CRF-based sieves. We then overview feature functions used by our model and explain the sieve-based system architecture, which is an end-to-end procedure that consists of data preprocessing, linear-chain CRF execution, rule-based relationship identification and data cleaning. \n\n### Conditional random fields with skip-mentions \n  \nCRF [ ] is a discriminative model, which estimates distribution of the objective sequence   y   conditioned on the input sequence   x  , that is,   p  (  y   |   x  ). Following is an example of the input sequence from the GRN BioNLP 2013 training dataset, where the potential attributes (i.e., mentions) are shown in bold: \n\n\"  spo0H   RNA and   sigma H levels   during growth are not identical to each other or to the pattern of   expression   of   spoVG  , a gene   transcribed   by   E sigma H  .\" \n\nThe corresponding objective sequence for this example is   y   - [  O, O, EVENT, O, EVENT, O, TranscriptionBy  ], which also corresponds to tokens in   x   - [  spo0H, sigma H, levels, expression, spoVG, transcribed, E sigma H  ]. Thus, both sequences are of the same length. \n\nWe retrieve additional information for input sequence   x   and generate sequences   x  ,   x  ,   x  that contain lemmas, parse trees, tokens and part-of-speech tags for each corresponding token in   x  . The CRF considers feature functions   f  , where   j   denotes   j  -th feature function,   j   = 1, 2  , . . . , m   (Figure  ). Feature functions employ text sequences to model target sequence   y  . The design of appropriate feature functions is the most important step in training CRF models. They contribute substantially to the improved performance of the system. We implement feature functions as templates and generate the final feature set by evaluating feature functions on a training dataset. The feature functions used by our model are described in the following section. \n  \n A feature function example  . The feature function indicates whether the current label is   Gene  , the previous is   Other   and the previous word is \"  transcribes  \", which returns 1 or otherwise it returns 0. \n  \nTraining of a CRF model involves estimating the most probable objective sequence  \u02c6 given the input   x  . In particular, we estimate \n\n\n\nwhere   w   is a vector of model parameters, weights, that have to be learned. Here, the conditional distribution   p  (  y   |   x  ,   w  ) is written as \n\n\n\nwhere   n   represents the length of input sequence   x  ,   m   the number of feature functions and   C  (  x  ,   w  ) is a normalization constant over all possible objective sequences   y  . Here,   f  (  y  ,   x   , i  ) denotes a   j  -th feature that is fired for   i  -th place in the input sequence. In our computations we avoid the need of computing normalization constant   C  . Instead of using the exact probabilities we rather rely on ranking of the sequences relative to their probabilities and return a sequence that is ranked first. use features that are fired at least five times on the training data (a parameter to our system). \n\nThe structure of a linear-chain model depends on the references to the target sequence labels that are used by the input feature functions. Figure   shows the graphical representation of the linear-chain CRF model. From the figure we can observe that the   i  -th factor can depend only on the current   y    label and the previous label   y    in a sequence. The training of linear CRFs is fast and efficient. This is in contrast to more complex CRF models, whose model inference is in general intractable and requires approximate probabilistic methods. \n  \n The linear-chain conditional random fields model representation  . The model is represented with an input sequence   x   (e.g., words) and target sequence   y   (i.e., relationship names) containing   n   tokens. \n  \n#### Model definition \n  \nWe formulate the task of relationship extraction as identification of relationships between two arguments. Linear-chain CRF model with standard data representation lacks the modeling of dependencies between mentions on longer distances (i.e., arguments that have at least one other token in-between). By analyzing the example from the previous section, \"gene   transcribed   by   E sigma H  \", we conclude that untransformed data representation can only identify relationships between two consecutive tokens. Thus, we cannot extract all possible relationships using a linear model. Rather than extracting relationship descriptors (i.e., parts of text that identify a relationship), we would like to extract categorized relationships between pairs of mentions. To overcome the limitation of linear models, we introduce new sequences that contain only mentions. We refer to these sequences as mention sequences. Mentions are a type of arguments that can form a relationship. In Figure   we present a conversion of the text excerpt into a mention sequence. Transformed sequence   x   consists of consecutive entity mentions. Notice that entity mentions are included in the training dataset. \n  \n Zero skip-mention sequence  . The initial mention sequence that contains all the mentions (i.e., zero skip-mention) from the document \"spo0H RNA and sigma H levels during growth are not identical to each other or to the pattern of expression of spoVG, a gene transcribed by E sigma H.\" A sentence from the GRN BioNLP 2013 training dataset, article PMID-1898930-S9. \n  \nWe label target sequence   y   with the name of a relationship (e.g.,   Interaction.Transcription, EVENT  ) or with the none symbol (i.e.,   O  ) when no relationship is present. Each relationship label represents a relationship between the current and the previous mention. \n\nFrom the mention sequence generated in Figure  , we cannot identify relationships between mentions that are not consecutive. This limitation becomes exacerbated when mentions that are arguments of a certain relationship appear on longer distances. For example, mentions   spoVG   and   E sigma H   should be related via the   Interaction.Transcription   relationship. However, this relationship cannot be extracted from representation that considers only consecutive mention pairs. Furthermore, a linear model can only detect relationships between directly consecutive mentions. To overcome this problem, we introduce a novel sequence representation called   skip-mention   sequences. The number of skip-mentions defines the number of mentions from the original text that exist between two consecutive mentions in a given skip-mention sequence. Thus, the original mention sequence (Figure  ) is a zero skip-mention sequence, because there are zero other mentions between any two consecutive mentions. This is opposed to a one skip-mention sequence, which considers relationships that are one mention apart. For example, to prepare the input data for extracting relationships between every second mention, we create two one skip-mention sequences for each input document. In the example in Figure   we extract relationship   Interaction.Transcription   based on one skip-mention sequence. \n  \n One skip-mention sequence  . One out of two possible one skip-mention sequences, generated from the initial zero skip-mention sequence [  spOH, sigma H, levels, expression, spoVG, transcribed, E sigma H  ]. The other one consists of tokens   sigma H, expression   and   transcribed  . \n  \nIn a general setting we consider skip-mention sequences for mentions at distance   s  . For a given skip-mention number,   s  , we create   s   + 1 mention sequences of length  . After the sequences are created, one independent linear-chain CRF model is trained for each value skip-mention number. As the generated sequences are independent, we can infer prediction models in parallel. From the models we read the extracted relationships between the mentions and form an undirected graph, where each connected component represents a relationship. Figure   shows a high level representation of data flow and relation extraction used in our approach. The time complexity of the proposed method is mainly determined by the time needed for training linear CRF models, since other routines can be run in linear time. Due to the parallel execution of the for loop (0, 1, 2  , . . . , s  ), we need to find the longest lasting execution. Let us suppose that CRF training and inference has time complexity of   O  (  EL  ) [ ], where   E   is the number of edges in the graph,   L   is the number of labels, and   Q   is the size of the maximal clique. In our type of CRF model, we use one label for each relationship type. The number of edges   E   depends on the sequence input to the algorithm. Let further assume there are   n   mentions in a document, which results in a zero skip-mention sequence with 2  n \u2212   1 =   O  (  n  ) edges. Moreover, every other generated   s   skip-mention sequence contains   edges. We conclude that by employing parallelization, CRF models would use   O  (  nL  ) =   O  (  n  ) of time (number of labels   L   is small and fixed). In addition to other linear time procedures, it is also important to consider the time for initialization of feature functions, which takes on the order of   O  (  nm  ), where   m   is the number of input feature functions. Figure   shows the distribution of distances between the relationship mention arguments (i.e., agents and targets) from the BioNLP 2013 Gene Regulatory Network training dataset. The labeled arguments represent entity mentions or events, depending on the sieve setting. Event is a type of relation that contains only mentions as their attributes. Events are extracted using the event extraction sieve. The distribution of distances between mentions is shown in the part A of Figure  . In the sieve (iv) we identify relationships that have only mentions as their attributes (B). In the training data there are 153 relations that have another relation or an event as their attribute. Of these, there are 11 such relations that have another relation as their attribute. Seven contain a regular relation as an attribute, while four represent negated relations, which are not scored. Relations that contain events as attributes are extracted by the event relations processing sieve (v) and the distribution of distances between the attributes is shown in part C of the figure. To use the same approach as for the other sieves, we transform events into mentions (see the sieve (v) for details). Since hierarchies of events or relations are not considered in model evaluation, we include the gene relations processing sieve (vi). Sieve (vi) extracts relations only between mentions, that are identified as   B. subtilis   genes. The distribution of distances between such mentions is presented in part D in the figure. We notice a drop of number of relationships on distance one for parts A, B and C. This is due to the fact of all the mentions we take into account when forming mention sequences. Differently, in part D, we take only gene mentions into account which also results in not having a drop at distance one. \n  \n Data flow in CRF-based relation extraction sieves  . First, the initial skip-mention sequence is transformed into the selected skip-mention sequences. Then, for each of the skip-mention sequence type, a different CRF model is trained and then used to label the appropriate skip-mention sequences. After labeling, the relations are instantiated from the tagged sequences and returned as a result. \n    \n Distributions of distances between relation attributes on BioNLP GRN train dataset  . (A) Mention distance distribution for events. (B) Mention distance distribution for relations. (C) Mention and event distance distributions for relations. Events are transformed into mentions. (D) A distribution of distances for relations in which subject and object mentions refer only to   B. subtilis   genes. \n  \nFrom all of the distance distributions we observe that relationships are mostly connected by the attributes on distance of two entity mentions. These distributions demonstrate the need to transform our data into skip-mention sequences. Without the transformation the linear-chain CRF model would, at best, uncover relations with attributes at zero distance (i.e., directly consecutive mentions). \n\nFor our final results we train the linear CRF models against skip-mention sequences from zero to ten skip-mentions. We decide to use this range after observing the distance distributions between attributes of the relations. By using up to ten skip-mentions we can retrieve most of relations and do not overfit the model. The findings in our previous work [ ] show that after reaching the tail of distance distributions the results do not further improve. \n\nThe feature functions that we consider are thoroughly explained in Table   and Table  . The tables contain short descriptions of the functions and parameters that are used for their instantiation. Additionally, the feature function generators generate a number of different functions from the training data and for them we also include the label types from which they are generated. \n  \nFeature functions description. \n  \nThe feature functions are used by all CRF-based sieves for all selected skip-mention CRF models. All extracted features are modeled both as unigram and bigram features. Unigram features are used for current label factor and bigram features are used for transition factor between two labels. \n    \nFeature function generators description. \n  \nAccording to the implementation, different options and observable values, the generators generate specific feature functions using a single scan over training data. The feature functions are used by all CRF-based sieves for all selected skip-mention CRF models. All extracted features are modeled both as unigram and bigram features (except prefix and suffix, which are of unigram type only). Unigram features are used for current label factor and bigram features are used for transition factor between two labels. \n  \n\n\n### Data processing components \n  \nWe introduce a pipeline-like data processing system that combines multiple data processing sieves (see Figure  ). Each is a separate data processing component. The whole system consists of nine sieves. The first two deal with data preprocessing and data preparation for efficient relationship extraction. The main ones then consist of linear CRF-based and rule-based relationship detection. The last one cleans the data before returning it as a result. The whole implementation of this proposed pipeline is available in a public source code repository [ ]. CRFSuite [ ] is used for fast CRF training and inference. \n\nThe proposed system can be easily adapted to another domain or other relation extraction task. In order to use it for other purposes, we would need to adapt the preprocessing part to enable the import of the new data. Also, the rule-based processing sieve would need to be discarded or populated with specific rules according to a new problem. All other sieves that extract relations could be the same because they use trained models and those would be specific to a domain and task. We also employed the use of skip-mention sequences to the task of coreference resolution and achieved comparable results to existing approaches [ ]. The pipeline starts by transforming the input text into the internal data representation, which could be used for further processing and enriches the data with additional labels, such as part-of-speech tags, parse trees and lemmas. After that we detect also action mentions, which are attributes within events. Next, we employ linear CRF models for event detection. We represent events as a special relationship type. Then the main relationships processing sieves detect relationships. We propose several processing sieves for each of the relationship type based on the argument types or hierarchy support. After each relationship extraction step we also use rules to set the agent and target attributes in the right direction. The last relationship processing sieve performs rule-based relationship extraction and therefore detects relationships of higher precision and boosts recall levels. In the last step the extracted data is cleaned and exported. \n\nThe sieves of our system are run in the same order as shown in Figure  . We provide detailed description of the processing sieves in the following sections, where we refer to the relationship attributes as subjects and objects, as shown in Figure  . Notice that sieves can depend on each other if they use data extracted by sieves executed earlier in the system pipeline (i.e., sieve (iii) and (v)). The initial set of the mentions is produced by the mention extraction sieve. This set is then used throughout the system and represent relation attributes used by extracted relations. \n  \n General relation representation  . Each relation (e.g., gerE inhibits cotD) is defined with a name (e.g.,   Interaction.Regulation  ) and subject (e.g.,   gerE  ) and object (e.g.,   cotD  ) attributes. \n  \n#### Preprocessing sieve \n  \nPreprocessing phase includes data importation, detection of sentences and tokenization of input text. Additionally, we tag the data with new labels, which are lemmas [ ], parse trees [ ] and part-of-speech tags. \n\n\n#### Mention extraction sieve \n  \nThe entity mention can belong to any of the following types:   Protein, GeneFamily, ProteinFamily, ProteinComplex, PolymeraseComplex, Gene, Operon, mRNA, Site, Regulon   and   Promoter  . Entity mentions are provided with the corpus, however, action mentions (e.g.,   expresses, transcribes  ) are not included in the corpus. We automatically detect action mentions. They are needed to represent relationship arguments within events during the event extraction. To identify action mentions we gather action mention lemmas from the training dataset and select new candidate mentions from the test dataset by exact matching of the lemmas. \n\n\n#### Event extraction sieve (iii) \n  \nAn event can be defined as a change in the state of biological entities, such as genes or complexes (e.g., \"the pattern of   expression   of   spoVG  \"). We encode events as a special relationship with a type name \"  EVENT  \". In the dataset, the event subject types can be of   Protein, GeneFamily, PolymeraseComplex, Gene, Operon, mRNA, Site, Regulon   and   Promoter   types, while the objects are always of the action mention type (e.g., \"  expression  \"), which are discovered in the mention extraction sieve. After the event type relationships are identified, we employ manual rules that change the order of arguments - they set an action mention as the object and a gene as the subject attribute for all extracted events. \n\n\n#### Relation processing sieves (iv, v, vi, vii) \n  \nDue to the existence of different relationships (i.e., different subject and object types), we extract relationships in four phases (iv, v, vi, vii). This also enables us to extract hierarchical relationships (i.e., relationships that contain another relationship as its subject or object) in order to achieve higher precision. All the sieves in this step use the novel linear CRF-based relationship extraction method. Each processing sieve uses specific relationship properties and is executed in the following order (the shown examples are sample extractions from the above demonstrative document): \n\n(iv) First, we extract relationships with only mentions as arguments (e.g.,   transcribed \u2192 TranscriptionBy \u2192 E sigma H  ). Mentions can be either of the real or action type. By real mentions we refer to the entities that represent genes, proteins and aggregates, while action mentions could represent only arguments within events (e.g., transcription). \n\n(v) In this step, we extract relationships that consist of at least one event in their arguments (e.g.,   expression spoVG \u2192 Interaction.Transcription \u2192 E sigma H  ). Before the extraction we map events into mentions, which enables us to use the same approach as in previous step. These mentions consist of two tokens (i.e., event arguments). We treat the newly created   event mentions   the same as others and also include them in the list of other mentions. Their order within the list is determined by the lowest mention token from the event. We train the models using the same techniques as in every other CRF-based processing sieve. The new action mentions are treated as other mentions and from them we extract features using the same set of feature functions. Lastly, the final relationships are instantiated following the same procedure as in the previous step. \n\n(vi) The goal of the shared task is to extract   Interaction   relations between   B. subtilis   genes. Thus, we select only mentions that represent   B. subtilis   genes and train the algorithm to predict the appropriate   Interaction   relations (e.g.,   spoVG \u2192 Interaction.Transcription \u2192 E sigma H   if there was no transcription event). For the mention selection step we exploit a public database of the   B. Subtilis   genes from the NCBI available at  . \n\n(vii) We propose this new processing sieve in addition to the previous sieves, which we previously introduced in the BioNLP challenge submission [ ]. The goal of the challenge is to extract interactions between genes. When there exists a relationship between a gene   G  1 and and event   E  , the final result in a GRN networks looks exactly the same if our system extracts a relationship between a gene   G  1 and a gene   G  2, where   G  2 is the object attribute of the event   E  . By taking into account the latter, we train the models to extract relationships only between   B. subtilis   genes (e.g.,   spoVG \u2192 Interaction.Transcription \u2192 E sigma H  , where   spoVG   is the subject attribute within an event). \n\nThe challenge datasets include seven hierarchical relationship instances, which have another relationship as one of its arguments. Due to the small number of instances and newly introduced relationship extraction sieve between genes (vi, vii), we did not extract this type of relationship hierarchies. \n\nAdditionally, there exist four negated relation instances. The BioNLP task considers only positive relations and there is no performance gain if negated relations are extracted. Thus, we focus on extracting positive relations. Depending on the dataset and performance evaluation measure, we can add a separate sieve that can extract negated relations by applying manually defined rules that search for negation words such as   nor, neither, whereas   and   not  . \n\n\n#### Rule-based processing sieve \n  \nThe last phase of relationship extraction involves application of the rules to achieve higher precision. The rules operate directly on the input text with recognized mentions and use different data representation than extractors based on CRFs. We implemented the following four approaches: \n\n Mention triplets:   This method searches for the consequent triplets of mentions, where the middle mention is an action mention. As input to the rule we set the matching regular expression that searches for text that action mention must starts with, and a target relation. For example, from text \"The   rocG   gene of Bacillus subtilis, encoding a catabolic glutamate dehydrogenase, is   transcribed   by   SigL   . . . \", we extract a relation   rocG \u2192 Interaction.Transcription \u2192 SigL  . The mention triplet in this example is   rocG, transcribed   and   SigL  , where the middle mention is an action mention matching the regular expression. \n\n Consecutive mentions:   The method processes every two consequent   B. subtilis   entity mentions and checks whether the text in-between the mentions matches a specified regular expression used for extracting a target relation. By default, it forms relations that are extracted from active sentences, otherwise it supposes the passive type and changes the order of attribute types within the matched relation. For example, from text \"  GerE   binds to a site on one of these promoters,   cotX  , that. . . \", we extract relation   GerE \u2192 Interaction.Requirement \u2192 cotX  . Notice that mentions   GerE   and   cotX   represent the   B. subtilis   entities and text between the entities matches a regular expression \".*binds to.*\". \n\n List of consecutive mentions:   This method extends the technique designed for consecutive mentions by allowing potentially many entity mentions on both sides of matched regular expression. The list of mentions must be separated by one of the delimiters \",\", \", and\" or \"and\". For example, this rule extracts two relationships from the sentence \"the   cotG   promoter is induced under the control of the   sigma K   and the DNA-binding protein   GerE  .\" \n\n Sentences of consecutive mentions:   This method is similar to the rule for consecutive mentions. It first removes subsentences that exist between two mentions and then it extracts relationships. Subsentences are defined as parts of text between two commas. For example, the method extracts a relationship   GerR \u2192 Interaction.Requirement \u2192 SpoIIID   from the sentence \"The   sigma(E)   factor turns on 262 genes, including those for   GerR  , and   SpoIIID  .\". \n\nThe   Interaction   relationships are extracted using keywords and regular expressions that depend on the type of interaction. Biomedical literature uses many different language forms to express the same type of a genetic relationship. For example, some researchers prefer   to repress   to   to inactivate   or   to inhibit  . We use synonyms of this kind to extract additional relationships that are not identified by linear CRF models. The parameters used for rule-based extraction are shown in Table  . \n  \nRule-based processing sieve input parameters. \n  \nEach of the four different rule-based extraction methods takes a target relation name and a regular expression as input. Some of them also require to specify whether the extraction should be made from active or passive sentences. \n\nThe method is called with passive parameter set to true. \n  \n\n#### Data cleaning sieve \n  \nThe data cleaning sieve removes loops of relationships and eliminates redundancies. We call relationship a loop if and only if both relationship arguments refer to the same entity (i.e., mentions are coreferent). For example, the sentence \"...   sp0H   RNA and   sigma H   ...\" refers to the mentions   sp0H   and   sigma H  . Since both mentions refer to the same entity (i.e.,   sigH  ), they cannot form a relationship. Removal of the loops improves performance of the system as it contributes to the reduction of undesired insertions in the final prediction. Another step in data cleaning phase is removal of redundant relationships. Disregarding redundant relationships has no affect on predictive performance of our system but it improves the readability of the output. \n\n\n\n\n## Experimental setup \n  \n### BioNLP GRN 2013 challenge dataset \n  \nThe GRN dataset consists of sentences from PubMed abstracts, which are mostly related to the topic of sporulation in   B. subtilis   and from which an appropriate gene regulation network can be reconstructed. It contains annotated text-bound entities that we call mentions. These mentions include biochemical events and relationships that were result of already conducted research work on cellular mechanisms at the molecular level. The goal of BioNLP Shared Task was to identify interactions, which represent relations between biological entities, events or relations and are essential for construction of GRN. The interaction relations form a hierarchy of mechanism and effect relation types. We were required to predict the following fine-grained interaction relation classes:   regulation, inhibition, activation, requirement, binding   and   transcription  . \n\nIn Table   we report on the features of the train, development and test datasets that were used in our study. The test dataset does not include labeled data and thus we cannot perform the evaluation of each sieve against it. In the other two datasets the sentences are manually labeled with relationships, events and entity mentions. \n  \nBioNLP 2013 GRN Shared Task development, training and test dataset properties. \n  \nThe numbers of the Interaction relations that our system reads from the datasets is different than the real ones due to the import technique into our internal data representation. The dev dataset contains 67 and training dataset contains 131 reference Interaction relations. The test data contains 88 such relation instances (The number was retrieved from the output of the official BioNLP GRN Shared Task test evaluation service). \n  \n\n### Evaluation criterion \n  \nThe official evaluation criterion of the BioNLP challenge considers edge resemblance between the predicted and the reference gene regulatory network describing sporulation in   B. subtilis  . The performance of a relation extraction system is evaluated using the SER measure [ ] \n\n\n\nwhich is the ratio between the sum of relationship substitutions (S), insertions (I) and deletions (D), divided by the number of edges in the reference network (N). In short, systems that output as many wrong predictions as correct predictions achieve a SER value of 1. Notice that a system, which reports zero extracted relations, produces as many deletions as there are relations in a dataset (i.e.,   N   =   D  ). When a system extracts a true relation, the number of deletions decreases by one. If it detects a false relation then either the number of substitutions or the number of insertions increases by one. More accurate systems have a lower SER. A perfect system would correctly identify all relations and would achieve a SER of 0. Our goal is to maximize the number of matched relations and minimize the number of substitutions, deletions and insertions. \n\n\n\n## Results and discussion \n  \nWe represent the GRN relationship extraction challenge as a two-level task. First, we need to identify relationships among given labeled mentions and secondly, we need to correctly identify the argument types of extracted relationships (i.e., the direction of a relationship). For the challenge evaluation procedure, only results that match by relationship type and also by both argument types are counted as correct. \n\nOur approach consists of multiple submodules, i.e., sieves, whereas each is developed for extracting a specific relationship type (e.g., are both arguments mentions, are arguments an event and a mention, or are both of them gene mentions). For the CRF-based relation extraction sieves we use skip-mention distances from zero to ten. Thus, we first show the overall results and then discuss the contributions of each sieve and subsets of feature functions. \n\n### Predictive performance \n  \nWe evaluated the proposed solution against the GRN BioNLP 2013 Shared Task dataset using leave one out cross validation on the development data, where we achieved a SER score of 0.74, with no substitutions, 36 deletions, 14 insertions and 31 matches. According to the results reported on the development dataset at the BioNLP workshop [ ], this is improvement for one point in SER due to the additional sieve and new feature functions. \n\nThe challenge test dataset consists of 290 mentions from 67 sentences. We trained the models jointly on the development and train datasets to detect relationships against the test data. The challenge submission results of other participants in the shared task are listed in Table  . According to the official SER measure, our system (U. of Ljubljana) was ranked first. The other participants or participating systems were K. U. Leuven [ ], TEES-2.1 [ ], IRISA-TexMex [ ] and \n  \nBioNLP 2013 GRN Shared Task results on the test dataset. \n  \nThe table shows the number of substitutions (S), deletions (D), insertions (I), matches (M) and slot error rate (SER) metric. Best results per metric are highlighted in bold. Reported are results announced after the BioNLP 2013 GRN challenge was closed. \n  \nEVEX [ ]. All the participants were trying to achieve a low number of substitutions, deletions and insertions, while trying to increase the number of matched relationships. We obtained the lowest number of substitutions and good results in the other three counters, which resulted in the best SER score. In general also other participants generated a high number of deletions, which is a clear result that the relationships are encoded in many and ambiguous forms in the text. The IRISA-TexMex achieved the lowest number of deletions and the maximum number of matches but received a low final result due to a high number of insertions and substitutions. \n\nSince the submission of our entry to the BioNLP challenge, we have introduced some new feature functions and implemented an additional sieve. The new sieve (vii) extracts relations between   B. subtilis   genes from hierarchically encoded relations in the training dataset. We report the improved results in Table  . They all include new feature functions and are grouped by the inclusion of the new event-based gene processing (vii) sieve and data cleaning sieves. The result without both of them already outperforms our submitted result by one point, with a SER score of 0.72. The new feature functions extract more relations with increased precision. It is interesting that the inclusion of the sieve (vii) deteriorates the final result by about 4 SER points. However, the inclusion uncovers more matches, but it inserts a substantial number of non-correct relations, which results in a higher error rate. Thus, the best SER score of 0.68 was achieved without the sieve (vii) and with data cleaning. Compared to our winning result at the BioNLP Shared Task, this may further improve the system by 5 SER points. \n  \nResults on test data. \n  \nThe table shows results on the test set using new feature functions and the additional sieve (vii) with or without data cleaning. The abbreviations represent the number of substitutions (S), deletions (D), insertions (I) and matches (M). Best results per metric are highlighted in bold. \n  \nIn Figure   we show the gene regulation network, which is the visual representation of the results of our system against the test dataset. Compared to our shared task submission [ ], the improved system identifies two additional relations (i.e.,   spoIVFB \u2192 Inhibition \u2192 spoIVFA, sigE \u2192 Transcription \u2192 gerR  ) and deletes one (i.e.,   sigB \u2192 Transcription \u2192 yvyD  ). If the deleted relation is correct, we could merge the results and achieve a SER of 0.67 with 4 substitutions, 50 deletions, 5 insertions and 34 matches, given 88 relations in the test set. To the best of our knowledge, this result represents the most accurate prediction on BioNLP GRN dataset so far. We were able to retrieve 39% of interactions from the data, which suggests that automatic extraction of gene regulatory networks is still a challenging task with open opportunity for future research. \n  \n Predicted gene regulation network on test data  . The predicted gene regulation network, generated from extracted relations on the test dataset by our improved sieve-based system. For our winning extractions at the BioNLP 2013 GRN Shared Task see the workshop paper [ ]. \n  \n\n### Analysis of extractions per sieve \n  \nTable   shows the number of extracted relations by each sieve. The same relation can be extracted by multiple sieves. Thus, we apply data cleaning as the last sieve to remove loop and duplicate relations. \n  \nRelations extracted by each sieve on development and test datasets. \n  \nData cleaning results represent the number of loop relations and the number of redundant relations (separated by forward slash). Slot error rate (SER) results are cumulative. \n\nDue to additional analysis we saw that the event-based gene processing sieve does not improve the final results, therefore we do not employ this sieve on the test data for the final result. \n  \nThe event extraction sieve uncovers events, which we represent as relations. Events are not part of performance evaluation and thus their extraction does not directly affect the SER score. Extracted events are given as input to the event processing sieve, which extracts relations having an event as a relation attribute. The first two relation processing sieves (Figure  ) already achieve promising performance on the development dataset, while on the test set they extract seven correct and seven incorrect relations, that is, the SER score remains 1. The next two sieves extract more correct relations on the test set and achieve very good results on the development dataset. The event-based gene processing sieve shows substantial improvements on the development dataset, while there is a minor result change on the test set. The lowest SER score is achieved when not using this sieve for the test set (but the CRF models are trained on both training and development data). In this setting there are no further improvements when using rules on the development data. Notice that the rule-based sieve contributed importantly on the development data before we introduced the event-based gene processing sieve into the system. We observed that many relations previously extracted by rules are now detected by the event-based gene processing sieve. Contrary to development data, rules uncover substantially more relations on the test dataset than event-based sieves. \n\n\n### Assessment of subsets of feature functions \n  \nThe selection of the most informative feature functions is one of the key tasks in machine learning for improving the quality of results. In Table   we show the results on the development data when using different subsets of feature functions. Feature functions were grouped into subsets, ranging from more general (A-C) to more specific (D-H). As expected, the results improve when more feature functions are used. If only basic features (A) are applied, the system detects one wrong relation, which results in a SER higher than 1. Still, when using   B. subtilis  -related feature functions (C), the results show no improvement (Table  ). We notice a reduction of 0.12 in error rate when prefix and suffix feature functions (D) were added. Thus, we suspect that the improvement results from combining these functions with other feature functions (D) or it is due to D being generator feature functions that generate larger number of features than the previous (A-C) ones. Also, the next generator of mention values and mention pairs (E) substantially improves the result. This is expected, especially if the same type of relations exist in the development dataset and in the training dataset. We confirmed that D and E perform poorly if used separately, achieving a SER of 0.98 and 0.87, respectively. If D and E are used together, the system achieves a SER of 0.81. Thus, the inclusion of diverse feature functions is important. It may seem that the feature function subset H does not contribute to the results. This does not hold and can be seen if subset G is excluded. The latter configuration gives a SER of 0.74. \n  \nRelations extracted by different subsets of feature functions on a development dataset. \n  \nThe table shows the number of substitutions (S), deletions (D), insertions (I), matches (M) and slot error rate (SER) metric. The results are measured on the development dataset using CRF-based sieves only. Best results per metric are highlighted in bold. The feature function subsets are selected as follows: (A) target label distribution, starts upper, starts upper twice, Hearst co-occurence, mention token distance, (B) parse tree mention depth, parse tree parent value, parse tree path, (C) BSubtilis, IsBSubtilis, IsBSubtilisPair, (D) prefix value, suffix value, (E) consequent value, current value, (F) context value, (G) previous/next value combination, left/right/between value and (H) split to values. For their detailed descriptions see Table 1 and Table 2. \n  \n\n\n## Conclusions \n  \nWe presented a sieve-based system for relationship extraction from textual data. The system uses linear-chain conditional random fields (CRFs) and manually defined extraction rules. To enable extraction of relationships between distant mentions we introduced   skip-mention   linear CRF, which extends the applicability of a linear CRF model. We form skip-mentions by constructing many sequences of mentions, which differ in the number of mentions we skip. \n\nWith a SER score of 0.73 our approach scored best among the GRN BioNLP-ST 2013 submissions, outperforming the second-best system by a large margin. We described here a number of improvements of our approach and demonstrated their utility that may be used to further improve the result (to 0.67 SER score). The CRF-based sieves in our approach are independent processing components and can be trained against an arbitrary data domain for which labeled data exists. We anticipate the utility of our approach in related data domains and for tasks with corpora. \n\n\n## Competing interests \n  \nThe authors declare that they have no competing interests. \n\n\n## Authors' contributions \n  \nS.Z., M.Z., B.Z. and M.B. designed the experiments. S.Z. and M.Z. performed the experiments. S.Z., M.Z., B.Z. and M.B. wrote the main manuscript text. All authors read and approved the final manuscript. \n\n \n", "metadata": {"pmcid": 4642041, "text_md5": "648f1fea8833038ea7fc7c7360dd836f", "field_positions": {"authors": [0, 67], "journal": [68, 86], "publication_year": [88, 92], "title": [103, 189], "keywords": [203, 292], "abstract": [305, 2926], "body": [2935, 59152]}, "batch": 1, "pmid": 26551454, "doi": "10.1186/1471-2105-16-S16-S1", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4642041", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4642041"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4642041\">4642041</a>", "list_title": "PMC4642041  Sieve-based relation extraction of gene regulatory networks from biological literature"}
{"text": "Sahoh, Bukhoree and Choksuriwong, Anant\nJ Syst Sci Syst Eng, 2020\n\n# Title\n\nAutomatic Semantic Description Extraction from Social Big Data for Emergency Management\n\n# Keywords\n\nOntology\nnatural language processing\ninformation extraction\nsemantic index\nnamed entity recognition\ntriplestore\n\n\n# Abstract\n \nEmergency events are unexpected and dangerous situations which the authorities must manage and respond to as quickly as possible. The main objectives of emergency management are to provide human safety and security, and Social Big Data (SBD) offers an important information source, created directly from eyewitness reports, to assist with these issues. However, the manual extraction of hidden meaning from SBD is both time-consuming and labor-intensive, which are major drawbacks for a process that needs accurate information to be produced in real-time. The solution is an automatic approach to knowledge discovery, and we propose a semantic description technique based on the use of triple store indexing for named entity recognition and relation extraction. Our technique can discover hidden SBD information more effectively than traditional approaches, and can be used for intelligent emergency management. \n \n\n# Body\n\n", "metadata": {"pmcid": 7282698, "text_md5": "1039f07ec5e56c1e548c9c373acda3cf", "field_positions": {"authors": [0, 39], "journal": [40, 59], "publication_year": [61, 65], "title": [76, 163], "keywords": [177, 289], "abstract": [302, 1218], "body": [1227, 1227]}, "batch": 1, "pmid": 32837111, "doi": "10.1007/s11518-019-5453-5", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7282698", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7282698"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7282698\">7282698</a>", "list_title": "PMC7282698  Automatic Semantic Description Extraction from Social Big Data for Emergency Management"}
{"text": "Beck, Tim and Shorter, Tom and Hu, Yan and Li, Zhuoyu and Sun, Shujian and Popovici, Casiana M. and McQuibban, Nicholas A. R. and Makraduli, Filip and Yeung, Cheng S. and Rowlands, Thomas and Posma, Joram M.\nFront Digit Health, 2022\n\n# Title\n\nAuto-CORPus: A Natural Language Processing Tool for Standardizing and Reusing Biomedical Literature\n\n# Keywords\n\nnatural language processing\ntext mining\nbiomedical literature\nsemantics\nhealth data\n\n\n# Abstract\n \nTo analyse large corpora using machine learning and other Natural Language Processing (NLP) algorithms, the corpora need to be standardized. The BioC format is a community-driven simple data structure for sharing text and annotations, however there is limited access to biomedical literature in BioC format and a lack of bioinformatics tools to convert online publication HTML formats to BioC. We present Auto-CORPus (Automated pipeline for Consistent Outputs from Research Publications), a novel NLP tool for the standardization and conversion of publication HTML and table image files to three convenient machine-interpretable outputs to support biomedical text analytics. Firstly, Auto-CORPus can be configured to convert HTML from various publication sources to BioC. To standardize the description of heterogenous publication sections, the Information Artifact Ontology is used to annotate each section within the BioC output. Secondly, Auto-CORPus transforms publication tables to a JSON format to store, exchange and annotate table data between text analytics systems. The BioC specification does not include a data structure for representing publication table data, so we present a JSON format for sharing table content and metadata. Inline tables within full-text HTML files and linked tables within separate HTML files are processed and converted to machine-interpretable table JSON format. Finally, Auto-CORPus extracts abbreviations declared within publication text and provides an abbreviations JSON output that relates an abbreviation with the full definition. This abbreviation collection supports text mining tasks such as named entity recognition by including abbreviations unique to individual publications that are not contained within standard bio-ontologies and dictionaries. The Auto-CORPus package is freely available with detailed instructions from GitHub at:  . \n \n\n# Body\n \n## Introduction \n  \nNatural language processing (NLP) is a branch of artificial intelligence that uses computers to process, understand, and use human language. NLP is applied in many different fields including language modeling, speech recognition, text mining, and translation systems. In the biomedical realm NLP has been applied to extract, for example, medication data from electronic health records and patient clinical history from free-text (unstructured) clinical notes, to significantly speed up processes that would otherwise be extracted manually by experts ( ,  ). Biomedical research publications, although semi-structured, pose similar challenges with regards to extracting and integrating relevant information ( ). The full-text of biomedical literature is predominately made available online in the accessible and reusable HTML format, however, some publications are only available as PDF documents which are more difficult to reuse. Efforts to resolve the problem of publication text accessibility across science in general includes work by the Semantic Scholar search engine to convert PDF documents to HTML formats ( ). Whichever process is used to obtain a suitable HTML file, before the text can be processed using NLP, heterogeneously structured HTML requires standardization and optimization. BioC is a simple JSON (and XML) format for sharing and reusing text data that has been developed by the text mining community to improve system interoperability ( ). The BioC data model consists of collections of documents divided into data elements such as publication sections and associated entity and relation annotations. PubMed Central (PMC) makes full-text articles from its Open Access and Author Manuscript collections available in BioC format ( ). To our knowledge there are no services available to convert PMC publications that are not part of these collections to BioC. Additionally, there is a gap in available software to convert publishers' publication HTML to BioC, creating a bottleneck in many biomedical literature text mining workflows caused by having to process documents in heterogenous formats. To bridge this gap, we have developed an Automated pipeline for Consistent Outputs from Research Publications (Auto-CORPus) that can be configured to process any HTML publication structure and transform the corresponding publications to BioC format. \n\nDuring information extraction, the publication section context of an entity will assist with entity prioritization. For example, an entity identified in the Results Section may be regarded as a higher priority novel finding than one identified in the Introduction Section. However, the naming and the sequential order of sections within research articles differ between publications. A Methods section, for example, may be found at different locations relative to other sections and identified using a range of synonyms such as   experimental section, experimental procedures  , and   methodology  . The Information Artifact Ontology (IAO) was created to serve as a domain-neutral resource for the representation of types of information content entities such as documents, databases, and digital images ( ). Auto-CORPus applies IAO annotations to BioC file outputs to standardize the description of sections across all processed publications. \n\nVast amounts of biomedical data are contained in publication tables which can be large and multi-dimensional where information beyond a standard two-dimensional matrix is conveyed to a human reader. For example, a table may have subsections or entirely new column headers to merge multiple tables into a single structure. Milosevic and colleagues developed a methodology to analyse complex tables that are represented in XML format and perform a semantic analysis to classify the data types used within a table ( ). The outputs from the table analysis are stored in esoteric XML or database models. The communal BioC format on the other hand has limited support for tables, for example the PMC BioC JSON output includes table data in PMC XML format, introducing file parsing complexity. In addition to variations in how tables are structured, there is variability amongst table filetypes. Whereas, publication full-text is contained within a single HTML file, tables may be contained within that full-text file (inline tables), or individual tables may be contained in separate HTML files (linked tables). We have defined a dedicated table JSON format for representing table data from both formats of table. The contents of individual cells are unambiguously identified and thus can be used in entity and relation annotations. In developing the Auto-CORPus table JSON format, we adopted a similar goal to the BioC community, namely, a simple format to maximize interoperability and reuse of table documents and annotations. The table JSON reuses the BioC data model for entity and relation annotations, ensuring that table and full-text annotations can share the same BioC syntax. Auto-CORPus transforms both inline and linked HTML tables to the machine interpretable table JSON format. \n\nAbbreviations and acronyms are widely used in publication text to reduce space and avoid prolix. Abbreviations and their definitions are useful in text mining to identify lexical variations of words describing identical entities. However, the frequent use of novel abbreviations in texts presents a challenge for the curators of biomedical lexical ontologies to ensure they are continually updated. Several algorithms have been developed to extract abbreviations and their definitions from biomedical text ( \u2013 ). Abbreviations within publications can be defined when they are declared within the full-text, and in some publications, are included in a dedicated   abbreviations   section. Auto-CORPus adapts an abbreviation detecting methodology ( ) and couples it with IAO section detection to comprehensively extract abbreviations declared in the full-text and in the   abbreviations   section. For each publication, Auto-CORPus generates an abbreviations dictionary JSON file. \n\nThe aim of this article is to describe the open Auto-CORPus python package and the text mining use cases that make it a simple user-friendly application to create machine interpretable biomedical literature files, from a single publication to a large corpus. The authors share the common interest of progressing text mining capabilities across the biomedical literature domain and contribute omics and health data use cases related to their expertise in Genome-Wide Association Study (GWAS) and Metabolome-Wide Association Study (MWAS) data integration and analytics (see Author Contributions Section). The following sections describe the technical details about the algorithms developed and the benchmarking undertaken to assess the quality of the three Auto-CORPus outputs generated for each publication: BioC full-text, Auto-CORPus tables, and Auto-CORPus abbreviations JSON files. \n\n\n## Materials and Methods \n  \n### Data for Algorithm Development \n  \nWe used a set of 3,279 full-text HTML and 1,041 linked table files to develop and test the algorithms described in this section. Files for 1,200 Open Access (OA) GWAS publications whose data exists in the GWAS Central database ( ) were downloaded from PMC in March 2020. A further 1,241 OA PMC publications of MWAS and metabolomics studies on cancer, gastrointestinal diseases, metabolic syndrome, sepsis and neurodegenerative, psychiatric, and brain illnesses were also downloaded to ensure the methods are not biased toward one domain, more information is available in the  . This formed a collection of 2,441 publications that will be referred to as the \u201cOA dataset.\u201d We also downloaded publisher-specific full-text files, and linked table data were available, for publications whose data exists in the GWAS Central database. This collection of 838 full-text and 1,041 table HTML files will be referred to as the \u201cpublisher dataset.\u201d   lists the publishers and journals included in the publisher dataset and the number of publications that overlap with the OA dataset. This also includes publications from non-biomedical fields to evaluate the application in other domains. \n  \nPublishers and journals included in the publisher dataset. \n  \n The full-text files were downloaded in HTML format and the linked table files were downloaded when available in HTML formats. The full-text files that overlap with the OA dataset were used to assess the consistency of outputs generated from different sources  . \n  \n\n### Algorithms for Processing Publication Full-Text HTML \n  \nAn Auto-CORPus configuration file is set by the user to define the heading and paragraph HTML elements used in the publication files to be processed. Regular expressions can be used within the configuration file allowing a group of publications with a similar but not an identical structure to be defined by a single configuration file, for example when processing publications from journals by the same publisher. The heading elements are used to delineate the content of the publication sections and the BioC data structure is populated with publication text. All HTML tags including text formatting (e.g., emphasized words, superscript, and subscript) are removed from the publication text. Each section is automatically annotated using IAO (see Section Algorithms for Classifying Publication Sections With IAO Terms) and the BioC data structure is output in JSON format. The BioC specification requires \u201ckey files\u201d to accompany BioC data files to specify how the data files should be interpreted ( ). We provide key files to define the data elements in the Auto-CORPus JSON output files for full-text, tables, and abbreviations ( ).   gives an example of the BioC JSON output and the abbreviations and tables outputs are described below. \n  \nAn extract of the Auto-CORPus BioC JSON created from the PMC3606015 full-text HTML file. Each section is annotated with IAO terms. The \u201cautocorpus_fulltext.key\u201d file describes the contents of the full-text JSON file ( ). \n  \nAbbreviations in the full-text are found using an adaptation of a previously published methodology and implementation ( ). The method finds all brackets within a publication and if there are two or more non-digit characters within brackets it considers if the string in the brackets could be an abbreviation. It searches for the characters present in the brackets in the text on either side of the brackets one by one. The first character of one of these words must contain the first character within the bracket, and the other characters within that bracket must be contained by other words that follow the first word whose first character is the same as the first character in that bracket. An example of the Auto-CORPus abbreviations JSON is given in   which shows that the output from this algorithm is stored along with the abbreviations defined in the publication abbreviations section (if present). \n  \nAn extract from the Auto-CORPus abbreviations JSON created from the PMC4068805 full-text HTML file. For each abbreviation the corresponding long form definition is given along with the algorithm(s) used to detect the abbreviation. Most of the abbreviations shown were independently identified in both the full-text and in the abbreviations section of the publication. A variation in the definition of \u201cRP\u201d was detected: in the abbreviations section this was defined as \u201creverse phase,\u201d however in the full-text this was defined as \u201creversed phase.\u201d The \u201cautocorpus_abbreviations.key\u201d file describes the contents of the abbreviations JSON file ( ). \n  \n\n### Algorithms for Classifying Publication Sections With IAO Terms \n  \nA total of 21,849 section headers were extracted from the OA dataset and directed path graphs (DPGs) were created for each publication ( ). DPGs are a linear chain without any cycles. For example, at this point in this article the main headers are   abstract   (one paragraph) followed by   introduction   (five paragraphs) and   materials and methods   (four paragraphs, three sub-headers)\u2014this would make up a DPG with three nodes (  abstract, introduction, materials and methods  ) and two directed edges. For our   Introduction Section  , while the individual five paragraphs within a section would all be mapped to the main header (  introduction  ), only one node would appear in the DPG (relating to the header itself) without any self-edges. The individual DPGs were then combined into a directed graph (digraph,  ) and the extracted section headers were mapped to IAO (v2020-06-10)   document part   terms using the Lexical OWL Ontology Matcher (LOOM) method ( ). Fuzzy matching using the fuzzywuzzy python package (v0.17.0) was then used to map headers to the preferred section header terms and synonyms, with a similarity threshold of 0.8 (e.g., the typographical error \u201cexperemintal section\u201d in PMC4286171 is correctly mapped to   methods section  ). This threshold was evaluated by two independent researchers who confirmed all matches for the OA dataset were accurate. Digraphs consist of nodes (entities, headers) and edges (links between nodes) and the weight of the nodes and edges is proportional to the number of publications in which these are found. Here the digraph consists of 372 unique nodes and 806 directed edges ( ). \n  \nFlow diagram demonstrating the process of classifying publication sections with IAO terms. The unfiltered digraph is visualized in  , and the process of combining DPGs and mapping unmapped nodes using anchor points in  . DPG, directed path graph; G(V,E), graph(vertex, edge); IAO, information artifact ontology. \n  \nHowever, after direct IAO mapping and fuzzy matching, unmapped headers still existed. To map these headings, we developed a new method using both the digraph and the individual DPGs. The headers are not repeated within a document/DPG, they are sequential/a chain and have a set order that can be exploited. Unmapped headers are assigned a section based on the digraph and the headers in the publication (DPG) that could be mapped (anchor headers), an example is given in   where a header cannot be mapped to IAO terms. Any unmapped header that is mapped to an existing IAO term in this manner does not result in a self-edge in the network as subsequent repeated headers are collapsed into a single node. Auto-CORPus uses the LOOM, fuzzy matching and digraph prediction algorithms to annotate publication sections with IAO terms in the BioC full-text file. Paragraphs can be mapped to multiple IAO terms in case of publications without main-text headers (based on digraph prediction) or with ambiguous headers (based on fuzzy matching and/or digraph prediction). \n\n#### New IAO Terms and Synonyms \n  \nWe used the IAO classification algorithms to identify potential new IAO terms and synonyms. Three hundred and forty-eight headings from the OA dataset were mapped to IAO terms during the fuzzy matching or mapped based on the digraph using the publication structure and anchor headers. These headings were considered for inclusion in IAO as term synonyms. We manually evaluated each heading and   lists the 94 synonyms we identified for existing IAO terms. \n  \nNew synonyms identified for existing IAO terms from the fuzzy and digraph mappings of 2,441 publications. \n    \nDiagraph nodes that were not mapped to IAO terms but formed heavily weighted \u201cego-networks,\u201d indicating the same heading was found in many publications, were manually evaluated for inclusion in IAO as new terms. For example, based on the digraph, we assigned   data   and   data description   to be synonyms of the   materials section  . The same process was applied to ego-networks from other nodes linked to existing IAO terms to add additional synonyms to simplify the digraph.   shows the ego-network for   abstract  , and four main categories and one potential new synonym (  precis  , in red) were identified. From the further analysis of all ego-networks, four new potential terms were identified:   disclosure, graphical abstract, highlights  , and   participants  \u2014the latter is related to, but deemed distinct from, the existing   patients section   (IAO:0000635).   details the proposed definition and synonyms for these terms. The terms and synonyms described here will be submitted to the IAO, with our initial submission of one term and 59 synonyms accepted and included in IAO previously (v2020-12-09) ( ).   shows the resulting digraph with only existing and newly proposed section terms. A major unmapped node is   associated data  , which is a header specific for PMC articles that appears at the beginning of each article before the abstract. In addition, IAO has separate definitions for   materials   (IAO:0000633),   methods   (IAO:0000317), and   statistical methods   (IAO:0000644) sections, hence they are separate nodes in the graph. The   introduction   is often followed by these headers to reflect the   methods section   (and synonyms), however there is also a major directed edge from   introduction   directly to   results   to account for   materials and methods   placed after the   discussion   and/or   conclusion   sections in some publications. \n  \nUnmapped nodes in the digraph ( ) connected to \u201cabstract\u201d as ego node, excluding corpus specific nodes, grouped into different categories. Unlabeled nodes are titles of paragraphs in the main text. \n    \n (A)   Proposed new IAO terms to define publication sections that were derived from analyzing the sections of 2,441 publications.   (B)   Proposed new IAO terms to define parts of a table section. Elements in italics have previously been submitted by us for inclusion into IAO and added in the v2020-12-09 IAO release. \n    \nFinal digraph model used in Auto-CORPus to classify paragraphs after fuzzy matching to IAO terms (v2020-06-10). This model includes new (proposed) section terms and each section contains new synonyms identified in this analysis. \u201cAssociated Data\u201d is included as this is a PMC-specific header found before abstracts and can be used to indicate the start of most articles, all IAO terms are indicated in orange. \n  \n\n\n### Algorithms for Processing Tables \n  \n#### Auto-CORPus Table JSON Design \n  \nThe BioC format does not specify how table content should be structured, leaving this open to the interpretation of implementers. For example, the PMC BioC JSON output describes table content using PMC XML (see the \u201cpmc.key\u201d file at  ). Including markup language within JSON objects presents data parsing challenges and interoperability barriers with non-PMC table data representations. We developed a simple table JSON format that is agnostic to the publication table source, can store multi-dimensional table content from complex table structures, and applies BioC design principles ( ) to enable the annotation of entities and relations between entities. The table JSON stores table metadata of title, caption and footer. The table content is stored as \u201ccolumn headers\u201d and \u201cdata rows.\u201d The format supports the use of IAO to define the table metadata and content sections, however additional IAO terms are required to define table metadata document parts.   includes the proposed definition and synonyms for these terms. To compensate for currently absent IAO terms, we have defined three section type labels:   table title, table caption   and   table footer  . To support the text mining of tables, each column header and data row cell has an identifier that can be used to identify entities in annotations. Tables can be arranged into subsections, thus the table JSON represents this and includes subsection headings.   gives an example of table metadata and content stored in the Auto-CORPus table JSON format. In addition to the Auto-CORPus key files, we make a table JSON schema available for the validation of table JSON files and to facilitate the use of the format in text analytics software and pipelines. \n  \nExtracts of the Auto-CORPus table JSON file generated to store metadata and content for an example table.   (A)   The parts of a table stored in table JSON. The section titles are underlined. The table shown is the PMC version (PMC4245044) of Table 1 from ( ).   (B)   The title and caption table metadata stored in table JSON.   (C)   Each column heading in the table content is split between two rows, so the strings from both cells are concatenated with a pipe symbol in the table JSON. Headers that span multiple columns of sub-headers are replicated in each header cell as here with the pipe symbol.   (D)   The table content for the first row from the first section is shown in table JSON. Superscript characters are identified using HTML markup.   (E)   The footer table metadata stored in table JSON. The \u201cautocorpus_tables.key\u201d file describes the contents of the tables JSON file ( ). \n  \n\n#### Processing Table HTML \n  \nTables can used within HTML documents for formatting web page layouts and are distinct from the   data tables   processed by Auto-CORPus. The configuration file set by the user identifies the HTML elements used to define data table containers, which include title, caption, footer, and table content. The files processed can either be a full-text HTML file for inline tables and/or separate HTML files for individual linked tables. The Auto-CORPus algorithm for processing tables is based on the functional and structural table analysis method described by Milosevic et al. ( ). The cells that contain navigational information such as column headers and section headings are identified. If a column has header strings contained in cells spanning multiple rows, the strings are concatenated with a pipe character separator to form a single column header string. The \u201csuper row\u201d is a single text string that spans a complete row (multiple columns) within the table body. The \u201cindex column\u201d is a single text string in the first column (sometimes known as a stub) within the table body when either only the first column does not have a header, or the cell spans more than one row. The presence of a super row or index column indicates a table section division where the previous section (if present) ends, and a new section starts. The super row or index column text string provides the section name. A nested array data structure of table content is built to relate column headers to data rows, working from top to bottom and left to right, with section headings occurring in between and grouping data rows. The algorithm extracts the table metadata of title, footer and caption. Table content and metadata are output in the table JSON format. The contents of table cells can be either string or number data types (we consider \u201ctrue\u201d and \u201cfalse\u201d booleans as strings) and are represented in the output file using the respective JSON data type. Cells that contain only scientific notation are converted to exponential notation and stored as a JSON number data type. All HTML text formatting is removed, however this distorts the meaning of positive exponents in text strings, for example   n   =   10   is represented as   n   =   103  . To preserve the meaning of exponents within text strings, superscript characters are identified using superscript HTML element markup, for example   n = 10 <sup>3</sup>  . \n\nSome publication tables contain content that could be represented in two or more separate tables. These multi-dimensional tables use the same gridlines, but new column headers are declared after initial column headers and data rows have appeared in the table. New column headers are identified by looking down columns and classifying each cell as one of three types: numerical, textual, and a mix of numbers and text. The type for a column is determined by the dominant cell type of all rows in a column excluding super rows. After the type of all columns are determined, the algorithm loops through all rows except super rows, and if more than half of cells in the row do not match with the columns' types, the row is identified as a new header row, and the rows that follow the new headers are then regarded as a sub-table. Auto-CORPus represents sub-tables as distinct tables in the table JSON, with identical metadata to the initial table. Tables are identified by the table number used in the publication, so since sub-tables will share their table number with the initial table, a new identifier is created for sub-tables with the initial table number, an underscore, then a sub-table number such as \u201c1_1.\u201d \n\n\n\n### Comparative Analysis of Outputs \n  \nThe correspondence between PMC BioC and Auto-CORPus BioC outputs were compared to evaluate whether all information present in the PMC BioC output also appears in the Auto-CORPus BioC output. This was done by analyzing the number of characters in the PMC BioC JSON that appear in the same order in the Auto-CORPus BioC JSON using the longest common subsequence method. With this method, overlapping sequences of characters that vary in length are extracted from the PMC BioC string to find a matching sequence in the Auto-CORPus string. With this method it can occur that a subsequence from the PMC BioC matches to multiple parts of the Auto-CORPus BioC string (e.g., repeated words). This is mitigated by evaluating matches of overlapping/adjacent subsequences which should all be close to each other as they appear in the PMC BioC text. \n\nThis longest common subsequence method was applied to each individual paragraph of the PMC BioC input and compared with the Auto-CORPus BioC paragraphs. This method was chosen over other string metric algorithms, such as the Levenshtein distance or cosine-similarity, due to it being non-symmetric/unidirectional (the Auto-CORPus BioC output strings contain more information (e.g., figure/table links, references) than the PMC BioC output) and ability to directly extract different characters. \n\n\n\n## Results \n  \n### Data for the Evaluation of Algorithms \n  \nWe attempted to download PMC BioC JSON format for all 1,200 GWAS PMC publications in our OA dataset, but only 766 were available as BioC from the NCBI server. We refer to this as the \u201cPMC BioC dataset.\u201d For the 766 PMC articles where we could obtain a NCBI BioC file, we processed the equivalent PMC HTML files using Auto-CORPus. We used only the BioC output files and refer to this as the \u201cAuto-CORPus BioC dataset.\u201d To compare the Auto-CORPus BioC and table outputs for PMC and publisher-specific versions, we accessed 163 Nature Communication and 5 Nature Genetics articles that overlap with the OA dataset and were not present in the publisher dataset, so they were unseen data. These journals have linked tables, so full-text and all linked table HTML files were accessed (367 linked table files). Auto-CORPus configuration files were setup for the journals to process the publisher-specific files and the BioC and table JSON output files were collated into what we refer to as the \u201clinked table dataset.\u201d The equivalent PMC HTML files from the OA dataset were also processed by Auto-CORPus and the BioC and table JSON files form the \u201cinline table dataset.\u201d \n\n\n### Performance of Auto-CORPus Full-Text Processing \n  \nThe proportion of characters from 3,195 full-text paragraphs in the PMC BioC dataset that also appear in the Auto-CORPus BioC dataset in the same order in the paragraph string were evaluated using the longest common subsequence method. The median and interquartile range of the (left-skewed) similarity are 100% and 100\u2013100%, respectively. Differences between the Auto-CORPus and PMC outputs are shown in   and relate to how display items, abbreviations and links are stored, and different character encodings. A structural difference between the two outputs is in how section titles are associated to passage text. In PMC BioC the section titles (and subtitles) are distinct from the passages they describe as both are treated as equivalent text. The section title occurs once in the file and the passage(s) it refers to follows it. In Auto-CORPus BioC the (first level) section titles (and subtitles) are linked directly with the passage text they refer to, and are included for each paragraph. Auto-CORPus uses IAO to classify text sections so, for example, the introduction title and text are grouped into a section annotated as introduction, rather than splitting these into two subsections (introduction title and introduction text as separate entities in the PMC BioC output) which would not fit with the IAO structure. \n  \nDifferences between the Auto-CORPus BioC and PMC BioC JSON outputs. \n  \nThe Auto-CORPus BioC output includes the figure captions where they appear in the text and a separate table JSON file to store the table data, whereas the PMC BioC adds these data at the end of the JSON document and provides table content as a block of XML. Abbreviation sections are not included in the Auto-CORPus BioC output since Auto-CORPus provides a dedicated abbreviations JSON output. In the PMC BioC format the abbreviations and definitions are not related, whereas in the Auto-CORPus abbreviations JSON output the two elements are related. If an abbreviation does not contain a definition in the abbreviations section (perhaps due to an editorial error), PMC BioC will include the undefined thus meaningless abbreviation string, whereas Auto-CORPus will ignore it. Link anchor text to figures, tables, references and URLs are retained in the Auto-CORPus output but removed in the PMC BioC output. The most common differences between the two BioC versions is the encodings/strings used to reflect different whitespace characters and other special characters, with the remaining content being identical. \n\nThe proportion of characters from 9,468 full-text paragraphs in the publisher dataset that also appear in the Auto-CORPus PMC BioC dataset in the same order in the paragraph string were evaluated. The median and interquartile range of the (left-skewed) similarity is also 100 and 100\u2013100%, respectively, and differences between the PMC and publisher-versions are the same as those previously observed and reported in  . \n\nLast, we evaluated the section title mapping to IAO terms for publication from non-biomedical domains (physics, psychology). We observed that not all publications from these domains have standardized headers that can be mapped directly or with fuzzy matching and require the digraph to map headers. Most headers are mapped correctly either to one or multiple (potential) IAO terms ( ). Only one publication contained a mismatch where two sections were mapped to introduction and methods sections, respectively, where each of these contained sub-headers that relate to introduction, methods and results. In two physics publications we encountered the case where the \u201cproportional to\u201d sign (\u221d) could not be mapped by the encoder. \n\n#### Performance of Auto-CORPus Table Processing \n  \nWe assessed the accuracy of the table JSON output generated from non-PMC linked tables compared with table JSON output generated from the equivalent PMC HTML with inline tables. The comparative analysis method described above was used for comparing BioC output from the linked table and inline table datasets, except here it was applied to both strings (bidirectional, taking the maximum value of both outcomes). This is equivalent to the Levenshtein similarity applied to transform the larger string into the smaller string, with the exception that the different characters for both comparisons are retained for identifying the differences. The correspondence between table JSON files in the linked table and inline table datasets was calculated as the number of characters correctly represented in the publishers table JSON output relative to the PMC versions [also using the (symmetric) longest common subsequence method]. Both the text and table similarity are represented as the median (inter-quartile range) to account for non-normal distributions of the data. Any differences identified during these analyses were at the paragraph or table row level, enabling manual investigation of these sections in a side-by-side comparison of the files. \n\nThe proportion of characters from 367 tables in the linked table dataset that also appear in the inline table dataset in the same order in the cell or text string were evaluated. The median and interquartile range of the (left-skewed) similarity is 100 and 99.79\u2013100.00%, respectively. We found that there were structural differences between some of the output files where additional data rows were present in the JSON files generated from the publisher's files. This occurred because cell value strings in tables from the publisher's files were split across two rows, however in the PMC version the string was formatted (wrapped) to be contained within a single row. The use of different table structures to contain the same data resulted in accurate but differing table JSON outputs. Most of the differences between table content and metadata values pertain to the character encoding used in the different table versions. For example, we have found different uses of hyphen/em dash/en dash/minus symbols between different versions, and Greek letters were represented differently in the different table versions. Other differences are related to how numbers are represented in scientific notation. If a cell contains a number only, then it is represented as a JSON number data type in the output. However, if the cell contains non-numeric characters, then there is no standardization of the cell text and the notation used (e.g., the \u00d7 symbol or E notation) will be reproduced in the JSON output. When there is variation in notation between sources, the JSON outputs will differ. Other editorial differences include whether thousands are represented with or without commas and how whitespace characters are used. Despite these variations there was no information loss between processed inline and linked tables. \n\n\n\n### Application: NER on GWAS Publications \n  \nOur intention is that Auto-CORPus supports information extraction from the biomedical literature. To demonstrate the use of Auto-CORPus outputs within a real-world application and aligned to the authors' expertise to support the evaluation of the results, we applied named-entity recognition (NER) to the Auto-CORPus BioC full-text output to extract GWAS metadata. Study metadata are included in curated GWAS databases, such as GWAS Central, and the ability to extract these entities automatically could provide a valuable curation aid. Full details of the method and the rationale behind the application is provided in the  . In summary, we filtered out sentences in the methods sections from the BioC full-text output that contain information on the genotyping platforms, assays, total number of genetic variants, quality control and imputation that were used. We trained five separate algorithms for NER (one for each metadata type) using 700 GWAS publications and evaluated these on 500 GWAS publications of the test set. The F1-scores for the five tasks are between 0.82 and 1.00 ( ) with examples given in  . \n\n\n\n## Discussion \n  \n### Strengths and Limitations \n  \nWe have shown that Auto-CORPus brings together and bolsters several disjointed standards (BioC and IAO) and algorithmic components (for processing tables and abbreviations) of scientific literature analytics into a convenient and reliable tool for standardizing full-text and tables. The BioC format is a useful but not ubiquitous standard for representing text and annotations. Auto-CORPus enables the transformation of the widely available HTML format into BioC JSON following the setup of a configuration file associated with the structure of the HTML documents. The use of the configuration file drives the flexibility of the package, but also restricts use to users who are confident exploring HTML document structures. We make available the configuration files used in the evaluations described in this paper. To process additional sources, an upfront time investment is required from the user to explore the HTML structure and set the configuration file. We will be increasing the number of configuration files available for larger publishers, and we help non-technical users by providing documentation to explain how to setup configuration files. We welcome configuration files submitted by users and the documentation describes the process for users to submit files. Configuration files contain a section for tracking contributions made to the file, so the names of authors and editors can be logged. Once a configuration file has been submitted and tested, the file will be included within the Auto-CORPus package and the user credited (should they wish) with authorship of the file. \n\nThe inclusion of IAO terms within the Auto-CORPus BioC output standardizes the description of publication sections across all processed sources. The digraph that is used to assign unmapped paragraph headers to standard IAO terms was constructed using both GWAS and MWAS literature to avoid training it to be used for a single domain only. We have tested the algorithms on PMC articles from three different physics and three psychology journals to confirm the BioC JSON output and IAO term recognition extend beyond only biomedical literature. Virtually all header terms from these articles were mapped to relevant IAO terms even when not all headers could be mapped, however some sections were mapped to multiple IAO terms based on paths in the digraph. Since ontologies are stable but not static, any resource or service that relies on one ontology structure could become outdated or redundant as the ontology is updated. We will rerun the fuzzy matching of headers to IAO terms and regenerate the digraph as new terms are introduced to the   document part   branch of IAO. We have experience of this when our first group of term suggestions based on the digraph were included into the IAO. \n\nThe BioC output of abbreviations contains the abbreviation, definition and the algorithm(s) by which each pair was identified. One limitation of the current full-text abbreviation algorithm is that it searches for abbreviations in brackets and therefore will not find abbreviations for which the definition is in brackets, or abbreviations that are defined without use of brackets. The current structure of the abbreviation JSON allows additional methods to be included alongside the two methods currently used. Adding further algorithms to find different types of abbreviation in the full-text is considered as part of future work. \n\nAuto-CORPus implements a method for extracting table structures and data that was developed to extract table information from XML formatted tables ( ). The use of the configuration file for identifying table containers enables the table processing to be focused on relevant data tables and exclude other tables associated with web page formatting. Auto-CORPus is distinct from other work in this field that uses machine learning methods to classify the types of information within tables ( ). Auto-CORPus table processing is agnostic to the extracted variables, with the only distinction made between numbers and strings for the pragmatic reason of correctly formatting the JSON data type. The table JSON files could be used in downstream analysis (and annotation) of cell information types, but the intention of Auto-CORPus is to provide the capability to generate a faithful standardized output from any HTML source file. We have shown high accuracy (>99%) for the tables we have processed with a configuration file and the machine learning method was shown to recover data from ~86% of tables ( ). Accurate extraction is possible across more data sources with the Auto-CORPus rule-based approach, but a greater investment in setup time is required. \n\nAuto-CORPus focuses on HTML versions of articles as these are readily and widely available within the biomedical domain. Currently the processing of PDF documents is not supported, but the work by the Semantic Scholar group to convert PDF documents to HTML is encouraging as they observed that 87% of PDF documents processed showed little to no readability issues ( ). The ability to leverage reliable document transformation will have implications for processing supplementary information files and broader scientific literature sources which are sometimes only available in PDF format, and therefore will require conversion to the accessible and reusable HTML format. \n\n\n### Future Research and Conclusions \n  \nWe found that the tables for some publications are made available as images (see  ), so could not be processed by Auto-CORPus. To overcome this gap in publication table standardization, we are refining a plugin for Auto-CORPus that provides an algorithm for processing images of tables. The algorithm leverages Google's Tesseract optical character recognition engine to extract text from preprocessed table images. An overview of the table image processing pipeline is available in  . During our preliminary evaluation of the plugin, it achieved an accuracy of ~88% when processing a collection of 200 JPG and PNG table images taken from 23 different journals. Although encouraging, there are caveats in that the image formats must be of high resolution, the algorithm performs better on tables with gridlines than tables without gridlines, special characters are rarely interpreted correctly, and cell text formatting is lost. We are fine tuning the Tesseract model by training new datasets on biomedical data. An alpha release of the table image processing plugin is available with the Auto-CORPus package. \n\nThe authors are involved in omics health data NLP projects that use Auto-CORPus within text mining pipelines to standardize and optimize biomedical literature ahead of entity and relation annotations and have given examples in the   of how the Auto-CORPus output was used to train these algorithms. The BioC format supports the stand-off annotation of linguistic features such as tokens, part-of-speech tags and noun phrases, as well as the annotation of relations between these elements ( ). We are developing machine learning methods to automatically extract genome-wide association study (GWAS) data from peer-reviewed literature. High quality annotated datasets are required to develop and train NLP algorithms and validate the outputs. We are developing a GWAS corpus that can be used for this purpose using a semi-automated annotation method. The GWAS Central database is a comprehensive collection of summary-level GWAS findings imported from published research papers or submitted by study authors ( ). For GWAS Central studies, we used Auto-CORPus to standardize the full-text publication text and tables. In an automatic annotation step, for each publication, all GWAS Central association data was retrieved. Association data consists of three related entities: a phenotype/disease description, genetic marker, and an association   P  -value. A named entity recognition algorithm identifies the database entities in the Auto-CORPus BioC and table JSON files. The database entities and relations are mapped back onto the text, by expressing the annotations in BioC format and appending these to the relevant BioC element in the JSON files. The automatic annotations are then manually evaluated using the TeamTat text annotation tool which provides a user-friendly interface for annotating entities and relations ( ). We use TeamTat to manually inspect the automatic annotations and modify or remove incorrect annotations, in addition to including new annotations that were not automatically generated. TeamTat accepts BioC input files and outputs in BioC format, thus the Auto-CORPus files that have been automatically annotated are suitable for importing into TeamTat. Work to create the GWAS corpus is ongoing, but the convenient semi-automatic process for creating high-quality annotations from biomedical literature HTML files described here could be adapted for creating other gold-standard corpora. \n\nIn related work, we are developing a corpus for MWAS for metabolite named-entity recognition to enable the development of new NLP tools to speed up literature review. As part of this, the active development focuses on extending Auto-CORPus to analyse preprint literature and  , improving the abbreviation detection, and development of more configuration files. Our preliminary work on preprint literature has shown we can map paragraphs in Rxiv versions to paragraphs in the peer-reviewed manuscript with the high accuracy (average similarity of paragraphs >95%). Another planned extension is to classify paragraphs based on the text in the case where headers are mapped to multiple IAO terms. The flexibility of the Auto-CORPus configuration file enables researchers to use Auto-CORPus to process publications and data from a broad variety of sources to create reusable corpora for many use cases in biomedical literature and other scientific fields. \n\n\n\n## Data Availability Statement \n  \nPublicly available datasets were analyzed in this study. The Auto-CORPus package is freely available from GitHub ( ) and can be deployed on local machines as well as using high-performance computing to process publications in batch. A step-by-step guide to detail how to use Auto-CORPus is supplied with the package. Data from both Open Access (  via   PubMed Central) and publisher repositories are used, the latter were downloaded within university library licenses and cannot be shared. \n\n\n## Author Contributions \n  \nTB and JP designed and supervised the research and wrote the manuscript. TB contributed the GWAS use case and JP contributed the MWAS/metabolomics use cases. TS developed the BioC outputs and led the coding integration aspects. YH developed the section header standardization algorithm and implemented the abbreviation recognition algorithm. ZL developed the table image recognition and processing algorithm. SS developed the table extraction algorithm and main configuration file. CP developed configuration files for preprint texts. NM developed the NER algorithms for GWAS entity recognition. NM, FM, CY, ZL, and CP tested the package and performed comparative analysis of outputs. TR refined standardization of full-texts and contributed algorithms for character set conversions. All authors read, edited, and approved the manuscript. \n\n\n## Funding \n  \nThis work has been supported by Health Data Research (HDR) UK and the Medical Research Council   via   an UKRI Innovation Fellowship to TB (MR/S003703/1) and a Rutherford Fund Fellowship to JP (MR/S004033/1). \n\n\n## Conflict of Interest \n  \nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n\n## Publisher's Note \n  \nAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. \n\n \n", "metadata": {"pmcid": 8885717, "text_md5": "cda5bc3c425c2f5d95cef4c21a5371c4", "field_positions": {"authors": [0, 207], "journal": [208, 226], "publication_year": [228, 232], "title": [243, 342], "keywords": [356, 440], "abstract": [453, 2344], "body": [2353, 49666]}, "batch": 1, "pmid": 35243479, "doi": "10.3389/fdgth.2022.788124", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8885717", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8885717"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8885717\">8885717</a>", "list_title": "PMC8885717  Auto-CORPus: A Natural Language Processing Tool for Standardizing and Reusing Biomedical Literature"}
{"text": "Hong, S. K. and Lee, Jae-Gil\nBMC Bioinformatics, 2020\n\n# Title\n\nDTranNER: biomedical named entity recognition with deep learning-based label-label transition model\n\n# Keywords\n\nBioinformatics\nData mining\nNamed entity recognition\nNeural network\n\n\n# Abstract\n \n## Background \n  \nBiomedical named-entity recognition (BioNER) is widely modeled with conditional random fields (CRF) by regarding it as a sequence labeling problem. The CRF-based methods yield structured outputs of labels by imposing connectivity between the labels. Recent studies for BioNER have reported state-of-the-art performance by combining deep learning-based models (e.g., bidirectional Long Short-Term Memory) and CRF. The deep learning-based models in the CRF-based methods are dedicated to estimating individual labels, whereas the relationships between connected labels are described as static numbers; thereby, it is not allowed to timely reflect the context in generating the most plausible label-label transitions for a given input sentence. Regardless, correctly segmenting entity mentions in biomedical texts is challenging because the biomedical terms are often descriptive and long compared with general terms. Therefore, limiting the label-label transitions as static numbers is a bottleneck in the performance improvement of BioNER. \n\n\n## Results \n  \nWe introduce DTranNER, a novel CRF-based framework incorporating a deep learning-based label-label transition model into BioNER. DTranNER uses two separate deep learning-based networks: Unary-Network and Pairwise-Network. The former is to model the input for determining individual labels, and the latter is to explore the context of the input for describing the label-label transitions. We performed experiments on five benchmark BioNER corpora. Compared with current state-of-the-art methods, DTranNER achieves the best F1-score of 84.56% beyond 84.40% on the BioCreative II gene mention (BC2GM) corpus, the best F1-score of 91.99% beyond 91.41% on the BioCreative IV chemical and drug (BC4CHEMD) corpus, the best F1-score of 94.16% beyond 93.44% on the chemical NER, the best F1-score of 87.22% beyond 86.56% on the disease NER of the BioCreative V chemical disease relation (BC5CDR) corpus, and a near-best F1-score of 88.62% on the NCBI-Disease corpus. \n\n\n## Conclusions \n  \nOur results indicate that the incorporation of the deep learning-based label-label transition model provides distinctive contextual clues to enhance BioNER over the static transition model. We demonstrate that the proposed framework enables the dynamic transition model to adaptively explore the contextual relations between adjacent labels in a fine-grained way. We expect that our study can be a stepping stone for further prosperity of biomedical literature mining. \n\n \n\n# Body\n \n## Introduction \n  \nBiomedical named-entity recognition (BioNER) automatically identifies specific mentions of interest such as chemicals, diseases, drugs, genes, DNAs, proteins, viruses etc. in biomedical literature. As the fundamental step for various downstream linguistic tasks, e.g., adverse drug event extraction [ ], bacteria biotope task [ ], drug-drug interaction [ ], and protein-protein interaction detection [ ], the performance of BioNER is crucial in the overall biomedical knowledge discovery process [ ]. \n\nBioNER operates by predicting a class label for each token across biomedical literature. It is typically considered as a sequence labeling problem and is thus widely modeled by a first-order linear-chain conditional random field (CRF) [ ,  ]. CRF yields chain-structured label sequences by collectively assessing possible label-label transition relations between words over the entire input sequence. \n\nIn recent years, deep learning (briefly, DL) has become prevalent across various machine learning-based natural language processing (NLP) tasks since neural network-based learning systems can effectively identify prominent features in a data-driven way, replacing task-specific feature engineering based on high-level domain knowledge [ ,  ]. For NER tasks, recent methods [ \u2013 ] have reported state-of-the-art performance by introducing a bidirectional long short-term memory (BiLSTM) into CRF. Accordingly, the combination of BiLSTM and CRF has been widely considered as a standard architecture for various sequence labeling problems. \n\nThe combined models (i.e., BiLSTM-CRFs) for NER typically consist of two major components: a token-level BiLSTM and a real-valued transition matrix. The BiLSTM is dedicated to estimate the best-suited label on each token, while the transition matrix is solely responsible for describing the transition compatibility between all possible pairs of labels on neighboring tokens; in detail, the numerical score at the   i  th row and   j  th column of a transition matrix represents the transition compatibility from the   i  th label to the   j  th label. Note that the transition matrix is once established by being suited to the statistics of given training data via its parameter learning and is frozen afterward. As a result, the transition matrix cannot provide the contextualized compatibility for the relationship of neighboring labels in a fine-grained way. \n\nAccordingly, we contend that solely relying on the static transition matrix is not enough to explain the ever-changing label-label transition relations in BioNER, since biomedical entities are frequently descriptive, long or even contain conjunctions [ ], e.g., \u201cnormal thymic epithelial cells,\u201d \u201cperipheral sensor neuropathy,\u201d and \u201ccentral nervous system and cardiac toxicity.\u201d As a result, the boundaries of entity-mentions in biomedical texts are often too ambiguous to accurately segment them. Therefore, we argue that exploiting contextual information to describe label-label transition relations is important to facilitate the accurate identification of biomedical entities. Recently, Lin et al. [ ] studied that explicitly modeling relations between parts in a structured model is applicable to semantic image segmentation, whereas it has been rarely studied in recent DL-based NLP methods. \n\nTo this end, we propose a novel framework, called    D    ynamic     Tran    sition for     NER     (DTranNER)  , to incorporate a DL-based model, which adaptively identify label-label transition relations to further improve the accuracy of BioNER. Overall, DTranNER makes use of two separate DL-based models: Unary-Network and Pairwise-Network. The addition of Pairwise-Network makes it possible to assess the transition compatibility between adjacent labels by exploring the context of an input sentence. Meanwhile, as another DL-based model, Unary-Network is used for individual labeling as in previous works. After all, Unary-Network and Pairwise-Network are arranged to yield agreed label sequences via this novel framework. \n\nBecause DTranNER is orthogonal to a DL-based model, any type of DL-based models such as attention [ ] or transformer [ ] can be employed to play the role of Unary-Network or Pairwise-Network. In this study, we conduct experiments using a BiLSTM as the underlying DL networks since it has been widely adopted in various sequence labeling problems so far. \n\nWe evaluated DTranNER by comparing with current state-of-the-art NER methods on five benchmark BioNER corpora to investigate the effectiveness of the DL-based label-label transition model. The results show that DTranNER outperformed the existing best performer on four out of five corpora and showed comparable accuracy to the existing best performer on one remaining corpus, thereby demonstrating the excellent performance of DTranNER. \n\n\n## Background \n  \n### Problem definition: biomedical named entity recognition (BioNER) \n  \nAn instance of a BioNER corpus consists of an input token sequence   x  =  x  ,\u2026,  x   and its associated output label sequence   y  =  y  ,\u2026,  y  . We use the IOBES tagging scheme by which tokens are annotated with one of \u201cI,\u201d \u201cO,\u201d \u201cB,\u201d \u201cE,\u201d or \u201cS\u201d labels. In the case of an entity spanning over multiple tokens, \u201cB\u201d is tagged to the token to indicate the beginning of the entity, \u201cI\u201d stands for \u201cInside,\u201d and \u201cE\u201d indicates the ending token of the entity. For the case of an entity of a single token, the \u201cS\u201d label is tagged to it. The \u201cO\u201d label stands for \u201cOutside,\u201d which means that the token is not part of any named entity. To indicate the type of entities, one of the type tags, such as \u201cChemical,\u201d \u201cDisease,\u201d \u201cGene,\u201d or \u201cProtein,\u201d is additionally concatenated to each IOBES tag. \n\n\n### Linear-chain conditional random field (CRF) \n  \nAs a class of discriminative probabilistic graphical models, a linear-chain conditional random field (CRF) describes the joint probability   P  (  y  |  x  ) of the entire structured labels   y   with respect to the structure of an undirected graph, given a set of inputs   x  . CRF is widely used in various sequence labeling problems as well as BioNER by imposing the first-order Markov property on the output sequence labeling. There are two types\u2014unary and pairwise\u2014of elementary feature functions to organize an output label sequence. The unary feature functions are dedicated to estimating the suitability of candidate labels at each individual position, whereas the pairwise feature functions are designed to assess possible pairwise labels on two connected positions. Summing up, when an input sequence   x   of length   N   is given, the conditional distribution   P  (  y  |  x  ) is represented as a product of position-dependent unary and pairwise feature functions; thus, it is formulated as in the following equation:\n \n\nwhere   s  (  y  ,  x  ,  i  ) denotes a member of the unary feature functions (i.e.,   s  \u2208  S  ) at the position   i  , and   t  (  y  ,  y  ,  x  ,  i  ) indicates a member of the pairwise feature functions (i.e.,   t  \u2208  T  ) at two consecutive positions   i-1   and   i  . Traditionally, the unary and pairwise feature functions are manually designed to facilitate accurate sequence labeling, and they are usually real-valued binary indicators representing either true or false. The weights (i.e.,   \u03bb  \u2208  \u03b8   and   \u03bb  \u2208  \u03b8  ) associated to the feature functions are trainable parameters.   Z  (  x  ) is the partition function as a normalization constant over all possible label assignments. \n\n\n### Bidirectional long short-term memory (BiLSTM) \n  \nLong short-term memory (LSTM) [ ] is a specific variant of recurrent neural networks to mitigate the problem of vanishing and exploding gradients in modeling long-term dependencies of a sequence. LSTM is suited for modeling sequential data with recurrent connections of hidden states   H  ={  h  ,  h  ,\u2026,  h  } and have become ubiquitous in a wide range of NLP tasks. At every time step, LSTM yields a current hidden state   and internally updates a current cell state   based on   and   calculated in the previous time step. \n\nGiven that LSTM is limited to using past context in the forward direction, a bidirectional LSTM (BiLSTM) is employed to exploit future context as well as past context. BiLSTM processes an input sequence in both forward and backward directions with two separate LSTMs. That is, the hidden states from both directional LSTMs are concatenated to make final output vectors  . \n\n\n### Merger of BiLSTM and CRF: BiLSTM-CRF \n  \nBiLSTM-CRF has been widely employed in recent neural network-based NER studies [ \u2013 ,  ,  ] for sequence labeling. The architecture of BiLSTM-CRF is typically comprised of four layers: a token-embedding layer, a token-level BiLSTM layer, a binding layer, and a CRF layer. We denote an input token sequence of length   N   by   x  ={  x  ,\u22ef,  x  } and the corresponding output label sequence by   y  ={  y  ,\u22ef,  y  }. First, the token-embedding layer encodes input tokens into its fixed-dimensional vectors as   e  ,  e  ,\u2026,  e  . Next, the BiLSTM layer takes the token-embedding vectors as the inputs to generate the hidden-state vectors   h  ,  h  ,\u2026,  h  . Before being fed to the CRF layer, the hidden-state vectors are transformed to the score vectors   U  ,  U  ,\u2026,  U   with   L  -dimensionality, where   L   denotes the number of labels, via the binding layer so as to match the number of labels. The score vector contains the confidence values for possible labels on its corresponding token position. Namely, the stack from the token-embedding layer to the binding layer can be considered to play the role of the unary feature functions (i.e.,   s  \u2208  S  ) in Eq.  . Besides, a real-valued transition matrix, denoted as   A  , accounts for all the label-label transition relations; it is likewise regarded to play the role of the pairwise feature functions (i.e.,   t  \u2208  T  ) in Eq.  . Eventually, BiLSTM-CRF calculates the likelihood for a label sequence   y   given an input token sequence   x   via the following equation:\n \n\nwhere   U  (  y  ) denotes the unary score for assigning the label   y   on the   i  th token,   A   corresponds to the real-valued pairwise transition compatibility from   i  th label to   j  th label, and  . \n\n\n\n## Related work \n  \nRecent state-of-the-art CRF-based NER studies [ \u2013 ,  \u2013 ] have demonstrated the effectiveness of data-driven representation learning (i.e., DL) under CRF. We discuss several CRF-based methods for NER in terms of two kinds of feature functions: unary and pairwise feature functions. We also introduce BioBERT that showed the state-of-the-art performance in BioNER.\n   \nLample et al. [ ] proposed to bring BiLSTM into CRF for NER in general news domain. The model uses two BiLSTMs: one for token-level representation learning and the another for character-level representation learning. The BiLSTMs work as unary feature functions, whereas a static transition matrix comes in for pairwise feature functions. Afterward, Habibi et al. [ ] adopted the model of Lample et al. [ ] for BioNER. \n  \nLuo et al. [ ] adopted BiLSTM-CRF for NER in chemistry domain and applied an attention mechanism to leverage document-level context information. They employ abbreviation embeddings using a specific external library to handle abbreviations that frequently appear in chemical entities\u2019 naming. Their model also relies on a static matrix to retrieve all the label-label transition relations in CRF. \n  \nDang et al. [ ] developed D3NER to utilize various linguistic information under BiLSTM-CRF. D3NER creates a token embedding by aggregating several embeddings: a pre-trained word embedding, an abbreviation embedding, a POS embedding, and a character-level token embedding. Similarly, a transition matrix solely plays the role of pairwise feature functions. \n  \nWang et al. [ ] introduced a multi-task learning framework for BioNER. They trained a model using several biomedical corpora together to overcome a limited amount of annotated biomedical corpora. Their model also adopts BiLSTM-CRF with a transition matrix. \n  \nYoon et al. [ ] proposed aggregation of multiple expert models. They named it CollaboNet, where each expert model is mapped to a BiLSTM-CRF and is trained with each distinct corpus. Likewise, each BiLSTM-CRF has a transition matrix, corresponding to pairwise feature functions. \n  \nPeters et al. [ ] introduced ELMo as a pre-trained model. ELMo provides contextualized word embeddings for various downstream tasks. They also trained the ELMo-enhanced BiLSTM-CRF for NER. \n  \nLee et al. [ ] released BioBERT by training BERT [ ] for the use in the   Bioinformatics   domain. Similarly to ELMo, as a pre-trained model, BioBERT provides contextualized word embeddings and thus can be applied to downstream tasks. BioBERT achieved the state-of-the-art performance in several BioNER corpora. \n  \n\n\n## DTranNER: architecture and method \n  \nIn this section, we present the proposed framework DTranNER as shown in Fig.\u00a0 . For parameter learning, the components (i.e., Unary-Network and Pairwise-Network) of DTranNER are systematically trained via two separate CRFs (i.e., Unary-CRF and Pairwise-CRF). Once trained, Unary-Network and Pairwise-Network are combined into a CRF for BioNER label sequence prediction. First of all, we describe how to build the token embeddings in our models. Although DTranNER is not limited to a specific DL architecture in the places of the underlying networks, from now on, we evaluate our framework using BiLSTM, which has been typically adopted in a majority of NER studies.\n   \nThe overall architectures of the proposed framework DTranNER.   a   As a CRF-based framework, DTranNER is comprised of two separate, underlying deep learning-based networks: Unary-Network and Pairwise-Network are arranged to yield agreed label sequences in the prediction stage. The underlying DL-based networks of DTranNER are trained via two separate CRFs: Unary-CRF and Pairwise-CRF.   b   The architecture of Unary-CRF. It is dedicated to train Unary-Network.   c   The architecture of Pairwise-CRF. It is also committed to train Pairwise-Network. A token embedding layer is shared by Unary-Network and Pairwise-Network. A token-embedding is built upon by concatenating its traditional word embedding (denoted as \u201cW2V\u201d) and its contextualized token embedding (denoted as \u201cELMo\u201d) \n  \n\n### Token-embedding layer \n  \nGiven a sequence of   N   tokens (  x  ,  x  ,...,   x  ), they are converted token-by-token into a series of fixed-dimensional vectors (  e  ,  e  ,...,   e  ) via the token-embedding layer. Each token embedding is designed to encode several linguistic information of the corresponding token in the sentence. Each token embedding is thus built up by concatenating the traditional context-independent token embedding and its contextualized token embedding. These token embeddings are subsequently fed to Unary-Network and Pairwise-Network as the inputs. We do not consider additional character-level token embeddings unlike several models [ \u2013 ,  ,  ,  ], because ELMo [ ] as our contextualized token embedding provider basically includes a character-level CNN model. \n\n#### Context-independent token embedding \n  \nWe use the pre-trained token vectors,   Wiki-PubMed-PMC  , created by Pyysalo et al. [ ] to initialize the traditional token-embedding vectors. The pre-trained token vectors were made up by being trained on three different datasets: the abstracts of the PubMed database, the full-text articles of the PubMed Central (PMC) database, and the texts of a recent Wikipedia dump. It is available at [ ]. We replace every out-of-vocabulary (OOV) token with a special   <UNK>   vector. \n\n\n#### Contextualized token embedding \n  \nWe employ ELMo [ ] for the contextualized token embeddings. Unlike context-independent token embeddings based on GloVe [ ] or Word2Vec [ ], ELMo creates context-dependent token embeddings by reconsidering the syntax and semantics of each token under its sentence-level context. In particular, we adopt the in-domain ELMo model pre-trained on the PubMed corpus, which is available at [ ]. \n\n\n\n### Unary-Network \n  \nAs shown in Fig.\u00a0 b, Unary-Network takes token embeddings as inputs, put them into its own BiLSTM layer to extract task-specific contextual information in an ordered token-level sequence, and finally produces the   L  -dimensional score vectors as many as the number of tokens via its binding layer. The binding layer consists of two linear transformations with an activation function and a skip connection between them. That is, the binding layer is formulated as follows:\n \n\nwhere   U   denotes the   L  -dimensional score vector exhibiting the suitability over all possible labels on the   i  th token,   is the   i  -th hidden state from the BiLSTM layer,   and   are trainable weight matrices, and   and   are the bias vectors. Here,   projects the   d  -dimensional vector obtained by both the feed-forward network and the skip connection to the   L  -dimensional output vector. We use an ELU as the activation function   \u03c3  (\u00b7). As will be explained in the following section, Unary-Network is trained via the purpose-built CRF (i.e., Unary-CRF) for the parameter learning. \n\n\n### Pairwise-Network \n  \nPairwise-Network aims to extract contextual information related to pairwise labeling. This design explains why two consecutive hidden state vectors of the BiLSTM are involved in describing an edge connection in the CRF layer as shown in Fig.\u00a0 c. Pairwise-Network therefore generates   L  -dimensional score vectors to match the number of possible label pairs on two tokens. We employ a bilinear model-based method [ ] to exploit interactive features of two neighboring hidden state vectors. This method approximates a classical three-dimensional tensor with three two-dimensional tensors, significantly reducing the number of parameters. It is shown in the following equation:\n \n\nwhere   f   denotes the   m  -dimensional vector via the bilinear model of two neighboring hidden state vectors (i.e.,   and  ) of the underlying BiLSTM layer;  , and   are trainable matrices; and \u2218 denotes Hadamard product (i.e., element-wise product of two vectors). The binding layer has a skip connection as in Unary-Network. It is thus formulated as the following equation:\n \n\nwhere   denotes the score vector indicating the confidence values over all label combinations on the neighboring (  i  \u22121)th and   i  th tokens,   and   are trainable weight matrices,   and   are the bias terms, and   \u03c3  (\u00b7) is an ELU activation. Similarly to Unary-Network, Pairwise-Network is also trained via the purpose-built CRF (i.e., Pairwise-CRF) for the parameter learning. \n\n\n### Model training \n  \nHere, we explain how to train DTranNER. In order to facilitate the parameter learning of the two underlying networks (i.e., Unary-Network and Pairwise-Network), we establish two separate linear-chain CRFs, which are referred as Unary-CRF (Fig.\u00a0 b) and Pairwise-CRF (Fig.\u00a0 c), by allocating the two types of DL-based networks (i.e., BiLSTMs in our case) to the two purpose-built CRFs, respectively. The reason is that, when both Unary-Network and Pairwise-Network coexist in a single CRF, as Smith et al. [ ] and Sutton et al. [ ] claimed that the existence of a few indicative features can swamp the parameter learning of other weaker features, either one of the two networks starts to hold a dominant position, causing the other network to deviate from its optimal parameter learning. Our solution enables each network to notice own prediction error during the parameter learning. We explain in detail the effect of our training strategy in the Additional file\u00a0 . \n\nIn this study, note that each of Unary- and Pairwise-CRFs is a sufficient label sequence predictor or learner; in the sense, the conditional likelihood   P   of Unary-CRF is formulated as in Eq.  , and the conditional likelihood   P   of Pairwise-CRF given the input sequence   x   with the length   N   is formulated as the following equation:\n \n\nwhere   is the normalization constant. \n\nRather than individually training multiple CRFs offline as in [ ,  ], Unary-CRF and Pairwise-CRF are jointly trained in our training strategy by maximizing their product\u2014i.e.,  \u2014of the two likelihoods of Unary-CRF and Pairwise-CRF. By equivalently converting the objective function into the negative log likelihood, the optimization problem is written as the following equation:\n \n\nwhere   x   and   y   denote the   e  th training sentence example and its ground-truth label sequence, and   \u03b8   and   \u03b8   denote the model parameters of Unary-CRF and Pairwise-CRF respectively. \n\n\n### Prediction \n  \nWe explain the detail on how to infer label sequences with the trained DTranNER. Once trained via the two separate CRFs, Unary-Network and Pairwise-Network are arranged into a CRF to yield an agreed label sequence in the prediction stage. Note that Unary-Network and Pairwise-Network have distinct focuses derived by different roles, leading to learn their own specific representations. We combine them by multiplying them as a product of models [ ]. More specifically, all the components obtained through the aforementioned training process\u2014Unary-Network, Pairwise-Network, and the transition matrix\u2014are organized in a CRF, as shown in Fig.\u00a0 a. The combined model is formulated in terms of the probability for a label sequence   y   given an input sequence   x   via the following equation:\n \n\nAs a result, we obtain the most likely label sequence using the Viterbi decoding. \n\n\n\n## Experimental setup \n  \n### Datasets \n  \nWe conducted our experiments with five BioNER benchmark corpora: BC2GM, BC4CHEMD, BC5CDR-chemical, BC5CDR-disease, and NCBI-Disease, which are commonly used in the existing literature [ ,  ,  ,  ]. \n\nTable\u00a0  shows the overall description of the five benchmark BioNER corpora. They are publicly available and can be downloaded from [ ]. The BioCreative II Gene Mention (  BC2GM  ) task corpus [ ] consists of 20,128 sentences from biomedical publication abstracts and is annotated for mentions of the names of proteins, genes, and related entities. The BioCreative IV Chemical and Drug (  BC4CHEMD  ) task corpus [ ] contains the annotations for chemical and drug mentions in 10,000 biomedical abstracts. The BioCreative V Chemical Disease Relation (  BC5CDR  ) corpus [ ] is composed of mentions of chemicals and diseases that appeared in 1,500 PubMed articles. The NCBI-Disease corpus (  NCBI-Disease  ) [ ] is composed of 793 PubMed abstracts annotated for disease mentions. The aforementioned corpora cover four major biomedical entity types: gene, protein, chemical, and disease.\n   \nBioNER corpora in experiments \n  \n\n\n### Training setup \n  \nIn model training, we added L2 regularization penalty to the loss (i.e., Eq.  ) with the decay factor of 1\u00d710 . The   Glorot   uniform initializer of Glorot and Bengio [ ] is used for initializing our weight matrices, and the biases are initialized with 0. All the activation functions are ELU (exponential linear unit). We set the minibatch size of model training to ten examples across all experiments. Our models are differentiable; thereby, the CRF and its underlying neural networks can be jointly trained end-to-end by backpropagation. We use the   Adam   optimizer of [ ] with the learning rate of 0.001. In the training process, we renormalize all gradients whenever the L2 norm of the gradients exceeds 5 in every minibatch update. We applied layer normalization [ ] to the outputs of the token embedding layer, and also applied weight normalization [ ] to all the weight matrices of the binding layers of Unary-Network and Pairwise-Network. We used Dropout [ ] with keep probability 0.5 in both the binding layers. We established our models within at most 50 epochs for all the corpora. \n\n\n### Evaluation metrics \n  \nWe evaluated all the methods using the precision, recall, and F1 score on the test sets of all corpora. We defined each predicted entity as correct if and only if both the entity type and the boundary were exactly matched to the ground-truth annotation. We used the python version of the evaluation script designed for CoNLL-2000 Benchmark Task, which can be downloaded from [ ]. To get reliable results, we repeated every test   five times   with different random initialization and report the arithmetic mean. \n\n\n\n## Results \n  \n### Overall performance comparison \n  \nWe compared DTranNER with five state-of-the-art methods:   (1)   Att-BiLSTM-CRF [ ],   (2)   D3NER [ ],   (3)   Collabonet [ ],   (4)   the multi-task learning-based model of Wang et al. [ ], and   (5)   BioBERT [ ]. Note that all the models except BioBERT employ a CRF as their top layer and rely on a static transition matrix. The performance values in terms of the   precision  ,   recall  , and   F1  -score over all the corpora are presented in Table\u00a0 . DTranNER outperformed the current state-of-the-art models on four of five corpora\u2014BC2GM, BC4CHEMD, BC5CDR-Disease, and BC5CDR-Chemical\u2014in terms of F1 scores.\n   \nPerformance values in terms of the   precision   (%),   recall   (%) and   F1  -score (%) for the state-of-the-art methods and the proposed model   DTranNER  \n  \n Note:   The highest performance in each corpus is highlighted in   Bold  . We quoted the published scores for the other models. For Wang et al. [ ], we conducted additional experiments to obtain the performance scores for two corpora (i.e., BC5CDR-Chemical and BC5CDR-Disease) using the software on their open source repository [ ] \n  \n\nDTranNER achieved a much higher F1 score with higher precision than the current best performer (94.16% vs. 93.44%) for BC5CDR-Chemical, where its NER process was confused owing to many abbreviations despite its shorter average entity length as shown in Table\u00a0 . Thus, the pairwise transition network of DTranNER is shown to be advantageous in discovering abbreviation-formed entities. \n\n\n### Ablation studies \n  \nWe investigated the effectiveness of main components of our proposed method DTranNER through ablation studies. \n\n#### Impact of unary- and pairwise-Networks \n  \nTo investigate the contribution of Unary- and Pairwise-Networks to DTranNER, we trained experimental models by deactivating each component (i.e., either Unary-Network or Pairwise-Network) in turn from DTranNER and then measured the performance of the variant models on three benchmark corpora: BC5CDR-Chemical, BC5CDR-Disease, and NCBI-Disease. The results are shown in Table\u00a0 .\n   \nImpact of Unary-Network and Pairwise-Network in terms of the F1-score (%) \n  \n Note:   \u201cUnary-CRF\u201d denotes a variant model excluding Pairwise-Network from DTranNER, \u201cPairwise-CRF\u201d denotes a variant model excluding Unary-Network from DTranNER, and \u201cUnary+Pairwise ensemble\u201d is an ensemble model of \u201cUnary-CRF\u201d and \u201cPairwise-CRF.\u201d In the ensemble model, \u201cUnary-CRF\u201d and \u201cPairwise-CRF\u201d were independently trained, and they voted over the sequence predictions by their prediction scores \n  \n\nThe removal of either Unary-Network or Pairwise-Network from DTranNER caused the overall performance degradation in all the corpora by up to 1.91 percent points. That is, this ablation study presents that the performance achievement of DTranNER is attributed to not only an individual component but also the mutual collaboration of Unary-Network and Pairwise-Network. The relative importance between the two networks was not very clear. \n\nWe also compared DTranNER with an ensemble model of Unary-CRF and Pairwise-CRF, denoted as \u201cUnary+Pairwise ensemble,\u201d which were separately trained. The sequence prediction of the ensemble model was decided by voting with their sequence output scores. As shown in Table\u00a0 , the performance improvement of the ensemble model was marginal in BC5CDR-Chemical and NCBI-Disease. More important, the ensemble model was much worse than DTranNER in all corpora. This result indicates that yielding agreed label sequences between the two networks, which have separate views, as in DTranNER is more effective than their ensemble via simple voting. \n\n\n#### Impact of separate BiLSTM layers of Unary- and Pairwise networks \n  \nUnary-Network and Pairwise-Network have an independent underlying layer which learns its role-specific representations. We investigate the impact of the separate underlying layers in the peer networks. For this purpose, we additionally built a variant model of DTranNER, denoted as \u201cDTranNER-shared,\u201d that forced Unary-Network and Pairwise-Network to share the parameters of their BiLSTM layers. As shown in Table\u00a0  for the comparison result, it turned out that Unary-Network and Pairwise-Network benefit from the exclusive underlying layer.\n   \nImpact of separate BiLSTM layers in terms of the F1-score (%) \n  \n Note:   \u201cDTranNER-shared\u201d is a variant model that shares the BiLSTM layer in \u201cUnary-Network\u201d and \u201cPairwise-Network.\u201d \n  \n\n\n#### Embedding layer \n  \nWe here investigate the impact of each element in the token embedding layer of DTranNER. For this purpose, we built two variants of DTranNER: (1) a model (denoted as \u201cW2V\u201d) whose token embedding consists of only 200-dimensional pre-trained token embedding [ ] and (2) another model (denoted as \u201cELMo\u201d) whose token embedding is solely comprised of 1024-dimensional ELMo embedding, which is obtained from the ELMo model [ ] pre-trained on the PubMed corpus. The comparison results are presented in Table\u00a0 . The context-dependent token embeddings via the ELMo model bring significant performance improvement on the four benchmark corpora, especially on NCBI-Disease. Nevertheless, the best performance is consistently achieved by the combination of the context-dependent ELMo embedding and the traditional context-independent embedding.\n   \nImpact of each component in the token embedding composition in terms of the F1-score (%) \n  \n Note:   \u201cW2V\u201d is a variant model of DTranNER whose embedding layer uses only traditional context-independent token vectors (i.e.,   Wiki-PubMed-PMC   [ ]), \u201cELMo\u201d is another variant model of DTranNER whose embedding layer uses only ELMo, and \u201cELMo + W2V\u201d is equivalent to DTranNER \n  \n\n\n\n### Case studies \n  \nTo demonstrate the advantage of the DL-based label-label transition model, which is the main feature of DTranNER, we compared several example outcomes yielded by DTranNER and Unary-CRF as shown in Table\u00a0 . Note that Unary-CRF is not equipped with this main feature. In addition, the label sequence predictions of DTranNER in Table\u00a0  coincide with the ground-truth annotations.\n   \nCase study of the label sequence prediction performed by DTranNER and Unary-CRF \n  \n Note  : Unary-CRF is the purpose-built model excluding Pairwise-Network from DTranNER. The named entities inferred by each model are underlined in sentences \n  \n\nFor Case 1, Unary-CRF failed to detect one of the boundaries of the disease-type entity \u201cureteric stones or obstruction\u201d because of the intervention of the inner conjunction \u201cor,\u201d while DTranNER precisely determined both boundaries. For Case 2, Unary-CRF failed to identify the chemical-type entities enumerated via the conjunctions \u201c/\u201d and \u201cand,\u201d whereas DTranNER exactly identified all the separate terms. For Case 3, Unary-CRF failed to determine the left boundary of the single-token entity \u201chepatitis\u201d by mistakenly regarding \u201cacute\u201d and \u201ccytolytic\u201d as its constituent elements, whereas DTranNER exactly distinguished them from this entity by understanding the contextual relations. For Case 4, DTranNER correctly identified the two entities, where the latter is the abbreviation of the former, but Unary-CRF failed. For Case 5, Unary-CRF ignored the gene-type entity \u201ccoagulase type IV\u201d by mistakenly regarding \u201ctype\u201d and \u201cIV\u201d as generic terms, whereas DTranNER correctly identified it by reflecting the contextual correlations between its constituent elements. For Case 6, DTranNER correctly identified both boundaries of the gene-type entity \u201cRNA polymerase I\u201d by exploiting the contextual clues on the consecutive pairs, \u2329\u201cpolymerase\u201d and \u201cI\u201d \u232a and \u2329\u201cI\u201d and \u201ctranscription\u201d \u232a, though \u201cI\u201d solely looks ambiguous; in contrast, Unary-CRF failed to determine the right boundary because it classified \u201cI\u201d as a generic term. For Case 7, DTranNER correctly extracted the lengthy entity by grasping the correlation between the neighboring tokens (i.e., \u201chydroxylase\u201d and \u201chomologue\u201d), whereas Unary-CRF failed to handle this lengthy entity. \n\nSumming up, DTranNER successfully supports various cases which would be very difficult without the contextual information, and these cases indeed show the benefit of DTranNER for BioNER. \n\n\n\n## Conclusion \n  \nIn this paper, we proposed a novel framework for BioNER, for which we call   DTranNER  . The main novelty lies in that DTranNER learns the label-label transition relations with deep learning in consideration of the context in an input sequence. DTranNER possesses two separate DL-based networks: Unary-Network and Pairwise-Network; the former focuses on individual labeling, while the latter is dedicated to assess the transition suitability between labels. Once established via our training strategy, these networks are integrated into the CRF of DTranNER to yield agreed label sequences in the prediction step. In other words, DTranNER creates the synergy leveraging different knowledge obtained from the two underlying DL-based networks. As a result, DTranNER outperformed the best existing model in terms of the F1-score on four of five popular benchmark corpora. We are extending DTranNER to utilize unlabeled biomedical data. This extension is meaningful in several aspects: (1) building a more-generalized model using a wide range of biomedical literature, (2) rapidly incorporating up-to-date biomedical literature by skipping time-consuming annotation, and (3) reducing annotation cost. \n\n\n## Supplementary information \n  \n\n\n\n\n \n", "metadata": {"pmcid": 7014657, "text_md5": "f957b56c9bded3cf793341b439a95839", "field_positions": {"authors": [0, 28], "journal": [29, 47], "publication_year": [49, 53], "title": [64, 163], "keywords": [177, 244], "abstract": [257, 2786], "body": [2795, 36719]}, "batch": 1, "pmid": 32046638, "doi": "10.1186/s12859-020-3393-1", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7014657", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7014657"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7014657\">7014657</a>", "list_title": "PMC7014657  DTranNER: biomedical named entity recognition with deep learning-based label-label transition model"}
{"text": "Kolchinsky, Artemy and Louren\u00e7o, An\u00e1lia and Wu, Heng-Yi and Li, Lang and Rocha, Luis M.\nPLoS One, 2015\n\n# Title\n\nExtraction of Pharmacokinetic Evidence of Drug\u2013Drug Interactions from the Literature\n\n# Keywords\n\n\n\n# Abstract\n \nDrug-drug interaction (DDI) is a major cause of morbidity and mortality and a subject of intense scientific interest. Biomedical literature mining can aid DDI research by extracting evidence for large numbers of potential interactions from published literature and clinical databases. Though DDI is investigated in domains ranging in scale from intracellular biochemistry to human populations, literature mining has not been used to extract specific   types of experimental evidence  , which are reported differently for distinct experimental goals. We focus on   pharmacokinetic evidence   for DDI, essential for identifying causal mechanisms of putative interactions and as input for further pharmacological and pharmacoepidemiology investigations. We used manually curated corpora of PubMed abstracts and annotated sentences to evaluate the efficacy of literature mining on two tasks: first, identifying PubMed abstracts containing pharmacokinetic evidence of DDIs; second, extracting sentences containing such evidence from abstracts. We implemented a text mining pipeline and evaluated it using several linear classifiers and a variety of feature transforms. The most important textual features in the abstract and sentence classification tasks were analyzed. We also investigated the performance benefits of using features derived from PubMed metadata fields, various publicly available named entity recognizers, and pharmacokinetic dictionaries. Several classifiers performed very well in distinguishing relevant and irrelevant abstracts (reaching F1\u22480.93, MCC\u22480.74, iAUC\u22480.99) and sentences (F1\u22480.76, MCC\u22480.65, iAUC\u22480.83). We found that word bigram features were important for achieving optimal classifier performance and that features derived from Medical Subject Headings (MeSH) terms significantly improved abstract classification. We also found that some drug-related named entity recognition tools and dictionaries led to slight but significant improvements, especially in classification of evidence sentences. Based on our thorough analysis of classifiers and feature transforms and the high classification performance achieved, we demonstrate that literature mining can aid DDI discovery by supporting automatic extraction of specific types of experimental evidence. \n \n\n# Body\n \n## Introduction \n  \nDrug-drug interaction (DDI) is one of the major causes of adverse drug reaction (ADR) and a threat to public health. Pharmaco-epidemiology studies [ ] and recent National Health Statistics Report publications [ ,  ] indicate that each year an estimated 195,000 hospitalizations and 74,000 emergency room visits are the result of DDI in the United States alone [ ]. DDI has been implicated in nearly 3% of all hospital admissions [ ] and 4.8% of admissions among the elderly [ ] and is a common consequence of medical error, representing 3% to 5% of all inpatient medication errors [ ]. With increasing rates of polypharmacy, which refers to the use of multiple medications or more medications than are clinically indicated [ ], the incidence of DDI will likely increase in the coming years. \n\nResearchers link molecular mechanisms underlying DDI to their clinical consequences through three types of studies:   in vitro  ,   in vivo  , and clinical [ \u2013 ].   In vitro   pharmacology experiments use intact cells (e.g. hepatocytes), microsomal protein fractions, or recombinant systems to investigate molecular interaction mechanisms within the cell (i.e. metabolic, transport- or target-based).   In vivo   studies evaluate whether such interactions impact drug exposure in humans. Finally, clinical studies use a population-based approach and large electronic medical record databases to investigate the contribution of DDI to drug efficacy and ADR. \n\nAutomated biomedical literature mining (BLM) methods offer a promising approach for uncovering evidence of possible DDI in published literature and clinical databases [ ]. BLM is a biomedical informatics methodology that holds the promise of tapping into the biomedical collective knowledge [ ] by extracting information from large-scale literature repositories and by integrating information scattered across various domain-specific databases and ontologies [ \u2013 ]. It has been used for knowledge discovery in many biomedical domains, including extraction of protein-protein interactions [ ,  ], protein structure prediction [ ], identification of genomic locations associated with cancer [ ], and mining drug targets [ ]. In the domain of DDI, putative interactions uncovered by BLM can serve as targets for subsequent investigation by   in vitro   pharmacological methods as well as   in vivo   and clinical studies [ ]. \n\nBLM has previously been used for DDI information extraction [ \u2013 ], as overviewed by the literature on recent DDI challenges [ \u2013 ] and   Pacific Symposium on Biocomputing   sessions [ ,  ]. However, much remains to be done in automatic extraction of   experimental evidence of DDI   from text. Importantly, experimental evidence of DDI is reported differently for the different types of studies described above. For instance,   in vivo   pharmacokinetic experiments report parameters such as the \u2018area under the concentration-time curve\u2019, while clinical studies may instead report population-level statistics of adverse drug reactions. It is important for BLM pipelines to be able to identify these different kinds of evidence independently. \n\nTo address this situation, we demonstrate the use BLM for reliable extraction of   pharmacokinetic   evidence   for DDI from reports of   in vitro   and   in vivo   experiments. Pharmacokinetic experimental evidence refers to measures of pharmacokinetic parameters such as the inhibition constant (Ki), the 50% inhibitory concentration (IC50), and the area under the plasma concentration-time curve (AUCR). Such evidence is particularly important in identifying or dismissing causal mechanisms behind DDIs and in providing support for putative DDIs extracted from mining patient records, where biases and confounds in reporting often give rise to non-causal correlations [ ]. In order to pursue the goal of using BLM to uncover pharmacokinetic DDI evidence, a collaboration was developed between Rocha\u2019s lab, working on literature mining, and Li\u2019s lab, working on pharmacokinetics. Though this work is focused on pharmacokinetic evidence, in subsequent studies we will approach other types of DDI evidence (e.g. clinical evidence). \n\nOur approach is different from previous BLM approaches to DDI information extraction [ \u2013 ,  ] because our ultimate goal is not to identify interacting drugs themselves but rather abstracts and sentences containing a   specific type   of   evidence of drug interaction  . Existing DDI-extraction methods and corpora\u2014including those evaluated under the DDI Extraction challenges [ \u2013 ,  ,  ]\u2014are not well suited for this task because they do not attempt to extract   experimental evidence   of drug interactions, nor specifically label distinct kinds of evidence. For instance, the DDI Extraction challenge \u201811 [ ] used a corpus of several hundred documents from DrugBank [ ], but interacting drug pairs were annotated without regard for the presence of experimental evidence. More recently, the DDI Extraction challenge \u201813 [ ] provided a corpus annotated with pharmacokinetic and pharmacodynamic interactions [ ], but the goal of the text mining task was the extraction and classification of interacting drug pairs, not the extraction of the experimental evidence of interactions. Other related work has used DrugBank data for large-scale extraction of drug-gene and drug-drug relationships [ ,  ], and for predicting DDI using a drug-drug network based on phenotypic, therapeutic, chemical, and genomic feature similarity [ ], but neither study aimed to identify or extract specific kinds experimental evidence of DDI. \n\nWe have previously shown that BLM can be used for automatic extraction of numerical pharmacokinetics (PK) parameters from the literature [ ]. However, that work was not oriented specifically toward the extraction of evidence of DDI. Recently, we reported high performance in a preliminary work on automatically classifying PubMed abstracts that contain pharmacokinetic evidence of DDI [ ] (details below). Because identifying relevant abstracts is only a first step in the process of extracting pharmacokinetic evidence of DDI, in this work we consider both the problem of identifying abstracts containing pharmacokinetic evidence of DDI and that of extracting from abstracts sentences that contain this specific kind of evidence. In addition to evidence sentence extraction, we also provide a new assessment of abstract classification using an updated version of a separately published corpus [ ], leading to substantially better classification performance than reported in our preliminary study [ ]. The updated corpus is described below and is publicly available. Finally, we provide a new comparison of classifiers, a new evaluation methodology using permutation-based significance tests and Principal Component Analysis (PCA) [ ] of feature weights, and a detailed study of the benefits of including features derived from PubMed metadata, named entity recognition tools and specialized dictionaries. \n\nWe created abstract and sentence corpora using annotation criteria for identifying pharmacokinetic evidence of DDI. We consider positive (indicating the presence of interactions)   and   negative (indicating the absence) DDI evidence as relevant (see \u201c \u201d section), since both provide important information about possible DDI. Because the criteria considered here are different from those used in previously available DDI corpora, our results are not directly comparable to other BLM approaches to DDI. Therefore, we pursued a thorough evaluation of the performance of different types of classifiers, feature transforms, and normalization techniques. For both abstract and sentence classification tasks we tested several linear classifiers: logistic regression, support vector machines (SVM), binomial Naive Bayes, linear discriminant analysis, and a modification of the Variable Trigonometric Threshold (VTT) classifier, previously developed by Rocha\u2019s lab and found to perform well on protein-protein interaction text mining tasks [ ,  ,  ]. As we describe in the results and discussion sections, classifiers fall into two main classes based on whether or not they take into account feature covariances. In addition, we compared different feature transform methods, including normalization techniques such as \u2018Term Frequency, Inverse Document Frequency\u2019 (TFIDF) and dimensionality reduction based on Principle Component Analysis (PCA). We also compared performance when including features generated by several Named Entity Recognition (NER) tools and specialized dictionaries. \n\nIn the experiments reported, our goal is to measure the quality of automated methods in identifying pharmacokinetic evidence of DDIs reported in the literature. More generally, we seek to demonstrate that literature mining can be successful in automatically extracting experimental evidence of interactions as part of DDI workflows. We show that many classifier configurations achieve high performance on this task, demonstrating the robustness and efficacy of BLM on extracting pharmacokinetic evidence of DDI. \n\n\n## Materials and Methods \n  \nThe following sections describe the methods used in our literature mining pipeline. Its basic steps are visually diagrammed in  . They include the selection of corpus documents, hand-labeling of ground truth assignments, extraction and normalization of textual features, and computation of unigram/bigram occurrences matrices. Cross-validation folds are used to estimate generalization performance of classifier and feature transform configurations, while nested (inner) cross-validation folds are used to choose classifier hyperparameters. The software consisted of custom   Python   scripts unless otherwise noted. \n   Literature mining pipeline.  \nThe basic steps of the literature mining pipeline include selection of corpus documents, hand-labeling of ground truth assignments, extraction and normalization of textual features, and computation of unigram/bigram occurrences matrices. Cross-validation folds are used to estimate generalization performance of classifier and feature transform configurations, while nested (inner) cross-validation folds are used to choose classifier hyperparameters. \n  \n### Abstract Corpus \n  \nFor the training corpus, Li\u2019s lab selected 1203 pharmacokinetics-related abstracts by searching PubMed using terms from a previously developed ontology for PK pharmacokinetic parameters [ ]. Therefore, all retrieved articles describe and contain some form of pharmacokinetic evidence, though not necessarily of DDI. We kept   in vitro   studies but removed any animal   in vivo   studies. Abstracts were labeled according to the following criteria: abstracts that reported   the presence or absence of drug interaction supported by explicit experimental evidence of pharmacokinetic parameter data   were labeled as   DDI-relevant   (909 abstracts) while the rest were labeled as   DDI-irrelevant   (294 abstracts). DDI-relevance was established regardless of whether the relevant enzymes were presented or not. Importantly, the concept of DDI-relevance employed here updates the criteria used in a previous preliminary study [ ]. Interactions between a drug and food, fruit, smoking, alcohol, and natural products are now classified as drug interactions because their pharmacokinetics studies are designed similarly. For the same reason, studies dealing with interactions between drug metabolites (instead of parent compounds) are now also considered relevant, as well as studies reporting inhibition of induction of a drug on a drug metabolism enzyme or drug transporter. Classification was done by three graduate students with M.S. degrees and one postdoctoral annotator; any inter-annotator conflicts were checked by a Pharm D. and an M.D. scientist with extensive pharmacological training. The corpus is publicly available as \u201cPharmacokinetics DDI-Relevant Abstracts V0\u201d in [ ] (see also [ ]). \n\nWe extracted textual features from PubMed article title and abstract text fields as well as the following metadata fields: the author names, the journal title, the Medical Subject Heading (MeSH) terms, the \u2018registry number/EC number\u2019 (RN) field, and the \u2018secondary source\u2019 field (SI) (the latter two fields contain identification codes for relevant chemical and biological substances). For each PubMed entry, the content of the above fields was tokenized, processed by Porter stemming [ ], and converted into textual features (unigrams and, in certain runs, bigrams). Strings of numbers were converted into \u2018#\u2019, short textual features (with length of less than 2 characters) and infrequent features (that occurred in less than 2 documents) were omitted. Author names, journal titles, substance names, and MeSH terms were treated as single textual tokens. \n\nThe corpus was represented as binary term-document occurrence matrices. We evaluated classification performance under two different conditions: in the first\u2014referred to as \u2018unigram runs\u2019\u2014only word unigram features were used; in the second\u2014referred to as \u2018bigram runs\u2019\u2014word bigram features were used in addition to unigram features. Bigram runs included a much larger number of parameters (i.e. the bigram feature coefficients) that needed to be estimated from training data, which can potentially increase generalization error arising from increased model complexity [ ]. Testing the classifiers exclusively with unigram features as well as with both unigram and bigram features evaluated whether the class information provided by bigrams outweighed their cost in complexity. \n\n\n### Sentence Corpus \n  \nThe evidence sentence task consisted in identifying those sentences within a PubMed abstract that reported experimental evidence for the presence or absence of a specific DDI. For this purpose, Li\u2019s group developed a training corpus of 4600 sentences extracted from 428 PubMed abstracts. All abstracts contained (positive or negative) pharmacokinetic evidence of DDIs. Sentences were manually labeled as DDI-relevant (1396 sentences) if they   explicitly mentioned pharmacokinetic evidence for the presence or absence of drug-drug interactions  , and as DDI-irrelevant (3204 sentences) otherwise. The same pre-processing and annotation procedures were followed for the sentence corpus as for the abstract corpus (see section \u201cAbstract Corpus\u201d). This corpus is publicly available as \u201cDeep Annotated PK Corpus V1\u201d in [ ] (see also [ ]). \n\n\n### Classifiers \n  \nSix different linear classifiers were tested:\n   \n VTT  : a simplified, angle-domain version of the   Variable Trigonometric Threshold   Classifier, previously developed in Rocha\u2019s lab [ ,  ,  ]. Given a document vector   x   = <  x  , \u2026,   x  > with features (i.e. dimensions) indexed by   i  , the separating hyperplane is defined as\n \nHere,   \u03bb   is a threshold (bias) and   \u03c6   is the \u2018angle\u2019 of feature   i   in binary class space:\n \nwhere   p   is the probability of occurrence of feature   i   in relevant-class documents and   n   is the probability of occurrence of feature   i   in irrelevant-class documents. The threshold parameter   l   is chosen so that a neutral \u2018pseudo-document\u2019 defined by   x   = (  p  +  n  )/2 falls exactly onto the separating hyperplane. \n\nThe full version of VTT, which includes additional parameters to account for named entity occurrences and which we have previously used in protein-protein interaction classification, is evaluated in combination with various NER tools in section \u201cImpact of NER and PubMed metadata on abstract classification\u201d below. VTT performs best on sparse, positive datasets; for this reason, we do not evaluate it on dense dimensionality-reduced datasets. Notice that in previous work, we used a different version of VTT with a cross-validated threshold parameter; its performance on the tasks was very similar, and is reported in the Supporting Information as the \u2018VTTcv\u2019 classifier (section 1 and 2 in  ). \n  \n SVM  : a linear   Support Vector Machine   with a cross-validated regularization parameter (implemented using the   sklearn   [ ] library\u2019s interface to the   LIBLINEAR   package [ ]). \n  \n Logistic regression   classifier with a cross-validated regularization parameter (also implemented using   sklearn  \u2019s interface to   LIBLINEAR  ). \n  \n Naive Bayes   classifier with smoothing provided by a Beta-distributed prior with a cross-validated concentration parameter. \n  \n LDA  : a regularized   Linear Discriminant Analysis   classifier, following [ ]. Singular value decomposition (SVD), a dimensionality reduction technique, is first used to reduce any rank-deficiency, after which the covariance matrix is shrunk toward a diagonal, equal-variance structured estimate. The shrinkage parameter is determined by cross-validation. \n  \n dLDA  : a \u2018diagonal\u2019 LDA, where only the diagonal entries of the covariance matrix are estimated and the off-diagonal entries are set to 0. A cross-validated parameter determines shrinkage toward a diagonal, equal-variance estimate. This classifier can offer a more robust estimate of feature variances; it is equivalent to a Naive Bayes classifier with Gaussian features [ ]. \n  \n\nGenerally, linear classifiers fall into one of two types. Classifiers of the first type\u2014sometimes called \u2018naive\u2019 in the literature, which in our case include VTT, dLDA, and Naive Bayes\u2014learn feature weights without considering feature covariances. While covariance information can be useful for distinguishing classes, naive classifiers often perform well with small amounts of training data, when covariances are difficult to estimate accurately. Classifiers of the second type\u2014which we refer to as \u2018non-naive\u2019, and which in our case included SVM, LDA, and Logistic Regression\u2014do consider feature covariances (often in combination with regularization techniques to smooth covariance estimates) and can achieve superior performance when provided with sufficient training data. \n\n\n### Feature Transforms \n  \nFor both unigram and bigram runs, we evaluated classification performance on several transforms of the document matrices:\n   \nNo transform: raw binary occurrence matrices (see section \u201cAbstract Corpus\u201d). \n  \nIDF: occurrences of feature   i   were transformed to its Inverse Document Frequency (IDF) value:  , where   c   is the total number of occurrences of feature   i   among all documents. This reduced the influence of common features on classification. \n  \nTFIDF: the Term Frequency, Inverse Document Frequency transform (TFIDF); same as above, but subsequently divided by the total number of features that occur in each document. This reduced the impact of document size differences. \n  \nNormalization: the non-transformed, IDF, and TFIDF document matrices underwent a length-normalization transform, where each document vector was inversely scaled by its L2 norm. L2 normalization has been argued to be important for good SVM performance [ ]. \n  \nPCA: The above matrices were run through a Principal Component Analysis (PCA) dimensionality reduction step. Projections onto the first 100, 200, 400, 600, 800, and 1000 components were tested. \n  \n\nFeature transforms can improve classification performance by making the surfaces that separate documents in different classes more linear and by decreasing the weight of non-discriminating features. PCA, on the other hand, reduces the number of parameters that need to be estimated from training data. If class membership information is contained in the subspace spanned by the largest principal components, then this kind of dimensionality reduction can improve generalization performance by reducing noise and model complexity. \n\n\n### Performance evaluation \n  \nThe abstract and sentence corpora described above were used both for training classifiers and for estimating generalization performance on out-of-sample documents. In order to estimate out-of-sample performance, we used the following cross-validation procedure for each possible classifier and feature transform:\n   \nEach corpus was randomly partitioned into 4 document folds (75%\u201325% splits). This was repeated 4 times, yielding 16   outer folds  . All classifiers and transforms were evaluated using the same partitions. \n  \nFor each fold, the 75% split was treated as the \u2018training\u2019 split and the 25% split was treated as the \u2018testing split\u2019. If a feature transform was used, it was applied to both splits but was computed using statistics (such as IDF or principal components) from the training split. Finally, classifiers were trained on the training split and evaluated based on their prediction performance on the testing split. \n  \nMeasures of classification performance (see below) on the testing split were collected. The 16 sets of performance measures were averaged to produce an estimate of generalization performance. \n  \n\nBecause training and testing documents are always separated, for each cross-validation fold the above procedure is equivalent to calculating performance on an independent testing corpus. \n\nExcept for VTT, the classifiers listed in section \u201cClassifiers\u201d used cross-validated regularization parameters. These parameters were not chosen using cross-validation on the outer folds because this would lead to a biased estimate of out-of-sample performance. Instead, regularization parameters were chosen using nested cross-validation within each of the 75% blocks of the above outer folds:\n   \nThe 75%-block was itself partitioned into 4 folds (75%\u201325% splits of the outer 75% block). This is repeated 4 times, producing a total of 16   inner folds   for each outer fold training split. \n  \nOver a range of values of the cross-validated parameter, the procedure described in step 2 above was used, but now applied to the 75%/25% splits of each   inner fold  . Mean performance on inner fold testing splits were measured using the Matthews Correlation Coefficient [ ] (MCC), which is particularly well-suited for the unbalanced scenarios of our corpora [ ]. \n  \nThe parameter value giving the highest mean MCC was chosen as the regularization parameter value for training the classifier in the outer fold. \n  \n\nWe evaluated the performance of the classifiers using three different measures: the balanced F1 score (the harmonic mean of precision and recall), the iAUC or \u2018area under the interpolated precision/recall curve\u2019 [ ], and the MCC. In addition, we computed and reported the rank product of these three measures (RP3) as a single inclusive metric of classification performance. The RP3 measure provides a well-rounded assessment of classifier performance, as it combines the ranking of the different individual measures [ ,  ]. \n\nFor displaying results, we focus primarily on the iAUC measure (in cases where only plots of iAUC performance are provided, F1 and MCC plots are found in the Supporting Information,  ). iAUC does not depend on predicted class assignments but rather on the ranking of test set documents according to classifier confidence scores from most relevant to most irrelevant. iAUC offers three major advantages as a measure of classification performance. First, it provides a more comprehensive measure of classifier performance because it evaluates the entire ranking of documents, as opposed to just class assignments. Second, iAUC is less sensitive to variation driven by random-sampling differences in the training corpus, which may lead to fluctuations in the class assignments of low confidence documents and, correspondingly, high variability in measures such as F1 and MCC. Finally, it is more relevant in a frequently-encountered situation where a human practitioner uses a BLM pipeline to retrieve only the most relevant documents (which should have high positive-class confidence scores) or to identify likely-to-be-misclassified documents (which should have low confidence scores). \n\nBoth the abstract and sentence classification tasks are characterized by imbalanced datasets, with more relevant-class abstracts and more irrelevant-class sentences respectively. For simplicity, and because we are primarily concerned how ranking performance (as measured by iAUC) changes between different machine learning configurations on the same dataset, we do not perform resampling or re-weighting of training items. We also report MCC values, a measure which is known to be stable in the face of unbalanced classes [ ]. \n\nThe performance of a classifier and feature transform configuration varies both due to random sampling of folds and due to the inherent performance bias of the configuration over the entire distribution of folds. Since we are only interested in the latter, observed performance differences between pairs of configurations were tested for statistical significance using a non-parametric paired-sample permutation test. First, the assignments of performance scores for each of the 16 outer folds were permuted between the two classifier/transform configurations under consideration. For each of the 2  possible permutations, the difference in across-fold mean performance was calculated; this formed the distribution of performance differences under the null hypothesis that the two configurations have equal performance. Finally, the   p  -value was computed as the probability (one- or two-tailed, as indicated) of observing a difference under the null hypothesis distribution equal to or greater than the actual difference. \n\n\n\n## Results \n  \n### Abstract classification performance \n  \n shows classifier performance on the abstract task for the unigram and bigram runs with no feature transform applied. The best classifier configuration, as well as those configurations not significantly different from the best (  p  >0.05, one-tailed test), are marked with an asterisk. In addition, the performance results, ranks, and the rank-product (RP3) measure are reported in  . The best classifier achieves F1\u22480.93, iAUC\u22480.98, MCC\u22480.73, which constitutes a substantial and significant improvement over our previous preliminary results reported in [ ], where we had reached F1\u22480.8, iAUC\u22480.88, MCC\u22480.6 (notice that these performance values would be well below the lowest reported levels in  ). This demonstrates that the corpus used in this work\u2014which is more carefully curated and now also considers interactions between drugs and food, fruit, smoking, alcohol, and natural products to be relevant (details in \u201c  Section\u201d)\u2014improves the classification of abstracts with pharmacokinetic evidence of DDI. The levels of performance achieved are excellent when compared to similar abstract classification tasks in other biomedical domains. For instance, in the BioCreative Challenge III, considered one of the premier forums for assessment of text mining methods, the best classifiers of abstracts with Protein-Protein Interaction yielded performances of F1\u22480.61, iAUC\u22480.68, MCC\u22480.55 [ ]. Naturally, our results are not directly comparable to results obtained on different corpora and on a different problem; rather, these numbers provide guidance on what is typically considered good results in biomedical article classification. \n   Classification performance on abstracts.  \nPerformance for both unigram and bigram runs on non-transformed features. Left: F1 measure. Middle: MCC measure. Right: iAUC measure. The best classifier configuration, and configurations not significantly different (  p  >0.05, one-tailed test) from it, marked with asterisk \u2018*\u2019. \n     Classification performance on abstracts.  \nPerformance for both unigram and bigram runs on non-transformed features according to F1, MCC, and iAUC performance measures. The rank of the classifiers according to each measure is reported in parenthesis in the respective column. Classifiers are ordered according to the rank product (RP3) of the three measures (last column). \n    \nFor each classifier, the inclusion of bigram features improved performance according to the RP3 measure. The best classifier according to all measures was LDA using bigrams. The performance of this classifier was significantly better than all others for the MCC, but not significantly better that Logistic Regression according to iAUC, and not significantly better that Logistic Regression and SVM according to F1 score. According to RP3, these three classifiers using bigrams yield the best performance. Naive Bayes, VTT, and dLDA\u2014classifiers that make a \u2018naive\u2019 independence assumption about features (see \u201c \u201d section)\u2014performed below the top three. However, the performance levels they achieved are still quite high, which indicates that such simple classifiers are also capable of classifying documents with pharmacokinetic DDI evidence in our corpus. The in-house VTT classifier is the only classifier among these that does not use cross-validated parameters; when used with NER features and cross-validated parameters (the configuration for which it was originally designed [ ,  ,  ]), its performance improved (see below). \n\n#### Feature Transforms and Dimensionality Reduction \n  \nThe different feature transforms and PCA-based dimensionality reductions (section \u201c \u201d) significantly improved performance for several classifiers, though they could not beat the performance of the best non-transformed classifier. Details are provided in Supporting Information (section 1.1 in  ). To summarize, according to most measures only dLDA and SVM improved performance significantly with either an IDF or TFIDF transform plus L2 normalization and dimensionality reduction (top   n   principal components). For instance, the best iAUC for SVM (0.984) occurs with a dimensionality reduction to the top 800 principal components and no feature transform; this is a significant improvement over the no-transform, no dimensionality reduction SVM classifier reported in   and  , but not a significant improvement over the overall best classifiers reported there (LDA and Logistic regression). The dLDA classifier significantly improves its iAUC performance with almost all feature transform and dimensionality reduction combinations, but not above that of the top performing classifiers. We conclude that feature transforms and dimensionality reduction does not lead to the best classification performance on the abstract task. \n\n\n#### Pharmacokinetics DDI Features in abstract classification \n  \nWe looked at which textual features play the largest role in the abstract classification task. A linear classifier separates document classes with a hyperplane defined by a set of feature coefficients. The impact of a feature on classification is quantified by the sign and amplitude of its hyperplane coefficient. A feature with a large positive coefficient contributes strongly to a document\u2019s propensity to be classified as relevant, while a feature with a large negative coefficient contributes strongly to a document\u2019s propensity to be classified as irrelevant. In  , we show the top 20 most distinctive features of the relevant and irrelevant classes in the abstract task, as chosen in the bigrams runs by the LDA classifier (left) and Logistic Regression classifier (right), the two top-performing classifiers in this task according to the RP3 measure (see  ). Notice that textual features are stemmed. \n   Top 20 relevant and irrelevant abstract features.  \nThe stemmed textual features most discriminative of relevant and irrelevant classes on the abstract task, as chosen by two of the top-performing classifiers according to the RP3 measure: LDA with bigrams (left) and Logistic Regression with bigrams (right). \n    \nSome of the most relevant features come from MeSH term metadata (such as the MeSH term   Drug interactions  ) and terms that explicitly indicate interactions (\u2018interact\u2019, \u2018inhibit\u2019, \u2018interact between\u2019, \u2018decreas\u2019, \u2018increas\u2019). Other relevant terms deal with administration protocols and study design (\u2018oral\u2019, \u2018day\u2019, \u2018receiv\u2019, \u2018mg\u2019, \u2018treatment\u2019, \u2018alon\u2019, \u2018combin\u2019). Some of the irrelevant features concern genetics terminology (\u2018allel\u2019, \u2018genotyp\u2019, \u2018polymorph\u2019, and MeSH term   Phenotype  ), indicating that the irrelevant class was enriched with genetics or pharmacogenetics vocabulary. Several generic biomedical terms (such as \u2018patient\u2019, \u2018healthi subject\u2019, \u2018higher\u2019) terms are also highly irrelevant. In addition, highly irrelevant features also contain some non-DDI-specific pharmacokinetic terms (for example, \u2018area\u2019, \u2018rate\u2019, \u2018clearance of\u2019), which is not surprising given that both relevant and irrelevant articles were drawn from pharmacokinetics-related literature. One surprising result is the observation that while the MeSH term   Mal  e is one of the top relevant features, the MeSH term   Female   is one of the top irrelevant features. We have no explanation for the cause of this gender imbalance since the corpus was built from automatic searches to PubMed without any gender-specific query terms. \n\nFurther analysis of highly relevant and irrelevant features across all classifiers and feature transforms was performed and reported in the Supporting Information (section 1.2 in  ). We quantified and plotted the contribution of standardized coefficients [ ] of different features and show the most positively and negatively loaded features for different classifier and transform configurations. Top textual features obtained from all classifiers include additional terms falling under the categories described above, with features derived from PubMed metadata (MeSH, chemical substances) also appearing among both the most relevant and irrelevant sets. Other relevant MeSH terms, besides   Drug Interactions  , include   Cimetidine/pharmacology  ,   Cross-Over Studies  ,   Enzyme Inhibitors/PK  ,   Kinetics  , and   Proton Pump Inhibitors  . Additionally, a PubMed author entry corresponding to a prominent researcher in the pharmacokinetics DDI field (\u2018PJ Neuvonen\u2019) appears as highly relevant, as well as three substances from the RN field (see also section \u201cImpact of NER and PubMed metadata on abstract classification\u201d below):   Cimetidine  ,   Enzyme Inhibitors  , and   Proton Pump Inhibitors  . For the irrelevant set, additional MeSH (  Anti-Ulcer Agents/adm&dos  ;   Injections, Intravenous  ;   Phenotype  ;   Protein Binding  ;   Reference Values  ) and Substance terms also appear (  Anti-ulcer agents  ;   Hydrocarb. Hydroxylas  ). The Supporting Information   contains details of the analysis and lists of features. It also shows the results of a Principal Component Analysis of feature weight coefficients chosen by different classifiers. \n\n\n#### Impact of NER and PubMed metadata on abstract classification \n  \nWe have previously demonstrated improved classification performance on protein-protein interaction BLM tasks by supplementing textual features (such as the word unigram and bigram occurrences) with features built using   Named Entity Recognition   (NER) and domain-specific   dictionary   tools [ ,  ,  ]. To test if similar techniques are useful in the DDI domain, we counted mentions of named biochemical species (e.g. proteins, compounds and drugs) and concepts (e.g. pharmacokinetic terms) in each document and then included these counts as document features in addition to the bigram and unigram textual features. Counts were extracted using biomedical-specific NER extraction tools and dictionaries, with dictionary matches identified by internally-developed software. A preliminary study of the impact of NER/Dictionary features was reported in [ ] using a previous less-refined DDI corpus. Here, in addition to using the more fine-tuned corpus (see \u201cMethods and Data\u201d section), we study the impact of PubMed metadata features on classification performance. We also provide a new comprehensive analysis of the performance impact of including features from several publicly-available NER and metadata resources:\n   \nOSCAR4 [ ]: NER tool for chemical species, reaction names, enzymes, chemical prefixes and adjectives. \n  \nABNER [ ]: NER tool for genes, proteins, cell lines and cell types. \n  \nBICEPP [ ]: NER tool for clinical characteristics associated with drugs. \n  \n DrugBank   database [ ]: a dictionary list of drug names \n  \n Dictionaries   provided by Li\u2019s lab.   i-CYPS  : cytochrome P450 [CYP] protein names, a group of enzymes centrally involved in drug metabolism;   i-PkParams  : terms relevant to pharmacokinetic parameters and studies;   i-Transporters  : proteins involved in transport;   i-Drugs  : Food and Drug Administration\u2019s drug names. The dictionaries are available for download from [ ]. \n  \nFor each of these NER tools and dictionaries, we counted the number of occurrences of any of its entities/entries in a given abstract. These counts were treated as any other feature for SVM, Logistic Regression, diagonal LDA, and LDA classifiers. Naive Bayes was omitted since NER count features are non-binary. VTT incorporates NER features via a modified separating hyperplane equation:\n \nwhere   x   represents the occurrence of textual feature   i  ,   \u03c6   and   \u03bb   are textual feature and bias parameters as described in section \u201cClassifiers\u201d,   c   is the count of NER/Dictionary feature from resource   j  , and   \u03b2   is a weight for resource   j  , which is chosen by cross-validation. \n\nIn   (left), we plot the relative iAUC changes over the respective classifiers without NER/Dictionary count features (results for MCC and F1 in Supporting Information; section 1.3 in  ). Significant performance changes are indicated with an asterisk (  p  <0.05, two-tailed test). Some NER/Dictionary features improved performance significantly for several classifiers. However, the inclusion of two dictionary features (  DrugBank  , and   i-CYPS  ) actually decreased performance significantly for several classifiers, suggesting that these features contain little class information and instead contribute to over-fitting.   lists performance for configurations in which NER and dictionary features gave a significant performance increase for at least one of the three measures (F1, MCC, or iAUC), along with best classifier performance using only textual features (bigram runs). The BICEPP tool consistently yielded the best improvement for every classifier tested, followed by the   i-Drugs   dictionary. The OSCAR4 tool also significantly improved the performance of the VTT classifier (especially for the MCC measure as shown in Supporting Information,  ). With the inclusion of NER and dictionary features, the overall top classifiers (LDA and Logistic Regression), significantly improved their performance, now reaching F1\u22480.93, MCC\u22480.74, iAUC\u22480.99. Among the set of naive classifiers, VTT improved performance significantly with the inclusion of NER features, ranking above the other naive classifiers according to the RP3 measure. \n   Performance impact of abstract NER and metadata features.  \nLeft: Relative changes in iAUC scores on non-transformed bigram runs in combination with different NER/Dictionary features. Significant changes (  p  <0.05, two-tailed test) in performance over the respective classifiers without NER features are indicated with asterisk \u2018*\u2019. Right: Relative changes in iAUC when features from a given PubMed metadata field are included versus omitted (while including features from the other 4 metadata fields). Significant changes (  p  <0.05, two-tailed test) in performance are indicated with asterisk \u2018*\u2019. \n     Abstract classification performance using NER features.  \nPerformance of the best classifiers when specific NER and dictionary features are added; original (bigram runs) classifiers also listed with no NER features (indicated by -). F1, MCC, and iAUC performance measures are listed; the rank of the classifiers according to each measure is reported in parenthesis in the respective column. Classifiers are ordered according to the rank product (RP3) of the three measures (last column). \n    \nAs mentioned, word unigram and bigram features were extracted not only from article abstracts and titles, but also from five PubMed metadata fields: author names, journal titles, MeSH terms, and two fields referring to standardized substance names: the \u2018registry number/EC number\u2019 [RN] field and the \u2018secondary source\u2019 field [SI]. In fact, some PubMed metadata features were among those most distinguishing of relevant and irrelevant abstracts (for greater detail, see   and section \u201cPharmacokinetics DDI Features in abstract classification\u201d, as well Supporting Information; section 1.2 in  ). We tested the impact of PubMed metadata fields on abstract classification performance. In   (right), we plot the relative iAUC changes when features from a given PubMed metadata field are included versus omitted (while including features from the other 4 metadata fields). Significant changes (  p  <0.05, two-tailed test) in performance are indicated with an asterisk; results for MCC and F1 can be found in Supporting Information ( ). MeSH terms was the only metadata source whose omission decreased performance significantly. However, the performance increase of including MeSH data is rather small. Therefore, the methodology does not require the availability of human-annotated metadata such as MeSH terms and can still be deployed on recent articles that have not yet been annotated with MeSH terms. \n\n\n\n### Evidence sentence extraction performance \n  \n shows classification performance on the sentence task of the unigram and bigram runs without any feature transforms applied, according to F1, MCC, and iAUC measures. The best classifier configuration, as well as those configurations not significantly different from the best (  p  >0.05, one-tailed test), are marked with an asterisk. In addition, the numerical results, ranks, and the rank-product (RP3) measure are reported in  . \n   Sentence classification performance.  \nPerformance for both unigram and bigram runs on non-transformed features. Left: F1 measure. Middle: MCC measure. Right: iAUC measure. The best classifier configuration, and configurations not significantly different (  p  >0.05, one-tailed test) from it, marked with asterisk \u2018*\u2019. \n     Sentence classification performance.  \nPerformance for both unigram and bigram runs on non-transformed features according to F1, MCC, and iAUC performance measures. The rank of the classifiers according to each measure is reported in parenthesis in the respective column. Classifiers are ordered according to the rank product (RP3) of the three measures (last column). \n    \nAs with abstracts, including bigram features tended to improve sentence classification performance. LDA performed best, having the highest RP3 and being the best classifier according to the F1 and MCC measures and one of the two best classifiers (along with SVM) on the iAUC measure. Generally, the classifiers that performed well on the sentence task were those that took into account feature covariances: SVM, Logistic Regression, and LDA. The top classifier (LDA with bigrams) on the evidence sentence task reached performance of F1\u22480.75, MCC\u22480.64, iAUC\u22480.83. \n\nWe measured sentence classification performance in combination with different feature transforms and dimensionality reductions (see section 2.1 in  ). In general, the three classifiers that do best on non-transformed features (SVM, Logistic Regression, and LDA) show decreased performance with dimensionality reduction according to all measures, with more extreme dimensionality reduction leading to larger performance decreases. On the other hand, for dLDA (a \u2018naive\u2019 classifier that treats features as independent), PCA-based dimensionality reduction\u2014which uses feature covariances to choose optimal projections\u2014led to significant improvements in all measures, with more dimensions giving better performance. These findings indicate that the pattern of feature covariance carries important information about class membership in the sentence task, and that this pattern is distributed across a large number of dimensions. Generally, the LDA classifier achieved the best performance according to all three measures. Its baseline performance according to the iAUC measure was further improved significantly by an IDF-transform, and\u2014according to the F1 measure\u2014by any transform containing an L2 normalization. \n\nIn  , we show the top 20 features most distinctive of the relevant and irrelevant classes in the sentence task as chosen by the LDA classifier on bigrams (the top performing classifier according to the RP3 measure; features of other top classifiers are not shown because they were highly similar). Numerical features (indicated by \u2018#.#\u2019) were highly indicative of the relevant class, along with expressions of quantitative changes (\u2018decreas\u2019, \u2018increas\u2019) and interaction (\u2018inhibit\u2019, \u2018catalyz\u2019, \u2018interact with\u2019) as well as adverbs expressing significance of evidence (\u2018significantli\u2019). Also highly relevant were features referring to the area under the concentration-time curve (\u2018auc\u2019), which is often employed in pharmacokinetics to measure differences in drug clearance rates under different experimental conditions. Names of several drugs (\u2018ketoconazol\u2019, \u2018itraconazol\u2019, \u2018quiindin\u2019) were relevant in predicting DDI evidence sentences. These drugs are frequently used probe inhibitors for metabolism enzymes CYP3A4/5, CYP3A4/5 and CYP2D6 respectively and are routinely used in drug interaction studies. \n   Top 20 relevant and irrelevant sentence features.  \nThe most discriminative features of relevant and irrelevant classes in the sentence task, as chosen by the top-performing classifiers according to the RP3 measure: LDA on bigrams. \n    \nHighly irrelevant features refer to more generic pharmacokinetic or biomedical concepts such as \u2018investig\u2019, \u2018dose\u2019, \u2018enzym\u2019, \u2018studi\u2019, etc. Interestingly, some terms that are highly relevant in the abstract task are highly irrelevant in the sentence task (e.g., \u2018day\u2019). Notably, the unigram \u2018interact\u2019 is highly irrelevant for sentences, whereas the bigram \u2018interact with\u2019 is highly relevant. This may be because all sentences in this corpus come from abstracts containing pharmacokinetic DDI evidence (see \u201c \u201d section). Thus, general administration protocols and drug interaction terms are likely to occur in the abstract as a whole but not necessarily in the evidence sentences that actually report outcomes of the pharmacokinetic drug interaction experiments. Similar patterns are observed in the more extensive analysis provided in Supporting Information (section 2.2 in  ), where relevant and irrelevant features are analyzed across a wide range of classifier and feature transform configurations. There we also show the results of a Principal Component Analysis of feature weight coefficients chosen by different classifiers. \n\nFinally, we tested the impact of additional features on sentence classification. Though there is no metadata available in the sentence corpus, features from NER tools can still be computed. Six NER features were tested:   BICEPP, DrugBank, i-CYPS, i-Drugs, i-PkParams, i-Transporters   (see section \u201cImpact of NER and PubMed metadata on abstract classification\u201d for details). As before, we counted mentions of named biochemical species and concepts specified by different NER tools in each sentence and then included such counts as sentence features in addition to the bigram and unigram textual features.   shows relative iAUC changes when features from each of these NER tools were included. Significant improvements (  p  <0.05, two-tailed test) above the corresponding classifier\u2019s performance without NER features are indicated by an asterisk; performance according to MCC and F1 measures is shown in Supporting Information (section 2.3 of  ). Notice that Naive Bayes was omitted since NER count features are non-binary. \n   Performance impact of sentence NER features.  \nRelative changes in iAUC scores on sentence bigram runs (without transforms or dimensionality reductions) in combination with different NER features. Significant changes (  p  <0.05, two-tailed test) in performance over respective classifiers without NER features are indicated with asterisk \u2018*\u2019. \n  \nAs in the abstract task, a few NER/Dictionary features improved performance for several classifiers. The iAUC scores of nearly all classifiers were significantly improved by three NER features: BICEPP,   DrugBank  , and our internally developed   i-Drugs   dictionary. These three features represent counts of drugs names, showing that drug name counts are helpful for classifying sentences as DDI-relevant vs. DDI-irrelevant. Use of features from the BICEPP tool yielded the largest improvement for every classifier.   lists the performance according to all measures for classifiers using the BICEPP features; also listed are the corresponding best classifiers using only textual features (bigram runs). The overall top classifiers (LDA and SVM) showed significantly improved performance with the inclusion of these NER features, reaching F1\u22480.76, MCC\u22480.65, iAUC\u22480.83. In addition, VTT performance improved significantly for all three measures with the inclusion of NER features. Here VTT with bigrams performs better than other naive classifiers, as expected given that this classifier was designed specifically to handle such NER features [ ,  ,  ]. In contrast, dLDA (another naive classifier) did not benefit much from the inclusion of NER features. \n   Sentence classification performance with NER features.  \nPerformance of different sentence classifiers with the count features obtained via the BICEPP NER tool; also listed are the corresponding best classifiers using only textual features (bigram runs; indicated by -). F1, MCC, and iAUC performance measures are listed; the rank of the classifiers according to each measure is reported in parenthesis in the respective column. Classifiers are ordered according to the rank product (RP3) of the three measures (last column). \n    \n\n\n## Discussion \n  \nWe have demonstrated that current BLM methods for text classification can reliably identify PubMed abstracts containing pharmacokinetic evidence of drug-drug interactions, as well as extract specific sentences that mention such evidence from relevant abstracts. The performance reached on a corpus of carefully annotated pharmacokinetics literature is quite high for both abstract classification (reaching F1\u22480.93, MCC\u22480.74, iAUC\u22480.99) and evidence sentence extraction (F1\u22480.76, MCC\u22480.65, iAUC\u22480.83). To explore the capability of BLM in the pharmacokinetics DDI context, where there are no existing directly-relevant corpora or experiments, we pursued a thorough comparison of the performance of several linear classifiers using different combinations of unigrams, bigrams, PubMed metadata, and NER features. We also tested the effects of applying feature transforms and dimensionality reduction. \n\nFrom a classification performance perspective, some results are noteworthy: in terms of textual features, bigrams in combination with unigrams performed significantly better than unigrams alone. However, performance in unigram versus bigram runs for the same classifier differed by no more than one percent for iAUC and MCC. Thus, while bigram features did contain some additional information about class membership, the amount of this information was not large. \n\nIn our experiments, feature transforms and PCA-based dimensionality reduction significantly improved performance for several classifiers (especially \u201cnaive\u201d classifiers such as dLDA, which assume feature independence), but did not significantly improve the overall best performance. We also found that a sophisticated version of the LDA classifier dominated performance in both the abstract and sentence tasks. This classifier used SVD to eliminate rank-deficiency in the feature occurrence matrices and performed shrinkage of the feature covariance matrix for regularization (see \u201c \u201d). \n\nFrom the drug-interaction domain perspective, feature analysis in the abstract task revealed that pharmacokinetic DDI evidence in the literature is highly correlated with terms that explicitly indicate interaction (including MeSH terms), enzyme inhibitors (including substance names via the RN metadata field in PubMed), DDI administration protocols, and study design. At the sentence level, drug interaction evidence from a pharmacokinetics perspective is highly correlated with terms that express experimental results, such as numerical values, measures of drug clearance, expressions of quantitative changes, as well as adverbs expressing significance of evidence. Feature analysis at the abstract level also revealed that lack of DDI evidence in the pharmacokinetics literature (irrelevant class) is highly correlated with some terms from PubMed metadata fields, as well as those pertaining to genomic or general medical terminology. At the sentence level, sentences in relevant abstracts but without DDI evidence tend to include terminology relevant to pharmacokinetics protocols, as well as more generic interaction discourse or biomedical concepts. \n\nSince many important features came from PubMed metadata fields, we looked at changes in iAUC scores when features from different PubMed metadata fields were omitted. We found that only the omission of MeSH terms significantly affected abstract classification performance. Nonetheless, while statistically significant, the drop in performance was rather small (affecting only millesimals of the iAUC, iAUC\u22480.98 without), indicating that abstract classification does not depend strongly on the inclusion of MeSH term features. This is an important consideration since MeSH terms may not be immediately added to publications, with statistics indicating that only 50% of citations are annotated within 60 days of inclusion in PubMed [ ]. Therefore, classification and evidence extraction from brand new articles should not rely on such metadata. \n\nWe also tested the effect of including features extracted using named entity recognition (NER) and dictionary tools, namely those for detecting possibly-relevant chemical, genomic, metabolomic, drug, and pharmacokinetic entities. Generally, dictionaries like BICEPP,   i-Drugs  , and DrugBank, which counted the number of times drug names appeared, significantly improved performance for several classifiers on both the abstract classification and evidence sentence extraction tasks (an exception to this was the lack of improvement on abstracts when including DrugBank features, an effect that needs further investigation). Nonetheless, as for MESH term features in abstract classification, the resulting performance increases were modest, even if statistically significant. This again demonstrates that relevant-class information can be extracted from abstracts and sentences using solely the statistics of unigram and bigram textual features. \n\nNotably, relevant and irrelevant documents and sentences both derive from the pharmacokinetics literature and therefore share similar feature statistics. This makes distinguishing between them a nontrivial text classification problem, though also a more practically relevant one (e.g. for a researcher who needs to automatically label potentially relevant documents retrieved from PubMed). Nonetheless, several classifiers reached high performance; for example, the abstract ranking performance (iAUC\u22480.99) has little room for further improvement, though the classification performance\u2014while high for this type of problem\u2014can still be improved. \n\nWe observed that many different pipeline configurations reached near-optimal performance. Even though some performance differences between configurations were statistically significant, they were small. For instance, iAUC differences between best and worst classifiers varied by no more than 1 percent in the abstract task and 5 percent in the sentence task. This demonstrates that classification performance in our experiments was robust to the classifier utilized, and that a BLM pipeline for this problem would do similarly well independently of classifier chosen. In particular, while \u201cnon-naive\u201d classifiers (which consider feature covariances) performed better than naive classifiers, the latter are still capable of competitive performance. These results suggest a fundamental limit on the amount of statistical signal present in the labels and feature distributions of the corpora as extractable by linear classifiers. However, it is worth noticing that an analysis of both abstract- and sentence-trained feature weight coefficients shows systematic differences between weights selected by naive and non-naive classifiers (see Supporting Information,  ), indicating that different classifiers emphasize distinct semantic features. Furthermore, it is possible that performance could be improved by the use of non-linear classifiers or features produced by more finely DDI-tuned NER tools, relation extraction or NLP methods, or other sophisticated feature-generation techniques. Indeed, the larger performance variation observed in the sentence task suggests that sentence extraction performance may improve with larger amounts of training data (which would permit better estimates of feature covariances). \n\nIt is not trivial to compare our performance results with those previously reported in the literature. First, there is no gold standard for DDI evidence sentence extraction, especially for a specific evidence-type such as pharmacokinetics. Second, most sentence extraction tasks in the biomedical domain involve extraction of passages which can contain several sentences (e.g. the protein-protein interaction subtask in Biocreative II) or passages relevant for a set of specific targets (e.g. Gene Ontology annotations for specific gene names in Biocreative I [ ] and IV [ ]). Due to these difficulties, the performance on those tasks has been comparatively low, e.g. in BioCreative IV the best F1 score in the gene ontology evidence extraction task was 0.27 [ ] (in Biocreative II, due to possible overlap and multiple accepted passages, the preferred performance measure was the mean reciprocal rank which reached 0.87 [ ,  ]). Considering that our performance on the sentence task is higher than what is typically reported for the abstract classification in the biomedical domain (e.g. PPI abstract classification in the BioCreative Challenge III reached F1\u22480.61, MCC\u22480.55, iAUC\u22480.68 [ ]), the classifiers trained on our sentence corpus reached a very good level of performance, indicating that the corpus is well annotated and that the task is highly feasible. Given the performance of our approach in extracting pharmacokinetic evidence, the classification methodology and associated corpus may be useful in the previously explored task of extracting interacting drug pairs from the literature. For example, it may be more effective to first identify DDI sentences containing specific types of evidence and then extract the interacting drug names from them, using automated methods or human expertise tailored to that specific type of DDI evidence. \n\nTo conclude, we provide a thorough report of the capability of linear classifiers to automatically extract pharmacokinetics evidence of DDI from an abstract- and sentence-level annotated corpus. Given the high performance observed on both abstract and sentence classification for all classifiers, including the simplest ones, we conclude that under realistic classification scenarios automatic BLM techniques can identify PubMed abstracts reporting DDI backed by pharmacokinetic evidence, as well as extract evidence sentences from relevant abstracts. These results are important because pharmacokinetic evidence can be essential in identifying causal mechanics of putative DDI and as input for further pharmacological and pharmaco-epidemiology investigation. More generally, our work shows that BLM can be safely included in DDI discovery pipelines where attention to distinct types of evidence is necessary. In future work, we intend to use our methodology to mine large corpora for both pharmacokinetic and other types of DDI experimental evidence. Such evidence can help fill knowledge gaps that exist in the DDI domain, with the ultimate goal of reducing the incidence of adverse drug reactions and contributing to the development of alternative safe treatments. \n\n\n## Supporting Information \n  \n \n", "metadata": {"pmcid": 4427505, "text_md5": "71253671a267bc3f71b0086d8af2dda6", "field_positions": {"authors": [0, 87], "journal": [88, 96], "publication_year": [98, 102], "title": [113, 197], "keywords": [211, 211], "abstract": [224, 2510], "body": [2519, 62819]}, "batch": 1, "pmid": 25961290, "doi": "10.1371/journal.pone.0122199", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4427505", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4427505"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4427505\">4427505</a>", "list_title": "PMC4427505  Extraction of Pharmacokinetic Evidence of Drug\u2013Drug Interactions from the Literature"}
{"text": "Wang, Ludi and Gao, Yang and Chen, Xueqing and Cui, Wenjuan and Zhou, Yuanchun and Luo, Xinying and Xu, Shuaishuai and Du, Yi and Wang, Bin\nSci Data, 2023\n\n# Title\n\nA corpus of CO2 electrocatalytic reduction process extracted from the scientific literature\n\n# Keywords\n\nElectrocatalysis\nScientific data\n\n\n# Abstract\n \nThe electrocatalytic CO  reduction process has gained enormous attention for both\u00a0environmental protection and chemicals production. Thereinto, the design of new electrocatalysts with high activity and selectivity can draw inspiration from the abundant scientific literature. An annotated and verified corpus made from massive literature can assist the development of natural language processing (NLP) models, which can offer insight to help guide the\u00a0understanding of these underlying mechanisms. To facilitate data mining in this direction, we present a benchmark corpus of 6,086 records manually extracted from 835 electrocatalytic publications, along with an extended corpus with 145,179 records in this article. In this corpus, nine types of knowledge such as material, regulation method, product, faradaic efficiency, cell setup, electrolyte, synthesis method, current density, and voltage are provided by either annotating or extracting. Machine learning algorithms can be applied to the corpus to help scientists find new and effective electrocatalysts. Furthermore, researchers familiar with NLP can use this corpus to design domain-specific named entity recognition (NER) models. \n \n\n# Body\n \n## Background & Summary \n  \nElectrocatalysis has garnered much attention in reducing fossil fuel consumption, decreasing greenhouse gas emissions, and producing sustainable fuels and chemicals . Critical to realizing these goals is the development of improved electrocatalysts with high activity and selectivity for the target product. In general, the property of catalysts depends on their compositions, structures, and regulation methods , and thus there is enormous synthesis and regulation space for catalyst exploration. Although extensive efforts have been devoted to the design and development of novel electrocatalysts , most of the previous exploration is based on heuristics and experience and still lacks effective design guidelines. Furthermore, it seems unreasonable to conduct enough attempts to cover a majority of the synthesis and regulation space to explore novel catalysts, even with the aid of high-throughput synthesis techniques. \n\nThe establishment of realm-specific datasets is a crucial step to promote the development of catalysts. A few existing catalyst datasets are built from density functional theory (DFT) calculations and mainly encompass features related to surface adsorption and electronic structure . Researchers in catalytic science have proposed various kinds of descriptors for catalyst screening through a mass of calculations . However, the real surface structure of catalysts is not ideal as theoretical calculations assume and is fairly complex, thus lowering the reliability of these datasets for catalyst design. In fact, an enormous amount of knowledge has been hidden in a large volume of scientific publications. If the concerned information related to catalysts can be extracted and collected into datasets, the efficiency of developing new catalysts can be greatly improved. \n\nCompositions, structures, regulation methods, and properties that can describe specific catalysts generally exist in the unstructured and heterogeneous form of scientific literature. Data-driven approaches exhibit great potential to deal with these data. These approaches can complement experimental and theoretical studies and have been successfully applied in materials discovery , material synthesis approaches , and the interpretation of experimental spectra . However, manual extraction of these data is nearly impractical and costs too much labor . Natural language processing (NLP) and text-mining approaches have made great progress in the past decades, and various cutting-edge tools have been employed in biology, chemistry, and materials science . \n\nIn this data descriptor, we present an open-source corpus of electrocatalytic CO  reduction. This database contains two types of corpus: (1) the benchmark corpus, which is a collection of 6,086 records extracted from 835 publications by catalysis postgraduates; (2) the extended corpus, which includes 145,179 records extracted from the full text of the 372 literature by intelligent model. In the benchmark corpus, we extracted nine types of knowledge, including material, regulation method, product, faradaic efficiency, cell setup, electrolyte, synthesis method, current density, and voltage. The extended corpus contains four types: material, regulation method, product, and\u00a0faradaic efficiency. Moreover, the extended corpus was evaluated and revised by domain experts. A schematic of the pipeline devised for this extraction is shown in Fig.\u00a0 .   \nExtract pipes and samples. Top panel: Schematic diagram of standard text mining pipeline: (i) Collect papers by keyword search; (ii) Expert notes to build a benchmark corpus; (iii) Extract key information of the synthesis process and build an extended corpus; (iv) Stored in a database for future data mining. Bottom panel: Sample entities extracted from the summary. \n  \n\nThe advantage of the benchmark corpus is that it is a dataset annotated entirely by domain experts, thus the reliability and accuracy of its label can be guaranteed to a certain extent. Therefore, this kind of corpus can be used as a benchmark to guide the evaluation of NLP systems. The extended corpus, on the other hand, has the advantage of an automatic annotation system that can save the labor of manual annotation. Its extensive data resource can help experts to derive further information from it and provide guidance for some downstream tasks, such as faradaic efficiency prediction models. \n\n\n## Methods \n  \nIn the current work, we built a more advanced extraction pipeline (Fig.\u00a0 ) that combines manual annotations and various advanced machine learning and NLP techniques to extract complete data for CO  electrocatalytic reduction process from scientific literature. We first collected literature related to copper-based catalytic CO  reduction procedures following a series of progressively finer-meshed filters. Then according to predefined entity labels, we published a manually annotated benchmark corpus and an automatically annotated extended corpus. The final resulting dataset can be used for domain data mining and further downstream NLP tasks. Each of the steps is described in detail below. \n\n### Content acquisition \n  \nThe first step in the database generation workflow was using Web of Science to find the\u00a0DOIs of scientific literature that will be used in the following steps. Specifically, over 22,000 metadata of articles were exported from Web of Science using the keywords \u201cCO \u201d, \u201cReduction\u201d and \u201cElectro*\u201d as subject index, such as article title, article DOI, article abstract, etc. Web of Science provides filtering and export functions on the website. The metadata of literature exported is then filtered step-by-step according to rules defined by experts, with each step of the filtering process consisting of a simple regular expression query .The process of literature screening is illustrated in Fig.\u00a0 . The title of every article was queried for words starting with \u201celectro\u201d, followed by any number of characters or whitespace, which yielded 9,474 articles; The title of every article was queried for words \u201cCO \u201d, \u201ccarbon dioxide\u201d or \u201cCO(2)\u201d, which yielded 7125 articles; The title of every article was queried for words \u201cCu\u201d or \u201ccopper\u201d, which yielded 1637 articles; The title of every article was queried for words \u201cphoto\u201d or \u201cvisible\u201d and then removed, which yielded 1465 articles. Finally on this basis, combined with further manual screening by experts, 835 articles were obtained on experimental works related to the electrocatalytic reduction of CO  over copper-based catalysts.   \nThe process of literature screening. \n  \n\nAfter the filtered step, the related 835 publications were downloaded manually from the web according to their DOI. These publications were obtained through agreements with publishers Elsevier, the Royal Society of Chemistry, American Chemical Society, Wiley, Acta Physico-Chimica Sinica & University Chemistry Editorial Office (Peking University), MDPI, the Electrochemical Society, Springer Nature, Informa, Hindawi Limited, Frontiers Media SA, China Science Publishing & Media Ltd., IOP publishing, NACE International, Proceedings of the National Academy of Sciences, Shanghai Institute of Ceramics, American Institute of Physics, American Scientific Publishers, the Chemical Society of Japan, the Electrochemical Society of Japan, Journal of New Materials for Electrochemical Systems, HARD Publishing Company, Taylor & Francis, American Association for the Advancement of Science (AAAS), ESG, Sycamore Global Publications, from which we received permissions to download the articles. For each publisher, we manually identified all materials science related journals available for download. We acquired papers in PDF format, which include the full text of the article as well as its metadata such as article title, public year, authors, etc. After filtered step described above, we imported the related articles to AutoDive, our self-developed annotation tool, which allows experts to annotate on PDF format directly. \n\n\n### Full-text preparation \n  \nThe full papers were operated differently according to the way the different corpora were constructed. As the annotation tool AutoDive is an online annotation platform, it is only necessary to import the literature into the platform in PDF format, organised in their DOI order, so that the experts can annotate entities directly. The extended corpus contains automatically generated entities based on the full text of the collected articles above. We used a PDF parsing tool, PyMuPDF library , to automate the batch extraction processing of these literature data. Because the processed documents contained irrelevant markups, we developed a customized function for parsing article markup strings into text paragraphs while keeping the structure of paper and section headings. \n\n\n### Entity annotation \n  \nThe definition of regulation methods and related properties for the electrocatalytic reduction of CO  is the key challenge in constructing the benchmark corpus. A prerequisite for the manual annotation for the provided corpus was that annotators had to have a background in CO  electrocatalytic reduction to guarantee that the annotations are correct. Thus we invited 5 postgraduates with an average experience of at least 3 years in experimental catalysis from National Center for Nanoscience and Technology to do the work after the annotation tool training. \n\nAn easy-to-use annotation tool with graphical user interface which allows labeling of text efficiently and consistently is crucial and necessary. We found that on-site annotation in PDF format is an effective way after consulting domain experts. Thus, we explored alternative ways on how to present the documents to the annotator in a way that is supported by existing annotation tools. Finally we decided to adapt our own annotation tool, AutoDive, as the application for the construction of this corpus. AutoDive provides the label interface in the form of PDF, which can ensure the layout of the original documents that can keep the original habit of reading literature. This tool does not require local installation on the curators side and can be used through a web-browser to make the annotation process as easy and fast as possible. \n\nFigure\u00a0  provides a general flowchart of the annotation process. The main three steps of the annotation process are annotate, evaluate and revise:   \nOverview of the construction of the benchmark corpus process. \n  \n\n Stage 1: Annotate  . As mentioned before, we invited 5 annotators who have research background of electrocatalysis to annotate the entities with AutoDive followed by the guidelines. The documents are randomly and evenly assigned to these annotators by a senior expert. Three important things are emphasized in the annotation guidelines. The first is what kind of entity is needed to label. The second is the mention boundaries of those labels. The third is how to classify those mentions into label categories. \n\n Stage 2: Evaluate  . After manual annotation, we used multiple statistical methods to evaluate the quality of annotation results, such as distribution statistics, average statistics (for numerical value) and abnormal value statistics, etc. The evaluate results were provided to the senior expert for quality verification. The senior expert tagged the entity annotation which maybe incorrect. \n\n Stage 3: Revise  . The AutoDive tool can export the annotated data in CSV format, which is provided to annotators to revise and correct the mis-identified annotations and add missing entity label manually. \n\nThe annotation data underwent three rounds of modification in this project. Finally, we associated the all kinds of labeled entity and meta data of paper for further analysis, as well as to refine the annotation data. \n\n\n### Entity extraction \n  \nIn this corpus, we present nine types of entity labels, including material, regulation method, product, faradaic efficiency, cell setup, electrolyte, synthesis method, current density, and voltage. In addition, we provide a more detailed label subclass in some entity labels, such as material, regulation method and product. The description of label category is shown in Fig.\u00a0 , as well as the subclass of material, regulation method and product. For instance, when an annotator located one material that is described as electrocatalyst, he/she needs to specify what kind of this material is, such as Cu, Cu/C, CuO , etc.   \nNine kinds of label categories with three of them show specific subclasses. \n  \n\n\n### Construction of extended corpus \n  \nAs the manual annotation process is laborious, a lower quality corpus, also known as a   silver standard corpus   (SSC) , was constructed using automated techniques. In this paper, we generate an extended corpus according to the construction standard of the   silver standard corpus   (SSC). The main types of entities involved in the CO  electrocatalytic reduction process include materials, products, regulation methods and the corresponding Faraday efficiencies. The other physical information including cell setup, catalyst synthesis methods, current density and faradaic efficiency voltage are additional information about the CO  electrocatalytic reduction process and have less annotation information, so we did not extract these information in the automatically annotated extended dataset. A schematic representation of the procedure is shown in the bottom panel in Fig.\u00a0 . In the sections below, we provide a brief overview of the methods used for each step of the Entity extraction. \n\n#### Coarse-grained entity recognition \n  \nTo identify and extract coarse-grained category entities from the full text of the literature, we implemented a bidirectional short-term memory neural network with a conditional random field layer on top of it (BiLSTM-CRF) , which is able to recognize the semantic information of a word based on both the word itself and its context. SciBERT module  is a scientific domain-oriented variant of BERT , which remains the original architecture of BERT and pre-trained on scientific corpora. In such a manner, domain knowledge would be consolidated into SciBERT and therefore improves its performance on downstream tasks. First, each word token was transformed into a digitized SciBERT embedding vector. A bi-directional long-short-term memory neural network with a conditional random field top layer (BiLSTM-CRF) was used to determine the corresponding entity class labels. The annotated dataset was split into training, validation, and test sets with a paper-wise ratio of 8:1:1 to train the aforementioned neural network. \n\nWhen assuming that the automated tools have an acceptable performance, the combination of multiple systems can generate labels with an acceptable quality. Considering that some material and product entities are usually described in terms of chemical formula and faradaic efficiency entity is often described in the form of numerical \u201cvalue unit\u201d, we proposed a rule-based approach to assist the model in its identification . Typically, the creation of an extended corpus required corpus harmonization to merge multiple predictions. Here we consider the simplest case, applying voting schemes  and various reference boundary coordination strategies (for example, accurate, nested, continuous similarity metrics for reference alignments ) for the final decision. \n\n\n#### Fine-grained entity classification \n  \nFine-grained entity categories divide entities in a more granular way. In order to identify and classify entities obtained from the previous task, we implemented a classification algorithm combining dictionary and maximum entropy model. The dictionary-based recognizer used a word list established on the expert-annotated data . The maximum entropy model was used to extract features from the data that cannot be matched by the dictionary. The features of each entity were obtained from its word embedding vectors, context vectors, word cluster clustering information and coarse-grained entity category information through a simple mapping function. Sentences were tokenized using ChemDataExtractor\u2019s ChemWordTokenizer  in order to obtain word embedding vectors. The context vector of each word was obtained through mask training of the SciBERT model mentioned above. \n\nOur system utilised features derived from Brown clustering , which is a form of hierarchical clustering of words based on the contexts in which words occur. This has been proved to improve the performance of part-of-speech tagging and named entity recognition in various domains . Clustering was performed on the full text and titles of 2123 material articles published by ACS, RSC and Springer. This collection contained about 20 million words out of about 700,000 sentences, with tokenization from ChemDataExtractor\u2019s ChemWordTokenizer. Liang C++ implementation  was used to perform clustering and generate 1,500 clusters containing 372,799 unique words. This clustering information was also used as the classification feature of entities for model training. \n\n\n#### Calibration of the extended corpus \n  \nFirst we automatically revised the annotation results for the extended corpus to cross check the mention boundaries, trim whitespace characters, and ensure their technical consistence with the annotation rules. We then selected a 50% random sample from the entire dataset to be manually proofread by the main annotation team of the Golden Corpus. For potentially inconsistent cases where a given chemical name was annotated in automatic labelling as one entity class and in manual annotation as another entity class, we relied primarily on the annotations of the main annotator team because these curators had a higher degree of experience in this task and they did provide active feedback for the refinement of the annotation. After one round of a rather rough proofreading process, this corpus contained only the crude annotations. By doing this we intend to encourage follow-up researchers to explore their own downstream NER tasks, such as cross comparison, mention alignment and consensus annotations strategies. A total of 145,179 automatic annotations were generated for these 8184 paragraphs. On average, the number of entity mentions per abstract was of 17.74, almost four times when compared to the benchmark corpus. A possible reason for this was that the automatic model identified eligible entities, but the context of the entities mentioned in the text was not relevant to the CO  electrocatalytic reduction process. However, it was useful to examine more difficult or easier cases and to detect potential annotation errors when examining consensus predictions generated by multiple systems. \n\n\n\n\n## Data Records \n  \nThe two types of datasets presented in this paper are available at Science Data Bank (ScienceDB), which is a public, general-purpose data repository aiming to provide data services for researchers, research projects/teams, journals, institutions, universities, etc. \n\nThe benchmark corpus is publicly available at 10.57760/sciencedb.07106 . The extended corpus is publicly available at 10.57760/sciencedb.07139 . The two Standard Corpus are provided as a file in CSV format, and the details of them are shown in Table\u00a0 .   \nSummary of the two corpus. \n  \n\nMetadata contained in the dataset for an article include: article DOI, the year of publication, and the title. Each record metadata includes: entity extracted from the paper, label of the entity, and the sentence where the entity is located. Expanded details for the format of the dataset are given in Table\u00a0 .   \nMetadata of the corpus. \n  \n\n\n## Technical Validation \n  \n### Extraction accuracy \n  \nTo ensure high accuracy of the dataset, we only included data from the CO  electrocatalytic reduction process obtained at the final filtering step of the pipeline. This strategy reduced potential errors in the dataset that may have been caused by combination-parsing failure, incomplete extraction, or incomplete information provided by the text. We applied the extraction line to 200 randomly selected documents in the material field, 150 of which were relevant to electrocatalytic reduction of CO  processes over copper-based catalysts, giving an extraction rate of approximately 75%. Although these excluded documents are also relevant to the topic of our concern, they are primarily concerned with theoretical calculations, mechanism investigations, and experimental studies in organic solutions, all of which are beyond our consideration. \n\nTo demonstrate the practicality of our annotated corpus, we explored two machine learning methods for extracting actions and entities: a maximum entropy model and several neural network tagging models. We used standard precision, recall and F1 indicators to evaluate and compare performance. In the maximum entropy model , we used two types of features based on the current word and context words within a window of size 2: the part-of-speech feature generated by GENIA part-of-speech Tagger , which is specially adjusted for biomedical texts, and the Lexical features, including unigrams, bigrams as well as their lemmas and synonyms from WordNet . Neural network annotators included the most advanced bidirectional LSTM with conditional random field (CRF) layer , bidirectional recurrent neural network Bi-GRU  and BERT model with conditional random field (CRF). Table\u00a0  shows the experimental comparison results. We found that the BERT-BiLSTM-CRF model consistently outperformed other methods, achieving an overall F1 score of 81.95 at identifying four coarse-grained category entities.   \nCompare the F1 scores of entity recognition in various models. \n  \n\nIn order to demonstrate the utility of the multi-task entity extraction, we conducted ablation experiments on the maximum entropy classification model to verify that the new features introduced are effective in improving the results . Table\u00a0  shows the precision, recall and F1 score of the maximum entropy classification model using various features. Parts of speech features alone are the most effective in capturing entity words. This is largely due to entity words appearing as verbs or nouns in the majority of the sentences. Cluster features are less effective in classifying method entities, because they usually have long spans and do not share similar semantic features. Our empirical results on using common machine learning algorithms such as maximum entropy model and neural network models show plenty of room for improvement when compared with the estimated domain experts\u2019 performance, and suggest that our corpus could serve as a benchmark for evaluating material specific natural language processing research. We leave further investigation for future work, and hope the release of our dataset can help draw more attention to NLP research on instructional languages.   \nPrecision, Recall and F1 (micro-average) of the maximum entropy model for fine-grained entity classification, as each feature is added. \n  \n\n\n### Dataset mining \n  \nIn order to illustrate the current status and future trends of Cu-based electrocatalysts in the field of CO  electroreduction, the nine types of entities in this dataset were visualized and analyzed. \n\nFirst, we presented the overall development course of Cu-based electrocatalysts for CO  reduction in the last 12 years (Fig.\u00a0 ). The number of publications on Cu-based electrocatalysts has gradually grown from several articles in 2011 to about 200 articles in 2021. The catalysts that researchers are most interested in focus on Cu, Cu-M, and CuO , and various composite catalysts such as Cu/C, Cu(O )-MO , and M\u2009+\u2009CuO  are given increasing attention nowadays. Apart from catalysts, the test condition of CO  electroreduction should also be considered due to its important role in performance. In terms of electrolytes, the KHCO  electrolyte is most commonly used in CO  electroreduction, with KOH, and NaHCO  following (Fig.\u00a0 ). Furthermore, the type of cell setup is another important test condition. As shown in Fig.\u00a0 , the statistical distribution of the count of current densities measured by different cell setups is presented. The current density of the H-type cell is largely concentrated in the values less than 20\u2009mA\u2009cm , while that of the flow cell exhibits an average value of close to 200\u2009mA\u2009cm , revealing the dependence of performance on cell setup.   \n(  a  ) Stacked frequencies of Cu-based electrocatalysts for CO  reduction in the last 12 years. (  b  ) Frequencies of different electrolytes used in CO  electroreduction. \n    \nThe statistical distribution of the count of current densities measured by different cell setups. Inset: The percentage of different cell setups applied in CO  electroreduction. \n  \n\nIn addition to catalysts and test conditions, we also analyzed the regulation method and performance of catalysts. Figure\u00a0  shows a heatmap depicting the number of publications of Cu-based electrocatalysts with different regulation methods. The structure control approach exhibits widespread use to modify the surface morphology, structure, and crystal phase of catalysts. Other approaches deliver a high degree of correlation with the type of catalysts. For example, only the binary Cu-M systems contain the alloy form of catalysts, surface/interface modification is mainly applied in Cu surfaces, and the atomic level dispersion of Cu atoms mostly takes place in Cu molecular complexes, Cu/C, and Cu-MOF. Figure\u00a0  shows a heatmap presenting the relationships between materials and products for CO  reduction. It can be seen that Cu and CuO  show a clear tendency to produce C H  and the Cu-M catalysts tend to produce C  products such as CO and HCOOH. The blank area of this heatmap also presents some potential research directions for researchers. As shown in Fig.\u00a0 , we drew violin plots of Faradaic efficiency as a function of product to illustrate the associations between products and corresponding Faradaic efficiencies in the scientific literature. The Faradaic efficiency of C  products is statistically higher than C  products. Specifically, most of the articles reporting CO and HCOOH as the main products realize a Faradaic efficiency of more than 80%, while the articles related to C H  and C H OH only report a Faradaic efficiency of about 40%. These results demote the difficulty of C-C coupling to product C  products. Furthermore, we also analyzed the correlations between the potential of CO  reduction and product (Fig.\u00a0 ). C  products are commonly produced at a lower overpotential than C  products. This result experimentally implies that a higher reaction energy barrier is needed to generate C  products. An interesting finding needed to pay attention is that CH COOH shows the lowest overpotential, and this result needs further study.   \n(  a  ) Heatmap depicting the number of publications of Cu-based electrocatalysts with different regulation methods. (  b  ) Heatmap depicting the number of publications of Cu-based electrocatalysts with various products. (  c  ) Violin plots of Faradaic efficiency as a function of product. (  d  ) Box plots of the potential of CO  reduction as a function of product. The points alongside the boxes present the distribution of experimental results in the dataset. \n  \n\nFinally, we presented the relationships between catalysts and synthesis methods based on the data from the benchmark corpus. We first labeled the synthesis methods of Cu-based electrocatalysts from the full text of articles and then divided them into eight categories including balling milling, wet chemical method, electrochemical method, solvothermal method, thermal treatment, sol-gel method, mechanical mixing, physical vapor deposition, and molecular/polymer coating. As shown in Fig.\u00a0 , the wet chemical method, electrochemical method, solvothermal method, and thermal treatment are the most commonly used approaches to prepare Cu-based electrocatalysts. Some relationships between catalysts and synthesis methods can also be found in Fig.\u00a0 . For instance, the electrochemical method is the most popular approach to preparing Cu and Cu-M catalysts, Cu/C is mainly prepared by thermal treatment, and physical vapor deposition is mainly used to obtain biphase Cu-M catalysts. These results can provide an intuitional guideline for the preparation of Cu-based electrocatalysts.   \nAlluvial plot showing the relationships between catalysts and synthesis methods. \n  \n\n\n \n", "metadata": {"pmcid": 10060421, "text_md5": "1963f926a986ee2571658ad61c7393b5", "field_positions": {"authors": [0, 139], "journal": [140, 148], "publication_year": [150, 154], "title": [165, 256], "keywords": [270, 303], "abstract": [316, 1510], "body": [1519, 30030]}, "batch": 1, "pmid": 36991006, "doi": "10.1038/s41597-023-02089-z", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10060421", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=10060421"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10060421\">10060421</a>", "list_title": "PMC10060421  A corpus of CO2 electrocatalytic reduction process extracted from the scientific literature"}
{"text": "He, Jiayuan and Nguyen, Dat Quoc and Akhondi, Saber A. and Druckenbrodt, Christian and Thorne, Camilo and Hoessel, Ralph and Afzal, Zubair and Zhai, Zenan and Fang, Biaoyan and Yoshikawa, Hiyori and Albahem, Ameer and Cavedon, Lawrence and Cohn, Trevor and Baldwin, Timothy and Verspoor, Karin\nFront Res Metr Anal, 2021\n\n# Title\n\nChEMU 2020: Natural Language Processing Methods Are Effective for Information Extraction From Chemical Patents\n\n# Keywords\n\nnamed entity recognition\nevent extraction\ninformation extraction\nchemical reactions\npatent text mining\ncheminformatics\n\n\n# Abstract\n \nChemical patents represent a valuable source of information about new chemical compounds, which is critical to the drug discovery process. Automated information extraction over chemical patents is, however, a challenging task due to the large volume of existing patents and the complex linguistic properties of chemical patents. The Cheminformatics Elsevier Melbourne University (ChEMU) evaluation lab 2020, part of the Conference and Labs of the Evaluation Forum 2020 (CLEF2020), was introduced to support the development of advanced text mining techniques for chemical patents. The ChEMU 2020 lab proposed two fundamental information extraction tasks focusing on chemical reaction processes described in chemical patents: (1)   chemical named entity recognition  , requiring identification of essential chemical entities and their roles in chemical reactions, as well as reaction conditions; and (2)   event extraction  , which aims at identification of event steps relating the entities involved in chemical reactions. The ChEMU 2020 lab received 37 team registrations and 46 runs. Overall, the performance of submissions for these tasks exceeded our expectations, with the top systems outperforming strong baselines. We further show the methods to be robust to variations in sampling of the test data. We provide a detailed overview of the ChEMU 2020 corpus and its annotation, showing that inter-annotator agreement is very strong. We also present the methods adopted by participants, provide a detailed analysis of their performance, and carefully consider the potential impact of data leakage on interpretation of the results. The ChEMU 2020 Lab has shown the viability of automated methods to support information extraction of key information in chemical patents. \n \n\n# Body\n \n## 1. Introduction \n  \nDiscovery and development of new drugs is continually needed by our society. New drugs are required by our healthcare systems to address unmet medical needs, and pharmaceutical industries strive to bring better drugs to market. However, the development of new drugs is an expensive process, which may take more than 10 years and cost more than 2.6 billion dollars, with a success rate of <12% . In order to accelerate the process, reduce the overall cost, and improve the success rate of novel formulations, there has been an increasing interest in leveraging artificial intelligence techniques for drug discovery (Smalley,  ; Mak and Pichika,  ). \n\nArtificial intelligence techniques may benefit the drug development process in various ways. One active research area is the development of information extraction tools over chemical literature. Chemical literature contains valuable information about the latest advancements in the chemistry domain that is important to make findable and accessible. However, due to the rapid growth in chemical literature, new discoveries are easily missed while manual extraction of this information is increasingly infeasible (Muresan et al.,  ). Therefore, the development of automatic information extraction systems over chemical literature has attracted extensive research interest. The main idea in this work is to leverage natural language processing (NLP) techniques to build systems that can automatically and effectively process lengthy chemical texts, including scientific literature or patents, in order to extract key information out of them. The extracted information can be used directly for related research, such as drug target ranking (Hamed et al.,  ), or to construct a structured knowledge base that can be searched. \n\nChemical patents are acknowledged as a critical source of information about new discoveries in chemistry. Discoveries of new chemical compounds are typically disclosed in chemical patents (Bregonje,  ; Senger et al.,  ) and patents may lead other chemical literature, such as scientific journals by up to 3 years. Moreover, some information about new chemical compounds, e.g., their detailed synthesis processes, is exclusively provided in chemical patents. These details are important for understanding the compound prior art, and provide a means for novelty checking and validation (Akhondi et al.,  ,  ). \n\nDue to the significant value of information in chemical patents, many research efforts have been made toward the development of more effective information extraction systems specifically for chemical patents (Parapatics and Dittenbach,  ; Akhondi et al.,  ; Chen et al.,  ). Several fundamental information extraction tasks, such as named entity recognition (NER) (Zhai et al.,  ), and relation extraction (Peng et al.,  ) have been extensively investigated. There also exist several shared tasks that focus on information extraction in the chemistry domain, such as ChemDNER (Krallinger et al.,  , ). \n\nThe ChEMU (Cheminformatics Elsevier Melbourne University) lab is an initiative to encourage research on methods for automated information extraction from chemical patents. As a first running of ChEMU, ChEMU2020 lab focused on extraction of   chemical reactions   from patents (He et al.,  ; Nguyen et al.,  ). We prepared two fundamental information extraction tasks. Task 1\u2014named entity recognition (NER)\u2014focused on identifying the set of   named entities   that are essential to describe chemical reaction process. Task 2\u2014event extraction (EE)\u2014addressed identifying the sequence of   event steps   in a chemical reaction which transforms the starting material to the reaction compound. Compared with existing shared tasks, such as BioNLP, we primarily focus on information extraction in the context of chemical patents rather than scientific literature, which introduces some challenges due to the complex linguistic properties of patents (Zhai et al.,  ). Compared with the shared task ChemDNER (Krallinger et al.,  ), which also addressed chemical named entity recognition, we go beyond the scope of entity mentions and chemical entity passage detection, and require the identification of the specific roles of chemical entities within reactions, such as whether the chemicals serve as starting materials or products. Moreover, we consider data from full patent texts in our task, instead of solely focusing on titles and abstracts of patents. \n\nA high-quality new corpus was made publicly available to support the two tasks in ChEMU2020 lab. The corpus was prepared using 1,500 text segments sampled from 180 English patents from the European Patent Office and the United States Patent and Trademark Office. Three chemical experts were hired to manually annotate the corpus, labeling named entities and event steps in these text segments. Two of them reviewed all text segments independently and the third annotator acted as an adjudicator who resolved their disagreements and merged their annotations into the final gold-standard corpus. The inter-annotator-agreement (IAA) score reaches 0.9760 and 0.9506 for the two tasks, respectively. \n\nThe ChEMU2020 lab was held during April to June 2020. Three tracks were made available to participants: one track each for the NER and EE tasks individually, and a third track for end-to-end systems which address both tasks simultaneously. We received registrations of 37 teams from 13 countries in total. We received 26 runs (including one post-evaluation run) from 11 teams in Task 1, 10 runs from five teams in Task 2, and 10 runs from four teams in the third track. In this paper, we provide a detailed overview of the activities within ChEMU2020 lab, including the new ChEMU corpus, the tasks, the evaluation framework, the evaluation results, and a summary of participants' approaches. This paper is an extension of our previous overview papers (He et al.,  , ) and thereby the task descriptions (section 4) and core evaluation results (section 5) are repeated here from those papers. Our focus is to provide additional detail about the preparation of the corpus we developed (section 3) and to provide more comprehensive analysis of the evaluation results. The corpus is available for use (Verspoor et al.,  ); the test data can be submitted for evaluation through the shared task website at  . \n\n\n## 2. Related Work \n  \nTo assess and advance the natural language processing (NLP) techniques in the biochemical domain, many shared tasks/labs have been organized, including n2c2 , TREC , BioCreative , BioNLP , and CLEF workshops . These shared tasks have covered a range of benchmark text mining tasks:   information retrieval  , such as document retrieval [CLEF eHealth 2014 (Kelly et al.,  )] and text classification [CoNLL 2010 (Farkas et al.,  )];   word semantics  , such as named entity recognition [BioCreative II (Morgan et al.,  ) Task 1] and mention normalization [BioCreative III (Arighi et al.,  ; Lu et al.,  ) Gene Normalization Task];   relation semantics  , such as event extraction [GENIA Event Extraction (Kim et al.,  )] and interaction extraction [Drug-Drug Interaction (Herrero-Zazo et al.,  )]; and   high-level applications  , such as question answering [Semantic QA (Tsatsaronis et al.,  )] and document summarization [Biomed-Summ (Jaidka et al.,  )]. \n\nNevertheless, most of these shared tasks/labs did not focus on the domain of chemical patents. These shared tasks mainly focused on the text mining over biomedical texts (e.g., scientific literature, such as PubMed abstracts) or clinical data (e.g., clinical health records). Text mining techniques that are developed for biomedical or biochemical texts, such as scientific journals and clinical records may not be effective for chemical patents. This is because their purpose is distinct\u2014chemical patents are written for protection of intellectual property related to chemical compounds\u2014and their content has different scope and characteristics, including variations in linguistic structures. Thus, it is critical to develop text mining techniques that are tailored for chemical patents. \n\nOnly two shared tasks have previously considered chemical patents. TREC 2009 (Lupu et al.,  ) provided a chemical information retrieval track for the tasks of   ad hoc   retrieval of chemical patents and prior art search. However, this track differs significantly from the subtasks in our ChEMU lab: it addresses document-level retrieval and relevance to queries instead of considering the detailed content of each document. The ChemDNER-patents task (Krallinger et al.,  ) at the BioCreative V workshop was the task that is most similar with ours. It aimed at detection of chemical compounds and genes/proteins in patent text. However, the ChemDNER-patents task only considered entity detection within patent abstracts while we consider data extracted from the full texts of patents. Moreover, our definition of chemical compound entities is much richer as our label set defines not only that a chemical or drug compound is mentioned, but also what its specific role is with respect to the chemical reaction that it is related to in the description, e.g., starting material, catalyst, or product. \n\nChEMU lab 2020 also contributes a new corpus on chemical text mining for the research community . Most existing benchmark datasets for biochemical text mining focus on biomedical texts, i.e., texts that consider the interaction of chemicals with molecular biology or human disease. CHEMProt (Krallinger et al.,  ) consists of 1,820 PubMed  abstracts with chemical-protein interactions, DDI extraction 2013 corpus (Herrero-Zazo et al.,  ) is a collection of 792 texts selected from the DrugBank database  and other 233 PubMed abstracts, and BC5CDR is a collection of 1,500 PubMed titles and abstracts selected from the CTD-Pfizer corpus, just to give a few examples. \n\nThe number of public datasets that focus on the chemistry domain is limited. Further, several existing chemical datasets are based on structured/semi-structured texts rather than free, natural language, texts. For example, the ZINC 15 250k corpus  is a collection of 250,000 molecules with their Simplified Molecular Input Line Entry System (SMILES) strings. The Tox21 dataset contains roughly 7,000 molecules and typical 120 characteristics, such as atomic number, aromicity, donor status. There are two datasets that are constructed from free patent texts: (1) the dataset released by the ChemDNER patents task and (2) the dataset created by Akhondi et al. ( ). However, these two datasets only contain entity annotations. Our chemical reaction corpus is further enriched by the relations between the annotated entities. \n\nDespite the limited number of shared tasks on chemical patent mining, there is an increasing interest in developing information extraction models for patents in general research communities (Tseng et al.,  ; Akhondi et al.,  ; Yoshikawa et al.,  ). Various text mining techniques have been proposed for information extraction over chemical patents (Krallinger et al.,  ), addressing fundamental NLP tasks, such as named entity recognition and relation extraction (Tseng et al.,  ; Vazquez et al.,  ; Akhondi et al.,  ; Zhai et al.,  ). Early techniques for chemical text mining, such as dictionary-based methods (Rebholz-Schuhmann et al.,  ; Hettne et al.,  ; Akhondi et al.,  ) and grammar-based methods (Narayanaswamy et al.,  ; Liu et al.,  ; Akhondi et al.,  ), heavily rely on expert knowledge in the chemical domain. Recently, machine learning-based techniques have reported state-of-the-art effectiveness in chemical text mining (Hemati and Mehler,  ; Zhai et al.,  ). However, such techniques require a large amount of annotated text data, which still remains limited. Thus, ChEMU lab 2020 was hosted to provide an opportunity for NLP experts to develop information extraction systems over chemical patents. The new ChEMU reaction corpus was also made publicly available to all researchers as an important benchmark dataset for future research in this domain (Verspoor et al.,  ). \n\n\n## 3. The ChEMU Chemical Reaction Corpus \n  \n### 3.1. Data Selection \n  \nThe ChEMU chemical reaction corpus was built with the aid of Elsevier Reaxys\u00ae database. Reaxys is a rich information resource for chemical reactions, which contains detailed descriptions of chemical reactions that are extracted via an \u201cexcerption\u201d process (Lawson et al.,  ), i.e., manual selection of information from literature sources, such as patents and scientific publications. We selected 180 English patents from the European Patent Office and the United States Patent and Trademark Office, for which information had been included in the Reaxys database. From these patents, 1,500 text segments were sampled from chemical reaction descriptions pre-identified by expert domain annotators, available as a product of the process used to populate information in Reaxys. We refer to each text segment as a patent \u201csnippet\u201d and use the two expressions interchangeably in the remainder of this paper. The key statistics of the sampled snippets are summarized in  , including the total numbers of words and sentences in all snippets (Total), and the minimum (Min), maximum (Max), median (Median), and mean (Mean) of the number of words/sentences per snippet. Note that the numbers of words and sentences are computed using the NLTK tool . \n  \nStatistics of the selected snippets. \n  \n depicts an example of an extracted patent snippet in the ChEMU corpus. This example snippet describes the synthesis process of the chemical compound \u201cN-(3-chloro-4-fluorophenyl)-N-(2-fluoro-4-(hydrazinecarbonyl)benzyl)tetrahydro-2H-thiopyran-4-carboxamide 1,1-dioxide.\u201d The described reaction process consists of five event steps: \n  \nMethyl 4-((N-(3-chloro-4-fluorophenyl)-1,1-dioxidotetrahydro-2H-thiopyran-4-carboxamido)methyl)-3-fluorobenzoate and hydrazine monohydrate were dissolved in ethanol at room temperature. \n  \nThe solution was stirred at 80\u00b0 for 5 h. \n  \nThe solution was cooled to room temperature. \n  \nThe reaction mixture was concentrated. \n  \nThe title compound of 1.180 g, 95.2% was used without further purification. \n    \nAn example of one patent snippet in ChEMU chemical reaction corpus. \n  \nGiven the patent snippet in  , the ChEMU lab 2020 aims at identification of these five event steps, and our ChEMU chemical reaction corpus is constructed to support this target. Therefore, all patent snippets were annotated to label three types of information: (1) named entities, e.g., chemical compounds participating in these event steps; (2) trigger words signaling the occurrences of event steps; and (3) relations between trigger words and named entities that reflect how entities are involved in event steps. The first type of annotations were used to support Task 1, serving as the ground-truth entities in our evaluation. The next two types of annotations were used to support Task 2, which form the ground-truth events. \n\n\n### 3.2. Annotation Guidelines \n  \nIn order to obtain high-quality and consistent annotations, comprehensive annotation guidelines were prepared before the official annotation started (Verspoor et al.,  ). \n\n#### 3.2.1. Named Entity Annotation Guidelines \n  \nAs discussed, our NER task requires not only detection of named entities, but also the assignment of correct labels for detected named entities according to their roles in chemical reactions. Four categories of entities are defined: (1) chemical compounds; (2) reaction conditions; (3) yields; and (4) example labels. Furthermore, ten entity labels are defined under the four categories. In particular, five labels are defined for chemical compound entities: STARTING_MATERIAL, REAGENT_CATALYST, REACTION_PRODUCT, SOLVENT, and OTHER_COMPOUND. Two labels are defined for reaction conditions: TIME and TEMPERATURE, two labels for yields: YIELD_PERCENT and YIELD_OTHER, and another label for example labels: EXAMPLE_LABEL. The detailed definitions of the ten labels are presented in Entity Annotations in  . \n  \nDefinitions of entity, trigger word, and relation types, i.e., labels. \n  \nIn addition to label definitions, more detailed instructions on annotations are provided in the annotation guidelines including: (1) how to correctly identify the text spans of named entities; and (2) how to assign correct labels to entities where there is ambiguity; and (3) annotation decisions for problematic examples encountered during the annotation process. \n\n##### 3.2.1.1. Text span identification \n  \nDetailed instructions are given in the annotation guidelines on how to determine the text spans of entities. For example, when annotating an EXAMPLE_LABEL entity, only the actual example index should be annotated, and any word preceding the index, such as \u201cExample,\u201d \u201cStep,\u201d or \u201cIntermediate\u201d should be excluded. In our example snippet  , \u201c[Step 3]\u201d indicates an example label, but only \u201c3\u201d should be annotated as an EXAMPLE_LABEL entity. \n\n\n##### 3.2.1.2. Label assignment \n  \nLabel definitions in   should be used to choose the correct label for each entity. In addition, more detailed rules have been set out to cover the cases where annotators may have disagreement. These additional rules are especially important since we have the label \u201cOTHER_COMPOUND\u201d which covers all chemical compounds that do not belong to the other four compound labels. Thus, the decision boundaries between the compound labels need to be rigorously defined. One example of such rules is that solvents that are used in work-up procedures should not be annotated as SOLVENT but rather as OTHER_COMPOUND. \n\n\n\n#### 3.2.2. Event Annotation Guidelines \n  \nA chemical reaction process is usually a sequence of steps, and these steps can be categorized into two types: (1)   reaction steps  , i.e., the steps required to convert the starting materials to the target reaction product; and (2)   work-up steps  , i.e., the manipulations required to purify or isolate a chemical product. For example, in  , the step of stirring the solution at 80\u00b0 is a reaction step while the step of concentrating the reaction mixture is a work-up step. \n\nIn our corpus, events are quantified by two types of information: the trigger words that flag the occurrences of event steps, and the relations between named entities and trigger words that tell us how entities are involved in event steps. Two labels of trigger words, WORKUP and REACTION_STEP are defined for the two types of event steps, respectively. To capture the relationships between trigger words and named entities, we adapt semantic argument role labels   Arg1   and   ArgM   from the Proposition Bank (Palmer et al.,  ) to label relations. The label Arg1 represents argument roles, a.k.a. thematic roles, being causally affected by another participant in the event (Jurafsky and Martin,  ), and is therefore used to label the relation between a trigger word and a chemical compound. The label ArgM represents adjunct roles with respect to an event, and thus, is used to label the relation between a trigger word and a temperature, time or yield entity. The definitions of trigger word types and relation types are summarized in  . \n\n\n\n### 3.3. Annotation Process \n  \nTo facilitate the annotation process, a   silver standard   set was first prepared based on information captured in the Elsevier Reaxys\u00ae database.  The extracted records from Reaxys are linked to the IDs of their source patents. However, the precise locations of the key entity and relation information in these records in source patents are needed to construct the gold-standard corpus. The silver-standard dataset was prepared by automatically mapping elements of the records in the Reaxys database to the source patents from which the records were extracted. This mapping process was performed by scanning patent texts and searching for excerpted entity mentions. \n\nThree chemical experts were hired to prepare the   gold standard   corpus. They manually reviewed all texts and pre-annotations in the silver-standard dataset to add or correct precise locations of the relevant entities and relations in the texts, according to annotation guidelines in section 3.2.1. Two of the experts first independently reviewed and updated the silver standard annotations. Then, a third chemical expert served as an adjudicator who resolved their disagreements to produce the final gold-standard corpus. See section 3.5 for further details on corpus statistics and data quality. \n\nThe annotation process was conducted using the BRAT annotation tool,  which is an interactive web-based tool for adding annotations to input texts. Continuing with the example snippet shown in  , a visualization of the snippet after annotation is presented in  . The visualization of our sample dataset, which is a subset of ChEMU chemical reaction corpus and consists of 50 snippets, is available in a dedicated website . \n  \nVisualization of the annotations in the snippet in  . \n  \n\n### 3.4. Data Format \n  \nThe ChEMU chemical reaction corpus is delivered in the BRAT standoff format (Stenetorp et al.,  ). For each snippet, two files were generated by the BRAT annotation tool: a text file (  .txt  ) for the plain text in the snippet, and an annotation file (  .ann  ) consisting of all the annotated entities and events. In the annotation file, each entity/trigger word is represented with a 5-tuple \u2329ID, label,   s, e  , text\u232a. Here, ID represents the index of the annotation, and   s   and   e   represent the offset (position relative to the first character in the snippet) of the starting and ending character of the entity, respectively. Each relation is represented with a 4-tuple: \u2329ID, label, Arg1, Arg2\u232a, where ID represents the index of the annotation, and Arg1 and Arg2 correspond to the IDs of the two entities linked by this relation. \n\nTo give an example, the annotation file for the snippet in   is presented in  . Note that in  , the texts of the two entities   T8   and   T9   are abbreviated for ease of presentation. \n  \nThe annotation file for the patent snippet in  . Entities and trigger words are indexed from   T0   to   T16  , and relations are indexed from   R0   to   R10  . \n  \n\n### 3.5. Annotated Corpus Statistics \n  \n#### 3.5.1. Overall Statistics \n  \nThe overall statistics of our annotated corpus are presented in  . In the 1,500 selected patent snippets, 26,857 entities and 11,236 trigger words were annotated, with 23,445 relations identified between them. The numbers of instances annotated for each label defined in   are summarized in  . \n  \nOverall statistics of the annotated corpus. \n    \nNumber of instances for each label defined in  . \n  \n\n#### 3.5.2. Inter-Annotator Agreement \n  \nWe measure the inter-annotator agreement (IAA) of our corpus using two metrics: (1) Cohen's Kappa (Cohen,  ) and (2) F -scores. Cohen's Kappa is a standard metric for the evaluation of inter-annotator agreement. But for the tasks in ChEMU 2020 lab, i.e., named entity recognition (NER) and event extraction (EE), Cohen's Kappa score may not the best metric to quantify the extent of agreement between annotators (Hripcsak and Rothschild,  ; Grouin et al.,  ). This is because the computation of Kappa requires the number of negative cases to be known, which is not explicitly given in the tasks of NER or EE. Thus, the pairwise F -scores are also commonly used to measure the IAA scores. In this paper, we report the IAA scores using both metrics. In the computation of Cohen's Kappa scores, we only consider the set of entities that are annotated by at least one annotator. The IAA scores in both metrics are summarized in  . Note that in this table, \u201canno3\u201d represents the third annotator (the adjudicator) as discussed in section 3.3 and thus, the annotations by \u201canno3\u201d are the same as the annotations in gold-standard corpus. \n  \nSummary of inter-annotator agreement scores. \n  \n\n\n### 3.6. Data Partitions \n  \nThe ChEMU chemical reaction corpus was randomly partitioned into train/development/test splits with the ratio of 0.6/0.15/0.25. The training and development sets were released to participants for model development and the test set was withheld for use in the evaluation stage. \n\nTo ensure a fair split of data, two statistical tests on the resultant train/dev/test splits were conducted. The distributions of entity labels (10 entity labels plus two trigger word labels in  ) in train/dev/test sets are presented in  . As shown in this figure, the distributions over entity labels on train/dev/test sets are quite similar. Specifically, only slight fluctuations in label distributions (\u22640.004) are found across the three splits. \n  \nDistributions of entity labels on the training/development/test data splits. The labels are indexed according to their order in  . \n  \nWe further compare the train/dev/test splits in terms of the International Patent Classifications (IPCs)  of their source patents. The IPC information of a patent reflects its application category. For example, the IPC code \u201cA61K\u201d is assigned to patents for preparations for medical, dental, or toilet purposes. Since patents with different IPCs may differ in the vocabulary and linguistic properties, we want to make sure that the patent snippets in the train, dev, and test set have similar distributions over IPCs. For each patent snippet, we extract the primary IPC of its corresponding source patent, and summarize the IPC distributions of the snippets in train/dev/test sets in  . As shown in  , the IPC distributions across the three splits are also similar. \n  \nDistributions of IPCs on the training/development/test data splits. Only dominating IPC groups that take up more than 1% of at least one data split are included in this figure. Other IPCs are grouped as \u201cOther\u201d. \n  \n\n\n## 4. The Tasks \n  \nWe provided two tasks in ChEMU lab: Task 1\u2014Named Entity Recognition (NER), and Task 2\u2014Event Extraction (EE). We also hosted a third track where participants can work on development of end-to-end systems which address both tasks jointly. \n\n### 4.1. Overview of Tasks \n  \n#### 4.1.1. Task 1: Named Entity Recognition (NER) \n  \nThe first task aims to identify named entities that occur in the descriptions of chemical reactions. The task requires (1) detection of text spans of named entities and (2) assigning the correct labels to detected entities from the set of labels defined in  . For example, in  , the entity \u201chydrazine monohydrate\u201d (line 4) needs to be detected and assigned with the label REACTION_CATALYST, according to its role in the chemical reaction. \n\n\n#### 4.1.2. Task 2: Event Extraction (EE) \n  \nA chemical reaction usually consists of an ordered sequence of event steps that transforms a starting material into an end product, such as the five reaction steps for the example snippet in  . The event extraction task (Task 2) targets identifying these event steps. \n\nSimilar with conventional event extraction problems (Kim et al.,  ), Task 2 involves three subtasks: (1) trigger word prediction and (event) typing; (2) argument prediction; and (3) semantic role typing. First, it requires the identification of trigger words. For each trigger word detected, one label out of the trigger labels defined in   needs to be assigned. Second, it requires the determination of argument entities that are associated with the trigger words, i.e., which entities identified in Task 1 participate in event or reaction steps. This is done by labeling the connections between event trigger words and their arguments. Finally, Task 2 requires the assignment of correct role types (Arg1 or ArgM) to each of the detected relations. \n\n\n#### 4.1.3. Task 3: End-to-End Systems \n  \nThe third track allows participants to develop end-to-end systems that address both tasks simultaneously, i.e., the extraction of reaction events including their constituent entities directly from chemical patent snippets. This is a more realistic scenario for an event extraction system to be applied for large-scale annotation of events. \n\n\n#### 4.1.4. Workflow of the Three Tracks \n  \nThe workflows of the three tracks is illustrated in  , where the input and output of each track is illustrated using a the last sentence in   as an example. The input of Task 1 NER is the plain text of the snippet. Participants need to identify entities defined in  , e.g., the text span \u201ctitle compound\u201d need to be identified as \u201cREACTION_PRODUCT.\u201d The input of Task 2 EE is the plain text plus the ground-truth named entities. Participants are required to firstly identify the trigger words and their types (e.g., the text span \u201cused\u201d is identified as \u201cREACTION_STEP\u201d) and then identify the relations between the trigger words and the provided entities (e.g., a directed link from \u201cused\u201d to \u201ctitle compound\u201d is added and labeled as \u201cArg1\u201d). In the track of end-to-end systems, participants are only provided with the plain text. They are required to identify both the entities and the trigger words, and predict the event steps directly from the text. \n  \nIllustration of the three tasks. Shaded text spans represent annotated entities or trigger words. Arrows represent relations between entities. \n  \n\n\n### 4.2. Organization of Tracks \n  \n#### 4.2.1. Training Stage \n  \nThe training and development data sets were released to all participants for model development. Two different versions of training data, namely Data-NER and Data-EE, were provided. Data-NER was prepared for participants in Task 1, where the gold-standard entities were included. Data-EE was prepared for Tasks 2 and 3, where both the gold-standard entities, trigger words and entity relations were included. \n\n\n#### 4.2.2. Testing Stage \n  \nSince the gold-standard entities of the test set needed to be provided to participants in Task 2, the testing stage of Task 2 was delayed until after the testing of Tasks 1 and 3 are completed, so as to prevent data leakage. \n\nTherefore, our testing stage consists of two phases. In the first phase, the text (  .txt  ) files of all test snippets were released. Participants in Task 1 are required to use the released patent texts to predict the entities. Participants in Task 3 were required to also predict the trigger words and entity relations. \n\nIn the second phase, the gold-standard entities of all test snippets were released. Participants in Task 2 can use the released gold-standard entities, along with the text files released in the first phase, to predict the event steps in test snippets. \n\n\n#### 4.2.3. Submission Website \n  \nA submission website was developed and maintained, which allows participants to submit their runs during the testing stage.  In addition, the website offers several other important functions to facilitate organization of the lab. \n\nFirst, it hosts the download links for the training, development, and test data sets so that participants can access the data sets conveniently. Second, it allows participants to test the performance (against the development set) of their models before the testing stage starts, which also offers a chance for participants to familiarize themselves with the evaluation tool BRATEval  (detailed in section 5). The website also hosts a private leaderboard for each team that ranks all runs submitted by each team, and a public leaderboard that ranks all runs that have been made public by teams. \n\n\n#### 4.2.4. Timeline \n  \nThe timeline of each stage is summarized as follows. \n  \nRelease of sample data: 09 March 2020 \n  \nRelease of training data: 10 April 2020 \n  \nTesting stage (phase 1): 22 May 2020\u201328 May 2020 \n  \nTesting stage (phase 2): 29 May 2020\u20133 June 2020 \n  \nEnd of evaluation cycle and feedback to participants: 05 June 2020 \n  \n\n\n\n## 5. Evaluation Framework \n  \nIn this section, we describe the evaluation framework of the ChEMU lab. We introduce three baseline algorithms for Task 1, Task 2, and end-to-end systems, respectively. \n\n### 5.1. Evaluation Methods \n  \nWe use BRATEval to evaluate all the runs that we receive. Three metrics are used to evaluate the performance of all the submissions for Task 1: Precision, Recall, and F -score. Specifically, given a predicted entity and a ground-truth entity, we treat the two entities as a match if (1) the types associated with the two entities match; and (2) their text spans match. The overall Precision, Recall, and F -score are computed by micro-averaging all instances (entities). \n\nIn addition, we exploit two different matching criteria, exact-match and relaxed-match, when comparing the texts spans of two entities. Here, the exact-match criterion means that we consider that the text span of an entity matches with that of another entity if both the starting and the end offsets of their spans match. The relaxed-match criterion means that we consider that the text span of one entity matches with that of another entity as long as their text spans overlap. \n\nThe submissions for Task 2 and end-to-end systems are evaluated using Precision, Recall, and F -score by comparing the predicted events and gold standard events. We consider two events as a match if (1) their trigger words, event types and semantic roles are the same; and (2) the entities involved in the two events match. Here, we follow the method in Task 1 to test whether two entities match. This means that the matching criteria of exact-match and relaxed-match are also applied in the evaluation of Task 2 and of end-to-end systems. Note that the relaxed-match will only be applied when matching the spans of two entities; it does not relax the requirement that the entity type of predicted and ground truth entities must agree. Since Task 2 provides gold entities but not event triggers with their ground truth spans, the relaxed-match only reflects the accuracy of spans of predicted trigger words. \n\nTo somewhat accommodate a relaxed form of entity type matching, we also evaluate submissions in Task 1\u2014NER using a set of high-level labels shown in the hierarchical structure of entity classes in  . The higher-level labels used are highlighted in gray. In this set of evaluations, given a predicted entity and a ground-truth entity, we consider that their labels match as long as their corresponding high-level labels match. For example, suppose we get as predicted entity \u201cSTARTING_MATERIAL, [335, 351), boron tribromide\u201d while the (correct) ground-truth entity instead reads \u201cREAGENT_CATALYST, [335, 351), boron tribromide,\u201d where each entity is presented in the form of \u201cTYPE, SPAN, TEXT.\u201d In the evaluation framework described earlier this example will be counted as a mismatch. However, in this additional set of entity type relaxed evaluations we consider the two entities as a match, since both labels \u201cSTARTING_MATERIAL\u201d and \u201cREAGENT_CATALYST\u201d specialize their parent label \u201cCOMPOUND.\u201d \n  \nIllustration of the hierarchical NER class structure used in evaluation. \n  \n\n### 5.2. Baselines \n  \nWe released one baseline method for each task as a benchmark method. Specifically, the baseline for Task 1 is based on retraining   BANNER   (Leaman and Gonzalez,  ) on the training and development data; the baseline for Task 2 is a co-occurrence method; and the baseline for end-to-end systems is a two-stage algorithm that first uses BANNER to identify entities in the input and then uses the co-occurrence method to extract events. \n\n#### 5.2.1. BANNER \n  \nBANNER is a named entity recognition tool for biomedical data. In this baseline, we first use the GENIA Sentence Splitter (GeniaSS) (S\u00e6tre et al.,  ) to split input texts into separate sentences. The resulting sentences are then fed into BANNER, which predicts the named entities using three steps, namely tokenization, feature generation, and entity labeling. A simple tokenizer is used to break sentences into either a contiguous block of letters and/or digits or a single punctuation mark. BANNER uses a conditional random field (CRF) implementation derived from the MALLET toolkit  for feature generation and token labeling. The set of machine learning features used consist primarily of orthographic, morphological and shallow syntax features. \n\n\n#### 5.2.2. Co-occurrence Method \n  \nThis method first creates a dictionary   D   for the observed trigger words and their corresponding types from the training and development sets. For example, if a word \u201cadded\u201d is annotated as a trigger word with the label of \u201cWORKUP\u201d in the training set, we add an entry \u2329added, WORKUP\u232a to   D  . In the case where the same word has been observed to appear as both types of \u201cWORKUP\u201d and \u201cREACTION_STEP,\u201d we only keep as entry in   D   its most frequent label. The method also creates an event dictionary   D   for the observed semantic roles in the training and development sets. For example, if an event \u2329ARG1, E1, E2\u232a is observed where \u201cE1\u201d corresponds to trigger word \u201cadded\u201d of type \u201cWORKUP\u201d and \u201cE2\u201d corresponds to entity \u201cwater\u201d of type \u201cOTHER_COMPOUND,\u201d we add an entry \u2329ARG1, WORKUP, OTHER_COMPOUND\u232a to   D  . \n\nTo predict events, this method first identifies all trigger words in the test set using   D  . It then extracts two events \u2329ARG1, T1, T2\u232a and \u2329ARGM, T1, T2\u232a for a trigger word \u201cE1\u201d and an entity \u201cE2\u201d if (1) they co-occur in the same sentence; and (2) the relation type \u2329ARGx, T1, T2\u232a is included in   D  . Here, \u201cARGx\u201d can be \u201cARG1\u201d or \u201cARGM,\u201d and \u201cT1\u201d and \u201cT2\u201d are the entity types of \u201cE1\u201d and \u201cE2,\u201d respectively. \n\n\n#### 5.2.3. BANNER + Co-occurrence Method \n  \nThe above two baselines are combined to form a two-stage method for end-to-end systems. This baseline first uses BANNER to identify all the entities in Task 1. Then it utilizes the co-occurrence method to predict events, except that gold standard entities are replaced with the entities predicted by BANNER in the first stage. \n\n\n\n\n## 6. Results \n  \nIn total, 39 teams registered for the ChEMU shared task, of which 36 teams registered for Task 1, 31 teams registered for Task 2, and 28 teams registered for both tasks. The 39 teams are spread across 13 different countries, from both the academic and industry research communities. In this section, we report the results of all the runs that we received for each task. \n\n### 6.1. Task 1\u2014Named Entity Recognition \n  \nTask 1 received considerable interest with the submission of 25 runs from 11 teams. The 11 teams include one team from Germany (OntoChem), three teams from India (AUKBC, SSN_NLP, and JU_INDIA), one team from Switzerland (BiTeM), one team from Portugal (Lasige_BioTM), one team from Russia (KFU_NLP), one team from the United Kingdom (NextMove Software/Minesoft), two teams from the United States of America (Melaxtech and NLP@VCU), and one team from Vietnam (VinAI). We evaluate the performance of all 25 runs, comparing their predicted entities with the ground-truth entities of the patent snippets in the test set. We report the performances of all runs under both matching criteria in terms of three metrics, namely Precision, Recall, and F -score. \n\nWe report the overall performance of all runs in  . The baseline of Task 1 achieves 0.8893 in F -score under exact match. Nine runs outperform the baseline in terms of F -score under exact match. The best run was submitted by team Melaxtech, achieving a high F -score of 0.9570. There were sixteen runs with an F -score >0.90 under relaxed-match. However, under exact-match, only seven runs surpassed 0.90 in F -score. This difference between exact-match and relaxed-match may be related to the long text spans of chemical compounds, which is one of the main challenges in NER tasks in the domain of chemical documents. \n  \nOverall performance of all runs in Task 1\u2014Named entity recognition. \n  \n Here, P, R, and F represents the Precision, Recall, and F -score, respectively. For each metric, we highlight the best result in   bold   and the second best result in italic. The results are ordered by their performance in terms of F -score under exact-match  . \n  \nNext, we evaluate the performance of all 25 runs using the high-level labels in   (highlighted in gray). We report the performances of all runs in terms of Precision, Recall, and F -score in  . \n  \nOverall performance of all runs in Task 1\u2014Named entity recognition where the set of high-level labels in   is used. \n  \n Here, P, R, and F represents the Precision, Recall, and F -score, respectively. For each metric, we highlight the best result in   bold   and the second best result in italic. The results are ordered by their performance in terms of F -score under exact-match  . \n  \n\n### 6.2. Task 2\u2014Event Extraction \n  \nWe received ten (10) runs from five (5) teams. Specifically, the five teams include one team from Portugal (Lasige_BioTM), one team from Turkey (BOUN_REX), one team from the United Kingdom (NextMove Software/Minesoft) and two teams from the United States of America (Melaxtech and NLP@VCU). We evaluate all runs using the metrics Precision, Recall, and F -score. Again, we utilize the two matching criteria, namely exact-match and relaxed-match, when comparing the trigger words in the submitted runs and ground-truth data. \n\nThe overall performance of each run is summarized in  .  The baseline (co-occurrence method) scored relatively high in Recall, i.e, 0.8861. This was expected, since the co-occurrence method aggressively extracts all possible events within a sentence. However, the F -score was low due to its low Precision score. Here, all runs outperform the baseline in terms of F -score under exact-match. Melaxtech ranks first among all official runs in this task, with an F -score of 0.9536. \n  \nOverall performance of all runs in Task 2\u2014Event extraction. \n  \n Here, P, R, and F represent the Precision, Recall, and F -score, respectively. For each metric, we highlight the best result in   bold   and the second best result in italics. The results are ordered by their performance in terms of F -score under exact-match  . \n  \n\n### 6.3. End-to-End Systems \n  \nWe received 10 end-to-end system runs from four teams. The four teams include one team from Germany (OntoChem), one team from Portugal (Lasige_BioTM), one team from the United Kingdom (NextMove Software/Minesoft), and one team from the United States of America (Melaxtech). \n\nThe overall performance of all runs is summarized in   in terms of Precision, Recall, and F -score under both exact-match and relaxed-match.  Since gold entities are not provided in this task, the average performance of the runs in this task are slightly lower than those in Task 2. Note that the Recall scores of most runs are substantially lower than their Precision scores. This may reveal that the task of identifying a relation from a chemical patent is harder than the task of typing an identified relation. The first run from Melaxtech team ranks best among all runs received for this task. \n  \nOverall performance of all runs in end-to-end systems. \n  \n Here, P, R, and F represent the Precision, Recall, and F -score, respectively. For each metric, we highlight the best result in bold and the second best result in italics. The results are ordered by their performance in terms of F -score under exact-match  . \n  \n\n\n## 7. Participants' Approaches \n  \nWe received paper submissions from eight teams include team BiTeM, VinAI, BOUN-REX, NextMove/Minesoft, NLP@VCU, AU-KBC, LasigBioTM, and Melaxtech. The tasks that the eight teams have participated in are summarized in  . In this section, we compare and summarize how the eight teams address these tasks. More details are available in the CLEF2020 workshop papers (He et al.,  ). \n  \nThe task participation of the 8 teams BiTeM, VinAI, BOUN-REX, NextMove/Minesoft, NLP@VCU, AU-KBC, LasigBioTM, and Melaxtech. \n  \n### 7.1. BiTeM (Copara et al.,  ) \n  \nThe BiTeM team participated in Task 1. They explored various popular structures including Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al.,  ) and its variants, such as ChemBERTa model,  and Convolutional Neural Network (CNN) (Lecun,  ). Their best system is an ensemble created using a majority vote strategy (Copara et al.,  ) of three models: the BERT-base-cased model, BERT-base-uncased model, and a CNN model. The BiTeM submitted three runs to Task 1, and their best run was ranked the 7-th among all 26 runs. \n\n\n### 7.2. VinAI (Dao and Nguyen,  ) \n  \nThe VinAI team participated in Task 1. They addressed this task utilizing the BiLSTM-CNN-CRF model (Ma and Hovy,  ) and further leveraged the C ELM  word embeddings released by Zhai et al. ( ). C ELM  is a contextualized ELMo language model trained from scratch with a corpus of 84K chemical patents. A Word2vec skip-gram model trained on the same corpus being used in the input layer of C ELM .  The VinAI team submitted two runs to Task 1 including one post-evaluation run, and their post-evaluation run was ranked the 4-th among all 26 runs. \n\n\n### 7.3. BOUN-REX (K\u00f6ksal et al.,  ) \n  \nThe BOUN-REX team participated in Task 2. The team broke the task into two steps: (1) trigger word detection; and (2) determination of trigger type/event type (The event type is determined by trigger type). The team modeled the first step as a question-answering problem, which aims to find the starting and ending index of trigger word spans. The second step was modeled as a classification problem. BERT structures were used to convert input texts into deep latent features, and an objective function that jointly considers the loss w.r.t. the two steps was used in fine-tuning. Various BERT architectures were explored, including BioBERT, BERT-large-uncased, and BERT-large-cased. The BOUN-REX team submitted one run to Task 2, which was ranked the 6-th among all 11 runs. \n\n\n### 7.4. NextMove Software/Minesoft (Lowe and Mayfield,  ) \n  \nThe NextMove Software/Minesoft team participated in all three tasks. They employed the open source tool ChemicalTagger (Hawizy et al.,  ) to detect chemical reactions from patent texts, which directly provides information on chemical entities, chemical properties, trigger words, and Part-of-Speech (PoS) information within input texts. They further adapted ChemicalTagger for tasks in ChEMU lab 2020 in several ways : (1) LeadMine (Lowe and Sayle,  ) was used to recognized chemicals, i.e., ChemicalTagger's tokenization was adjusted accordingly such that all LeadMine entities were treated as single tokens; and (2) Rules were used to cover the differences between the annotation guidelines of ChEMU and outputs of ChemicalTagger, e.g., the definitions of \u201ccatalyst.\u201d The NextMove Software/Minesoft team submitted three runs, two runs, and three runs to Tasks 1\u20133, respectively. Their best run for each task was ranked the 8-th, 4-th, and 2-nd in Tasks 1\u20133, respectively. \n\n\n### 7.5. NLP@VCU (Mahendran et al.,  ) \n  \nThe NLP@VCU team participated in two tasks: Task 1 and Task 2. The team utilized a BiLSTM-CRF model in Task 1, with a concatenated input of pre-trained word embeddings (Mikolov et al.,  ) and character embeddings. In Task 2, the team proposed two models to identify relations between the trigger words and the entities. Their first approach is a rule-based method, which connects a named entity with its closest trigger word within the same sentence. Their second approach is a CNN-based model, which performs a binary classification to determine if a given trigger-entity pair is related or not. The NLP@VCU team submitted three runs to Task 1 and 2, respectively. Their best run for Task 1 was ranked the 11-th among all 26 runs, and their best run for Task 2 was ranked the 7-th among all 11 runs. \n\n\n### 7.6. AU-KBC (Pattabhi et al.,  ) \n  \nThe AU-KBC team participated in Task 1. They investigated two different model architectures to address this task: (1) a CRF model; and (2) an Multi-Layer Perceptron (MLP) model. The inputs of their systems are the syntactic features extracted from patent texts, such as PoS and Phrase Chunk information (noun phrase, verb phrase). Then a CRF model and an MLP model were used to learn to map these input features to desired target outputs, which resulted in the two runs submitted by AU-KBC. They have shown that the CRF model is more effective in terms of capturing contextual information in the inputs, compared with the MLP model. The AU-KBC team submitted two runs to Task 1, and their better run was ranked the 20-th among all 26 runs. \n\n\n### 7.7. LasigBioTM (Ruas et al.,  ) \n  \nThe LasigBioTM team participated in all three tasks. Their system for Task 1 was developed by fine-tuning the BioBERT NER model using the training set plus half of the development set, and using the rest of the dev set as validation set for hyper-parameter tuning. For Task 2, they combined the BioBERT NER model and BioBERT Relation Extraction (RE) model, where the NER model was used for trigger word detection and BioBERT RE model was used for classifying relations between trigger words and named entities. The LasigBioTM team submitted three runs, one run for each task. Their run for Task 1 was ranked the 5-th among all 26 runs. Their runs submitted for Tasks 2 and 3 had technical issues and were not included in the final leaderboard, unfortunately. \n\n\n### 7.8. Melaxtech (Wang et al.,  ) \n  \nThe Melaxtech team participated in all three tasks. Their systems for all tasks were based on the same core model, named Patent_BERT. The Patent_BERT model is a BioBERT pre-trained language model that was trained further using the patent snippets released by ChEMU on the task of masked language modeling. For Task 1, they fine-tuned Patent_BERT using the Bi-LSTM-CRF (Ma and Hovy,  ) model. For Task 2, they adopted a two-staged approach: they first followed the approach for Task 1 to detect trigger words. Afterwards, another binary classifier was built by fine-tuning Patent_BioBERT to determine relations between event triggers and named entities in the same sentence. For Task 3, they combined their models in Tasks 1 and 2 to form a pipeline system to produce end-to-end predictions. Melaxtech submitted three runs to each task. They were ranked the 1-st in all three tasks. \n\n\n### 7.9. Summary of Participants' Approaches \n  \nDifferent approaches were explored by the participating teams. In  , we summarize the key strategies in terms of three aspects: tokenization method, token representations, and core model architecture. \n  \nSummary of participants' approaches. \n  \n BiTeM (Copara et al.,  ); VinAI (Dao and Nguyen,  ); BOUN-REX (K\u00f6ksal et al.,  ); NextMove/Minesoft (Lowe and Mayfield,  ); NLP@VCU (Mahendran et al.,  ); AU-KBC (Pattabhi et al.,  ); LasigBioTM (Ruas et al.,  ); and Melaxtech (Wang et al.,  )  . \n  \nFor teams who participated in Tasks 2 and 3, a common two-step strategy was adopted for relation extraction: (1) identify trigger words; and (2) extract the relation between identified trigger words and entities. The first step is essentially an NER task, and the second step is a relation extraction task. As such, NER models were used by all these teams for Tasks 2 and 3 as well as by the teams participating in Task 1. Therefore, in what follows, we first discuss and compare the approaches of all teams without considering the target tasks, subsequently considering relation extraction approaches. \n\n#### 7.9.1. Tokenization \n  \nTokenization is an important data pre-processing step that splits input texts into words/subwords, i.e., tokens. We identify three general types of tokenization methods used by participants: (1) rule-based tokenization; (2) dictionary-based tokenization; and (3) subword-based tokenization. Specifically, rule-based tokenization applies pre-defined rules to split texts into tokens. The rules applied can be as simple as \u201cwhite-space tokenization,\u201d but can also be a complex mixture of a set of carefully designed rules (e.g., based on language-specific grammar rules and common prefixes). Dictionary-based tokenization requires the construction of a vocabulary and the text splitting is performed by matching the input text with the existing tokens in the constructed vocabulary. Subword-tokenization allows a token to be a sub-string of a word, i.e., subword units, and thus provides a way of handling out-of-vocabulary words. A subword tokenizer learns a set of common subwords based on the word distribution of the training corpus. Thereafter, it can encode rare words by splitting them into meaningful subword units. Popular subword tokenization methods include WordPiece (Schuster and Nakajima,  ) and Byte Pair Encoding (BPE) (Radford et al.,  ). For each participating team, we consider whether their approach belong to one or multiple of the three categories, and summarize our findings in  . Finally, we also indicate whether their tokenization methods consider domain-specific knowledge in  . \n\nFour teams utilized tokenization methods that are purely rule-based. Specifically, VinAI used the Oscar4 tokenizer (Jessop et al.,  ). This tokenizer is particularly designed for chemical texts, and is made up of a set of pattern matching rules (e.g., prefix matching) that are designed based on domain knowledge from chemical experts. NLP@VCU used spaCy tokenizer,  which consists of a collection of complex normalization and segmentation logic and has been proven to work well with general English corpus. NextMove/Minesoft used a combination of the Oscar4 and LeadMine (Lowe and Sayle,  ) tokenziers. LeadMine was first run on untokenized text to identify entities using auxiliary grammars or dictionaries. Oscar4 was then used for general tokenization but is adjusted so that each entity recognized by LeadMine corresponds to exactly one token. Four teams, BiTeM, BOUN-REX, LasigBioTM, and Melaxtech, chose to leverage the pre-trained model BERT (or variants of BERT) to address our tasks, and thus, the four teams used the subword-based tokenizer, WordPiece, that is built-in within BERT. BOUN-REX, LasigBioTM, and Melaxtech used BioBERT model which is language model pre-trained on biomedical texts. Since this model is a continual training based on the original BERT model, the vocabulary used in BioBERT does not differ from BERT, i.e., domain-specific tokenization is not used. However, since Melaxtech performed a pre-tokenization using a toolkit CLAMP (Soysal et al.,  ), we consider their approach as domain-specific, since CLAMP is tailored for clinical texts. BiTeM used the model ChemBERTa that is pre-trained on the ZINC corpus. It is unclear yet whether the tokenization is domain-specific due to the lack of documentation of ChemBERTa. Finally, since WordPiece needs an extra pre-tokenization step, we consider it as a hybrid of rule-based and subword-based method. \n\n\n#### 7.9.2. Representations \n  \nWhen transforming tokens into machine-readable representations, two types of methods are used: (1) feature extraction that represents tokens with their linguistic characteristics, such as word-level features (e.g., morphological features) and grammatical features (e.g., PoS tags); and (2) embedding methods in which token representations are randomly initialized as numerical vectors (or initialized from pre-trained embeddings) and then learned (or fine-tuned) from provided training data. Two teams, NextMove/Minesoft and AU-KBC adopted the first strategy and the other teams adopted the second strategy. Among the teams that used embeddings to represent tokens, two teams, VinAI and NLP@VCU further added character-level embeddings to their systems. All of these six teams used pre-trained embeddings, and five teams used embeddings that are pre-trained for related domains: VinAI and NLP@VCU used the embeddings that are pre-trained on chemical patents (Zhai et al.,  ), BOUN-REX and LasigBioTM used the embeddings from BioBERT that are pre-trained on PubMed corpus. MexlaxTech also used embeddings from BioBERT, but they further tuned the embeddings using the patent documents released in the test phase. \n\n\n#### 7.9.3. Model Architecture \n  \nVarious architectures were employed by participating teams. Four teams, BiTeM, BOUN-REX, LasigBioTM, and Melaxtech, developed their systems based on transformers (Vaswani et al.,  ). BiTeM submitted an additional run using an ensemble of a Transformer-based model and a CNN-based model. They also had a third run that is built based on CRF. The other two teams Melaxtech and BOUN-REX added rule-based techniques into their systems. Melaxtech added several pattern-matching rules in their post-processing step. BOUN-REX focused on Task 2 and their system used rule-based methods to determine the event type of each detected event. Two teams, VinAI and NLP@VCU, used the architecture of BiLSTM-CNN-CRF for Task 1. NLP@VCU also participated in Task 2 and they proposed two systems based on rules, and a CNN architecture, respectively. NextMove/Minesoft utilized Chemical Tagger (Hawizy et al.,  ), a model based on Finite State Machine (FSM), and a set of comprehensive rules are applied to generate predictions. AU-KBC proposed two systems for Task 1, based on multi-layer perceptron and CRF, respectively. \n\n\n#### 7.9.4. Approaches to Relation Extraction \n  \nFour of the above teams participated in Task 2 or Task 3. As mentioned before, these teams utilized their NER models for trigger word detection. Thus, here, we only discuss their approaches for relation extraction assuming that the trigger words and entities are known. \n\nNextMove/Minesoft again made use of ChemicalTagger for event extraction. ChemicalTagger is able to recognize WORKUP and REACTION_STEP words, thus, assignment of relationships was achieved by associating all entities in a ChemicalTagger action phrase with the trigger word responsible for the action phrase. A set of post-processing rules were also applied to enhance the accuracy of ChemicalTagger. \n\nLasigBioTM, NLP@VCU, and Melaxtech formulated the task of relation extraction as a binary classification problem. That is, given each candidate pair of trigger word and named entity that co-locate within an input sentence, the goal of the task is to determine whether the candidate pair of entities are related or not. \n\nLasigBioTM developed a BioBERT-based model to accomplish this classification. The input of BioBERT is the sentence containing the candidate pair but the trigger word and named entity of the candidate pair were replaced with the tags \u201c@TRIGGER$\u201d and \u201c@LABEL$,\u201d respectively. The output of BioBERT is modified as a binary classification layer which aims to predict the existence of relation for the candidate pair. \n\nNLP@VCU proposed two systems for relation extraction. Their first system is a rule-based system. Given a named entity, a relation is extracted between the named entity and its nearest trigger word. Their second system is developed based on CNNs. They split the sentence containing the candidate pair into five segments: the sequence of tokens before/between/after the candidate pair, the trigger word, and the named entity of the candidate pair. Separate convolutional units were used to learn the latent representations of the five segments, and a final output layer was used to determine if the candidate pair is related or not. \n\nMelaxtech continued the use of the BioBERT model re-trained on the patent texts released during the test phase. Similar to LasigBioTM, the input to their model is the sentence containing the candidate pair but only the candidate named entity is generalized by its semantic type in the sentences. Furthermore, rules were also applied in the post-processing step to recover long distance relations, including relations across clauses and across sentences. \n\n\n\n### 7.10. Summary of Observations \n  \nThe various approaches adopted by teams and the resulting performances have provided us with valuable experiences in how to address the tasks and what choices of methods are more suitable for our tasks. \n\n#### 7.10.1. Tokenization \n  \nIn general, domain-specific tokenization tools perform better than tokenization methods that work for general English corpora. This is as expected since the vocabulary of chemical patents contains a large number of domain-specific terminology, and a machine can better understand and learn the characteristics of input texts if the texts are split into meaningful tokens. Another observation is that subword-based tokenization may contribute to overall accuracy. Chemical names are usually long, which make subword-based tokenization a suitable method for breaking down long chemical names. But further investigation is needed to support this claim. \n\n\n#### 7.10.2. Representation \n  \nPre-trained embeddings are shown to be effective in enhancing system performances. Specifically, the Melaxtech and Lasige_BioTM systems are based on BioBERT (Lee et al.,  ) and ranked the first and third place in Task 1. The VinAI system leveraged embeddings pre-trained on chemical patents (Zhai et al.,  ) and ranked second place. Character-level embeddings are also beneficial, shown by the ablation study in Dao and Nguyen ( ) and Mahendran et al. ( ). \n\n\n#### 7.10.3. Model Architecture \n  \nThe most popular choice of model is BERT (Devlin et al.,  ), which is based on Transformer (Vaswani et al.,  ). The model has demonstrated its effectiveness in sequence learning again. The Melaxtech system adopted this architecture and ranked first place in all three tasks. However, it is also worthwhile to note that the architecture of BiLSTM-CNN-CRF is still very competitive w.r.t. BERT. The VinAI system ranked the first place in F -score when relaxed-match is used. \n\n\n\n\n## 8. Error analysis \n  \nWe perform an error analysis on the test set to understand common errors in different tasks. \n\n### 8.1. Task 1\u2014NER \n  \nTo understand the errors in Task 1\u2014NER, we use the top-ranking system as an example. We present the confusion matrix of the top system on the test set in  . The figure shows that the ambiguity between chemical entities is much higher than non-chemical entities. Considering the predicted entities that have labels in the ground-truth set, 141 chemical entities are assigned with wrong labels, while none of non-chemical entities are assigned with wrong labels. In particular, we find it most difficult for the system to tell the differences between STARTING_MATERIAL and REAGENT_CATALYST, REACTION_PRODUCT and OTHER_COMPOUND, and SOLVENT and REAGENT_CATALYST, which correspond to 47 (28 + 19) errors, 32 (20 + 12) errors, and 18 (8 + 10) errors out of the total 141 errors. Next, we present some examples of these errors. \n  \nConfusion matrix of the top system on Task 1\u2014NER. NEG (negative entity): ground-truth (or predicted) entities whose text spans are not annotated as entities in the predicted (or ground-truth) set. This confusion matrix is computed under the scenario of exact span-matching. \n  \n#### 8.1.1. STARTING_MATERIAL vs. REAGENT_CATALYST \n  \nAn example of the system misclassifying STARTING_MATERIAL as REAGENT_CATALYST is shown in  . The snippet in this figure describes the synthesis process of the reaction product \u201c3-[1-(2-hydroxyacetyl)-piperidin-4-yl]-3,4-dihydro-1H-quinazolin-2-one.\u201d Given that the chemical expression of the reaction product contains the subword \u201chydroxyacetyl,\u201d it can be deducted that \u201chydroxyacetic acid\u201d (line 3) is the starting material since   it provides atoms to the reaction product  . However, the system labels hydroxyacetic acid as REAGENT_CATALYST. \n  \nAn example of the system misclassifying STARTING_MATERIAL as REAGENT_CATALYST. \n  \n\n#### 8.1.2. REACTION_PRODUCT vs. OTHER_COMPOUND \n  \nWe present an example of the misclassification between REACTION_PRODUCT as OTHER_COMPOUND in  . The title of this snippet is the name of the chemical compound   N6-(piperidin-4-yl)-N4-(3-chloro-4-fluorophenyl)-7-methoxyquinazoline-4,6-diamine  , but the following chemical reaction describes the synthesis process for preparing   the trifluoroacetate of the title compound  . Thus, in this chemical reaction, the reaction product is no longer the title compound but   the trifluoroacetate of the title compound  . As such, the text span \u201cN6-(piperidin-4-yl)-N4-(3-chloro-4-fluorophenyl)-7-methoxyquinazoline-4,6-diamine\u201d in the first line should be labeled as OTHER_COMPOUND. The system made two mistakes: (1) mislabeled the title compound as REACTION_PRODUCT; and (2) failed to identify the correct span of the reaction product. Both mistakes show that the system is not able to understand the word \u201ctrifluoroacetate\u201d correctly. Here, \u201ctrifluoroacetate\u201d is not an independent chemical compound but an adjective implying that a different variant of the title compound is obtained. Thus, the word \u201ctrifluoroacetate\u201d should not be labeled as an independent reaction product. The word also implies that the actual reaction product is different from the title compound. Thus, the title compound should not be labeled as REACTION_PRODUCT. \n  \nAn example of misclassification between REACTION_PRODUCT and OTHER_COMPOUND. \n  \n\n#### 8.1.3. SOLVENT vs. REAGENT_CATALYST \n  \nWe present an example of the system misclassifying SOLVENT as REAGENT_CATALYST in  . In most training instances, the role of SOLVENT is implied by expressions, such as \u201cA dissolved in B\u201d or \u201ca solution of A in B.\u201d In this figure, however, the sentence structure changes slightly: the solute (HCI) and solvent (aqueous) are consecutive words with no preposition phrases (e.g., in) between them. The role of SOLVENT is instead implied by the expression within the brackets, i.e., \u201c26 mL of a 8 M solution.\u201d In this case, the system failed to understand the role of \u201cAqueous\u201d and mislabels it as REAGENT_CATALYST. \n  \nAn example of the system misclassifying REAGENT_CATALYST as SOLVENT. \n  \n\n\n### 8.2. Task 2\u2014EE \n  \nMost teams broke down this task to two sub-tasks: trigger word identification and predicting relations between trigger words and entities. We first investigate the typical errors for trigger word prediction in Task 2. We present the confusion matrix of the top ranking system in  . The figure shows that more errors (154 errors) are related to trigger word detection and a relatively smaller number of errors (32 errors) are related to trigger word classification. Next we present some examples of different types of errors. \n  \nConfusion matrix of the top system for trigger word prediction in Task 2\u2014EE. WU: WORKUP. RS: REACTION_STEP. NEG (negative trigger word): ground-truth (or predicted) trigger words whose text spans are not annotated as trigger words in the predicted (or ground-truth) set. This confusion matrix is computed under the scenario of exact span-matching. \n  \n#### 8.2.1. Trigger Word Classification \n  \nWe present an example, where the system misclassified a REACTION_STEP as a WORKUP in  . In this figure, the word \u201cadded\u201d (line 6) is predicted as REACTION_STEP but its true label is WORKUP. If we look at its next sentence, we can see that the action of adding sodium thiosulphate solution and hydrochloric acid solution is to dissolve the material so that the desired components can be filtered out. Thus, the word \u201cadded\u201d is part of the procedure to isolate the product, and needs to be labeled as WORKUP. We suspect that this error was caused by sentence segmentation in data preprocessing, since the information needed to make correct decision is only provided by the next sentence. If the two sentences were separated and were not fed into the system simultaneously, the system could not make the correct prediction. \n  \nAn example of misclassifying WORKUP as REACTION_STEP. \n  \n\n#### 8.2.2. Trigger Word Detection \n  \nWe present an example of false positives/negatives in trigger word detection in  . In this chemical reaction, the starting material 2-(2-bromophenyl)acetic acid was dissolved in DCM, and the solution was added with three chemicals: (1) DCC; (2) DMAP; and (3) N-(3,5-dimethyl-1-phenyl-1 H-pyrazol-4-yl)-2-hydroxyacetamide. Here, the three chemicals should be all associated with the reaction step \u201cadded\u201d and the expression \u201cfollowed by\u201d is only to describe the order of the three chemicals being added. However, the system mislabels the word \u201cfollowed\u201d as a REACTION_STEP, leading to a false positive error. The work-up procedure for purifying the chemical compounds contains two steps. First, the obtained suspension was filtered. Then the filtrate was purified. The system fails to detect the first step, leading to a false negative error. \n  \nFalse positive and false negative examples. \n  \n\n#### 8.2.3. Relation Prediction \n  \nThe most significant challenge in relation prediction lies in the fact that many related entities are distant to each other.   gives an example. The word \u201csynthesized\u201d (line 2) is related with the chemical \u201c45\u201d (line 2) and the yield entities \u201c0.83 g,\u201d \u201c0.72 mmol,\u201d and \u201c46%\u201d (line 6). However, the system only discovered the relation between \u201csynthesized\u201d and \u201c45\u201d and missed all other three relations, since the three yield entities are very far from the word \u201csynthesized.\u201d \n  \nAn example of errors in relation prediction. Only the ground-truth labels are included in the figure. In terms of relation prediction, the top ranking system misses three relations: (1) synthesized \u2192 0.83 g; (2) synthesized \u2192 0.72 mmol; (3) synthesized \u2192 46%. \n  \n\n\n\n## 9. Discussion of Results \n  \nThe ChEMU2020 workshop at CLEF was held during 22\u201326 September 2020. Worldwide participants attended the workshop and presented their systems for the tasks. During the discussion session, the Nextmove Software/Minesoft team contributed an important observation: some pairs of training and test snippets are sampled from the same source document, which results in high similarity in their contents. \n\nIn the meantime, the issue of \u201cdata leakage\u201d in existing NLP shared tasks and benchmark datasets has been raised in the NLP community. The data leakage here is not limited to direct leakage of training data, where training instances are repeated in the test set, but is extended to include those more general scenarios where testing instances have significant overlap with the training instances, e.g., the overlap due to the same source documents. \n\nSolutions for controlling data leakage are still under exploration. However, the extent of data leakage needs to be taken into consideration when we interpret our evaluation results. That is, when there is significant similarity between train and test data, models that have huge capacity to memorize training instances are more advantageous than others. On the contrary, a test set that has low similarity with the training set will promote those models that go deep in the training data and learn the knowledge required to generalize. Elangovan et al. ( ) present a study on the data leakage issue in various existing benchmark datasets, including the ChEMU 2020 data, and show that unconscious data leakage may lead to inflated evaluation results, i.e., inadvertently interpreting a model's ability to memorize as the ability to generalize. \n\nIn this section, we provide an extensive study on the train-test overlaps in our ChEMU chemical reaction corpus, and its impact on our evaluation results in Task 1. We investigate three forms of data leakage: (1) leakage caused by texts being from the same source patents; (2) leakage caused by similar text inputs; and (3) leakage caused by similar target entities. \n\n### 9.1. Impact of Source Patents \n  \nPatent snippets extracted from the same source patent may be more similar in terms of linguistic properties, such as their vocabulary, sentence structures, and topical distributions. The snippets in our corpus are sampled from a relatively small set of source patents: 1,500 snippets sampled from 180 patents. The unique source patents that are used in the training, development and test sets are summarized in  . There are 180 unique source patents used in the training set, out of which 101 overlap with the test set, 81 overlap with the development set, and 69 overlap with both the test and development sets. There are 13 source patents in the test set that have not been used in either the train or dev snippets. These 13 source patents have been used to generate 14 patent snippets in the test set. \n  \nNumber of unique source patents in training, development, and test set. Each number represents the number of unique source patents within the sector partitioned by the gray circle boundaries. \n  \nLet   represents the set of 14 patent snippets with new source patents. We evaluate all runs on Task 1 using   and report their F -scores in the column \u201cF\u201d of  . We also evaluate the runs by only considering the compound entities in   and report their F -scores in the column \u201cF .\u201d We also present their performance change on   compared with those on the original test set by measuring their (columns \u201c\u0394F,\u201d \u201c\u0394F ,\u201d \u201c%,\u201d and \u201c% \u201d). \n  \nEvaluation results on  . \n  \n F: F -scores; \u0394F: change in F -scores (compared with  ); %: percentage of absolute change in F -scores; F : F -scores on COMPOUND entities; \u0394F : change in F -scores on COMPOUND entities (compared with  ); % : percentage of absolute change on COMPOUND entities  . \n  \nAs shown by the column\u201c\u0394F\u201d, most runs are found to have drops in performance. But the general trend is that the runs with higher rankings are more robust on the new test set compared with those with lower rankings. The top five runs show a fairly consistent drop in F -score of ~0.08 (a ratio of ~8.0%). The lower ranking runs seem to have much higher variances, and the maximum change observed reaches more than 100%. There are some runs that do not follow this trend, for example, the runs submitted by KFU_NLP, SSN_NLP, and NextMove/Minesoft. The runs submitted by KFU_NLP show very small performance changes (<5%), and so do SSN_NLP runs. The runs submitted by NextMove/Minesoft were found to have more performance changes on  , considering their ranking in  . But more investigations are needed to confirm the reasons behind these observations. \n\n\n### 9.2. Impact of Input Text Similarity \n  \nThe vocabulary similarity between train and test instances in terms of the input text also impact model performances. Models with high memorization capacity may gain more benefits when the input texts of test instances are significantly similar with those of training instances. Herein, we investigate the impact of text similarity by dividing the original test set into several subsets, each of which has a different level of similarity with the training set, and observe the performances of all runs on these subsets. \n\n#### 9.2.1. Text Similarity Computation \n  \nFor each test snippet, we choose the training snippet that is most similar with the test snippet, and use similarity between the two snippets to represent the similarity of the test snippet with the entire training set. To quantify the similarity between two snippets, we convert their input texts into their bag-of-words vector representations. Then the similarity is computed as the cosine similarity between the two bag-of-words. As a result, the similarity value between two snippets is within the range [0, 1] and higher value indicates higher similarity between two snippets. \n\nWe split the test snippets into four groups Q1 to Q4, where the snippets within each group have a similarity of [0.0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1.0], respectively. Thus, Q1 represents the set of test snippets that are most different from the training set, and Q4 represents the set of test snippets that are most similar to the training set. We find that 10% of test snippets belong to Q2, 60% belong to Q3 and 30% belong to Q4. There are no test snippets with <0.25 similarity with the training set, and thus, Q1 is empty. \n\nThe performances on each resultant group are summarized in  . For ease of comparison, for each run, we present its absolute change in F -score on each group of test snippets compared with the original test set, and the percentage of such change. We also present their maximum changes in F -scores across different groups. \n  \nF  scores on Q2-4. \n  \n Q1 is omitted since it's empty. Qi: change in F -scores on set Qi; Qi(%): percentage of absolute change on set Qi, where i\u2208[2, 4]. \u0394: maximum change in F -scores across Q2-4  . \n  \nThe average change in F -scores across Q2-4 (i.e., average value of column \u201c\u0394\u201d) is 0.0659. Only the teams NLP@VCU, NextMove/Minesoft, OntoChem, and AUKBC have slightly greater fluctuations across different quartiles. On average, NLP@VCU has a change of 0.0871, NextMove/Minesoft has a change of 0.0739, OntoChem has a change of 0.0920, and AUKBC has a change of 0.0827. But in general, the fluctuations of all runs across different subsets are similar and relatively small. \n\nThe results in   show that different runs have different trends in F -scores across Q2-4. In general, we expect that a machine learning method will benefit from the train-test similarity and there may be consistent increase in F -scores when the test set is switched from Q2 to Q3, and then Q3 to Q4. Indeed, this phenomenon is observed on most runs. However, there are also a few teams that do not follow this distribution. The teams NextMove/Minesoft, KFU_NLP, and OntoChem achieve their best accuracy on Q3. \n\n\n\n### 9.3. Impact of Entity Similarity \n  \nThe overlap between target entities in the test set may also have influence on model performances. In an extreme case where the target entities in the test set are identical with the entities in the training set, a model can perform well by simply memorizing the entities in the training set and performing dictionary look-up when predicting. Therefore, a test set where the entities highly overlap with the training set will promote the models that are powerful in memorization, and misinterpret their capabilities of memorization as their capabilities of capturing deep contextual information in the input. \n\nTo quantify the train-test overlap in terms of target entities, a straightforward method is to find out the entities that appear in both training and test set and treat these overlapped entities as \u201ceasy entities\u201d that are predictable simply by memorization. However, if an overlapped entity occurs multiple times in the training set and is assigned with different labels in these multiple occurrences, the entity may not be easy to predict by memorization. For example, if an entity \u201cwater\u201d occurs 10 times in the training set, and five of them are tagged as \u201cSOLVENT\u201d and the others as \u201cO\u201d (none-entity token in the BIO scheme), simply memorizing the entity \u201cwater\u201d is not enough to predict its label. Thus, we quantify the   entity predictability w.r.t. memorization   by computing the information-theoretic entropy of an entity in the training set. \n\n#### 9.3.1. Computation of Entity Entropy \n  \nFor each entity in the training set, we compute a probability distribution of it being tagged with the 10 entity labels ( ). Given an entity   e   with a probability distribution   p  (  e  ) = [  p  , \u2026,   l  ] where   p   represents the probability of   e   being tagged as the   i  th label, the entropy   E  (  e  ) of   e   is computed as follows, where   l   represents the total number of labels: \n\nThe entropy value range of an entity is [0.0, 1.0]. The entropy value of 0.0 refers to the case where the entity is always tagged with the same label, and the entropy value of 1.0 refers to the case where the label of an entity is extremely random. If an entity in the test set does not occur in the training set, we set its entropy as 1.0 since we cannot obtain any information about the entity by memorizing its occurrence(s) in the training set. \n\nWe compute the entropy of entities in both the training, development, and test sets. Note that when computing the entropy of all entities, we only consider their occurrences in the training and development set, since we aim to understand the predictability of these entities by memorizing their occurrences in the training and development sets. \n\nThere are 2,393 unique entities in the test set, and 1,160 of them do not appear in the training or test set. Among the rest of the entities, 238 of them have an entropy value ranging within [0.25, 1.0), 310 of them have an entropy value ranging within (0.0, 0.25), and 685 of them have the entropy value equal to 0. \n\nAmong the entities with an entropy of 0, the entity \u201ctitle compound\u201d appears as most frequent in the training and development set: all its 388 occurrences are labeled as REACTION_COMPOUND. Another example is the entity \u201cbrine,\u201d which occurs 216 times and is always labeled as OTHER_COMPOUND. The entity \u201cmethanol\u201d appear a lot in the training and development set, but is much harder to predict (entropy of 0.542), since the diversity of its labels is quite high: 125 of its occurrences are labeled as OTHER_COMPOUND, 66 are labeled as SOLVENT, 37 are labeled as \u201cO\u201d (none-entity), 11 as STARTING_MATERIAL, 10 as REAGENT_CATALYST. Another example is the entity \u201caqueous\u201d with entropy of 0.412: 261 of its occurrences are labeled as OTHER_COMPOUND, 123 as \u201cO,\u201d 42 as \u201cSOLVENT\u201d and 6 as REAGENT_CATALYST. \n\nWe split the entities in the test set into four sets S1 to S4. Set S1 contains the entities with an entropy value of 0.0, S2 contains the entities whose entropy value is within (0.0, 0.25), S3 contains the entities whose entropy value is within [0.25, 1.0), and S4 contains the entities whose entropy value is equal to 1.0. We evaluate all runs on these the four sets. Suppose we are evaluating a model using S1. In this case, we only use the entities in S1 when counting true positives and false negatives. When counting false positives, if an entity predicted by a submitted run has not appeared in the test set, we compute its entropy on the fly. If its entropy lies within the entropy interval of S1, we include it as a false positive, and ignore it otherwise. \n\nThe evaluation results of all runs are summarized in  . For ease of comparison, we only show the differences between the F -scores observed in this experiment and the scores reported in   Table 6  . As shown in the table, the F -scores of most runs decrease as the test set changes from S1 to S4. There are not many exceptions, and only AUKBC-run1 performs slightly better on S3 than S2. This is as expected, since entities in S1 have lower information entropy and are easier to predict compared those in other sets. \n  \nF  scores of all runs on S1 to S4. \n  \n S  i  : change in F -scores on set S  i  , where   i  \u2208  [1, 4]  . \u0394: maximum change in F -scores across four sets  . \n  \nThe average change in F -scores across S1\u20134 (i.e., average value of column \u201c\u0394\u201d) is 0.1367, which is much higher than the average change of 0.0659 in  . Many more teams have higher fluctuations across S1 to S4 compared to the fluctuations across Q2 to Q4. Moreover, the range of the fluctuations of all teams is [0.0312, 0.2619], which is a much wider change compared with  . This may indicate that models are more sensitive to what they need to predict, compared with what they can use for prediction. The top five runs are relatively more robust against the changes in test sets, given their small fluctuations in both  ,  . \n\n\n\n### 9.4. Summary \n  \nIn the above experiments, we generate stratified test sets with controlled similarity over the training set. We use these test sets to re-evaluate the runs we received in Task 1 NER and investigate their capability of generalizing on new data. The above experimental results show that the performances of different systems do change significantly with the test sets. Most models perform better on test sets that are more similar with the training set. On the test set in which only the test instances from new source documents are included, the top ten runs in Task 1 have ~10-point drop on average in F -scores compared with the original test set. On the stratified test subsets where the input text similarity is controlled within each subset, the top ten runs have ~6-point difference in F -scores across all subsets. Similar phenomena is also observed on the stratified subsets where the similarity of target entities is controlled. \n\nAlthough the absolute performances of different runs change with the test set, the ranking of these runs does not change much. For example, the top three teams in Task 1 remain the same across  \u2013 , with almost the same ranking. This shows that the current test set still correctly reflect the ranking of how each model generalize on new test data. \n\nHowever, the strikingly different performances of the same model on different test sets show that avoiding unconscious data leakage is still important. We believe that controlling the source documents of the test instances is crucial in our future shared tasks, since a key feature of a good model is its ability to process   unseen   documents. How to avoid other types of data leakage, e.g., train-test overlap in terms of input texts and target entities, and to what extent we need to control such train-test overlaps, remain a question to us. On one hand, having control over the train-test overlaps is crucial when we interpret our evaluation results: we want to know if a model can generalize well on new data. But extreme elimination of train-test overlap may be infeasible or unnecessary. Ultimately, machine learning models are trying to learn representative distributions underlying the data by capturing such similarities and correlations amongst the training data. We are still exploring methods for mitigating/controlling train-test overlaps. But we believe at least using stratified test sets instead of a single test set will provide more comprehensive evaluation results. \n\n\n\n## 10. Conclusions \n  \nThis paper presents an overview of the activities in the ChEMU 2020 evaluation lab. We introduced our motivation of hosting the lab, the tasks provided by the lab, and the evaluation framework used. We also summarized the evaluation results, discussed participants' approaches, and presented analysis of the results. \n\nThe ChEMU 2020 evaluation lab was hosted to provide tasks that focus on information extraction over chemical patents. Two key information extraction tasks were provided: named entity recognition, which aims to identify chemical compounds and their specific roles in chemical reactions, and event extraction, which aims to identify the single event steps that form a chemical reaction. A new high-quality chemical reaction corpus annotated by chemical experts was made available to the public. The corpus is annotated with fine-grained chemical entities and the relations between reaction steps and these entities. Analysis of the inter-annotator agreement demonstrates high reliability of the annotation. \n\nThe task was held between April 2020 to June 2020. We received registrations from 39 teams, 46 runs from 11 teams, and 8 paper submissions from 8 teams detailing their approaches to address the various tasks. Many effective solutions were reported: the best systems achieved up to nearly 0.98 macro-averaged F -score on the NER task (and up to 0.99 F -score on a relaxed match), 0.95 F -score on the isolated relation extraction task, and around 0.92 F -score for the end-to-end systems. These results strongly outperformed baselines. \n\nComparison of participants' approaches to the tasks confirmed the effectiveness of pre-trained models/embeddings, and indicate that incorporation of domain-specific knowledge is crucial to model performance. We found that the use of domain-specific tokenizers, such as Oscar4 is beneficial to model performance. We also found that systems that used domain-specific embeddings, such as embeddings trained on biochemical texts performed better than those which used embeddings trained on a general English corpus. \n\nFinally, we investigated how the similarity between the training and test sets affect our evaluation results. We investigated three types of train-test similarities, including similarity in source patents, similarity in the patent texts, and similarity in the target entities. We found that train-test similarity had observable influence on model performances. Most runs that we received show a degradation in model performances when the test set has lower similarity with the training set. However, we observed that grammar-based models may behave differently compared with machine learning models, which are purely data-driven, with our results suggesting that manual tuning of rule-based methods may result in some overfitting. We also confirmed that the top ranking runs were the most robust against the changes in the test set. \n\nThe ChEMU 2020 shared task makes an important contribution to progressing the state of the art in automatic extraction of reaction information in chemical patents, with very strong performance exhibited on the key chemical entity recognition and the relations connecting these entities. However, certain limitations in the definition of the current task preclude direct application to the broader full-text chemical patent literature. One such limitation is the reliance on the pre-identified reaction snippets, artificially eliminating significant quantities of additional text in patents from analysis by the model. This pre-segmentation of the full patents clearly simplifies the task by identifying reaction descriptions, thereby eliminating segments potentially confusing to the models. This may explain the high performance of models in the shared task. Another limitation is the restriction to consideration of explicitly mentioned entities, when indirect or generic references abound in these texts. \n\nBoth of these limitations will be addressed in the ChEMU 2021 shared task (He et al.,  ). The creation of a gold standard data set of full patents annotated with reaction spans and references between them, building on the work of Yoshikawa et al. ( ), is underway to facilitate subsequent analysis of reaction spans with the models developed for ChEMU 2020. The ChEMU-Ref dataset (Fang et al.,  ) is also in development to support analysis of anaphoric relations that occur in the reaction texts (Fang et al.,  ). We are looking forward to increased capabilities for text mining of chemical patents for critical chemical reaction information. \n\n\n## Data Availability Statement \n  \nThe Data and Annotation Guidelines for the ChEMU 2020 shared task are available under the CC BY NC 3.0 license on Mendeley Data (Verspoor et al.,  ). The gold standard annotations for the test data will remain \u201cblind\u201d until they are released in late 2021. The evaluation system for the test data will remain open at:   until such time as they are released publicly. \n\n\n## Author Contributions \n  \nJH: managing day-to-day activities of ChEMU lab, evaluation of shared task results, baseline design, and paper writing. DN, ZZ, BF, and HY: data preparation, paper revision, and baseline design. SA, CD, CT, RH, and ZA: data preparation, paper revision, and organization of ChEMU lab. AA: development of submission website. LC, TC, and TB: Contributions to ChEMU team member (staff/student) supervision; paper revision. KV: organization of ChEMU lab, data preparation, baseline design, and paper writing. All authors contributed to the article and approved the submitted version. \n\n\n## Conflict of Interest \n  \nSA, CD, CT, RH, and ZA are employed by the company Elsevier. DN is employed by the company VinAI. HY is employed by the company Fujitsu. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n \n", "metadata": {"pmcid": 8028406, "text_md5": "079c2eccc1f23dcc190f6b6ad9458163", "field_positions": {"authors": [0, 293], "journal": [294, 313], "publication_year": [315, 319], "title": [330, 440], "keywords": [454, 573], "abstract": [586, 2362], "body": [2371, 94035]}, "batch": 1, "pmid": 33870071, "doi": "10.3389/frma.2021.654438", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8028406", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8028406"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8028406\">8028406</a>", "list_title": "PMC8028406  ChEMU 2020: Natural Language Processing Methods Are Effective for Information Extraction From Chemical Patents"}
{"text": "Kononova, Olga and He, Tanjin and Huo, Haoyan and Trewartha, Amalie and Olivetti, Elsa A. and Ceder, Gerbrand\niScience, 2021\n\n# Title\n\nOpportunities and challenges of text mining in aterials research\n\n# Keywords\n\nData Analysis\nComputing Methodology\nComputational Materials Science\nMaterials Design\n\n\n# Abstract\n \nResearch publications are the major repository of scientific knowledge. However, their unstructured and highly heterogenous format creates a significant obstacle to large-scale analysis of the information contained within. Recent progress in natural language processing (NLP) has provided a variety of tools for high-quality information extraction from unstructured text. These tools are primarily trained on non-technical text and struggle to produce accurate results when applied to scientific text, involving specific technical terminology. During the last years, significant efforts in information retrieval have been made for biomedical and biochemical publications. For materials science, text mining (TM) methodology is still at the dawn of its development. In this review, we survey the recent progress in creating and applying TM and NLP approaches to materials science field. This review is directed at the broad class of researchers aiming to learn the fundamentals of TM as applied to the materials science publications. \n   Graphical Abstract      \nData Analysis; Computing Methodology; Computational Materials Science; Materials Design \n \n\n# Body\n \n## Introduction and background \n  \nThe first example of statistical analysis of publications dates back to 1887 when Thomas C. Mendenhall suggested a quantitative metric to characterize authors' writing styles ( ). At that time, the analysis of the literature was widely used to resolve authorship disputes, and, of course, was entirely manual. In the 1940-1960s, the development of computers gave a significant boost to the growth of linguistic analysis. The work of Stephen C. Kleene on regular expressions and finite automata ( ), subsequent formal language theory described by Noam  , and the important fundamental work on information theory by Claude   became the foundation for what is now known as natural language processing (NLP). The following decades brought diverse research results along different aspects of text mining (TM) and NLP: automated generation of article abstracts ( ), regular expressions compilers ( ), automated dialog assistant ( ), the first structured text collection \u2013 the Brown University Standard Corpus of American English ( ), and many others ( ). \n\nIn the 1990s, technological progress permitted storage and access to large amounts of data. This shifted NLP and machine learning (ML) from a knowledge-based methodology toward data-driven approaches ( ). The accelerated development of the Internet and the Web during this decade facilitated information sharing and exchange. This is also reflected in the rapid growth of scientific publications ( ) over this period. Our analysis of the papers indexed in the Web of Science repository shows that since the beginning of 2000s, the number of publications in different fields of materials science has increased exponentially ( ).   \nPublication trend over the past 14 years \n\n Top panel:   Number of publications appearing every year in different fields of materials science. All data were obtained by manually querying Web of Science publications resource. The analysis includes only research articles, communications, letters, and conference proceedings. The number of publications is on the order of 10 .   Bottom panel:   Relative comparison of the fraction of scientific papers available on-line as image PDF or embedded PDF versus articles in HTML/XML format. The gray arrow marks time intervals for both top and bottom panels. \n  \n\nThere are significant opportunities in leveraging data to guide materials research, which is driven by such aspects as property prediction, the search for novel materials, identifying synthesis routes, or determining device parameters. Data are central to the materials informatics enterprise as the availability of large quantities of machine-readable data is a prerequisite to leverage statistical approaches to accelerate materials research ( ). Not surprisingly, early work on data-driven learning approaches therefore focused on the few highly curated datasets in the materials field, such as crystal structure data ( ;  ) or on computed property data which can be generated homogeneously and at high rate ( ;  ;  ). \n\nHowever, knowledge acquisition in materials science must generally be performed across insufficient, diverse, and heterogeneous data. These data range across disparate materials systems and a multitude of characterization approaches to comprehend thermomechanical, electromagnetic and chemical properties ( ). Publications are still the primary way to communicate within the scientific discipline. Therefore, there is substantial potential in capturing unstructured information from the vast and ever-growing number of scientific literature. \n\nTextual information exists in an unstructured or highly heterogeneous format. Manual data extraction is expensive, labor-intensive, and error-prone (although some powerful examples exist in the materials community ( ;  ;  )). As a result, there are tremendous opportunities for large-scale automated data extraction to transform materials science into a more quantitative and data-rich field. \n\nThis review discusses recent advances in automated text processing and information extraction from a large corpus of chemical, physical and materials science publications. We first discuss the methods and approaches widely used in TM and NLP ( ). Then we survey some prominent case studies that are focused on data collection and data mining ( ). We highlight some major challenges and obstacles in scientific TM ( ). Lastly, we discuss potential future research developments for NLP in its application to materials science ( ). \n\n\n## Text mining of scientific literature \n  \nModern computers encode text as a monotonic sequence of bits representing each character but without reflecting its internal structure or other high-order organization (e.g. words, sentences, paragraphs). Building algorithms to interpret the sequences of characters and to derive logical information from them is the primary purpose of TM and NLP. Unlike standard texts on general topics, such as newswire or popular press, scientific documents are written in specific language requiring sufficient domain knowledge to follow the ideas. Application of general-purpose TM and NLP approaches to the chemical or materials science domain requires adaptation of both methods and models, including development of an adequate training sets that comply with the goals of the TM project. \n\nGenerally, a scientific TM pipeline breaks down into the following steps ( ): (i) retrieval of documents and conversion from markup languages or PDF into plain text; (ii) text pre-processing, i.e. segmentation into sentences and tokens, text normalization, and morphological parsing; (iii) text analysis and information extraction; (iv) data normalization and database structuring. The resulting collection either serves as a final product of the TM or provides a source of data for further mining and analysis.   \nSchematic representation of the standard text mining pipeline for information extraction from the scientific publications \n  \n\nWhile a comprehensive discussion of the algorithms and methods used to accomplish each task of the pipeline is beyond the scope of this review, we cover in this Section those methods that are widely applied in scientific TM. We also revise state-of-the-art NLP parsing tools needed to handle chemical and materials science texts. We emphasize the challenges arising along the way and discuss possible solutions. For details and theoretical background on TM and NLP models in general, we refer the reader to the following books ( ): and ( ). \n\n### Obtaining the text corpus \n  \nIn computational linguistics, a large organized set of human-created documents is referred to as a   text corpus  . Scientific discourse generally occurs across a wide variety of document formats and types: abstracts in proceedings, research articles, technical reports, and pre-prints, patents, e-encyclopedias, and many more. There are two primary ways to obtain the text corpus: (i) by using existing indexed repositories with the available text-mining application programming interfaces (APIs) and search tools; or (ii) by having access to an individual publisher's content. \n\n#### Text databases \n  \nA comprehensive overview of scientific text resources can be found in review of  .   lists some common repositories for scientific texts in the domain of chemistry and material science, their document types, and access options. The main advantage of using established databases for TM is the uniform format of their metadata, a convenient API, and sometimes analysis tools. However, the majority of the publications in these repositories are heavily biased toward biomedical and biochemical subjects with a smaller fraction belonging to physics, (in)organic chemistry, and materials science. Moreover, the access to the content is limited: it either requires having a subscription or provides a search over open-access publications only.   \nList of some common text repositories in chemistry and material science subjects that provide an API for querying \n    \n\n\n#### Individual publisher access \n  \nImplementation of a customized scraping routine to screen the publisher's web-pages and download the content requires more effort. However, this approach allows for accessing content from those resources that are not providing an API, for example, e-print repositories. In most cases, downloading and accessing significant publisher content require text and data mining (TDM) agreements. We note that this TDM agreement differs from a standard academic subscription granted to the libraries of the institutions because scraping and downloading large volumes, affect the operation of the publishers' server. \n\nWeb-scraping not only requires a substantial amount of work, but it also has to respond to dynamic web pages in which content is generated by a client browser. In our recent work, we implemented such a solution for Elsevier, RSC, ECS, and AIP publishers ( ). Similarly, ChemDataExtractor ( ) provides the web-scrapers for Elsevier, RSC, and Springer. In the research fields where most of the literature has an open access repository, e.g. physics, mathematics or the rapidly growing literature collection on COVID-19 ( ), the corpus acquisition step will be considerably easier. \n\n\n\n### Conversion into raw text \n  \nIn general, the retrieved content includes the targeted text and other metadata, such as journal name, title, authors, keywords, and others. Querying text databases, as those in  , provide a structured output with raw text ready for processing and analysis. In contrast, web-scraped content usually consists of a complete paper files requiring the additional step to convert it into a raw text. Nowadays, most of the text sources provide as HTML/XML/JSON documents, whereas older papers are usually available as embedded or image PDFs ( ). \n\nWhile parsing of HTML/XML markups can be performed with various programming tools, extraction of the plain text from PDF files is more laborious. Embedded PDFs usually have a block structure with the text arranged in columns and intermixed with tables, figures, and equations. This affects the accuracy of conversion and text sequence. Some work has been done attempting to recover a logical text structure from PDF-formatted scientific articles by utilizing rule-based ( ) and ML ( ;  ) approaches. However, the accuracy of these models measured as F1-score is still below \u223c80%. The authors' experience demonstrates that this can dramatically impact the final output of the extraction pipeline ( ). Hence, the decision on whether to include PDF text strongly depends on the tasks that are being solved. \n\nA great number of documents, in particular, those published before the 1990s, are only available as an image PDF ( ). Conversion of these files into a raw text requires advanced optical character recognition (OCR), and, to the best of our knowledge, the currently available solutions still fail to provide high enough accuracy to reliably extract chemistry ( ;  ). Often, interpretation errors in PDFs originate from subscripts in chemical formulas and equations, and from confusion between symbols and digits. Creating a rigorous parser for PDF articles, and especially an OCR for scientific text is an area of active research in the computer science and TM community ( ;  ). \n\n\n### Text pre-processing, grammatical, and morphological parsing \n  \nThe raw documents proceed through normalization, segmentation, and grammar parsing. During this step, the text is split into logical constitutes (e.g. sentences) and   tokens   (e.g. words and phrases), that are used to build a grammatical structure of the text. Depending on the final text and data mining goal, the text tokens may be normalized by   stemming   or   lemmatization   and processed through the   part of speech tagging   (POS tagging), and   dependencies parsing   to build the sentences structure. These are explained below. \n\n Paragraph segmentation and sentence tokenization   identify, respectively, the boundaries of the sentences and word phrases (tokens) in a text. In general, finding the start/end of a sentence segment requires recognition of certain symbolic markers, such as period (\u201c.\u201d), question mark (\u201c?\u201d), and exclamation mark (\u201c!\u201d), which is usually performed with (un)supervised ML models ( ). State-of-the-art implementations attain \u223c95-98% accuracy (measured as F1-score). However, applying these models to scientific text requires modification. Commonly used expressions such as \u201cFig.\u00a0X\u201d, \u201cet\u00a0al.\u201d and a period in chemical formulas often result in over-segmentation of a paragraph. Conversely, citation numbers at the end of a sentence promote the merging of two sentences together. There is no generally accepted solution to this problem, and it is usually approached by hard-coding a set of rules that capture particular cases ( ). \n\nSentence tokenization, i.e. splitting a sentence into logical constituents, is a crucial step on the way to information extraction, because the errors produced in this step tend to propagate down the pipeline ( ) and affect the accuracy of the final results. Tokenization requires both unambiguous definition of grammatical tokens and robust algorithms for identification of the token boundaries. For general-purpose text, tokenization has been the subject of extensive research resulting in the development of various advanced methods and techniques ( ). However, for chemical and materials science text, accurate tokenization still requires substantial workarounds and revision of the standard approaches.   displays some typical examples of sentence tokenization produced by general-purpose tokenizers such as NLTK ( ) and SpaCy ( ). As in the case of sentence segmentation, the major source of errors is the arbitrary usage of punctuation symbols within chemical formulas and other domain-specific terms. The chemical NLP toolkits such as OSCAR4 ( ), ChemicalTagger ( ), and ChemDataExtractor ( ) implement their own rules- and dictionaries-based approaches to solve the over-tokenization problem. The advantage of chemical NLP toolkits is that they provide good performance on chemical terms, even if the rest of the text may have lower tokenization accuracy.   \nExamples of how different tokenizers split sentences into tokens \n    \n\nHowever, another prominent reason for tokenization errors is the lack of generally accepted rules regarding tokenization of chemical terms consisting of multiple words. For instance, complex terms such as \u201clithium battery\u201d or \u201cyttria-doped zirconium oxide\u201d or \u201c(Na K )NbO \u00a0+ x wt% CuF \u201d often become split into separate tokens \u201clithium\u201d and \u201cbattery\u201d, \u201cyttria-doped\u201d and \u201czirconium\u201d and \u201coxide\u201d, \u201c(Na K )NbO \u201d and \u201c+\u201d and \u201cx wt% CuF \u201d. This significantly modifies the meaning of the tokens and usually results in lowered accuracy of the named entity recognition (see below). Currently, this problem is solved case-by-case by creating task-specific wrappers for existing tokenizers and named entity recognition models ( ;  ;  ). Building a robust approach for chemistry-specific sentence tokenization and data extraction requires a thorough development of standard nomenclature for complex chemical terms and materials names. We discuss this challenge in detail in Section   below. \n\n Text normalization, part-of-speech tagging, and dependency parsing   are often used to reduce the overall document lexicon and to design words' morphological and grammatical features used as an input for entity extraction and other TM tasks ( ). Text normalization usually consists of lemmatization and/or its simpler version \u2013 stemming. While during the stemming the inflected word is cut to its stem (e.g. \u201cchanged\u201d becomes \u201cchang\u201d), lemmatization aims to identify a word's lemma, i.e. a word's dictionary (canonical) form (e.g. \u201cchanged\u201d becomes \u201cchange\u201d) ( ). Stemming and/or lemmatization help to reduce the variability of the language, but the decision whether to apply it or not, depends on the task and expected outcome. For instance, recognition of chemical terms will benefit less from stemming or lemmatization ( ) as it may truncate a word's ending resulting in a change of meaning (compare \u201cmethylation\u201d vs. \u201cmethyl\u201d). But when a word identifies, for example, a synthesis action, lemmatization helps to obtain the infinitive form of the verb and avoids redundancy in the document vocabulary ( ). \n\nPart-of-speech (POS) tagging identifies grammatical properties of the words and labels them with the corresponding tags, i.e. noun, verb, article, adjective, and others. This procedure does not modify the text corpus but rather provides linguistic and grammar-based features of the words that are used as input for ML models. A challenge in identifying the POS tags in scientific text often arises due to the ambiguity introduced by the word's context. As an example, compare two phrases: \u201cthe chemical tube is on the ground\u201d and \u201cthe chemical was finely ground\u201d. In the first case, the general-purpose POS tagger will work correctly, while in the second example, it will likely misidentify \u201cchemical\u201d and \u201cground\u201d as adjective and noun, respectively. Therefore, using a standard POS tagger often requires re-training of the underlying NLP model, or post-processing and correction of the obtained results. \n\nDependency parsing creates a mapping of a linear sequence of sentence tokens into a hierarchical structure by resolving the internal grammatical dependencies between the words. This hierarchy is usually represented as a   dependency tree  , starting from the   root   token and going down to the terminal nodes. Parsing grammatical dependencies helps to deal with the arbitrary order of the words in the sentence and establishes semantic relationships between words and parts of the sentence ( ). Grammatical dependency parsing is a rapidly developing area of NLP research providing a wealth of algorithms and models for general-purpose corpus (see   for specific examples and evaluation). \n\nApplication of the currently existing dependency parsing models to scientific text comes with some challenges. First, sentences in science are often depersonalized, with excessive usage of passive and past verbs tense, and limited usage of pronouns. These features of the sentence are not well captured by general-purpose models. Secondly, the accuracy of the dependency tree construction is highly sensitive to punctuation and correct word forms, particularly verb tenses. As the scientific articles do not always exhibit perfect language grammar, the standard dependency parsing models can produce highly unpredictable results. To the best of our knowledge, these specific challenges of dependency parsing for scientific text have not yet been addressed or explored in detail. \n\n\n### Text representation modeling \n  \nThe application of ML models requires mapping the document into a linear (vector) space. A common approach is to represent a text as a collection of multidimensional (and finite) numerical vectors that preserve the text features, e.g. synonymous words and phrases should have a similar vector representation, and phrases having an opposite meaning should be mapped into dissimilar vectors ( ). Modeling of the vectorized text representation is a broad and rapidly developing area of research ( ). In this section, we highlight only some of the approaches applied to scientific TM, whereas a more detailed discussion of the methods can be found elsewhere ( ). \n\nThe   bag-of-words model   is one of the simplest models of text representation. It maps a document into a vector by counting how many times every word from a pre-defined vocabulary occurs in that document. While this model works well for recognizing specific topics defined by keywords, it does not reflect word context and cannot identify the importance of a particular word in the text. The latter can be solved by introducing a normalization factor and applying it to every word count. An example of such normalization is the   tf-idf model   (  term frequency-inverse document frequency  ) which combines two metrics: the frequency of a word in a document and the fraction of the documents containing the word. The method can thereby identify the terms specific to a particular document. Bag-of-words and tf-idf are the most commonly used models to classify scientific documents or to identify parts of text with relevant information ( ;  ;  ). \n\nWhile bag-of-words and tf-idf are relatively versatile, they do not identify similarity between words across documents. This can be done through   topic modeling   approaches ( ). Topic modeling is a statistical model that examines the documents corpus and produces a set of abstract topics \u2013 clusters of the keywords that characterize a particular text. Then, every document is assigned with a probability distribution over topical clusters. Latent Dirichlet Allocation, a specific topic modeling approach ( ), has been applied to analyze the topic distribution over materials science papers on oxide synthesis ( ) and to classify these papers based by synthesis method used in the paper ( ). \n\nSignificant progress in TM and NLP has been achieved with the introduction of   word embedding   models which construct a vectorized representation of a single word rather than of the entire document. These approaches use the distributional hypothesis ( ) and are based on neural networks trained to predict word context in a self-supervised fashion. Multiple variations of word embeddings models include GloVe ( ), ELMo ( ), word2vec ( ), and FastText ( ). Besides being intuitively simple, the main advantage of word embedding models is their ability to capture similarity and relations between words based on mutual associations. Word embeddings are applied ubiquitously in materials science TM and NLP to engineer words features that are used as an input in various named entity recognition tasks ( ;  ;  ;  ). Moreover, they also seem to be a promising tool to discover properties of materials through words association ( ). \n\nRecently, research on text representation has shifted toward context-aware models. A breakthrough was achieved with the development of   sequence-to-sequence models   ( ) and, later, an   attention mechanism   ( ) for the purpose of neural machine translation (NMT). The most recent models such as Bidirectional Encoder Representations from Transformers (BERT) ( ) and Generative Pre-trained Transformer (GPT) ( ;  ) are multi-layered deep neural networks trained on very large unlabeled text corpora and demonstrate state-of-the-art NLP performance. These models offer fascinating opportunities for the future NLP development in domain of materials science ( ;  ). We discuss them in greater details in the Section 5. \n\n\n### Retrieval of information from the text \n  \nInformation retrieval (IR) represents a broad spectrum of NLP tasks that extract various types of data from the pre-processed corpus ( ). The most ubiquitous IR task is   named entities recognition   (NER) which classifies text tokens in a specific category. In general-purpose text, these categories are usually names of locations, persons, etc., but in scientific literature the named entities can include chemical terms as well as physical parameters and properties. Extraction of action graphs of chemical synthesis and materials fabrication is another class of IR task that is closely related to NER. This task requires identification of action keywords, linking of them into a graph structure, and, if necessary, augmenting with the corresponding attributes characterizing the action (e.g. the action \u201cmaterial mixing\u201d can be augmented with the attribute \u201cmixing media\u201d or \u201cmixing time\u201d). Lastly, data extraction from figures and tables represents another class of information that can be retrieved from scientific literature. This requires not only TM methods but also image recognition approaches. In this section we will mainly review the recent progress for chemical and materials NER and action graphs extraction and will provide a brief survey of the efforts spent on mining of scientific tables and figures.   \nSchematic representation of various information types that can be extracted from a typical materials science paper \n  \n\n Chemical NER   is a broadly defined IR task. It usually includes identification of chemical and materials terms in the text but can also involve extraction of properties, physical characteristics, and synthesis actions. The early applications of chemical NER were mainly focused on extraction of drugs and biochemical information to perform more effective document searches ( ;  ;  ;  ). Recently, chemical NER has shifted toward (in)organic materials and their characteristics ( ;  ;  ;  ), polymers ( ), nanoparticles ( ), synthesis actions and conditions ( ;  ;  ;  ). The methods used for NER vary from traditional rule-based and dictionary look-up approaches to modern methodology built around advanced ML and NLP techniques, including conditional random field (CRF) ( ), long short-term memory (LSTM) neural networks ( ), and others. A detailed survey on the chemical NER and its methods can be found in recent reviews ( ;  ;  ). \n\nExtraction of chemical and materials terms has been a direction of intensive development in the past decade ( ;  ). The publicly available toolkits use rules- and dictionaries-based approaches (e.g LeadMine ( )), statistical models (e.g OSCAR4 ( )), and, predominantly, the CRF model (e.g. ChemDataExtractor ( ), ChemSpot ( ), tmChem ( )) to assign labels to chemical terms. Some recent works implemented advanced ML models such as bidirectional LSTM models ( ;  ;  ) as well as a combination of deep convolutional and recurrent neural networks ( ) to identify chemical and material terms in the text and use context information to assign their roles.   shows a few examples of the NER output obtained using some of these tools and compares it to non-scientific NER models implemented in NLTK ( ) and SpaCy ( ) libraries.   \nExamples of chemical NER extraction \n    \n\nOften, the objective of scientific NER task is not limited to the identification of chemicals and materials, but also includes recognition of their associated attributes: structure and properties, amounts, roles, and actions performed on them. Assigning attributes to the entities is usually accomplished by constructing a graph-like structure that links together all the entities and build relations between them. A commonly used graph structure is the grammatical dependency tree for a sentence (see  ). Traversing the sentence trees allows for resolving relations between tokens, hence, link the entities with attributes. ChemicalTagger ( ) is one of the most robust frameworks that extends the OSCAR4 ( ) functionality and provides tools for grammatical parsing of chemical text to find the relation between entities and the corresponding action verbs. Similarly, ChemDataExtractor ( ) can identify the chemical and physical characteristics (e.g. melting temperature) in the text and assign it to a material entity. A rules- and dictionaries-based relation-aware chemical NER model has been proposed by   to build a search engine for publications.   used the random forest decision model to resolve synonyms between chemical entities and materials-related terms.   applied a two-step LSTM model to resolve the role of materials in a synthesis procedure.   used convolutional neural network model to build relations between materials, their mechanical properties and processing conditions which were extracted from publications by keywords search. Lastly, a combination of advanced NLP models has been recently used to extract the materials synthesis steps and link them into an action graph of synthesis procedures for solid-state battery materials ( ) and inorganic materials in general ( ). \n\nDespite significant effort, the accuracy of the NER for chemical names and formulas is still relatively low compared to the general state-of-the-art NER models ( ;  ).  A displays the overall precision and recall for different chemical NER models reported in the corresponding publications. Both, precision and recall of the models vary from 60% to 98% ( A), whereas for the general-purpose NER, these values are >91% (see  ). There are two major challenges that obstruct training of high-accuracy chemical NER models: (i) the lack of unambiguous definitions of the chemical tokens and their boundaries, and (ii) the lack of the robust annotation schema as well as comprehensive labeled training sets for the supervised ML algorithms. Oftentimes, researchers manually create their own training set for specific tasks but with limited use for more general goals. Therefore, the success of chemical NER becomes a trade-off between the size of the annotated set and model complexity: either using simple model with limited capabilities on a small set of labeled data, or investing effort into annotation of a large dataset and using it with advanced models providing a higher accuracy of data extraction.   \nAccuracy of chemical NER extraction \n\n(A) Precision and recall of the published models for chemical NER manually extracted from the reports \n\nColor denotes the primary algorithm underlying the model. \n\n(B) Accuracy of the data extracted from materials synthesis paragraphs plotted against the complexity of the paragraphs. The accuracy is computed using chemical NER models developed by our team ( ;  ) to the manually annotated paragraphs. The text complexity is calculated as a Flesch-Kincaid grade level (FKGL) score indicating the education level required to understand the paragraph ( ). \u03c1 is a Pearson correlation coefficient between the accuracy of NER model and the FKGL score. \n  \n\nAn early attempt in creating a labeled data set for the chemical NER task was done by   and  . The GENIA and CHEMDNER sets provide annotation schema and labeled data of chemicals and drugs extracted from MEDLINE and PubMed abstracts, respectively. However, these corpora are heavily biased toward biomedicine and biochemical terms with only a small fraction of organic materials names present. The progress of the past few years brought a variety of annotated corpora to the materials science domain. Among the publicly available labeled dataset, there is the NaDev corpus consisting of 392 sentences and 2,870 terms on nanocrystal device development ( ), the data set of 622 wet lab protocols of biochemical experiments and solution syntheses ( ), a set of 9,499 labeled sentences on solid oxide fuel cells ( ), and an annotated set of 230 materials synthesis procedures ( ). \n\n Extraction of information from tables and figures   is another branch of scientific IR that has been rapidly developing in the past few years. The specific format of the figures and tables in scientific papers imposes substantial challenges for the data retrieval process. First, it is common that images (and sometimes the tables) are not directly embedded in the HTML/XML text but instead contain a link to an external resource. Second, connecting tables/images to the specific part of the paper text is an advanced task that does not have a robust solution to date. Third, both tables and images can be very complex: images can include multiple panels and inserts that require segmentation, while tables may have combined several rows and columns imposing additional dependencies on the data. To the best of our knowledge, only a few publications have attempted to parse tables from the scientific literature using heuristics and machine learning approaches ( ;  ). \n\nImage recognition methods have been broadly used in materials science but have so far been primarily focused on extracting information about the size, morphology, and the structure of materials from microscopy images. To date, the existing solutions for interpretation of microscopy images use variations of convolutional neural networks, and address diverse spectra of materials science problems ( ;  ;  ;  ). While these models demonstrate a remarkable accuracy when applied directly to microscopy output, they are not intended to separate and process the images embedded in scientific articles. Steps toward parsing of article's images were reported recently.   developed the ImageDataExtractor tool that uses a combination of OCR and CNN to extract the size and shape of the particles from microscopy images.   used Google Inception-V3 network ( ) to create the Livermore SEM Image Tools for electron microscopy images. This tool was later applied by   to \u223c35,000 publications to obtain information about the variability of nanoparticles sizes and morphologies. \n\n\n\n## Using text mining in materials science: case studies \n  \nData-driven materials discovery usually relies either on computational methods to calculate the structure and properties of materials and collect them in databases ( ), or on experimental datasets that have been painstakingly collected and curated. Development of advanced approaches for scientific TM creates broad opportunities to augment such data with a large amount of reported but uncollected experimental results. A few large-scale data sets extracted from the scientific publications have become available over the last few years ( ;  ;  ;  ;  ). In this Section, we survey the publicly available data sets created by retrieval of information from chemistry, physics, and materials science publications and discuss the most interesting results obtained from them. \n\n### Publicly available collections of text-mined data \n  \nWhile recently several data collections have been obtained by automated TM and NLP-based pipelines, there are a few large-scale data sets that have been manually extracted from scientific publications and are worth mentioning here. \n\nThe Pauling File Project ( ) is one of the biggest manually curated collections of data for inorganic crystalline substances, covering crystallographic data, physical properties, and phase diagrams. The Pauling File Project provides data for the Materials Platform for Data Science ( ), Pearson\u2019s Crystal Data ( ), and Springer Materials ( ). Together, it contains more than 350,000 crystalline structures, 150,000 physical properties, and 50,000 phase diagrams extracted from the scientific literature in materials science, engineering, physics, and inorganic chemistry from 1891 to present. The quality and accuracy of the extracted records are high, and they include expert interpretation and a summary of the original text. Nonetheless, significant human labor is required to maintain and update this database. Moreover, due to the human interpretation of the data, the records are highly heterogeneous and may require additional processing and normalization. \n\nThe Dark Reactions Project ( ) is another prominent dataset extracted manually from laboratory journals containing 3,955 parameters of failed hydrothermal synthesis experiments ( ). So-called \u201cnegative\u201d sampling data are critical for ML applications that need to predict a \u201cyes/no\u201d answer. Unfortunately, the \u201cno\u201d results, i.e. unsuccessful experimental outcomes, are rarely published or made available to the broad research community. The Dark Reaction Project represents the first attempt to demonstrate the importance of sharing negative-result data within the chemistry and materials science domain. \n\nA substantial effort in the automated extraction of materials properties from scientific publications has been done by the research group of J. Cole (University of Cambridge, UK). They developed ChemDataExtractor ( ), an NLP toolkit for chemical text and used it to build a large collection of phase transition temperatures of magnetic materials ( ), and a dataset of electrochemical properties of battery materials ( ). The first set contains 39,822 records of Curie and N\u00e9el temperatures for various chemical compounds retrieved from 68,078 research articles ( ). These data augment the MAGNDATA database \u2013 a collection of \u223c1,000 magnetic structures manually extracted from publications by Gallego et\u00a0al. ( ,  ). The battery data set includes 292,313 records collected from 229,061 papers covering electrochemical properties of battery materials such as capacity, voltage, conductivity, Coulombic efficiency, and energy density. It enhances by more than an order of magnitude the manually constructed data set of   containing 16,000 property entries for Li-ion battery materials extracted from 200 publications. \n\nA large-scale text-mined data collection of materials synthesis parameters has been developed by our team during the past few years.   generated a data set of synthesis operations and temperatures for 30 different oxide systems mined from 640,000 full-text publications. Later on, this set was extended by 1,214 sol-gel-synthesis conditions for germanium-based zeolites ( ). A\u00a0collection of 19,488 solid-state ceramics synthesis reactions containing precursors chemicals, synthesis\u00a0steps and their attributes was generated from 53,538 materials science papers by  . \n\nIt is important to highlight that although the TM and NLP methods help to generate large-scale data sets, the output can suffer from lower accuracy of extraction as compared to any manually curated data set. For instance, the extraction precision of the Curie and N\u00e9el temperatures are \u223c82% ( ), and that of the electrochemical properties \u2013 \u223c80% ( ), meaning that up to \u223c20% of the obtained records have one or more attributes incorrectly extracted. The dataset of oxides synthesis parameters shows categorical accuracy (i.e. the fraction of the predicted labels of the text tokens that match the true labels) for the chemical NER task of \u223c81% ( ). For the data set of solid-state synthesis reactions, precision (i.e. fraction of correctly extracted entities) of extracted synthesis parameters varies from \u223c62% for fully accurate retrieval of synthesis conditions, to \u223c97\u201399% for extraction of precursor materials and final products ( ). \n\n\n### Text-mining-driven materials discoveries \n  \nResearch exploring TM-based data-driven approaches to provide insights on materials emerged well before any progress in the development of robust NLP tools had been made. Several groups have attempted manual information extraction from a narrow set of publications with a specific scope. \n\nThe group of T. Sparks (University of Utah, US) explored the correlation between materials performance and the elemental availability for high-temperature thermoelectric materials ( ) and Li-ion battery materials ( ). In both of these publications, the sets of physical parameters for materials classes were manually retrieved from queried materials science literature, and augmented with data on market concentration and Earth abundance for chemical elements. Based on this data the importance of considering global market state and geopolitical factors when designing materials was discussed. \n\nAn analysis of cellular toxicity of cadmium-containing semiconductor quantum dots was performed by applying random forest models to the 1,741 data samples manually collected from 307 relevant publications ( ). The authors found that the toxicity induced by quantum dots strongly correlates with their intrinsic properties, such as diameter, surface ligand, shell, and surface modification. \n\nThe data set of failed hydrothermal synthesis reactions collected in the course of the Dark Reactions Project (see above) was used to explore synthesis routes for organically templated vanadium selenites and molybdates ( ). In particular, the authors applied support vector machine and decision tree models to define the upper/lower boundaries of the synthesis parameters that lead to formation of crystals from solution. The suggested synthesis routes were tested against real experiments and showed 89% success rate exceeding human intuition by 11%. \n\nAlthough the manual approach to abstract a large text corpus is very laborious, it allows for obtaining high-quality data from the tables and figures as well as from the text, thus justifying the small size of these data sets. Nonetheless, a growing amount of research uses the automated TM pipelines to obtain a collection from which to initiate data-driven materials discoveries. \n\n developed a semi-automated TM pipeline to extract and analyze the growth conditions for four different oxide materials synthesized with pulsed laser deposition technique. They were able to obtain the range of growth temperatures and pressures and predict the relative values of critical temperatures by applying a decision tree classifier. \n\n applied a TM pipeline to effectively screen and sort organic dyes for panchromatic solar cells. Their approach identified 9,431 dye candidates which were then narrowed down to five prospective molecules for experimental validation. This work is an important step toward a so-called \u201cdesign-to-device\u201d approach to fabrication of advanced materials ( ). The approach consists of the four steps of (i) data extraction from literature, (ii) data augmentation with computations, (iii) AI-guided materials design, and (iv) experimental validation. \n\nIn other work,   used the records of Curie and N\u00e9el temperatures text-mined from the scientific literature ( ) (see previous section) to reconstruct the phase diagrams of magnetic and superconducting materials. They used the materials bulk and structural properties as descriptors in ML models to predict the critical temperature for a magnetic phase transition. The trained models are formulated into a web application that provides multiple options for predicting and exploring magnetic and superconducting properties of arbitrary materials ( ). \n\nOur team has extensively used TM aiming to uncover insights about materials synthesis from scientific publications.   explored the parameters of hydrothermal and calcination reactions for metal oxides by analyzing the data extracted from 22,065 scientific publications. They found a strong correlation between the complexity of the target material and the choice of reaction temperature. A decision tree model applied to predict synthesis routes for titania nanotubes identified the concentration of NaOH and synthesis temperature as the most important factors that lead to nanotube formation. A similar approach was used to predict the density of germanium-containing zeolite frameworks and to uncover their synthesis parameters ( ). \n\nIn other work,   applied a variational autoencoder to learn the latent representation of synthesis parameters and to explore the conditions for the synthesis of TiO  brookite and for polymorph selection in the synthesis of MnO . Their results showed that the use of ethanol as a reaction medium is a sufficient but not necessary condition to form the brookite phase of TiO . Their latent representation of synthesis parameters also captures the requirement of alkalai ions for the generation of certain MnO  polymorph, consistent with   ab initio   findings ( ). A conditional variational autoencoder was also used to generate a precursors list for some perovskite materials ( ). \n\nBuilding relations between materials, their properties and applications and combining them into a so-called   knowledge graph   structure is an emerging area of research in materials science that became enabled by the development of scientific TM.   implemented the Computer-Aided Material Design (CAMaD) system which is an elegant TM framework that reconstructs and visualizes a knowledge graph in the form of a process-structure-property-performance chart for desired materials. While the presented performance of the CAMaD system is still limited, it demonstrates the capabilities of TM to create a comprehensive knowledge-based structure that can be used for optimization of materials design. \n\nThe relation between materials reported in the different application areas of materials science was explored by  . They applied the word2vec model ( ) to 3 million abstracts to learn a vectorized representation of words and materials specifically. Interestingly, the model was able to not only learn some aspects of the chemistry underlying the relations between materials but also to draw a similarity between materials for different applications. In particular, it was demonstrated that such a cross-field correlation between the material properties required in different application could be used to predict novel thermoelectric materials. This work highlights an important aspect of scientific TM and NLP: its capability to uncover latent knowledge about a subject by comprehending a large amount of unstructured data \u2013 a task that is not possible for a human. \n\nThe question of materials similarity was also studied by  . In their work, a measure of similarity for synthesis precursors was defined by two parameters: (i) the probability to substitute one precursor with another in the synthesis reaction for a common target material, and (ii) the area of overlap of synthesis temperature distributions for two precursors. The results demonstrate that some of the empirical rules widely used by researchers when choosing the precursors for materials synthesis can be learned from text data. \n\n\n\n## Challenges and Caveats of the Text-Mining-Driven Research \n  \nWhile TM and NLP are tremendously promising tools to extract the enormous amount of information locked up in published research, several challenges for the approach remain. We categorize these below. \n\n### Lack of annotated data \n  \nThe lack of a large dataset corresponding to a \u201cgold standard\u201d of annotated data significantly slows down the development of robust high-precision methods for chemical NER. The majority of the existing annotated sets have been created to serve a specific purpose or subfield of materials science and their broad application is not straightforward. Current attempts to create standardization for annotated data in materials science are limited to chemical named entities with emphasis on organic chemistry ( ;  ;  ). Building more structured databases of experimental data that can be related to the papers from which the data are sourced, could potentially help to test the performance of NLP methods. One can even conceive creating machine-annotated data based on an existing relation between data and publications. We are, however, not hopeful that the scientific community can come together around central data deposition without an incentive structure from publishers or government agencies, which further stresses the important role that TM will have in generating large amounts of materials data. \n\n\n### Ambiguity and lack of standard nomenclature to describe and categorize complex materials \n  \nAn engineering material is not merely a compound that requires a chemical description. It can be a doped system, inhomogeneous, a multi-phase system, or a composite. Each of these complexities comes with its morphology and length scale. While for common chemical terms, IUPAC provides nomenclature recommendations, writers usually prefer to simplify them or use arbitrary notations for materials names if no standard terminology is established. For instance, even for a basic concept such as a doped material, various nomenclatures are used e.g. \u201cSc (MoO ) :Eu \u201d, \u201cSc (MoO ) \u00a0+ x% Eu \u201d or \u201cEu -doped Sc (MoO )\u201d. Composites and mixtures can be written in various ways (e.g. (1-x)Pb(Zr Ti )O -xBaTiO  or Pb(Zr Ti )O \u00a0+ x wt% BaTiO ). The abbreviated names of chemicals and materials (e.g. EDTA, BNT-BT-KNN, LMO) are also ubiquitous. Even within one journal or publisher no standards are applied. This complicates comparison and aggregation of extracted data across papers and requires substantial data post-processing in order to normalize and unify the results. In some cases it creates ambiguity that cannot be resolved, or whose resolution leads to errors. \n\n\n### Positive bias \n  \nAuthors often \u201ccherry-pick\u201d data in the main body of a paper, either leaving out less successful data or moving it to supplementary information (which is often only available as PDF and with too low information content to do meaningful automated data extraction). This positive bias introduces substantial problems for ML models trained on these data, and requires caution when choosing the questions which one asks from ML models. In recent work,   explored the effect of human bias in the choice of starting materials for the synthesis of metal organic crystals. They found a strong preference in the literature for selecting some reagents over others which was attributed to historically established rule-of-thumbs. In their explicit experimental testing they found no value of the implied precursor selection bias, something that an ML based on the published data would not have been able to resolve without additional data. In our own work on the prediction of novel compounds ( ;  ) or their synthesis methods ( ), the lack of negative information is severely limiting. For example, the lack of a known compound at a given composition in a complex phase diagram may mean that no compound exists at that composition, or, that nobody has looked carefully for it. These are very different pieces of input information for an ML model that tries to predict which compositions are compound forming or not. One can imagine that some researchers may have investigated the specific composition, but because they did not find anything, the investigation was not reported. In a similar problem, failed synthesis experiments are rarely reported. This lack of negative data prevents one from capturing the boundaries on the space of possible ML outcomes. The effect of human bias on the quality of ML model predictions has not been investigated in detail and remains a challenging aspect of NLP-based data collections. \n\n\n### Language complexity and error accumulation \n  \nThe narrative of a research paper is known to have a very specific style and language. It was shown for the corpus of newspapers of various subjects that the texts covering a scientific topic have the lowest readability score as compared to other topics, such as sports or weather ( ). To explore the dependence between complexity of a scientific paragraph and the quality of the data extraction, we computed the categorical accuracy (fraction of predicted values that match with actual values) of data extraction for \u223c100 manually annotated paragraphs on materials synthesis and their corresponding Flesch-Kincaid grade level (FKGL) score ( ).  B shows the extraction accuracy of synthesis steps and material entities per each paragraph obtained using the NLP models developed by our team previously ( ;  ), plotted against the corresponding FKGL score. Although the data are highly scattered, the negative correlation trend between the extraction accuracy and the FKGL score can be noticed. The computed Pearson correlation coefficients between the value of the FKGL score and the extraction accuracy of synthesis steps and materials entities are \u22120.42 and \u22120.38, respectively. It is worth noting that the correlation is stronger when the NLP model is applied to extract synthesis steps rather than materials entities. This can be explained with the fact that the context of a sentence defining a synthesis action is more ambiguous than that for materials terms ( ). This complexity stresses the need to improve the general NLP tools to deal with scientific text. The accuracy of the text processing along the TM pipeline is crucial as errors usually accumulate from step to step, leading to a strong reduction in quality and size of the output ( ). As was noted before, the problem with sentence tokenization significantly affect the outcome of information extraction, in particular, chemical NER. Overcoming this problem may be possible by developing a hybrid NLP methods that introduces domain knowledge. \n\nThe accuracy of scientific NLP imposes constraints on the potential range of questions that the extracted data can address.   have investigated the viability and fidelity of ML modeling based on a text-mined dataset. They used various ML algorithms and material structure models to predict the discharge capacity of battery materials after 25 cycles based on a dataset extracted from the literature and found inconclusive results. While one can speculate on the origin of this outcome, it is clear that the high level of uncertainty of the predictions can arise from invalid descriptors or models, as well as from the human bias and imperfectness of the experimental measurements ( ). As the \u201cno-free-lunch\u201d theorem states, there is no any particular ML model that will work best for a given task. Therefore, interpretation of results obtained by application of ML algorithms to text mined data should always be treated with caution and keeping the limitations of the input data in mind. In general, limitations of ML predictions are much more likely to be caused by limitations of input data than by problem with the ML method. \n\n\n\n## Future directions \n  \nData are considered the fourth paradigm of science ( ). Access to a large amount of data allows the quantification and more accurate testing of hypothesis, and even potentially the machine learning of the relation between composition, structure, processing and properties of materials. The Materials Genome Initiative (MGI) ( ) led to some highly successful data-driven research projects (e.g.  ,  funding/pgm_summ.jsp?pims_id\u00a0= 505073 and   and  ). But the personal experience of one of the authors in helping launch MGI is that experimental data is unlikely to be collected one piece at a time, by having scientists enter it in databases, the way it was envisioned by some when MGI started. While ML is an exciting new direction for materials research, it is telling that much of published ML work is either on computed data sets (which can be generated with high-throughput computing) ( ), or on very small experimental datasets, often containing no more than 50\u2013100 data items. Because of this failure to collect experimental data in more organized ways, TM and NLP are likely to play a critical role in enabling more data-driven materials research. The willingness of publishers to share access to their large corpus for TM and several new developments in the NLP field are likely to lead to increased volume and quality of extracted information from scientific text. \n\nThe most notable advance in NLP in recent years has been the advent of   transformer models  , which have dramatically improved state-of-the-art performance on almost all benchmark tasks. The transformer uses an idea of sequence encoding-decoding ( ) and creates a latent vectorized representation of a text. The advantage of the model is its attention functionality ( ) that allows for the model to recognize the key parts of a sequence that are crucial for understanding the meaning of text. The transformers have ushered in a new paradigm in NLP, whereby very large general-purpose models (with typically hundreds of millions of parameters) are pre-trained on publicly available corpora with unsupervised objective, before being fine-tuned to individual tasks. This so-called   transfer learning   approach allows the transformer to have high performance on supervised-training tasks with only a small number of training examples, significantly reducing the burden on human annotation. \n\nFrom a materials science perspective, the transfer learning still meets some difficulties. The publicly available transformer models are pre-trained on general-purpose corpora, thus performing poorly on tasks involving scientific language. Moreover, the computational cost to train them \u201cfrom scratch\u201d is also significant: training BERTLarge on a corpus of 3.3 billion words with 64 TPU cores took 4\u00a0days ( ). There have been a number of recent efforts to pre-train domain-specific transformer models on scientific text, including SciBERT ( ), BioBERT ( ), and MedBERT ( ). Although the corpus of available materials science publications ( ) is of comparable size to the corpora used to train the original BERT models, no materials science-specific pre-trained BERT-style model is publicly available to date. Training and release of such a model would be of tremendous impact for the materials science community. \n\nProminent progress has been also achieved for Neural Machine Translation (NMT), providing an opportunity to apply TM on scientific literature written in non-English languages. While NMT has reached parity with human translation in a number of languages ( ), the dominant methodology relies on supervised training on a large bilingual corpus with parallel texts in source and target languages. However, there are significant difficulties in implementing the parallel-translation approach tailored specifically to the peculiarities of the scientific text. The domain-specific vocabulary of scientific texts requires a significant bilingual corpora for training the parallel-translation model ( ). The latest development in unsupervised NMT models ( ,  ;  ) utilizes monolingual corpora, escaping the need for parallel texts. This opens possibilities for domain-specific training of the NMT and its application to the non-English scientific text. \n\nAs mentioned previously, the lack of large-scale annotated datasets often obstructs application of advanced NLP techniques for scientific TM. Crowd-sourcing for data collection may be a solution to this problem. Diverse approaches to collaborative data management have been widely used in projects such as OpenEI ( ), Folding@home ( ) and others ( ;  ), as well as have proven to be highly efficient for gathering a large amount of data. To date, only a few projects have utilized crowd-sourcing in materials science TM research ( ;  ). But development of a collaborative data collection platform for application of NLP in materials science meets several challenges. First, building and maintenance of the software part requires a substantial labor investment one for which government science agencies do not seem quite ready for. Second, efficient data collection and annotation requires well established standards for labeling of scientific texts that can be unambiguously applied to a wide variety of research tasks. \n\nThe accelerated development of high-throughput computations and emergence of \u201cbig data\u201d in materials science in the past few years has shifted focus toward data management and curation. This has resulted in engineering and production of high-quality databases with flexible graphical interfaces and programming APIs that provide facile and convenient access to the data for their mining and analysis ( ). Rapidly growing sets of the data extracted from scientific publications call for development of a similar advanced infrastructure for representations, maintenance and distribution of these data. \n\nPrevalent, broad and accurate data are a pillar of science. It inspires, negates, or validates theories. In society and business, data has become a highly valued commodity from which to take strategic decision, construct more effective marketing campaigns, or to improve products. For materials science to fully benefit from the new data paradigm significantly more effort will need to be directed toward data collection. TM and NLP are clearly a tool to make the results of hundred years of materials research available toward the realization of this paradigm. \n\n \n", "metadata": {"pmcid": 7905448, "text_md5": "3408d6a71ae4cd84426e1bdb2ce1832f", "field_positions": {"authors": [0, 109], "journal": [110, 118], "publication_year": [120, 124], "title": [135, 199], "keywords": [213, 298], "abstract": [311, 1465], "body": [1474, 60948]}, "batch": 1, "pmid": 33665573, "doi": "10.1016/j.isci.2021.102155", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7905448", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7905448"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7905448\">7905448</a>", "list_title": "PMC7905448  Opportunities and challenges of text mining in aterials research"}
{"text": "Weber, Leon and S\u00e4nger, Mario and M\u00fcnchmeyer, Jannes and Habibi, Maryam and Leser, Ulf and Akbik, Alan\nBioinformatics, 2021\n\n# Title\n\nHunFlair: an easy-to-use tool for state-of-the-art biomedical named entity recognition\n\n# Keywords\n\n\n\n# Abstract\n \n## Summary \n  \nNamed entity recognition (NER) is an important step in biomedical information extraction pipelines. Tools for NER should be easy to use, cover multiple entity types, be highly accurate and be robust toward variations in text genre and style. We present   HunFlair  , a NER tagger fulfilling these requirements. HunFlair is integrated into the widely used NLP framework   Flair  , recognizes five biomedical entity types, reaches or overcomes state-of-the-art performance on a wide set of evaluation corpora, and is trained in a cross-corpus setting to avoid corpus-specific bias. Technically, it uses a character-level language model pretrained on roughly 24 million biomedical abstracts and three million full texts. It outperforms other off-the-shelf biomedical NER tools with an average gain of 7.26 pp over the next best tool in a cross-corpus setting and achieves on-par results with state-of-the-art research prototypes in in-corpus experiments.   HunFlair   can be installed with a single command and is applied with only four lines of code. Furthermore, it is accompanied by harmonized versions of 23 biomedical NER corpora. \n\n\n## Availability and implementation \n  \n HunFlair   ist freely available through the   Flair   NLP framework ( ) under an MIT license and is compatible with all major operating systems. \n\n\n## Supplementary information \n  \n are available at   Bioinformatics   online. \n\n \n\n# Body\n \n## 1 Introduction \n  \nRecognizing biomedical entities (NER) such as genes, chemicals or diseases in unstructured scientific text is a crucial step of all biomedical information extraction pipelines. The respective tools are typically trained and evaluated on rather small gold standard datasets. However, in any real application they are applied \u2018in the wild\u2019, i.e. to a large collection of texts often varying in focus, entity distribution, genre (e.g. patents versus scientific articles) and text type (e.g. abstract versus full text). This mismatch can lead to severely misleading evaluation results. To address this, we recently released the   HUNER   tagger ( ) that was trained jointly on a large collection of biomedical NER datasets, leading to a much better performance on unseen corpora compared to models trained on a single corpus. However,   HUNER   relies on a Docker installation and uses a client-server architecture. These design decisions do not hinder its own installation but make its integration into any of the major NLP frameworks, which is required for the construction of comprehensive information extraction pipelines, cumbersome. Moreover,   HUNER   does not build upon a pretrained language model (LM), although such models were the basis for many recent breakthroughs in NLP research ( ). \n\nHere, we present   HunFlair  , a redesigned and retrained version of   HUNER   integrated into the widely used   Flair   NLP framework.   HunFlair   builds upon a pretrained character-level language model. It recognizes five important biomedical entity types with high accuracy, namely   Cell Lines  ,   Chemicals  ,   Diseases  ,   Genes   and   Species  . Through its shipping as a Flair component, it can be easily combined with other IE tools (e.g. text parsing, document classification, hedge detection) or other language models and benefits from the experiences and future developments of the large user and developer base of   Flair  . Through its simple but extensible interface, it is easily accessible also for non-experts. Technically,   HunFlair   combines the insights from   and   by merging character-level LM pretraining and joint training on multiple gold standard corpora, which leads to strong gains over other state-of-the-art off-the-shelf NER tools. For   HunFlair  , we specially trained a character-level in-domain LM on a large corpus of biomedical abstracts and full-texts and make it publicly available to facilitate further research. \n\nIn addition, we integrate 23 biomedical NER corpora into   HunFlair   using a consistent format, which enables researchers and practitioners to rapidly train their own models and experiment with new approaches within   Flair  . Note that these are the same corpora that were already made available through   HUNER  . However, the integration into   Flair   has the additional benefits of more convenient automated downloading and flexible preprocessing. While   HUNER  \u2019s corpora came preprocessed with a particular method, users of   HunFlair   may process the corpora along with their own choices, for instance by using different sentence resp. word segmentation methods. \n\n\n## 2 Hunflair \n  \n HunFlair   was created by implementing the approach behind   HUNER   into the   Flair   NLP framework, along with its improvement by integrating a pretrained language model.   Flair   is an NLP framework designed to allow intuitive training and distribution of sequence labeling, text classification and language models.   Flair   achieves state-of-the-art performance in several NLP research challenges ( ), allows researchers to \u2018mix and match\u2019 various types of character, word and document embeddings and features a base of more than 120 contributors. In addition, more than 500 open-source projects and python libraries rely on Flair (see  ). \n\n shows the architecture of   HunFlair   and illustrates how little coding is required to use it. At the core, it relies on a Flair character-level language model trained on roughly 24 million abstracts of biomedical articles from PubMed and 3 million full texts originating from PMC as well as fastText word embeddings ( ). The inclusion of such pretrained character-level language models in NER models lead to strong improvements in other domains ( ). Prediction of named entities is performed by a BiLSTM-CRF model ( ). Following the HUNER approach, it consists of distinct models for each entity type which are trained on the union of all training sets of all integrated gold standard NER corpora with this type to achieve a more robust performance across other texts, text genres and biomedical sub-domains. See   S1 for details of the training process. \n  \nOverview of the   HunFlair   model and it\u2019s integration into the Flair ecosystem. The model is based on a biomedical Flair character-level language model and word embeddings from fastText. In total, the model was trained on 23 biomedical NER datasets spanning five distinct entity types. Furthermore, the simple installation and application of   HunFlair   as well as it\u2019s integration with other Flair components is shown exemplarily \n  \n\n## 3 Results \n  \nWe compare the tagging accuracy of   HunFlair   to two types of competitors: Other \u2018off-the-shelf\u2019 biomedical NER tools, and other recent research prototypes. Therefore, we classify a tool as off-the-shelf when it (i) comes with pretrained prediction models (ease of use), and (ii) can be locally installed (to allow the application to potentially large and potentially propriatary text collections). In contrast, we classify a tool as research prototype when its application requires a retraining of models or when it is only usable as a web service. \n\nOur primary comparisons to off-the-shelve tools are based on cross-corpus experiments, because these give insight into the generalization properties of a model across different text types (e.g. full text versus abstract) and scientific subdomains (e.g. human oncology, psychological diseases, biology of plants, etc.). Clearly, this comes at the price of introducing a bias against methods which were designed for specific annotation guidelines that differ from those of an evaluation corpus. Therefore, our comparisons to research prototypes are based on in-corpus experiments which evaluate the architecture of   HunFlair   also in this setting. \n\n### 3.1 Comparison to off-the-shelf tools \n  \nWe compare the performance of   HunFlair   in a cross-corpus setting to five other state-of-the-art biomedical NER tools using three gold standard corpora: CRAFT ( ), BioNLP13 Cancer Genetics ( ) and PDR ( ). None of these was used in the training of neither   HunFlair   nor any competitor tools and we checked that there are no significant textual overlaps between these corpora and any of   HunFlair\u2019  s trainings corpora. We compare (restricted to the supported entity types) against   SciSpacy   ( ),   HUNER   ( ),   tmChem   ( ),   GNormPlus   ( ) and   DNorm   ( ). As   SciSpacy   comes with several models for each entity type, we report the best performance among all of those models that were not trained on the evaluation corpus. Results can be found in  . \n  \nF1-scores of several off-the-shelf biomedical NER tools on three unseen corpora \n    \n HunFlair   outperforms all competitors in all but one comparison, with an average gain of 7.26 pp in F1. Note that this evaluation uses mention-level F1 scores and compares against the gold spans annotated in the original corpora, while allowing for a one-character offset which accounts for differences in the handling of special characters. Results for a slightly different evaluation protocol, which considers as match any overlap between gold standard and predicted spans, along with a more in-depth discussion of evaluation setups and results, can be found in   S2. Although especially   SciSpacy   and   HUNER   profit from this more lenient evaluation (+8.04 pp/+5.55 pp), the overall ranking of methods is not changed. \n\n\n### 3.2 Comparison to research prototypes \n  \nWe compare   HunFlair\u2019s   results in an in-corpus setting to those reported by four different research prototypes based on three different corpora: JNLPBA ( ), BioCreative V CDR ( ) and NCBI Disease ( ). These corpora were chosen because of the availability of published results on the test splits. Specifically, we compare to   BioBERT   ( ),   SciBERT   ( ),   CollaboNet   ( ) and   SciSpacy   ( ). To ensure a fair comparison, we proceed as follows when evaluating   HunFlair   in this setting. We first remove the three evaluation corpora from the pretraining set. We next pre-train   HunFlair   on all remaining corpora and then fine-tune it on the training and development portions of the respective corpus. \n\nThe results can be found in   and the detailed evaluation protocol is described in   S3.   HunFlair   sets the new state-of-the-art on   BioCreative V CDR   consisting of chemical and disease annotations with a macro-average F1 score of 90.57. For JNLPBA (gene) and NCBI Disease, it reaches on-par results with the competitor methods. We also investigate the effect of pretraining on multiple gold standard corpora, by comparing   HunFlair   to a non-pretrained version on all 23 NER corpora. On average, finetuning improves results on all entity types with the improvements in F1 ranging from 0.8 pp for chemicals to 4.75 pp for cell lines. The full results per corpus are provided in   S4. \n  \nComparison with the reported results of state-of-the-art research prototypes for BioNER \n    \n\n\n## 4 Conclusion \n  \nWe proposed   HunFlair  , a state-of-the-art biomedical NER tagger. Through its tight integration into the Flair NLP framework, it is easy to install, easy to use and easy to combine with other NLP modules. It comes comes along with 23 biomedical NER corpora in a single format while still enabling customized pre-processing.   HunFlair   is a redesign of   HUNER  , which it extends with pretrained domain-specific character-level language models. It outperforms a series of other off-the-shelf tools in a cross-corpus evaluation setting on different datasets, and achieves on-par results with current state-of-the-art research prototypes based on in-corpus experiments. \n\n\n## Funding \n  \nL.W. and J.M. were funded by the Helmholtz Einstein International Berlin Research School in Data Science (HEIBRiDS). M.H. was funded by the German Research Council [LE-1428/7-1]. \n\n Conflict of Interest  : none declared. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 8428609, "text_md5": "c2660b1946fdd8ddc9e4fa93771c2113", "field_positions": {"authors": [0, 102], "journal": [103, 117], "publication_year": [119, 123], "title": [134, 220], "keywords": [234, 234], "abstract": [247, 1669], "body": [1678, 12188]}, "batch": 1, "pmid": 33508086, "doi": "10.1093/bioinformatics/btab042", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8428609", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8428609"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8428609\">8428609</a>", "list_title": "PMC8428609  HunFlair: an easy-to-use tool for state-of-the-art biomedical named entity recognition"}
{"text": "Lamurias, Andre and Sousa, Diana and Clarke, Luka A. and Couto, Francisco M.\nBMC Bioinformatics, 2019\n\n# Title\n\nBO-LSTM: classifying relations via long short-term memory networks along biomedical ontologies\n\n# Keywords\n\nText mining\nDrug-drug interactions\nDeep learning\nLong short term memory\nRelation extraction\n\n\n# Abstract\n \n## Background \n  \nRecent studies have proposed deep learning techniques, namely recurrent neural networks, to improve biomedical text mining tasks. However, these techniques rarely take advantage of existing domain-specific resources, such as ontologies. In Life and Health Sciences there is a vast and valuable set of such resources publicly available, which are continuously being updated. Biomedical ontologies are nowadays a mainstream approach to formalize existing knowledge about entities, such as genes, chemicals, phenotypes, and disorders. These resources contain supplementary information that may not be yet encoded in training data, particularly in domains with limited labeled data. \n\n\n## Results \n  \nWe propose a new model to detect and classify relations in text, BO-LSTM, that takes advantage of domain-specific ontologies, by representing each entity as the sequence of its ancestors in the ontology. We implemented BO-LSTM as a recurrent neural network with long short-term memory units and using open biomedical ontologies, specifically Chemical Entities of Biological Interest (ChEBI), Human Phenotype, and Gene Ontology. We assessed the performance of BO-LSTM with drug-drug interactions mentioned in a publicly available corpus from an international challenge, composed of 792 drug descriptions and 233 scientific abstracts. By using the domain-specific ontology in addition to word embeddings and WordNet, BO-LSTM improved the F1-score of both the detection and classification of drug-drug interactions, particularly in a document set with a limited number of annotations. We adapted an existing DDI extraction model with our ontology-based method, obtaining a higher F1 score than the original model. Furthermore, we developed and made available a corpus of 228 abstracts annotated with relations between genes and phenotypes, and demonstrated how BO-LSTM can be applied to other types of relations. \n\n\n## Conclusions \n  \nOur findings demonstrate that besides the high performance of current deep learning techniques, domain-specific ontologies can still be useful to mitigate the lack of labeled data. \n\n \n\n# Body\n \n## Background \n  \nCurrent relation extraction methods employ machine learning algorithms, often using kernel functions in conjunction with Support Vector Machines [ ,  ] or based on features extracted from the text [ ]. In recent years, deep learning techniques have obtained promising results in various Natural Language Processing (NLP) tasks [ ], including relation extraction [ ]. These techniques have the advantage of being easily adaptable to multiple domains, using models pre-trained on unlabeled documents [ ]. The success of deep learning for text mining is in part due to the high quantity of raw data available and the development of word vector models such as word2vec [ ] and GloVe [ ]. These models can use unlabeled data to predict the most probable word according to the context words (or vice-versa), leading to meaningful vector representations of the words in a corpus, known as word embeddings. \n\nA high volume of biomedical information relevant to the detection of Adverse Drug Reactions (ADRs), such as Drug-Drug Interactions (DDI), is mainly available in articles and patents [ ]. A recent review of studies about the causes of hospitalization in adult patients has found that ADRs were the most common cause, accounting for 7% of hospitalizations [ ]. Another systematic review focused on the European population, identified that 3.5% of hospital admissions were due to ADRs, while 10.1% of the patients experienced ADRs during hospitalization [ ]. \n\nThe knowledge encoded in the ChEBI (Chemical Entities of Biological Interest) ontology is highly valuable for detection and classification of DDIs, since it provides not only the important characteristics of each individual compound but also, more importantly, the underlying semantics of the relations between compounds. For instance, dopamine (CHEBI:18243), a chemical compound with several important roles in the brain and body, can be characterized as being a catecholamine (CHEBI:33567), an aralkylamino compound (CHEBI:64365) and an organic aromatic compound (CHEBI:33659) (Fig.\u00a0 ). When predicting if a certain drug interacts with dopamine, its ancestors will provide additional information that is not usually directly expressed in the text. While the reader can consult additional materials to better understand a biomedical document, current relation extraction models are trained solely on features extracted from the training corpus. Thus, ontologies confer an advantage to relation extraction models due to the semantics encoded in them regarding a particular domain. Since ontologies are described in a common machine-readable format, methods based on ontologies can be applied to different domains and incorporated with other sources of knowledge, bridging the semantic gap between relation extraction models, data sources, and results [ ].\n   \nAn excerpt of the ChEBI ontology showing the first ancestors of dopamine, using \u201cis-a\u201d relationships \n  \n\n### Deep learning for biomedical NLP \n  \nCurrent state-of-the-art text mining methods employ deep learning techniques, such as Recurrent Neural Networks (RNN), to train classification models based on word embeddings and other features. These methods use architectures composed of multiple layers, where each layer attempts to learn a different kind of representation of the input data. This way, different types of tasks can be trained using the same input data. Furthermore, there is no need to manually craft features for a specific task. \n\nLong Short-Term Memory (LSTM) networks have been proposed as an alternative to regular RNN [ ]. LSTMs are a type of RNN that can handle long dependencies, and thus are suitable for NLP tasks, which involve long sequences of words. When training the weights of an RNN, the contribution of the gradients may vanish while propagating for long sequences of words. LSTM units account for this vanishing gradient problem through a gated architecture, which makes it easier for the model to capture long-term dependencies. Recently, LSTMs have been applied to relation extraction tasks in various domains. Miwa and Bansal [ ] presented a model that extracted entities and relations based on bidirectional tree-structured and sequential LSTM-RNNs. The authors evaluated this model on three datasets, including the SemEval 2010 Task 8 dataset, which defines 10 general semantic relations types between nominals [ ]. \n\nBidirectional LSTMs have been proposed for relation extraction, obtaining better results than one-directional LSTMs on the SemEval 2010 dataset [ ]. In this case, at each time step, there are two LSTM layers, one that reads the sentence from left to right, and another that reads from right to left. The output of both layers is combined to produce a final score. \n\nThe model proposed by Xu et al. [ ] combines Shortest Dependency Paths (SDP) between two entities in a sentence with linguistic information. SDPs are informative features for relations extraction since these contain the words of the sentence that refer directly to both entities. This model has a multichannel architecture, where each channel makes use of information from a different source along the SDP. The main channel, which contributes the most to the performance of the model, uses word embeddings trained on the English Wikipedia with word2vec. Additionally, the authors study the effect of adding channels consisting of the part-of-speech tags of each word, the grammatical relations between the words of the SDP, and the WordNet hypernyms of each word. Using all four channels, the F1-score of the SemEval 2010 Task 8 was 0.0135 higher than when using only the word embeddings channel. Although WordNet can be considered an ontology, its semantic properties were not integrated in this work, since only the word class is extracted, and the relations between classes are not considered. \n\nDeep learning approaches to DDI classification have been proposed in recent years, using the SemEval 2013: Task 9 DDI extraction corpus to train and evaluate their performance. Zhao et al. [ ] proposed a syntax convolutional neural network for DDI extraction, using word embeddings. Due to its success on other domains, LSTMs have also been used for DDI extraction [ \u2013 ]. Xu et al. [ ] proposed a method that combines domain-specific biomedical resources to train embedding vectors for biomedical concepts. However, their approach uses only contextual information from patient records and journal abstracts and does not take into account the relations between concepts that an ontology provides. While these works are similar to ours, we present the first model that makes use of a domain-ontology to classify DDIs. \n\n\n### Ontologies for biomedical text mining \n  \nWhile machine learning classifiers trained on word embeddings can learn to detect relations between entities, these classifiers may miss the underlying semantics of the entities according to their respective domain. However, the semantics of a given domain are, in some cases, available in the form of an ontology. Ontologies aim at providing a structured representation of the semantics of the concepts in a domain and their relations [ ]. In this paper, we consider a domain-specific ontology as a directed acyclic graph where each node is a concept (or entity) of the domain and the edges represent known relations between these concepts [ ]. This is a common representation of existing biomedical ontologies, which are nowadays a mainstream approach to formalize knowledge about entities, such as genes, chemicals, phenotypes, and disorders. \n\nBiomedical ontologies are usually publicly available and cover a large variety of topics related to Life and Health Sciences. In this paper, we use ChEBI, an ontology for chemical compounds with biological interest, where each node corresponds to a chemical compound [ ]. The latest release of ChEBI contains nearly 54k compounds and 163k relationships. Note that, the success of exploring a given biomedical ontology for performing a specific task can be easily extended to other topics due to the common structure of biomedical ontologies. For example, the same measures of metadata quality have been successfully applied to resources annotated with different biomedical ontologies [ ]. \n\nOther authors have previously combined ontological information with neural networks, to improve the learning capabilities of a model. Li et al. [ ] mapped each word to a WordNet sense disambiguation to account for the different meanings that a word may have and the relations between word senses. Ma et al. [ ] proposed the LSTM-OLSI model, which indexes documents based on the word-level contextual information from the DBpedia ontology and document-level topic modeling. Some authors have explored graph embedding techniques, converting relations to a low dimensional space which represents the structure and properties of the graph [ ]. For example, Kong et al. [ ] combined heterogeneous sources of information, such as ontologies, to perform multi-label classification, while Dasigi et al. [ ] presented an embedding model based on ontology concepts to represent word tokens. \n\nHowever, few authors have explored biomedical ontologies for relation extraction. Textpresso is a project that aims at helping database curation by automatically extracting biomedical relations from research articles [ ]. Their approach incorporates an internal ontology to identify which terms may participate in relations according to their semantics. Other approaches measure the similarity between the entities and use the value as a feature for a machine learning classifier [ ]. One of the teams that participated in the BioCreative VI ChemProt task used ChEBI and Protein Ontology to extract additional features for a neural network model that extracted relation between chemicals and proteins [ ]. To the best of our knowledge, our work is the first attempt at incorporating ancestry information from biomedical ontologies with deep learning to extract relations from text. \n\nIn this manuscript, we propose a new model, BO-LSTM that can explore domain information from ontologies to improve the task of biomedical relation extraction using deep learning techniques. We compare the effect of using ChEBI, a domain-specific ontology, and WordNet, a generic English language ontology, as external sources of information to train a classification model based on LSTM networks. This model was evaluated on a publicly available corpus of 792 drug descriptions and 233 scientific abstracts annotated with DDIs relevant to the study of adverse drug effects. Using the domain-specific ontology in addition to word embeddings and WordNet, BO-LSTM improved the F1-score of the classification of DDIs by 0.0207. Our model was particularly efficient with document types that were less represented in the training data. Moreover, we improved the F1-score of an existing DDI extraction model by 0.022 by adding our proposed ontology information, and demonstrated its applicability to other domains by generating a corpus of gene-phenotype relations and training our model on that corpus. The code and results obtained with the model can be found on our GitHub repository ( ), while a Docker image is also available ( ), simplifying the process of training new classifiers and applying them to new data. We also made available the corpus produced for gene-phenotype relations, where each entity is mapped to an ontology concept. These results support our hypothesis that domain-specific information is useful to complement data-intensive approaches such as deep learning. \n\n\n\n## Methods \n  \nIn this section, we describe the proposed BO-lSTM model in detail, as shown in Fig.\u00a0 , with a focus on the aspects that refer to the use of biomedical ontologies.\n   \nBO-LSTM Model architecture, using a sentence from the Drug-Drug Interactions corpus as an example. Each box represents a layer, with an output dimension, and merging lines represent concatenation. We refer to   a   as the Word embeddings channel,   b   the WordNet channel and   c   the ancestors concatenation channel and   d   the common ancestors channel \n  \n\n### Data preparation \n  \nThe objective of our work is to identify and classify relations between biomedical entities found in natural language text. We assume that the relevant entities are already recognized. Therefore, we process the input data in order to generate instances to be classified by the model. Considering the set of entities   E   mentioned in a sentence, we generate   instances of that sentence. We refer to each instance as a candidate pair, identified by the two entities that constitute that pair, regardless of the order. A relation extraction model will assign a class to each candidate pair. In some cases, it is enough to simply classify the candidate pairs as negative or positive, while in other cases different types of positive relations are considered. \n\nAn instance should contain the information necessary to classify a candidate pair. Therefore, after tokenizing each sentence, we obtain the Shortest Dependency Path (SDP) between the entities of the pair. For example, in the sentence \u201cLaboratory Tests Response to Plenaxis   should be monitored by measuring serum total testosterone   concentrations just prior to administration on Day 29 and every 8 weeks thereafter\u201d, the shortest path between the entities would be Plenaxis - Response - monitored - by - measuring - concentrations - testosterone. For both tokenization and dependency parsing, we use the spaCy software library ( ). The text of each entity that appears in the SDP, including the candidate entities, is replaced by the generic string to reduce the effect of specific entity names on the model. For each element of the SDP, we obtain the WordNet hypernym class using the tool developed by Ciaramita and Altun [ ]. \n\nTo focus our attention on the effect of the ontology information, we use pre-trained word embedding vectors. Pyysalo et al. [ ] released a set of vectors trained on PubMed abstracts (nearly 23 million) and PubMed Central full documents (nearly 700k), with the word2vec algorithm [ ]. Since these vectors were trained on a large biomedical corpus, it is likely that its vocabulary will contain more words relevant to the biomedical domain than the vocabulary of a generic corpus. \n\nWe match each entity to an ontology concept so that we can then obtain its ancestors. Ontology concepts contain an ID, a preferred label, and, in most cases, synonyms. While pre-processing the data, we match each entity to the ontology using fuzzy matching. The adopted implementation uses the Levenshtein distance to assign a score to each match. \n\nOur pipeline first attempts to match the entity string to a concept label. If the match has a score equal to or higher than 0.7 (determined empirically), we accept that match and assign the concept ID to that entity. Otherwise, we match to a list of synonyms of ontology concepts. If that match has a score higher than the original score, we assign the ID of the matched synonym to the entity, otherwise, we revert to the original match. It is preferable to match to a concept label since these are more specific and should reflect the most common nomenclature of the concepts. This way, every entity was matched to a ChEBI concept, either to its preferred label or to a synonym. Due to the automatic linking method used, we cannot assume that every match is correct, but fuzzy matching has been used for similar purposes [ ], so we can assume that the best match is chosen. We matched 9020 unique entities to the preferred label and 877 to synonyms, and 1283 unique entities had an exact match to either a preferred label or synonym. \n\nThe DDI corpus used to evaluate our method has a high imbalance of positive and negative relations, which hinders the training of a classification model. Even though only entities mentioned in the same sentence are considered as candidate DDIs, there is still a ratio of 1:5.9 positive to negative instances. Other authors have suggested reducing the number of negative relations through simple rules [ ,  ]. We excluded from training and automatically classify as negative the pairs that fit the following rules: \n   \nentities have the same text (regardless of case): in nearly every case a drug does not interact with itself; \n  \nthe only text between the candidate pair is punctuation: consecutive entities, in the form of lists and enumerations, are not interacting, as well as instances where the abbreviation of an entity is introduced; \n  \nboth entities have anti-positive governors: we follow the methodology proposed by [ ], where the headwords of entities that do not interact are used to filter less informative instances. \n  \n\nWith this filtering strategy, we used only 15,697 of the 27,792 pairs of the training corpus, obtaining a ratio of 1:3.5 positive to negative instances. \n\nWe developed a corpus of 228 abstracts annotated with human phenotype-gene relations, which we refer to as the HP corpus, to demonstrate how our model could be applied to other relation extraction tasks. This corpus was based on an existing corpus that were manually annotated with 2773 concepts of the Human Phenotype Ontology [ ], corresponding to 2170 unique concepts. The developers of the Human Phenotype Ontology made available a file that links phenotypes and genes that are associated with the same diseases. Each gene of this file was automatically annotated on the HP corpus through exact string matching, resulting in 360 gene entity mentions. Then, we assumed that every gene-phenotype pair that co-occurred in the same sentence was a positive instance if this relation existed in the file. While the phenotype entities were manually mapped to the Human Phenotype Ontology, we had to employ an automatic method to obtain the most representative Gene Ontology [ ,  ] concept of each gene, giving preference to concepts inferred from experiments. We applied the same pre-processing steps as for the DDI corpus, except for entity matching and negative instance filtering. This corpus is available at  . \n\n\n### BO-LSTM model \n  \nThe main contribution of this work is the integration of ontology information with a neural network classification model. A domain-specific ontology is a formal definition of the concepts related to a specific subject. We can define an ontology as a tuple <  C  ,  R  >, where C is the set of concepts and R the set of relations between the concepts, where each relation is a pair of concepts (  c  ,  c  ) with   c  ,  c  \u2208  E  . In our case, we consider only subsumption relations (is-a), which are transitive, i.e. if (  c  ,  c  )\u2208  R   and (  c  ,  c  )\u2208  R  , then we can assume that (  c  ,  c  ) is a valid relation. Then, the ancestors of concept   c   are given by \n \n\nwhere   T   is the transitive closure of   R   on the set   E  , i.e., the smallest relation set on   E   that contains   R   and is transitive. Using this definition, we can define the common ancestors of concepts   c   and   c   as \n \n\nand the concatenation of the ancestors of concepts   c   and   c   as \n \n\nWe consider two types of representations of a candidate pair based on the ancestry of its elements: the first consisting of the concatenation of the sequence of ancestors of each entity; and second, consisting of the common ancestors between both entities. Each set of ancestors is sorted by its position in the ontology so that more general concepts are in the first positions and the final position is the concept itself. Common ancestors are also used in some semantic similarity measures [ \u2013 ], since they normally represent the common information between two concepts. Due to the fact that in some cases there can be almost no overlap between the ancestors of two concepts, the concatenation provides an alternative representation. \n\nWe first represent each ontology concept as a one-hot vector   v  , a vector of zeros except for the position corresponding to the ID of the concept. The ontology embedding layer transforms these sparse vectors into dense vectors, known as embeddings, through an embedding matrix  , where   D   is the dimensionality of the embedding layer and   C   is the number of concepts of the ontology. Then, the output of the embedding layer is given by \n  In our experiments, we set the dimensionality of the ontology embedding layer as 50, and initialized its values randomly. Then, these values were tuned during training through back-propagation. \n\nThe sequence of vectors representing the ancestors of the terms is then fed into the LSTM layer. Figure\u00a0  exemplifies how we adapted this architecture to our model, using a sequence of ontology concepts as input. After the LSTM layer, we use a max pool layer which is then fed into a dense layer with a sigmoid activation function. We experimented with bypassing this dense layer, obtaining inferior results. Finally, a softmax layer outputs the probability of each class.\n   \nBO-LSTM unit, using a sequence of ChEBI ontology concepts as an example. Circle refers to sigmoid function and rectangle to tanh, while \u201cx\u201d and \u201c+\u201d refer to element-wise multiplication and addition.   h  : hidden unit;  : candidate memory cell;   m  : memory cell;   i   input gate;   f   forget gate;   o  : output gate \n  \n\nEach configuration of our model was trained through mini-batch gradient descent with the Adam algorithm [ ] and with cross-entropy as the loss function, with a learning rate of 0.001 We used the dropout strategy [ ] to reduce overfitting on the trained embeddings and weights. We used a dropout rate of 0.5 on every layer except the penultimate and output layers. We tuned the hyperparameters common to all configurations using only the word embeddings channel on the validation set. Each model was trained until the validation loss stopped decreasing. The experiments were performed on an Intel Xeon CPU (X3470 @ 2.93 GHz) with 16 GB of RAM and on a GeForce GTX 1080 Ti GPU with 11GB of RAM. \n\nThe ChEBI and WordNet embedding layers were trained along with the other layers of the network. The DDI corpus contains 1757 of the 109k concepts of the ChEBI ontology. Since this is a relatively small vocabulary, we believe that this approach is robust enough to tune the weights. For the size of the WordNet embedding layer, we used 50 as suggested by Xu et al. [ ], while for the ChEBI embedding layer, we tested 50, 100 and 150, obtaining the best performance with 50. \n\n\n### Baseline models \n  \nAs a baseline, we implemented a model based on the SDP-LSTM model of Xu et al. [ ]. The SDP-LSTM model makes use of four types of information: word embeddings, part-of-speech tags, grammatical relations and WordNet hypernyms, which we refer to as channels. Each channel uses a specific type of input information to train an LSTM-based RNN layer, which is then connected to a max pooling layer, the output of the channel. The output of each channel is concatenated, and connected to a densely-connected hidden layer, with a sigmoid activation function, while a softmax layer outputs the probabilities of each class. \n\nXu et al. show that it is possible to obtain high performance on a relation extraction task using only the word representations channel. For this reason, we use a version of our model with only this channel as the baseline. We employ the previously mentioned pre-trained word embeddings as input to the LSTM layer. \n\nAdditionally, we make use of WordNet as an external source of information. The authors of the SDP-LSTM model showed that WordNet contributed to an improvement of the F1-score on a relation extraction task. We use the tool developed by Ciaramita and Altun [ ] to obtain the WordNet classes of each word according to 41 semantic categories, such as \u201cnoun.group\u201d and \u201cverb.change\u201d. The embeddings of this channel were set to be 50-dimensional and tuned during the training of the model. \n\nWe adopted a second baseline model to make a stronger comparison with other DDI extraction models, based on the model presented by Zhang et al. [ ]. Their model uses the sentence and SDP of each instance to train a hierarchical LSTM network. This model is constituted by two levels of LSTMs which learn feature representations of the sentence and SDP based on word, part-of-speech and distance to entity. An embedding attention mechanism is used to weight the importance of each word to the two entities that constitute each pair. We kept the architecture and hyperparameters of their model, and added another type of input, based on the common ancestors and concatenation of each entity\u2019s ancestors. We applied the same attention mechanism, so that the most relevant ancestors have a larger weight on the LSTM. We ran the original Zhang et al. model to replicate the results, and then ran again with ontology information. \n\n\n\n## Results \n  \nWe evaluated the performance of our BO-LSTM model on the SemEval 2013: Task 9 DDI extraction corpus [ ]. This gold standard corpus consists of 792 texts from DrugBank [ ], describing chemical compounds, and 233 abstracts from the Medline database [ ]. DrugBank is a cheminformatics database containing detailed drug and drug target information, while Medline is a database of bibliographic information of scientific articles in Life and Health Sciences. Each document was annotated with pharmacological substances and sentence-level DDIs. We refer to each combination of entities mentioned in the same sentence as a candidate pair, which could either be positive if the text describes a DDI, or negative otherwise. In other words, a negative candidate is a candidate pair that is not described as interacting in the text. Each positive DDI was assigned one of four possible classes: mechanism, effect, advice, and int, when none of the others were applicable. \n\nIn the context of the competition, the corpus was separated into training and testing sets, containing both DrugBank and Medline documents. We maintained the test set partition and evaluated on it, as it is the standard procedure on this gold standard. After shuffling we used 80% of the training set to train the model and 20% as a validation set. This way, the validation set contained both DrugBank and Medline documents, and overfitting to a specific document type is avoided. It has been shown that the DDIs of the Medline documents are more difficult to detect and classify, with the best systems having almost a 30 point F1-score difference to the DrugBank documents [ ]. \n\nWe implemented the BO-LSTM model in Keras, a Python-based deep learning library, using the TensorFlow backend. The overall architecture of the BO-LSTM model is presented in Fig.\u00a0 . More details about each layer can be found in the \u201c \u201d section. We focused on the effect of using different sources of information to train the model. As such, we tuned the hyperparameters to obtain reasonable results, using as reference the values provided by other authors that have applied LSTMs to this gold standard [ ,  ]. We first trained the model using only the word embeddings of the SDP of each candidate pair (Fig.\u00a0 a). Then we tested the effect of adding the WordNet classes as a separate embedding and LSTM layer (Fig.\u00a0 b) Finally, we tested two variations of the ChEBI channel: first using the concatenation of the sequence of ancestors of each entity (Fig.\u00a0 c), and second using the sequence of common ancestors of both entities (Fig.\u00a0 d). \n\nTable\u00a0  shows the DDI detection results obtained with each configuration using the evaluation tool provided by the SemEval 2013: Task 9 organizers on the gold standard, while Table\u00a0  shows the DDI classification results, using the same evaluation tool and gold standard. The difference between these two tasks is that while detection ignores the type of interactions, the classification task requires identifying the positive pairs and also their correct interaction type. We compare the performance on the whole gold standard, and on each document type (DrugBank and Medline). The first row of each table shows the results obtained using an LSTM network trained solely on the word embeddings of the SDP of each candidate pair. Then, we studied the impact of adding each information channel on the performance of the model, and the effect of using all information channels, as shown in Fig.\u00a0 .\n   \nEvaluation scores obtained for the DDI detection task on the DDI corpus and on each type of document, comparing different configurations of the model \n  \nEvaluation metrics used: Precision (P), Recall (R) and F1-score (F). Each row represents the addition of an information source to the initial configuration \n\nBoldface indicates the configuration with highest score for each measure \n    \nEvaluation scores obtained for the DDI classification task on the DDI corpus and on each type of document, comparing different configurations of the model \n  \nEvaluation metrics used: Precision (P), Recall (R) and F1-score (F). Each row represents the addition of an information source to the initial configuration \n\nBoldface indicates the configuration with highest score for each measure \n  \n\nFor the detection task, using the concatenation of ancestors results in an improvement of the F1-score in the Medline dataset, contributing to an overall improvement of the F1-score in the full test set. The most notable improvement was in the recall of the Medline dataset, where the concatenation of ancestors increased this score by 0.246. The usage of ontology ancestors did not improve the F1-score of detection of DDIs in the DrugBank dataset. In every test set, it is possible to observe that the concatenation of ancestors results in a higher recall while considering only the common ancestors is more beneficial to precision. Combining both approaches with the WordNet channel results in a higher F1-score. \n\nRegarding the classification task (Table\u00a0 ), the F1-score was improved on each dataset by the usage of the ontology channel. Considering only the common ancestors led to an improvement of the F1-score in the DrugBank dataset and on the full corpus, while the concatenation improved the Medline F1-score, similarly to the detection results. \n\nTo better understand the contribution of each channel, we studied the relations detected by each configuration by one or more channels, and which of those were also present in the gold standard. Figures\u00a0  and   show the intersection of the results of each channel in the full, DrugBank, and Medline test sets. We compare only the results of the detection task, as it is simpler to analyze and show the differences in the results of different configurations. In Fig.\u00a0 , we can visualize false negatives as the number of relations unique to the gold standard and the false positives of each configuration as the number of relations that does not intersect with the gold standard. The difference between the values of this figure and the sum of their respective values in Fig.\u00a0  is due to the system being executed once for each dataset. Overall 369 relations in the full test set were not detected by any configuration of our system, out of a total of 979 relations in the gold standard. We can observe that 60 relations were detected only when adding the ontology channels.\n   \nVenn diagram demonstrating the contribution of each configuration of the model to the results of the full test set. The intersection of each channel with the gold standard represents the number of true positives of that channel, while the remaining correspond to false negatives and false positives \n    \nVenn diagram demonstrating the contribution of each configuration of the model to the DrugBank (  a  ) and Medline (  b  ) test set results. The intersection of each channel with the gold standard represents the number of true positives of that channel, while the remaining correspond to false negatives and false positives \n  \n\nIn the Medline test set, the ontology channel identified 7 relations that were not identified by any other configuration (Fig.\u00a0 b). One of these relations was the effect of quinpirole treatment on amphetamine sensitization. Quinpirole has 27 ancestors in the ChEBI ontology, while amphetamine has 17, and they share 10 of these ancestors, with the most informative being \u201corganonitrogen compound\u201d. While this information is not described in the original text, but only encoded in the ontology, it is relevant to understand if the two entities can participate in a relation. However, this comes at the cost of precision, since 10 incorrect DDIs were classified by this configuration. \n\nTo empirically compare our results with the state-of-the-art of the DDI extraction, we compiled the most relevant works on this task in Table\u00a0 . The first line refers to the system that obtained the best results on the original SemEval task [ ,  ]. Since then, other authors have presented approaches for this task, most recently using deep learning algorithms. In Table\u00a0  we compare the machine learning architecture used by each system, and the results reported by the authors. Since some authors focused only on the DDI classification task, we could not obtain the DDI detection results for those systems, hence the missing values. We were only able to replicate the results of Zhang et al. [ ]. Since this system followed an architecture similar to ours, we adapted the model with our ontology-based channel, as described in the \u201c \u201d section. This modification to the model resulted in an improvement of 0.022 to the F1-score. Our version of this model is also available on our page along with the BO-LSTM model.\n   \nComparison of DDI extraction systems \n  \nThe architectures mentioned are Support Vector Machines (SVM), Convolutional Neural Networks (CNN) and LSTMs \n  \n\nWe used the HP corpus to demonstrate the generalizability of our method. This case-study served only as a proof-of-concept, it was not our intent to measure the performance of the model, given the limited number of annotations and the dependence on the quality of using exact string matching to identify the genes. For example, we may have missed correct relations in the corpus, because they were not in the reference file or the gene name was not correctly identified. \n\nTherefore, we used 60% (137 documents) of the corpus to train the model and 40% (91 documents) to manually evaluate the relations predicted with that model. For example, in the following sentence: \n\n the model identified the relation between the phenotype \u201cangiofibromas\u201d and the gene \u201cMEN1\u201d. One recurrently identified relation by our model that was not present on the phenotype-gene associations file is between the phenotype \u2019neurofibromatosis\u2019 and the gene \u2019NF2\u2019: \n\n\n\nDespite this relation not being described in the previous sentence, it is predicted given its presence in the phenotype-gene associations files. With a larger number of annotations in the training corpus, we expect this error to disappear. \n\n\n## Discussion \n  \nComparing the results across the two types of documents, we can observe that our model was most beneficial to the Medline test set. This set contains only 1301 sentences from 142 documents for training, while the DrugBank set contains 5675 sentences from 572 documents. Naturally, the patterns of the DrugBank documents will be easier to learn than the ones of the Medline documents because more examples are shown to the model. Furthermore, the Medline set has 0.18 relations per sentence, while the DrugBank set has 0.67 relations per sentence. This means that DDIs are described much more sparsely than in the DrugBank set. This demonstrates that our model is able to obtain useful knowledge that is not described in the text. \n\nOne disadvantage of incorporating domain information in a machine learning approach is that it reduces its applicability to other domains. However, biomedical ontologies have become ubiquitous in biomedical research. One of the most successful cases of a biomedical ontology is the Gene Ontology, maintained by the Gene Ontology Consortium [ ]. The Gene Ontology defines over 40,000 concepts used to describe the properties of genes. This project is constantly updated, with new concepts and relations being added every day. However, there are ontologies for more specific subjects, such as microRNAs [ ], radiology terms [ ] and rare diseases [ ]. BioPortal is a repository of biomedical ontology, currently hosting 685 ontologies. Furthermore, while manually labeled corpora are created specifically to train and evaluate text mining applications, ontologies have diverse applications, i.e., they are not developed for this specific purpose. \n\nWe evaluate the proposed model on the DDI corpus because it is associated with a SemEval task, and for this reason, it has been the subject of many studies since its release. However, while applying our model to a single domain, we designed its architecture so it can fit any other domain-specific ontology. To demonstrate this, we developed a corpus of gene-phenotype relations annotated with Human Phenotype and Gene ontology concepts, and applied our model to it. Therefore, the methodology proposed can be easily followed to apply to any other biomedical ontology that describes the concepts of a particular domain. For example, the Disease Ontology [ ], that describes relations between human diseases, could be used with the BO-LSTM model on a disease relation extraction task, as long as there is an annotated training corpus. \n\nWhile we studied the potential of domain-specific ontologies based only on the ancestors of each entity, there are other ways to integrate semantic information from ontologies into neural networks. For example, one could consider only the ancestors with the highest information content, since those would be the most helpful to characterize an entity. The information content can be estimated either by the probability of a given term in the ontology or in an external dataset. Alternatively, a semantic similarity measure that accounts for non-transitive relations could be used to find similar concepts to the entities of the relation [ ], or one that considers only the most relevant ancestors [ ]. The quality of the ontology embeddings could also be improved by pre-training on a larger dataset, which would include a wider variety of concepts. \n\n\n## Conclusions \n  \nThis work demonstrates how domain-specific ontologies can improve deep learning models for classification of biomedical relations. We developed a model, BO-LSTM which combines biomedical ontologies with LSTM units to detect and classify relations in text. In this manuscript, we demonstrate that ontologies can improve the performance of deep learning techniques for biomedical relation extraction, in particular for situations with a limited number of annotations available, which was the case of the Medline dataset. Furthermore, we explored how it can be adapted to other relation extraction domains, for example, gene-phenotype relations. Considering that biomedical ontologies are openly available and regularly updated as the knowledge on the domain progresses, they should be considered important information sources for relation extraction. \n\n \n", "metadata": {"pmcid": 6323831, "text_md5": "c188e182972a70090b3f13071b69ec6c", "field_positions": {"authors": [0, 76], "journal": [77, 95], "publication_year": [97, 101], "title": [112, 206], "keywords": [220, 312], "abstract": [325, 2458], "body": [2467, 41720]}, "batch": 1, "pmid": 30616557, "doi": "10.1186/s12859-018-2584-5", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6323831", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6323831"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6323831\">6323831</a>", "list_title": "PMC6323831  BO-LSTM: classifying relations via long short-term memory networks along biomedical ontologies"}
{"text": "Jiang, Min and Sanger, Todd and Liu, Xiong\nJMIR Med Inform, 2019\n\n# Title\n\nCombining Contextualized Embeddings and Prior Knowledge for Clinical Named Entity Recognition: Evaluation Study\n\n# Keywords\n\nnatural language processing\nnamed entity recognition\ndeep learning\ncontextualized word embedding\nsemantic embedding\nprior knowledge\n\n\n# Abstract\n \n## Background \n  \nNamed entity recognition (NER) is a key step in clinical natural language processing (NLP). Traditionally, rule-based systems leverage prior knowledge to define rules to identify named entities. Recently, deep learning\u2013based NER systems have become more and more popular. Contextualized word embedding, as a new type of representation of the word, has been proposed to dynamically capture word sense using context information and has proven successful in many deep learning\u2013based systems in either general domain or medical domain. However, there are very few studies that investigate the effects of combining multiple contextualized embeddings and prior knowledge on the clinical NER task. \n\n\n## Objective \n  \nThis study aims to improve the performance of NER in clinical text by combining multiple contextual embeddings and prior knowledge. \n\n\n## Methods \n  \nIn this study, we investigate the effects of combining multiple contextualized word embeddings with classic word embedding in deep neural networks to predict named entities in clinical text. We also investigate whether using a semantic lexicon could further improve the performance of the clinical NER system. \n\n\n## Results \n  \nBy combining contextualized embeddings such as ELMo and Flair, our system achieves the F-1 score of 87.30% when only training based on a portion of the 2010 Informatics for Integrating Biology and the Bedside NER task dataset. After incorporating the medical lexicon into the word embedding, the F-1 score was further increased to 87.44%. Another finding was that our system still could achieve an F-1 score of 85.36% when the size of the training data was reduced to 40%. \n\n\n## Conclusions \n  \nCombined contextualized embedding could be beneficial for the clinical NER task. Moreover, the semantic lexicon could be used to further improve the performance of the clinical NER system. \n\n \n\n# Body\n \n## Introduction \n  \n### History of Clinical Named Entity Recognition \n  \n\nClinical named entity recognition (NER), an important clinical natural language processing (NLP) task, has been explored for several decades. In the early stage, most NER systems leverage rules and dictionaries to represent linguistic features and domain knowledge to identify clinical entities, such as MedLEE [ ], SymText/MPlus [ , ], MetaMap [ ], KnowledgeMap [ ], cTAKES [ ], and HiTEX [ ]. To promote the development of machine learning\u2013based system, many publicly available corpora have been developed by organizers of some clinical NLP challenges such as the Informatics for Integrating Biology and the Bedside (i2b2) 2009 [ ], 2010 [ - ], 2012 [ - ], 2014 [ - ], ShARe/CLEF eHealth Evaluation Lab 2013 dataset [ ], and Semantic Evaluation 2014 task 7 [ ], 2015 task 6 [ ], 2015 task 14 [ ], and 2016 task 12 [ ] datasets. Many machine learning\u2013based clinical NER systems have been proposed, and they greatly improved performance compared with the early rule-based systems [ , , ]. Most systems are implemented based on two types of supervised machine learning algorithms: (1) classification algorithms such as support vector machines (SVMs) and (2) sequence labeling algorithms such as conditional random fields (CRFs), hidden Markov models (HMMs), and structural support vector machines (SSVMs). Among all of the algorithms, CRFs play the leading roles due to the advantage of the sequence labeling algorithms over classification algorithms in considering context information when making the prediction; CRFs, as one type of discriminative model, tend to achieve better performance for the same source of testing data compared with generative model-based algorithms such as HMMs. Even though CRFs have achieved a huge success in the clinical NER area, they have some obvious limitations: CRF-based systems lie in manually crafted features, which are time consuming, and their ability to capture context in a large window is limited. \n\n\n### Deep Neural Network\u2013Based Named Entity Recognition Algorithms \n  \nIn recent years, deep neural network\u2013based NER algorithms have been extensively studied, and many deep learning\u2013based clinical NER systems have been proposed. They have an obvious advantage over traditional machine learning algorithms since they do not require feature engineering, which is the most difficult part of designing machine learning\u2013based systems. They also improve the ability to leverage the context information. Initially, word embedding [ ] is proposed as a method to represent the word in a continuous way to better support neural network structure. Then several new neural network structures including recurrent neural networks (RNNs) and long short-term memory (LSTM) [ ] have been introduced to better represent sequence-based input and overcome long-term dependency issues. Recently, contextual word representations generated from pretrained bidirectional language models (biLMs) have been shown to significantly improve the performance of state-of-the-art NER systems [ ]. \n\nIn biLMs, the language model (LM) can be described as: given a sequence of N tokens, (  t  ,   t  , ...,   t  ), the probability of token   t   can be calculated given the history (  t  , ...,   t  ), and the sequence probability can be computed as seen in  . \n\nRecent neural LMs usually include one layer of token input, which is represented by word embedding or a CNN over characters, followed by L layers of forward LSTMs. On the top layer, the SoftMax layer is added to generate a prediction score for the next token [ ]. The biLM combines two such neural LMs: the forward LM and backward LM; the backward LM is similar to the forward LM, except it runs over the reverse sequence. As a whole, the biLM tries to maximize the log-likelihood of the forward and backward directions as seen in  . \n  \nSequence probability in bidirectional language models. \n    \nLog-likelihood of the forward and backward directions language models. \n  \nWhere \u03b8  represents the token representation layer, \u03b8  represents the Softmax layer, and     and     represent the forward and backward directions of the LSTM layer. \n\nIn 2017, Peters et al [ ] introduced a sequence tagger called TagLM that combines pretrained word embeddings and biLM embeddings as the representation of the word to improve the performance of the NER system. Since the output of each layer of the biLM represents a different type of contextual information [ ], Peters et al [ ] proposed another embedding, a deep contexualized word representation, ELMo, by concatenating all the biLM layer outputs into the biLM embedding with a weighted average pooling operation. The ELMo embedding adds CNN and highway networks over the character for each token as the input. ELMo has been proven to enhance the performance of different NLP tasks such as semantic role labeling and question answering [ ]. \n\nSimilar to Peters\u2019 ELMo, Akbik et al [ ] introduced contextual string embeddings for sequence labeling, which leverages neural character-level language modeling to generate a contextualized embedding for each word input within a sentence. The principle of the character-level LM is that it is the same as biLMs except that it runs on the sequences of characters instead of tokens.   shows the architecture of extracting a contextual string embedding for the word \u201chypotensive\u201d in a sentence. We can see that instead of generating a fixed representation of the embedding for each word, the embedding of each token is composed of pretrained character embeddings from surrounding text, meaning the same token has dynamic representation depending on its context. \n  \nArchitecture of extracting a contextual string embedding. \n  \n\n### Deep Neural Network\u2013Based Clinical Named Entity Recognition Systems \n  \nIn the clinical domain, researchers investigated the performance of clinical NER tasks on various types of deep neural network structures. In 2015, researchers showed it is beneficial to use the large clinical corpus to generate word embeddings for clinical NER systems, and they comparatively investigated the different ways of generating word embeddings in the clinical domain [ ]. In 2017, Wu et al [ ] produced state-of-the-art results on the i2b2 2010 NER task dataset by employing the LSTM-CRF structure. Liu et al [ ] investigated the effects of two types of character word embeddings on LSTM-based systems on multiple i2b2/Veterans Administration (VA) NER task datasets. In 2018, Zhu et al [ ] employed a contextualized LM embedding on clinical data and boosted the state-of-the-art performance by 3.4% on the i2b2/VA 2010 NER dataset. The above studies show that, with the development of methods in text representation learning, especially contextual word embedding, more and more hidden knowledge can be learned from a large unannotated clinical corpus, which is beneficial for clinical NER tasks. According to the study by Peters et al [ ], contextual word representations derived from pretrained biLMs can learn different levels of information that vary with the depth of the network, from local syntactic information to long-range dependent semantic information. Even without leveraging traditional domain knowledge such as lexicon and ontology, deep learning\u2013based NER systems can achieve better performance than traditional machine learning\u2013based systems. \n\nBesides using pretrained representation from large unlabeled corpora, researchers started to integrate prior knowledge into deep learning frameworks to improve the performance of the NER system. For example, in the general domain, Yu and Dredze [ ] created a semantic word embedding based on WordNet and evaluated the performance on language modeling, semantic similarity, and human judgment prediction. In another example, Weston et al [ ] leveraged a CNN to generate a semantic embedding based on hashtags to improve the performance of the document recommendation task. In the clinical domain, Wu et al [ ] compared two types of methods to inject medical knowledge into deep learning\u2013based clinical NER solutions and found that the RNN-based system combining medical knowledge as embeddings achieved the best performance on the i2b2 2010 dataset. In 2019, Wang et al [ ] explored two different architectures that extend the bidirectional LSTM (biLSTM) neural network and five different feature representation schemes to incorporate the medical dictionaries. In addition, other studies also use prior knowledge to generate embeddings [ - ]. \n\nTo date, no detailed analysis has been published to investigate the value of combining different types of word embeddings and prior knowledge for clinical NER. In this study, we made the following contributions: (1) we proposed an innovative method to combine two types of contextualized embeddings to study their effects on the clinical NLP challenge dataset, (2) we incorporated prior knowledge from semantic resources such as medical lexicon to evaluate if it could further improve the performance of the clinical NER system, and (3) we conducted a thorough evaluation on our models with different sizes of data to gain knowledge on how much data are needed to train a high-performance clinical NER system. \n\n\n\n## Methods \n  \n### Datasets \n  \nFor this study, we used two datasets, the 2010 i2b2/VA concept extraction track dataset and the Medical Information Mart for Intensive Care III (MIMIC-III) corpus. The 2010 i2b2/VA challenge dataset is annotated with named entities, while the MIMIC-III corpus is unannotated data. \n\n#### 2010 i2b2/VA Concept Extraction Track Dataset \n  \nThe goal of the 2010 i2b2/VA concept extraction task is to identify three types of clinical named entities including problem, treatment, and test from clinical notes. The original dataset includes 349 notes in the training set and 477 notes in the testing set, which include discharge summaries and progress notes from three institutions: Partners HealthCare, Beth Israel Deaconess Medical Center, and University of Pittsburgh Medical Center. Since the University of Pittsburgh Medical Center\u2019s data have been removed from the original data set, the portion of discharge summaries that is available contains 170 notes for training and 256 for testing. In total, the training set contains 16,523 concepts including 7073 problems, 4844 treatments, and 4606 tests. The test set contains 31,161 concepts including 12,592 problems, 9344 treatments, and 9225 tests. \n\n\n#### Medical Information Mart for Intensive Care III Corpus \n  \nThe MIMIC-III corpus [ ] is from MIMIC-III database, which is a large, freely available de-identified health-related dataset that integrates de-identified, comprehensive clinical data of patients admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts. \n\nThe dataset comprises 2,083,180 notes from 15 different note types including \u201crehab services,\u201d \u201ccase management,\u201d \u201cgeneral,\u201d \u201cdischarge summary,\u201d \u201cconsult,\u201d \u201cradiology,\u201d \u201celectrocardiography,\u201d \u201cnutrition,\u201d \u201csocial work,\u201d \u201cpharmacy,\u201d \u201cechocardiography,\u201d \u201cphysician,\u201d \u201cnursing,\u201d \u201cnursing/other,\u201d and \u201crespiratory.\u201d \n\n\n\n### Embedding Generation \n  \nIn order to fit our text input into the deep neural network structure, we generated three types of embeddings: classic word embeddings, (2) contextualized LM\u2013based word embeddings, and semantic word embeddings. \n\n#### Training Classic Word Embeddings \n  \nWe generated two types of word embeddings based on the MIMIC-III corpus and a medical lexicon: MIMIC-III corpus-based embeddings and tagged MIMIC-III corpus-based embeddings. We adopted the Word2Vec implementation database from Github [ ] to train word embeddings based on the MIMIC-III corpus. We used a continuous bag-of-words architecture with negative sampling. In accordance with the results from the study by Xu et al [ ], we set the dimension of embedding as 50. \n\n\n#### Training Contextual Language Model\u2013Based Embeddings \n  \nBesides the word embeddings, we employed two recently proposed methods to generate contextual LM-based embeddings: ELMo embeddings and (2) contextual string embeddings for sequence labeling (Flair). \n\n\n\n### Training ELMo Embeddings \n  \nWe followed the method introduced by Zhu et al [ ] that uses a partial MIMIC-III corpus combined with a certain portion of Wikipedia pages as a training corpus to train the ELMo contextual LM in the clinical domain. In more detail, it combines discharge summaries and radiology reports from the MIMIC-III corpus and all the Wikipedia pages with titles that are items from the Systematized Nomenclature of Medicine\u2013Clinical Terms. Such a corpus is trained on a deep neural network that contains a character-based CNN embedding layer followed by a two-layer biLSTM. Details have been published elsewhere [ ]. \n\n\n### Training Contextual String Embeddings for Sequence Labeling \n  \nAkbik et al [ ] proposed a new method to generate a neural character-level LM. The paper shows the state-of-the-art performance on the Conference on Computational Natural Language Learning 2003 NER task dataset. The LM for the general domain is publicly accessible. The author also integrates all the codes into an NLP framework called Flair. It achieved great success on the data in the general domain. However, according to the research by Friedman et al [ ], clinical language has unique linguistic characteristics compared with general English, which make models generated from the public domain poorly adaptable to clinical narratives. It is demanding to train the LM on the clinical corpus to better support the clinical NER task. For training corpus preparation, we first did sentence segmentation on the entire corpus, then we randomly selected 1500 sentences as the testing set and another 1500 sentences for the validation set. The remaining part serves as the training set. For the hyperparameters, we kept the default setting: learning rate as 20.0, batch size as 32, anneal factor as 0.25, patience as 10, clip as 0.25, and hidden size as 1024. \n\n\n### Training Semantic Word Embeddings \n  \nInjecting domain knowledge into the deep learning model is a potential way to further improve the performance of the NER system. According to the results by Wu et al [ ], combining medical knowledge into the embedding outperforms the method of representing it as a one-hot vector. Therefore, we similarly created the embedding to represent medical lexicon and fed it into the deep learning framework in our study. More specifically, we initially generated a lexicon dictionary based on a subset of semantic categories in the Unified Medical Language System. We then identified all the lexicon occurrences in the corpus using the dictionary and replaced them with semantic categories.   shows an example of the conversion. In the example sentence of \u201cNo spontaneous thrombus is seen in the left atrium,\u201d \u201cthrombus\u201d is replaced with the tag \u201cDISORDER\u201d and \u201cleft atrium\u201d is replaced with two \u201cBODYLOC\u201d tags. In this way, we can integrate semantic information into the word embeddings. For the embedding generation, we use the same setting as in the previous section. \n  \nOne example of converting the sentence into the tagged sentence. \n  \n\n### Deep Neural Network Architecture \n  \nAfter we generated all the embeddings, we started to fit them as the input into our deep neural network for the supervised training stage. Since each type of embedding is generated using one method, meaning each represents different aspects of knowledge from the large corpus, combining them is an obvious solution to potentially further improve the performance, which has also been proven by clinical NER studies [ , ]. Although there are many options to combine multiple embeddings in the deep neural network system such as weighting [ ] and ensemble [ ], in this study, we adopted the most straightforward way, which is simply concatenating them as the input. \n\nWe used the biLSTM-CRF sequence labeling module proposed by Huang et al [ ].   shows the architecture of the whole deep neural network structure; the input is the embedding layer, which is concatenated by different types of embeddings as described in the previous section. Before we extracted embeddings for tagged word embedding, we used the same medical lexicon\u2013based tagger to replace the tokens with the semantic tags. All the embedding inputs went through the biLSTM layer to generate forward and backward output, which was used to calculate the probability score by CRF layers. On the top, the prediction was given by a SoftMax layer. \n  \nDeep neural network structure with combined embeddings. Bi-LSTM: bidirectional long short-term memory; CRF: conditional random field. \n  \n\n### Training the Deep Neural Network\u2013Based Sequence Tagger \n  \nFor the implementation, we employed Flair [ ], which is a simple framework for NLP tasks including NER and text classification. We used the default hyperparameter setting in Flair, and we used the following configuration: learning rate as 0.1, batch size as 32, dropout probability as 0.5, and maximum epoch as 500. The learning rate annealing method is basically the same as the default: we half the learning rate if the training loss does not fall for the consecutive \u201cpatience\u201d number of epochs. We set the patience number to 12 in this study. A TITAN V (NVIDIA Corporation) graphics processing unit was used to train the model. We took about 4 hours to train our model each time. \n\n\n### Evaluation \n  \nIn order to get more reliable results, we ran each model three times. For the measurement of each running, we used precision, recall, and F-1 score. \n\n\n\n## Results \n  \n shows the performance of the challenge winner system and different deep neural network systems. We used four benchmarks as our baseline systems, and then we reported the performance of the systems when adding ELMo embeddings, Flair embeddings, and tagged embeddings one at a time. All evaluation scores were based on exact matching. For the baseline systems, the first one is the semi-Markov model, developed by Debruijn et al [ ], which reported an F-1 score of 85.23%. The second and third baselines are both based on the LSTM model, and they reported F-1 scores of 85.78% and 85.94%, respectively. The last baseline is the best result for the nonensemble models from Zhu et al [ ], which used ELMo embedding. The three baseline systems used the original corpus (training: 349 notes; test: 477 notes), all other systems are based on the existing modified corpus (training: 170 notes; test: 256 notes). To start, we combined word embeddings with ELMo and Flair embeddings, respectively. Both models achieved an F-1 score of 87.01%, which is a little bit higher than what was reported by Zhu et al [ ]. After combining word embeddings with ELMo and Flair embeddings, the F-1 score increased to 87.30%. When the word embedding on the tagged corpus was incorporated, the performance was further improved to 87.44% for the F-1 score. \n\nIn order to test if the improvement between different results is statistically significant, we conducted a statistical test based on results from bootstrapping. From the prediction result of the test set, we randomly selected 1000 sentences with replacement for 100 times and generated 100 bootstrap data sets. For each bootstrap data set, we evaluated F-measures for three pairs of results: (1) \u201cbiLSTM + ELMo\u201d and \u201cbiLSTM + ELMo + Flair,\u201d (2) \u201cbiLSTM + ELMo + Flair\u201d and \u201cbiLSTM + ELMo + Flair + semantic embedding,\u201d and (3) \u201cbiLSTM + ELMo by Zhu et al [ ]\u201d and \u201cbiLSTM + ELMo + Flair + semantic embedding.\u201d After that, we adopted a Wilcoxon signed rank test [ ] to determine if the differences between F-measures from the three pairs were statistically significant. The results show that the improvement of F-measures for all three pairs were statistically significant (  P   values were .01, .02, and .03, respectively). \n  \nPerformance of all the models on the 2010 i2b2/VA dataset. \n    \n\n## Discussion \n  \n### Principal Findings \n  \nNER is a fundamental task in the clinical NLP domain. In this study, we investigated the effects of combinations of different types of embeddings on the NER task. We also explored how to use medical lexicon to further improve performance. Based on the result, we found that either ELMo or Flair embeddings could boost the system\u2019s performance, and combining both embeddings could further improve the performance. Although both ELMo and Flair embeddings use biLM to train the LM on MIMIC-III corpus, they actually generate the contextualized word embeddings in different ways. ELMo concatenates all the biLM layers to represent all different levels of the knowledge, while Flair embedding is generated by a character-level LM. Character-level LM is different from character-aware LM [ ] since it actually uses word-level LM while leveraging character-level features through a CNN encoding step. It was composed by the surrounding text\u2019s embedding in the character-level. The difference between ELMo and Flair embeddings could explain the reason why they can play complementary roles in the model. \n\nThe results show that adding semantic embeddings could further improve performance. According to the study by Peters et al [ ], the lower biLM layer specializes in local syntactic relationships, while the higher layers focus on modeling longer range relationships. Those relationships are learned from the pure clinical corpus without any resources from outside such as medical lexicons and ontologies. This study shows an effective way to incorporate domain knowledge into the deep neural network\u2013based NER system. \n\nA large amount of training data is required to achieve success when applying deep learning algorithms [ ]. Within the general domain, it is more difficult to accumulate a large size of the annotated corpus for most of the clinical NLP tasks since it usually requires the annotator to have in-depth domain knowledge. Contextualized word embeddings, as an effective way of transferring the knowledge from the large unlabeled corpus, could address the issue of lack of training data. According to the results, by only using the small size of the training corpus (170 notes), contextualized word embedding\u2013based models could achieve better performance than the models that use the large size training corpus (349 notes). To further investigate the effectiveness of transfer learning in our proposed models, we compared the performance of our best model generated from different sizes of the training data.   shows the F-1 score for the model \u201cbiLSTM + ELMo + Flair + semantic embedding\u201d on randomly selected 80%, 60%, 40%, 20%, and 10% of the training data. Surprisingly, we found that using only 40% of the training corpus could achieve comparable performance as the original state-of-the-art traditional machine learning\u2013based system. Even using 20% of the training corpus, the model\u2019s F-1 score is still more than 80%. This result indicates that contextualized word representation could potentially be an effective way to reduce the size of the training corpus, which could significantly improve the feasibility of applying deep learning to real practice. \n\nBesides the performance reported in the Results section, we also recorded the change of performance for our proposed models during the fine-tuning stage.   shows the F-1 score on 1, 20, 40, and 60 epochs for our three models. On epoch 1, comparing to only word embeddings, any contextualized word embedding boosts the F-1 score. This is mostly because pretraining on contextualized word embeddings is very beneficial for the task of named entity recognition. This proves that the LM is a good way for pretraining that can be adapted to different downstream NLP tasks. Another interesting finding is that even though the model ELMo achieved the best performance among our three models, it was surpassed by the other two models on later epochs, which indicates that during the optimization process, the best starting point does not necessarily lead to the best local optimal solution. \n  \nPerformance of the best model training, BiLSTM  + ELMo + Flair + semantic embedding, on different sizes of the training corpus. \n      \nF-1 score for our proposed models on different epochs. \n  \n\n### Limitations \n  \nThis study has some limitations. For contextualized embedding generation, we followed others\u2019 research methods and didn\u2019t test different configurations for LM training. For example, for ELMo embeddings, we followed the work of Zhu et al [ ] for Flair embedding generation and kept the same configuration as seen in the work by Akbik et al [ ]. For the fine-tuning stage, we only fine-tuned a limited set of hyperparameters including learning rate and patience. For domain knowledge integration, there are a lot of options that could be explored to merge the lexicon information into the input of the deep neural network structure. In this study, we only tried one way to represent it in the form of word embeddings. In this paper, we studied two contextualized embeddings: ELMo and Flair. In the future, we plan to test our framework by adding bidirectional encoder representations from transformers, which is another popular contextualized embedding [ ]. \n\n\n### Conclusions \n  \nIn this study, we investigated the effects of the combination of two contextualized word embeddings including ELMo and Flair and clinical knowledge for the clinical NER task. Our evaluation on the 2010 i2b2/VA challenge dataset shows that using both ELMo and Flair embeddings outperforms using only ELMo embeddings, which indicates its great potential for the clinical NLP research. Furthermore, we demonstrate that incorporating the medical lexicon into the word representation could further improve the performance. Finally, we found that adopting our best model would be an effective way to reduce the size of the required training corpus for the clinical NER task. \n\n\n \n", "metadata": {"pmcid": 6913757, "text_md5": "6d3006fb6b02e77c9a102023dc2e57d7", "field_positions": {"authors": [0, 42], "journal": [43, 58], "publication_year": [60, 64], "title": [75, 186], "keywords": [200, 332], "abstract": [345, 2241], "body": [2250, 28212]}, "batch": 1, "pmid": 31719024, "doi": "10.2196/14850", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6913757", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6913757"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6913757\">6913757</a>", "list_title": "PMC6913757  Combining Contextualized Embeddings and Prior Knowledge for Clinical Named Entity Recognition: Evaluation Study"}
{"text": "Zia, Amjad and Aziz, Muzzamil and Popa, Ioana and Khan, Sabih Ahmed and Hamedani, Amirreza Fazely and Asif, Abdul R.\nJ Pers Med, 2022\n\n# Title\n\nArtificial Intelligence-Based Medical Data Mining\n\n# Keywords\n\ntext mining\nartificial intelligence\nmachine learning\nmedical data\nhealthcare information\n\n\n# Abstract\n \nUnderstanding published unstructured textual data using traditional text mining approaches and tools is becoming a challenging issue due to the rapid increase in electronic open-source publications. The application of data mining techniques in the medical sciences is an emerging trend; however, traditional text-mining approaches are insufficient to cope with the current upsurge in the volume of published data. Therefore, artificial intelligence-based text mining tools are being developed and used to process large volumes of data and to explore the hidden features and correlations in the data. This review provides a clear-cut and insightful understanding of how artificial intelligence-based data-mining technology is being used to analyze medical data. We also describe a standard process of data mining based on CRISP-DM (Cross-Industry Standard Process for Data Mining) and the most common tools/libraries available for each step of medical data mining. \n \n\n# Body\n \n## 1. Introduction \n  \nWith the rapid growth in online available medical literature, it is almost hard for readers to obtain the desired information without an extensive time investment. For example, in the ongoing COVID-19 pandemic, the number of publications talking about COVID-19 increased very rapidly. In the first 2 years of the pandemic, there were 228,640 articles in PubMed, 282,883 articles in PMC, and 7551 COVID-19 clinical trials listed in   databases (Data accessed on 16 February 2022), and this is increasing at an amazing speed. Because of the high degree of dimensional heterogeneity, irregularity, and timeliness, these data are often underutilized. This exponential growth in the scientific literature has made it difficult for the researchers to (i) obtain relevant information from the literature, (ii) present information in a concise and structured manner from an unstructured literature pile, and (iii) fully comprehend the current state and the direction of development in a research field. \n\nThe rapidly increasing literature cannot be managed and/or processed using traditional technologies and methods within an acceptable period. This massive volume of data makes it rather difficult for researchers to explore, analyze, visualize, and obtain a concise outcome. The process of extracting hidden, meaningful, and engrossing patterns from unstructured text literature is known as text mining [ ]. Traditional text mining techniques are not sufficient to cope with the current large volumes of published literature. Therefore, a rapid increase in the development of new data mining techniques based on artificial intelligence can be seen on the horizon for the benefit of patients and physicians. The inclusion of artificial intelligence (also machine learning (ML), deep learning (DL), and natural language processing (NLP) as the subsets) empowers the data mining process with multifold benefits: Gaining new insights into the decision-making process, processing large dataset with increased accuracy and efficiency, and the ability to learn and improve continuously from the new data. \n\nThe current review sheds light on the role of different AI-based methods, i.e., NLP and neural network (NN) in medical text mining, the current data mining processes, different database sources, and various AI-based tools used in the text mining process along with various algorithms. We reviewed the latest text mining approaches, highlighted the key differences between medical and non-medical data mining, and presented a set of tools and techniques currently being used for each step of medical literature text mining. Additionally, we described the role of artificial intelligence and machine learning in medical data mining and pointed out challenges, difficulties, and opportunities along the road. \n\n### 1.1. Medical vs. Non-Medical Literature Text Mining \n  \nHuman medical data are unique and may be difficult when it comes to mining and analysis. First, due to the fact that humans are the most advanced and the most observed (in-depth) species on the globe, their observation is enriched because humans may provide their sensory input easily compared to the other species on the earth [ ]. However, medical data mining faces numerous key challenges, mainly due to the heterogeneity and verbosity of data coming from various non-standardized patient records. Similarly, the insufficient quality of data is also a known issue in medical science that needs to be handled with care for data mining. Such challenges can be met by standardization of the process of selection of patients, collection, storage, annotation, and management of data [ ]. However, sometimes this means that existing data and data acquired at multiple centers without good coordination and standard operating procedures (SOPs) could not be used. The major divergence between medical data and non-medical data mining is expected in ethical and legal aspects. The use of information that can be traced back to individuals involves privacy risks, which could result in legal issues. More than fifteen Federal US departments with the US Department of Health and Human Services have issued final revisions to the Federal Policy for the Protection of Human Subjects \u201cthe Common Rule, 45 CFR 46, Subpart A\u201d (Protection of Human Subjects, 45 CFR 46 (2018). The federal framework for privacy and security does not apply to the information, which is de-identified or anonymized [ ]. \n\nThe ownership of medical data is another critical issue, as the data are acquired by different entities where the individuals may have been during their treatment or for diagnostic purposes. These entities can gather and store the data as per the authorization of the individual at the time of data acquisition. However, this permission on consent can be withdrawn by the patient at any time, and/or the consent is only valid for a limited period and data must be erased after this time [ ]. Most of the clinical text is produced in a telegraphic way and the information is highly enriched. Additionally, it is written for the clinical staff and colleagues, therefore is full of incomplete sentences and abbreviations. Special tools are required to read, understand, and process this text [ ]. Electronic patient records, also known as clinical text, have a unique problem in that they are written in a highly specialized language that can only be processed with a few available tools. Secondly, patient records are sometimes written in a telegraphic and information-dense style for clinician-to-clinician communication, and there exists no developed dictionary for such communications to check grammar and spelling mistakes. In addition, doctors and medical staff frequently use rudimentary sentences and frequently fail to mention the object, such as the patient, because the patient is implied in the text. \u201cArrived with 38.3 fever and a pulse of 132\u201d, for example, could be written or simply mentioned. \n\n\n### 1.2. Use of Artificial Intelligence and Machine Learning in Medical Literature Data Mining \n  \nThe digital era has shown immense trust and growing confidence in machine learning techniques to increase the quality of life in almost every field of life. This is the case in health care and precision medicine, where a continuous feed of medical data from heterogeneous sources becomes a key enabler for AI/ML-assisted treatments and diagnosis. For instance, AI today can help doctors to bring better patient outcomes with early diagnosis and treatment plans as well as increased quality of life. Similarly, health organizations and authorities also aim for the timely execution of AI routines for the prognosis of outbreaks and pandemics at the national and international levels. Healthcare today is also witnessing the use of AI-aided procedures for operational management in the form of automated documentation, appointment scheduling, and virtual assistance for patients. In this section, we will see some real-life references of AI\\ML tools and technologies currently used in various areas of medical sciences ( ). \n\nBefore going into further detail, it is worth mentioning that data mining and machine learning concepts go hand in hand and overlap each other to an extent but with a clear distinction of the overall outcome of both technologies. Data mining is the process of discovering correlations, anomalies, and new patterns in a large set of data from an experiment or event to forecast results [ ]. The basis of data mining is statistical modeling techniques to represent data in some well-defined mathematical model and then use this model to create relationships and patterns among the data variables. Machine learning, on the other hand, is a one-step-ahead approach to data mining, where machine learning algorithms let the computer machine understand the data (with the help of statistical models) and make predictions of its own. That said, data mining techniques always require human interaction to find interesting patterns from a given dataset, whereas machine learning is a relatively modernized technique that enables computer programs to learn from the data automatically and provide predictions without any human interaction. \n\n#### Natural Language Processing \n  \nNatural Language Processing (NLP) is an artificial intelligence (AI) discipline that converts human language into machine language. With the increased usage of computer technology over the last 20 years, this sector has grown significantly [ ]. Clinical documentation, speech recognition, computer-assisted coding, data mining research, automated registry reporting, clinical decision support, clinical trial matching, prior authorization, AI chatbots and virtual scribes, risk adjustment models, computational phenotyping, review management and sentiment analysis, dictation and EMR implementations, and root cause analysis are some of the most popular applications of NLP in healthcare [ ]. In the literature, a wide range of applications of NLP have been illustrated. \n\nLiu et al. [ ] used clinical text for entity recognition using word embedding (WE)-skipgram and long short-term memory (LSTM) techniques and achieved an accuracy of 94.37 percent, 92.29 percent, and 85.81 percent for de-identification, event detection, and concept extraction, respectively, based on the micro-average F1-score. Deng et al. [ ] used concept embedding (CE)\u2013continuous bag of words (CBOW), skip-gram, and random projection to generate code and semantic representations from clinical text. Afzal et al. [ ] have developed a pipeline for question generation, evidence quality recognition, ranking, and summarization of evidence from biomedical literature and presented an accuracy of 90.97 percent. Besides these examples, Pandey et al. [ ] listed 57 papers published between 2017 and 2019 that used NLP techniques and various text sources, such as clinical text, EHR inputs, Chinese medical text, cancer pathology reports, biomedical text, randomized controlled trial (RCT) articles, clinical notes, and EMR text-radiology reports, among others. \n\n\n\n\n## 2. Standard Process for Data Mining \n  \nIn response to the demand for a standard data mining method, industry leaders collaborated with a diverse group of practitioners (service providers, management consultants, data mining users, data warehouse vendors) and data mining experts to develop a free, well-documented, and non-proprietary data mining model [ ]. Numerous methods are available for data mining, such as ASUM (Analytics Solutions Unified Method), CRISP-DM (Cross-Industry Standard Process for Data Mining), KDD (Knowledge discovery in databases), SEMMA (Sampling, Exploring, Modifying, Modelling, and Assessing), and POST-DS (Process Organization and Scheduling electing Tools for Data Science) [ ]. In this study, we employ the CRISP-DM model for data mining because it is a complete and comprehensive data mining approach. In 1997, the CRISP-DM consortium developed a generic process model for data mining to establish guidelines for data mining beginners, the community, and experts, which can be modified for any particular need [ ]. For example, to deal with the problem of multidimensional time-series data in a neonatal intensive care unit (NICU), the CRISP-DM model was modified to support and accommodate temporal data mining (TDM), which is named CRISP-TDM [ ]. In the lifecycle of a data mining process, the CRISP-DM reference model has six phases ( ): Business understanding, data understanding, data preparation, modeling, evaluation, and deployment. The details of the available tools and technologies for each phase are described in the rest of this article. \n\n### 2.1. Business Understanding \n  \nThe first and most critical part of data mining is business understanding, which includes setting project objectives, and targets, assessing the situation, execution plans, and risk assessments [ ]. Setting project objectives requires a complete grasp of the project\u2019s genuine goal to define the associated variables. The steps in the data understanding phase according to CRISP-DM are to (1) determine the business objectives (to fully comprehend the project\u2019s goal, identify the key players, and establish business success criteria), (2) assess the situation (to identify resource availability (especially data), identify project risks and potential solutions to those risks, and calculate the cost\u2013benefit ratio), (3) clarify the data mining goals (to establish project goals and success criteria), (4) produce a project plan (to develop detailed plans for each project segment, including a timeline and technology and tool selection). \n\nMartins et al. [ ] used a data mining approach to predict cardiovascular diseases (while using RapidMiner and Weka software). The main question addressed by the project is how to detect cardiovascular disease at an early stage in a person who is at a high risk of developing the disease and thus avoid premature death. As a result, the primary set of goals is to create a solution for predicting cardiovascular diseases in patients using patient data, to shorten the time required for disease diagnosis, and to provide the patients with immediate and adequate treatment. \n\n\n### 2.2. Data Understanding \n  \nThe emphasis in this phase (second phase), according to CRISP-DM, is on data source identification, data acquisition, initial data collection, familiarization with the data, and identifying problems in the acquired data. The steps in the data understanding phase are (1) acquire the initial data (to gather the data from various sources, insert it into the analysis program, and integrate it), (2) explain the data (to study and report on the acquired data\u2019s surface properties such as field identities, data format, data quantity, and the number of records, etc.), (3) explore the data (to delve deeper into the data by querying, visualizing, and identifying relationships between data points, as well as to generate an exploration report), and (4) verify data quality (to inspect and document the data quality and any quality-related issues) [ ]. In this phase, one focuses on identifying data sources for various types of data, the process of acquisition of the data, and handling access restrictions in data acquisition. A tremendous amount of data is generated by the health care industry and medical institutions every day from medical imaging, patient monitoring, and medical records [ ]. Some of the most common types of medical data are experimental data, medical literature, clinical textual data, medical records, images/videos (e.g., MRI), and omics data (e.g., genomics, proteomics). For example, Martins et al. [ ] used a data mining approach to predict cardiovascular diseases. For data understanding, the dataset for cardiovascular disease prediction came from the Kaggle data repository and focused on detecting cases of cardiovascular disease. The dataset included 70,000 registered patients with 12 disease-related attributes collected during the patients\u2019 medical examinations. \n\n#### 2.2.1. Literature Extraction/Data Gathering \n  \nThe first task in the data understanding phase is to identify data sources, acquire data from these sources, identify problems during data acquisition, such as data restrictions and data privacy policies, and document the solutions [ ]. Text/data mining frequently uses public Internet-based sources such as the World Wide Web. The retrieval of content from public sources is referred to as \u201cweb scraping\u201d or \u201cweb crawling\u201d. Web scraping can be performed manually, but it can also be performed automatically with the help of a web crawler. Manual scraping a large database such as PubMed, which contains millions of peer-reviewed publications, requires a lot of time and effort. Only automated processing can provide the necessary quality, response time, and homogeneity for their analysis with such a large database. As a result, there is always a high demand for web scraping techniques and tools tailored to customer requirements. PubMed, for example, is a massive database of biomedical literature that contains 34 million citations (as of 11 May 2022) collected from online books, life science journals, and MEDLINE, and a massive number of new publications are added every year [ ]. Web crawlers are used to search for and harvest the necessary data from it. Guo et al. [ ], for example, collected COVID-19 data published by local health authorities using a web crawler (developed using the Python language and connected with a MySQL database). \n\nAlthough web scraping and web crawling may seem to be identical, they have several distinctions ( ). While the terms \u201cweb scraping\u201d and \u201cweb crawling\u201d are sometimes interchanged, they refer to two distinct processes [ , ]. Web crawling is a broad term that refers to the process of downloading information from a website, extracting the hyperlinks included within, and following them ( ). Typically, downloaded information is saved in a database or indexed to enable searching. Essentially, search engines are crawlers. All that is required is to see a page in its entirety and indexing it. When a bot crawls a website, it scans each page and link, all the way to the website\u2019s last line, looking for any information. Web crawlers are primarily used by major search engines such as Google, Bing, and Yahoo, as well as statistics organizations and online aggregators. Typically, a web crawler collects general information, while scrapers collect particular datasets [ , ]. On the other hand, web scraping is the process of obtaining data from a web page and extracting specific information that can be saved almost anywhere (database, file, etc.) as shown in  . An online scraper, also known as a web data extractor, is similar to a web crawler in that it detects and locates website content. In contrast to a web crawler, which uses pseudo-random IDs, web scraping uses specific identifiers, such as the HTML structure of the web pages from which data must be collected. Web scraping refers to the use of robots to extract specific datasets from the internet. The obtained data can be compared, checked, and analyzed in accordance with the demands and objectives of a specific organization [ ]. \n\nSeveral text mining tools are now available. Kaur and Chopra [ ] compared 55 popular text mining tools and their features and discovered three categories: (1) Proprietary (company-owned\u201439 tools); (2) open source (free\u201413 tools); and (3) online text mining tools (run directly from a website\u20143 tools). Four tools that were not examined in the prior review but are now on the list of well-liked text mining tools are contrasted in  . All of these Python-based tools serve the same purpose, but with different goals and objectives. \u2018\u2019Requests\u2019\u2019 has an advantage over other tools in that it is easy to use, making it an excellent choice for any simple web scraping task. Scrapy is best suited for large-scale web scraping projects, as opposed to the other three tools (requests, beautiful soup, and selenium), which are best suited for small-scale scraping tasks. The \u201cBeautiful Soup\u201d tool has advantages such as being simple to understand, learn, and use, and it can extract information from a disorganized website. Selenium has a significant advantage over the other scraping tools described because it can scrape websites with heavy JavaScript.   provides descriptions of more hierarchical comparisons. \n\n##### Access Restriction \n  \nWhen a web crawler visits a website, some pages or the entire website possess access restrictions. These restrictions are implemented mainly by the site owners due to data confidentiality, data integrity, and data quality, as well as legal concerns. A crawler usually performs multiple requests per second and downloads large files to obtain the data in a short time, which can cause a website server to crash. To tackle this problem, numerous methods are available. Canonical tag, robots.txt, x-robots-tag, the metarobots tag, and others are files provided by the website owners to follow the instructions for scraping the website without creating any problem. For example, \u201crobots.txt\u201d files are frequently used by websites to convey their scraping and crawling intents. Robots.txt files enable scraping bots to crawl specific sites, while malevolent bots, on the other hand, are uninterested in robots.txt files (which act as a \u201cdo not enter\u201d sign) as explained below in  . \n\n\n##### Data Collection from Different Sources \n  \nThe pace at which medical data are being generated is increasing day by day during the massive information explosion year, and global information is being produced in massive quantities in every field, including healthcare [ , ]. Administrative records, biometric data, clinical registration, diagnostics, X-rays, electronic health records, patient report data, treatments, results, and other types of medical data are all included in medical data. These massive and complex characteristics make data difficult to deal with for a meaningful and unknown outcome. Healthcare centers and medical institutions around the world have proposed a variety of medical information systems to deal with rapidly growing data and provide the best possible services and care to patients [ ]. The most common way to collect and store the data is by management software, which can store all electronic and non-electronic records. Several software products are available, e.g., eHospital Systems ( , accessed on 11 April 2022) and the DocPulse Clinic/Hospital Information Management System ( , accessed on 11 April 2022). \n\nFor text mining, data collection from data sources is the key step. In medical science, various types of medical data, as well as trends, are generated at a rapid pace, which can be differentiated into five categories, as follows:   \nHospital management software (Patient data/Clinical narratives). \n  \nClinical trials. \n  \nResearch data in Medicine. \n  \nPublication platforms for Medicine (PubMed, for instance). \n  \nPharmaceuticals and regulatory data. \n  \n\n,   and   provide further details about the different types of data sources. Patient data generated by clinical trials is available from various sources, as shown in  . Medical researchers benefit from open-access databases because they have enormous volumes of data, rich data content, broad data coverage, and a cost-effective study strategy. There exist several datasets and databases publicly available related to various medical fields that contain many medical record variables ( ). Textual information is growing rapidly, and it is difficult to grab concise information fast and structured manner. The published literature is the most abundant and primary source of textual information in the health care field ( ). \n\n\n\n\n### 2.3. Data Preparation \n  \nIn the third phase (data preparation) of CRISP-DM, a final dataset is created from the raw data, which will be used in the modeling tool. This phase is the major part (ca. 80%) of a text/data mining project. The steps in the data preparation phase are (1) data selection (to choose the dataset along with its attributes that will be used for the analysis based on the project goals, quality, data type, and volume.), (2) data cleaning (to estimate missing data and improve the dataset by correcting, imputing, or removing incorrect values), (3) data construction (to create derived attributes or entirely new records, as well as to transform data as needed), (4) data integration (to create new datasets and aggregate new values by combining data from multiple sources), (5) data formation (to remove inappropriate characters from the data and change the data\u2019s format or design so that it fits into the model) [ ]. \n\n#### 2.3.1. Data Cleaning/Data Transformation \n  \nThe primary goal of data cleaning is to detect and remove duplicate data and errors from a dataset to create a reliable dataset. Cleaning data entails identifying and removing entries from a dataset that are corrupt, incorrect, duplicated, incomplete, or improperly formatted (see  ). Data cleaning is required to analyze information from multiple sources [ , , ]. \n\nVarious related tools and python libraries are discussed in the following sections. \n\nPython Libraries for Data Cleaning include the following:   \nNumPy is a quick and easy-to-use open-source Python library for data processing. Because many of the most well-known Python libraries, including Pandas and Matplotlib, are based on NumPy, it is a fundamentally crucial library for the data science environment. The primary purpose of the NumPy library is the straightforward manipulation of large multidimensional arrays, vectors, and matrices. For numerical calculations, NumPy also offers effectively implemented functions [ ]. \n  \nData processing tasks such as data cleaning, data manipulation, and data analysis are performed using the well-known Python library Pandas. The Python Data Analysis Library is referred to as \u201cPandas\u201d. Multiple modules for reading, processing, and writing CSV, JSON, and Excel files are available in the library. Although there are many data cleaning tools available, managing and exploring data with the Pandas library is incredibly quick and effective [ ]. \n  \nAn open-source Python library for automating data cleaning procedures is called DataCleaner. Pandas Dataframe and scikit-learn data preprocessing features comprise its two separate modules [ ]. \n  \n\nThe data are then transformed into the proper format after being cleaned (Excel, JSON, or XML). Data transformation makes it simpler to preprocess data and/or text. Depending on the modifications that must be made, the data transformation may be straightforward or complicated. The data are easier to use for both humans and computers after transformation because it is more structured and organized. Additionally, it becomes simpler to integrate into various programs and systems [ ]. \n\nVarious related tools are discussed in the following sections. \n  \nGeneration of Bibliographic Data is known as GROBID. It is a machine-learning library that has developed into a state-of-the-art open-source library for removing metadata from PDF-formatted technical and scientific documents. The library plans to reconstruct the logical structure of its original document in addition to simple bibliographic extraction in order to support large-scale advanced digital library processes and text analysis. \n  \nGROBID develops fully automated solutions based on machine learning models for that reason. ResearchGate, Mendeley, CERN Inspire, and HAL, France\u2019s national publication repository, are just a few of the commercial and open-access scientific services that the library is connected to. \n\nThe result is to extract and transform PDF documents into XML TEI format, supplement the extracted information with other online services, and illustrate the findings gathered in PDF documents of scientific papers [ , ]. \n  \nBioC is a straightforward and straightforward format for exchanging text data and annotations, as well as for simple text processing. Its primary goal is to provide an abundance of research data and articles for text mining and information retrieval. They are available in a variety of file formats, including BioC XML, BioC JSON, Unicode, and ASCII. These formats are available through a Web API or FTP [ ]. \n  \nTo summarize, data cleansing improves a dataset\u2019s consistency, while transformation simplifies data processing. Both processes improve the training dataset\u2019s quality for model construction. \n\n\n#### 2.3.2. Feature Engineering \n  \nChoosing, modifying, and converting raw data into features that may be utilized in supervised learning is a process of feature engineering, often referred to as feature extraction. This machine learning technique, feature engineering, uses data to generate new variables that are not present in the training set. To streamline and accelerate data transformations while also improving model accuracy, it can generate new features for both supervised and unsupervised learning. With machine learning models, feature engineering is necessary. Regardless of the architecture or the data, a bad feature will directly affect your model. Numerous tools are available to automate the entire feature engineering process and to generate a large pool of features in a short period for both classification and regression tasks. Some feature engineering tools are FeatureTools, AutoFeat, TsFresh, Turi, Azure Machine Learning Studio, ZOMBIE, FeatureFu, and OneBM [ , ]. \n\nVijithananda et al. [ ] extracted features from MRI ADC images of a brain tumor. The following features were extracted from labeled MRI brain ADC image slices from 195 patients: Skewness, cluster shade, pixel values (he demographics), prominence, Grey Level Co-occurrence Matrix (GLCM) features, energy, contrast, entropy, variance, mean, correlation, homogeneity, and kurtosis. Both GLCM homogeneity and skewness were excluded because they scored the lowest in the ANOVA f-test feature selection process. The Random Forest classifier outperformed Decision Trees, Nave Bayes, Linear Discriminant Analysis, K-Nearest Neighbors (KNN), and Logistic Regression and was chosen for further model development. The final model had an accuracy of 90.41 percent in predicting malignant and benign neoplasms. \n\n\n#### 2.3.3. Searching for Keywords \n  \nThe extraction of keywords or key phrases from text documents is known as keyword extraction. They are chosen from among the phrases in the text document and describe the topic of the document. Several popular methods are available for automatically extracting keywords. Those are used in processes that automatically extract keywords from documents to select the most frequently used and significant words or phrases from the text document. This classifies keyword extraction methods as part of the natural language processing field, which is important in machine learning and artificial intelligence. [ ]. Keyword extractors are used to extract words (keywords) or groups of two or more words that form a phrase (key phrases). \n\nFlashText, for example, is a free and open-source Python package that enables keyword search and replacement and is one of the recently described keyword extraction tools [ ]. It performs a full analysis using an Aho-Corasick algorithm and a Trie Dictionary. As a general rule, keyword matching entails scanning the corpus (human-created documents comprise a large, structured set of texts) for each term. Consider the following scenario: Someone has 100 keywords and needs to search through 2000 papers; a single term is selected at a time and a search of the 2k corpus is performed; the search is continued for 100 \u00d7 2000 is 200,000 iterations. In addition to this keyword search tool, four Python-based tools are selected from the various keyword and phrase extraction tools that are available, and their features, benefits, and NLP tasks are contrasted in  . \n\n\n\n### 2.4. Modeling \n  \nIn the fourth phase (known as modeling) of CRISP-DM, various modeling techniques are tested and calibrated by adjusting the model parameters to achieve the best results [ ]. The steps in the modeling process are (1) choosing a modeling technique to select one or more task-specific models/algorithms/assumptions, (2) the creation of test designs to determine the model\u2019s strength by evaluating the model\u2019s quality and validity, (3) the building of models (to use the modeling tool for building models from the prepared dataset, adjust the model parameter, and describe the model), and (4) the evaluation of models to explain the model outcome based on subject knowledge, the predetermined success norms, and the test design, rank the multiple generated models, and readjust the parameter settings\u2014if required. \n\nFrom several available models for organizing and analyzing the data, the selection of a model depends on the purpose (e.g., forecast) and the type of data used (unstructured or structured). A model is a set of data, patterns, and statistics. The available data-mining models are divided into two categories: Predictive and descriptive. Descriptive models are frequently used to determine patterns in data that can be explained by humans. Predictive models use known results from various datasets to forecast unidentified or future values of other variables of interest. Predictive models are usually based on the previously provided data and their results. Classification, prediction, regression, and time series analysis are tasks in the predictive models. Descriptive model data mining tasks comprise clustering, associating rules, sequence discovery, and summarization ( ). A number of algorithms/methods are available for the prediction and analysis of patterns in the data. However, the selection of the algorithm is mainly depending on the dependent variables whether labeled or unlabeled. If the dependent variable/s in the dataset are labeled, a supervised learning algorithm is used. Decision trees, the random forest (RF), support vector machines (SVMs), and competitive risk model are commonly used algorithms. In contrast, if the dependent variables in the data are not labeled, an unsupervised learning method is used. Clustering analysis, partition clustering, hierarchical clustering, principal component analysis (PCA), and association analysis are some of the unsupervised learning algorithms [ , ]. \n\nThe dataset is the primary distinction between supervised and unsupervised machine learning. It is referred to as supervised learning if the dataset employs a labeled dataset for input and output, whereas unsupervised learning techniques use unlabeled data. As the name suggests, supervised learning entails the external supervision of a model\u2019s training. Unsupervised learning, on the other hand, does not involve any supervision. Additionally, in the case of supervised learning, the goal is to predict the outcome of new data. In the case of unsupervised learning, the goal is to find hidden patterns and gain insight from enormous amounts of new data. In contrast to supervised learning models, which are straightforward, unsupervised learning models require a large training set to produce the desired results, making them computationally complex. Some of the applications of supervised learning models include diagnosis, identity fraud detection, image classification, price predictions, sentiment analysis, spam detection, market forecasting, and weather forecasting. Unsupervised learning models are used in the pipelines for anomaly detection, big data visualization, customer personas, feature elicitation, recommended systems, structure discovery, and targeted marketing [ , ]. \n\nAs an instance of the modeling example, the suitability of a WebCrawler (StormCrawler) for the acquisition of all health-related web content on the German Health Web (Germany, Austria, and Switzerland) was investigated by Zowalla et al. [ ]. For this purpose, a support vector machine classifier model was trained to distinguish between health-related and non-health-related web pages using the dataset created from the German health web. This model was tested for accuracy and precision on an 80/20 training/test split and against a crowd-validated dataset. For predicting cardiovascular diseases, the best-suited technique was the \u2018Decision Tree\u2019 compared with eight other techniques, i.e., Deep Learning, Nearest Neighbor (k-NN), Gradient Boosted Tree, Generalized Linear Model, Logistic Regression, Na\u00efve Bayes, Random Forest, and Rule Induction [ ]. Furthermore, some parameters were optimized using the optimized parameters operator to achieve better results when using the \u2018Decision Tree\u2019. \n\n\n### 2.5. Data Model Validation and Testing \n  \nThis step\u2019s primary goal is to validate and test the selected model for the data in the model development process. The validation procedure is used to ensure that the developed model is accurate enough for the intended use [ ]. The first half of this step, model validation, is important because the used/newly developed model cannot be relied on solely because it was designed to fit the training data and demonstrates that the training data fits the model well. To validate a model, output predictions are made in scenarios unrelated to the training set, and the same statistical measures of fit are computed. The second half of this step involves testing the model with test data and comparing its accuracy with the results of the validation step. Only when a model is compared to test data and statistical calculations show a satisfactory match is it considered \u201cready\u201d. For the classification of tumor and non-tumor samples, Dong et al. [ ] employed a training dataset (which consists of mass spectrometry (MS) raw data obtained from 194 paired tumor and non-tumor samples) to train different models and used a similar type of dataset (which consists of MS raw data obtained from 58 paired tumor and non-tumor samples) as a test dataset. The convolutional neural network (CNN), gradient boosting decision tree (GBDT), support-vector machine (SVM), principal component analysis (PCA) plus SVM, logistic regression (LR), and random forest (RF) were compared, and the CNN model showed the highest accuracy. Some of the ML model validation testing tools include Apache Spark, Excel, Hadoop, KNIME, Python, R, RapidMiner, SAS, SQL, and Tableau. \n\n\n### 2.6. Evaluation \n  \nIn the fifth phase (known as evaluation) of CRISP-DM, a more thorough evaluation and review of the model\u2019s construction is conducted to ensure that the model properly achieves the business objectives. The steps in the evaluation phase are (1) the assessment of outcomes to assess how well the model achieves the project\u2019s goals, discover additional constraints, information, or clues about future paths, and present the project\u2019s final statement, (2) the review process to conduct a more in-depth review of the project and address quality assurance concerns, and (3) the decision for further steps to determine whether or not to proceed with the deployment or to make changes for the improvement [ ]. \n\nAfter the analysis of text data, the next step is to visualize the data meaningfully for interpretation and communication purposes. Text visualization is primarily accomplished through the use of charts, graphs, maps, timelines, networks, word clouds, and so on. These visualized results allow humans to read the most important aspects of a large amount of information. There are several tools available to display the analyzed data. These tools make it easy to identify and discover patterns, outliers, trends, and insights in data straightforwardly and understandably. Effective data visualization has benefits and advantages such as easy understanding of the outcome, effortless and prompt decision-making, and a higher degree of engagement for a diverse audience over other communication methods (e.g., verbal communication). For successful data visualization, there are three main principles: (1) Depending on the purpose, select the appropriate visualization style, (2) the selected visualization style should be appropriate for the targeted audience, and (3) the chosen visualization style should be accompanied by an effective graphic design [ ]. The most important aspects of selecting the appropriate visualization style are considering the selected data and the aim of the visualization. For example, line and bar charts are suitable for comparing data points across a dataset. Diverse visualization styles are available for creating attractive and effective visual information, i.e., typographic visualization (e.g., word cloud), graph visualization (e.g., tree), chart visualization (e.g., bar/line chat), 3D visualization, etc. Below, in  , we provide a list of various visualization styles along with a few of the available tools in each category. \n\nBesides these tools, there are software available with gigantic capabilities to visualize the data, such as, Microsoft Excel\u2019s PivotTables, R, Tableau, Power-BI, datawrapper, and Google Charts. These tools are easy to use and very helpful in creating a clear and dynamic display of data because of their interactive graphical interface. Furthermore, different libraries written in different programming languages are also available for data visualization, which are easy to use for programmers, such as JavaScript libraries (e.g., D3.js, Chart.js, and Highcharts), python libraries (e.g., Matplotlib, Seaborn, and Plotly), and R libraries (e.g., ggplot2, Leaflet, and Esquisse). The major challenges of data visualization are the massive amount of data, the complexity of data, and missing/duplicate entries [ ]. \n\n\n### 2.7. Deployment \n  \nIn the deployment phase (sixth and final phase of CRISP-DM, Shearer [ ]), the knowledge gained from the project is organized and presented (e.g., live demonstrations) in a way that is useful for the project, the company, and the customer. This phase\u2019s complexity varies greatly. The steps in the deployment phase are as follows: (1) Create a deployment plan to formulate and note a deployment strategy for the model, (2) plan the monitoring and maintenance to create well-thought-out planning of maintenance and monitoring to shun problems during the operational phase of a model, (3) produce a final report to prepare and present a final report of the project in the form of a written document and verbal meeting, and (4) review the project to evaluate successes and failures, as well as potential areas for improvement in future projects. \n\n\n\n## 3. Conclusions and Future Outlook \n  \nThe amount of medical text data is rapidly increasing. From medical text data, data mining can be used to extract new and useful information or knowledge. The CRISP-DM system presented in this study focuses on each step of data mining while using medical examples to explain each step. The authors plan to develop an artificial intelligence-based web crawling system with 4D visualization of the data in a summarized and easy-to-understand manner and use these data as a source of information for researchers, as well as for the education of patients and medical staff in future work. \n\n \n", "metadata": {"pmcid": 9501106, "text_md5": "6666a600486b2a632e289de089d75584", "field_positions": {"authors": [0, 116], "journal": [117, 127], "publication_year": [129, 133], "title": [144, 193], "keywords": [207, 296], "abstract": [309, 1277], "body": [1286, 43583]}, "batch": 1, "pmid": 36143144, "doi": "10.3390/jpm12091359", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9501106", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9501106"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9501106\">9501106</a>", "list_title": "PMC9501106  Artificial Intelligence-Based Medical Data Mining"}
{"text": "Krallinger, Martin and Leitner, Florian and Rabal, Obdulia and Vazquez, Miguel and Oyarzabal, Julen and Valencia, Alfonso\nJ Cheminform, 2015\n\n# Title\n\nCHEMDNER: The drugs and chemical names extraction challenge\n\n# Keywords\n\nnamed entity recognition\nBioCreative\ntext mining\nchemical entity recognition\nmachine learning\nchemical indexing\nChemNLP\n\n\n# Abstract\n \nNatural language processing (NLP) and text mining technologies for the chemical domain (  ChemNLP   or chemical text mining) are key to improve the access and integration of information from unstructured data such as patents or the scientific literature. Therefore, the BioCreative organizers posed the CHEMDNER (chemical compound and drug name recognition) community challenge, which promoted the development of novel, competitive and accessible chemical text mining systems. This task allowed a comparative assessment of the performance of various methodologies using a carefully prepared collection of manually labeled text prepared by specially trained chemists as Gold Standard data. We evaluated two important aspects: one covered the indexing of documents with chemicals (  chemical document indexing   -   CDI   task), and the other was concerned with finding the exact mentions of chemicals in text (  chemical entity mention recognition   -   CEM   task). 27 teams (23 academic and 4 commercial, a total of 87 researchers) returned results for the CHEMDNER tasks: 26 teams for CEM and 23 for the CDI task. Top scoring teams obtained an F-score of 87.39% for the CEM task and 88.20% for the CDI task, a very promising result when compared to the agreement between human annotators (91%). The strategies used to detect chemicals included machine learning methods (e.g. conditional random fields) using a variety of features, chemistry and drug lexica, and domain-specific rules. We expect that the tools and resources resulting from this effort will have an impact in future developments of chemical text mining applications and will form the basis to find related chemical information for the detected entities, such as toxicological or pharmacogenomic properties. \n \n\n# Body\n \n## Background \n  \nUnstructured data repositories contain fundamental descriptions of chemical entities, such as their targets and binding partners, metabolism, enzymatic reactions, potential adverse effects and therapeutic use, just to name a few. Being able to extract information on chemical entities from textual data repositories, and particularly the scientific literature, is becoming increasingly important for researchers across diverse chemical disciplines [ ]. Manual curation of papers or patents to generate annotations and populate chemical knowledgebases is a very laborious process that can be greatly improved through the use of automated language processing pipelines. Text-mining methods have shown promising results in the biomedical domain, where a considerable amount of methods and applications have been published [ , ]. These attempts cover tools to rank articles for various topics of relevance [ ], detect mentions of bio-entities [ , ], index documents with controlled vocabulary terms [ ] or even extract complex relationships between entities like physical protein-protein interactions [ ]. Automatically transforming recognized entity mentions into structured annotations for biomedical databases has been studied in particular for genes or proteins [ ]. \n\nLinking chemical entities to the results obtained by biological/biomedical text mining systems requires first the automatic recognition and indexing of chemical entities in documents. Furthermore, knowing which compounds are described in a given paper, and where exactly those descriptions are, is key to select appropriate papers. Only with such fine-grained annotations, it is possible to directly point to relevant sentences and to extract more detailed chemical entity relations. The process of automatically detecting the mentions of a particular semantic type in text is known as named entity recognition (NER). Some of the first NER systems constructed where those that recognized entities from newswire texts. Among the classical entities detected by such domain-independent tools are names of persons, organizations or locations [ ]. In the context of biomedical literature mining, the bio-entities that attracted more interest were genes, proteins, cell lines, cell types, drugs, mutations and organisms or species [ , , ]. The recognition of gene and protein mentions was addressed in several community challenges (BioCreative I, II, JNLPBA) that served to determine the state of the art methodology and systems performance [ , ] in addition of providing valuable datasets for developing new systems [ ]. \n\nThe recognition of chemical entities has to cope with a considerable degree of naming variability encountered between and within different chemical sub-disciplines. A given chemical entity can appear in the literature as a trivial or trademark name of a drug, as a short form (abbreviation or acronym), or it can be referred to in text following standard naming nomenclature guidelines as provided by the IUPAC. In addition, alternative typographical expressions for chemical entity mentions and ambiguity of chemical names that can correspond to other entity types (depending on the context of mention) makes detection of chemical names a demanding task. \n\nManually annotated text collections that were exhaustively labeled with entity mentions are essential to implement NER tools, especially if they rely on statistical machine learning methods. The lack of publicly available, large, and manually annotated text corpora for chemical entities was one of the main reasons why only few chemical compound recognition applications were available [ ] before the CHEMDNER task. Details on the construction of high quality text corpora for chemistry, together with the underlying selection criteria and guidelines are presented in the CHEMDNER corpus paper of this same special issue. \n\nWe have organized the CHEMDNER task as part of the BioCreative community challenge (BioCreative IV [ ]), to promote the development of systems for the automatic recognition of chemical entities in text. The CHEMDNER task was the first community-wide attempt to evaluate chemical natural language processing methods. It offered manually annotated data that allowed participating researches to improve and evaluate their tools. This task could help to increase the performance of chemical NER systems and run a comparative assessment across heterogeneous strategies. A number of participating teams present updated versions of their systems in this special issue, showing that the CHEMDNER task was a successful environment to implement their systems and that they could actually further improve their tool by learning from other participating approaches. This article will present the obtained results and provides an overview of the methods used by participating teams. \n\n\n## Methods \n  \n### Task description \n  \nWe divided the BioCreative IV CHEMDNER track into two tasks, each of practical relevance for the retrieval and extraction of chemical information from the literature. One task was concerned with the ability to determine which compounds appear in a given document, a requirement for selecting articles that refer to a particular chemical of interest. Therefore, this assignment is called the   chemical document indexing   (CDI) task. For the CDI task, given a collection of PubMed abstracts, participants had to provide for each of them a non-redundant (unique) list of chemical entities. Chemical entities were defined for the CDI task as the (UTF-8 encoded) character strings found in the text. The entities had to be returned as a ranked list to express the entity's likelihood of being a relevant chemical mention in that document, together with a confidence score. Each entity mention in the list had to be unique (for a given document). Submissions containing multiple ranks for the same chemical name were discarded. Although of great practical importance, we did not require that the returned compounds had to be mapped to their chemical structures or database identifiers. Normalization - or grounding - of entities to a knowledgebase is particularly challenging in case of chemicals [ ], because many compounds found in literature (and patents) do not have any corresponding record in open access chemical databases. This means that only a subset of the compound mentions can be linked to an existing database. However, we consider the conversion of chemical names to structures a task of its own, more closely related to the cheminformatics domain. Therefore, we decided to not confound this research problem with the task of detecting chemicals in text. \n\nThe other CHEMDNER task was concerned with the ability to specifically locate exactly within a document every chemical entity mention, defined as their start and end character indices (counting the position of the characters in the document spanning the mention of a chemical). We called this the   chemical entity mention recognition   (CEM) task, a key step for any further chemical relation mining approaches or to induce chemical lexicons from the literature automatically. To specify mentions of entities, one popular option is to require labeling of individual tokens (mostly word units), as was done for the first gene mention task of the BioCreative challenge [ ]. In case of the biomedical literature, several different tokenization strategies have been tested [ ] and also specialized tokenizers have been proposed [ ]. Tokenization of chemical literature is even more difficult, mainly due to the variable use of hyphens, parenthesis, brackets, dashes, dots and commas. We therefore did not pre-impose any tokenization of the CHEMDNER text collection and defined entities only at the character offset level, similarly to the last gene mention recognition task of BioCreative [ ]. \n\nFor both tasks, teams had to provide ordered results (ranked entity names or ranked entity mention offsets) together with confidence scores that reflected how sure they were that the extracted entity was correct. This setting promoted the implementation of systems that are more efficient for the manual validation of automatically extracted results. It facilitates selecting any N top results for each document. All the chemical annotations and predictions for both tasks were derived exclusively from the PubMed titles and abstracts; information from full text articles was not annotated for the CHEMDNER tasks. Participating teams could submit up to five runs for each of the two tasks. It was not mandatory to send predictions for both tasks; they could send results for any of the two or for both the CDI and CEM tasks. One strict constraint posed to participating teams was that any manual (human) correction or adjustment of the official results that they submitted for the test set documents were forbidden (i.e. only fully automated results were allowed). Compliance with the CHEMDNER prediction format was checked by the BioCreative evaluation script that was distributed to assess consistency and performance of automated predictions [ ]. Table   shows two example team predictions for each of the two tasks. In that table, the first column is the PubMed identifier (PMID) and each line corresponds to one prediction for that document. In case of the CDI task, the second column contains the unique chemical entity mention string. For the CEM task, it corresponds to the chemical mention offset, specified as the part of the document record (T: Title, A: abstracts) followed by the offset of the starting character and the ending character of the mention span (separated by ':'). The third column (for the CDI and CEM predictions) corresponds to the rank for each prediction given the article. The fourth column of each task prediction contains the confidence score (Conf.). In these examples, only the top ten predictions per task and for the article with the PubMed identifier 23380242 are shown. \n  \nExample team predictions for the CDI (left) and CEM (right) tasks. \n  \nBoth task predictions have as first column the document identifier (PMID) and originally are formatted as tabulator-separated plain text columns. Only the top ten predictions are shown. \n  \n\n### Task data: CHEMDNER corpus \n  \nThe predictions generated by automated systems were compared against manually labeled annotations done by domain experts. This manually labeled collection of texts and annotations is called the CHEMDNER corpus. The task was temporally structured into four periods, associated with the release of the CHEMDNER corpus datasets (refer to [ ] for a detailed description of the corpus). The pre-release phase was a period before the actual data release, during which we distributed the initial annotation guidelines together with an annotated sample set. During the training phase, teams could explore the annotated training data to build their systems. Thereafter the development set was released, consisting of additional annotated abstracts useful for the evaluation and improvement of the participating systems. Finally, during the test set prediction phase, registered teams were provided with a collection of articles without annotations for which they had to return predictions within a short period of time, together with a technical system description. The entire CHEMDNER corpus consisted of a collection of 10,000 recently published PubMed abstracts representative of various chemistry-related disciplines. All abstracts were exhaustively annotated for chemical entity mentions by trained chemistry domain experts with experience in literature curation. The annotation process followed carefully defined annotation guidelines of rules for defining what actually was considered as a chemical entity and what not, as well as how to determine the individual mention boundaries of a chemical entity in text. As a minimum criteria, chemical entities had to correspond to chemical names that could be potentially linked to a chemical structure, excluding very general chemical terms and very large macromolecular entities, such as proteins. Additionally the annotations were manually classified into one of the following chemical mention classes: abbreviation (short form of chemical names including abbreviations and acronyms), formula (molecular formulas), identifier (chemical database identifiers), systematic (IUPAC names of chemicals), trivial (common names of chemicals and trademark names), family (chemical families with a defined structure) and multiple (non-continuous mentions of chemicals in text). These more granular annotation types should help participants to adapt their entity recognition strategies for particularities specific to each of the chemical entity mention classes. The entire CHEMDNER corpus was randomly sampled into three subsets, the training set (3,500 abstracts), development set (3,500 abstracts) and test set (3,000 abstracts). Overall, the CHEMDNER corpus contained 84,355 chemical entity mentions corresponding to 19,806 unique chemical names. The fraction corresponding to the test set used for evaluation purposes had 25,351 chemical entity mentions (7,563 unique chemical names). \n\n\n### Evaluation metrics \n  \nParticipating systems were permitted to integrate previously accessible third party tools and lexical resources relevant to chemistry in addition to the official CHEMDNER annotations. To keep the task simple, we did not request predictions of the class of chemical entity mentions (e.g. systematic, trivial etc.). We provided an evaluation script together with the data that calculated the performance of predictions against the Gold Standard data and returned all the evaluation scores that were used for the CHEMDNER task. \n\nThe metrics used for scoring the predictions were micro-averaged recall, precision and F-score. The balanced F-score was the main evaluation metric used. Three result types were examined: False negative (FN) results corresponding to incorrect negative predictions (cases that were part of the Gold Standard, but missed by the automated systems), False positive (FP) results being cases of incorrect positive predictions (wrong results predicted by the systems that had no corresponding annotation in the Gold Standard) and True positive (TP) results consisting of correct positive predictions (correct predictions matching exactly with the Gold Standard annotations). We did not examine partial hits of predictions that only in part overlapped with the manually defined Gold Standard annotations. \n\nRecall   r   is the percentage of correctly labeled positive results over all positive cases. \n\n\n\nIt is a measure of a systems ability to identify positive cases. \n\nPrecision   p   is the percentage of correctly labeled positive results over all positive labeled results. \n\n\n\nIt is a measure of a classifier's reproducibility of the positive results. \n\nThe F-measure   F  is the harmonic mean between precision and recall, where   \u03b2   is a parameter for the relative importance of precision over recall. \n\n\n\nThe balanced F-measure (  \u03b2   = 1, referred to as \"F-score\" in this work) can be simplified to: \n\n\n\n\n\n## Results \n  \nWe received predictions from a total of 27 teams for the CHEMDNER challenge: 26 for the CEM task and 23 for the CDI task. In total, 87 researchers took part in participating teams, most of the teams were affiliated to academic research institutions but also 4 commercial teams submitted predictions. Table   provides an overview of participating teams, the tasks for which they submitted results (number of runs and the rank of their best submission grouped into subsets based statistical significance between them) and the link to the corresponding software in case it is available. A number of teams published a systems description paper in this same special issue of the Journal of Chemoinformatics. Those papers provide additional details on the used methods as well as potential updates and improvements of the initial approach that was used for the task. Total of 91 submissions for the CDI and 106 submissions for the CEM were evaluated. For properly interpreting text mining results, it is important to put automated systems performance into context. A simple baseline for detecting entity mentions is to label those mentions in the test set that have been seen before in the training data. A widespread baseline approach for NER methods is the   vocabulary transfer  , defined as the proportion of entities (without repetition) that are present both in the training/development data as well as in the test corpus. The vocabulary transfer is an estimate of the lower boundary expected recall of automatic NER methods. In previous assessments, for instance in MUC-6 (Palmer & Day) it was 21%, while in case of the CHEMDNER task it was of 36.34% when using both the training and development set names and of 27.77% when using only the names from the training set. We generated a dictionary-lookup baseline system that used the chemical entity list derived from the training and development set to tag the test set abstracts. For the CDI task, this strategy obtained a micro-averaged F-score of 53.85% (with a precision of 53.71% and recall of 54.00%), while in case of the CEM task it reached a micro averaged F-score of 57.11 (precision of 57.22%, recall of 57.00%). These scores indicate that there are some frequently mentioned chemical compounds in PubMed that can be exploited for labelling text, but also that many of them are ambiguous and just using dictionary look-up is not enough to capture the novel compound mentions. The upper boundary of named entity recognition performance is commonly measured by comparing independent annotations carried manually by human curators. The resulting value, called inter-annotator agreement (IAA) or inter-coder agreement score is useful to assess how well the task is defined, how consistent the annotations are and it helps to quantity the difficulty of the annotation process. The simplest IAA score is based on the percentage agreement of manual annotations between two different annotators. In case of the CHEMDNER corpus the percentage agreement between curators for defining chemical entity mentions was of 91% (exact matches) based on manual annotations of 100 abstracts. Additional details on the chemical annotation consistency analysis and IAA are provided in the CHEMDNER corpus paper [ ]. \n  \nCHEMDNER team overview. \n  \nIn the CDI and CEM columns the number or runs and the rank group of the best submission are shown. The asterisks indicate teams with articles in this special issue. Id: team identifier, A: academic, C: commercial \n  \nTo examine the statistical significance of each prediction with respect to other submissions, we carried out a Bootstrap resampling simulation in a similar way to what was previously done for the gene mention task of Biocreative II [ ]. This statistical analysis was done for both the CDI and CEM tasks by taking 1,000 bootstrapped samples from all 2,478 articles in the test set that had annotations. We then calculated the micro-averaged F-scores for each team on each sample. These 1,000 resampled results were then used to calculate the standard deviation of each teams F-score (SDs). The evaluation tables of the CDI and CEM tasks illustrate the range of other teams (rows) that are covered within two standard deviations of a teams own F-score (Range). We used this range to group teams that have no statistically significant difference (at two SD) between results (Group). \n\nWe received 91 runs from the 23 teams that participated in the CDI task. The evaluation of the best performing test set predictions from each team against the manual Gold Standard annotations are shown in table   (for the complete list of results for all CDI runs refer to additional materials table   i.e. Additional file  ). The table   shows the micro-averaged precision (P), recall (R), and balanced F-score (F1) for the highest-scoring runs/team. The top-ranking F-score obtained for the CDI task was of 88.20% by team 231 (run 3) very closely followed by run 3 from team 184 (F-score of 88.15%). There was no significant difference between these two top scoring runs. Eight teams had predictions with CDI prediction F-scores above 80%. When looking at the precision and recall performance separately, the highest precision obtained by a participant was 98.66% (with a modest recall of 16,65%) while the highest recall was of 92.24% (with a precision of 76.29%). In general the precision scores of the team submissions were slightly better than the corresponding recall. \n  \nCDI evaluation results (best run per team only). \n  \n P  : precision;   R  : recall,   F  : F-score,   SD  : standard deviation in the bootstrap samples. \n  \nThe best result of the CEM task was marginally below the top result of the CDI task. The best scoring prediction (by team 173, run 2) obtained an F-score of 87.39, closely followed by team 231 with 87.11% (see table  ). These scores can be considered already competitive results taking into account the underlying IAA of 91% of the annotations. It is also important to keep in mind that this was the first time that such a task was carried out and that teams had a rather short timeframe to build/train their tools, indicating that these initial results could be further improved. Nine teams obtained an F-score above 80% and the highest precision of a submission was of 98.05 (recall 17.90). The top recall of systems for the CEM task was of 92.11 (with a precision 76.72). The complete list of evaluated CEM results can be seen in the additional materials table   (Additional file  ). Overall precision scores, when compared to their corresponding recall values were considerable better in case of the CEM task (even more than in case of the CDI task). This might indicate that there is still some room for improving the overall recall of participating systems. To better understand issues related to the chemical entity recognition recall we examined the subset of entities that were only present in the test set and did not have any previous mention neither in the training nor development set (novel chemical mentions). The highest recall of such novel chemical mentions was of 83.49% (team 173, run 3), more than eight percent lower than the recall on the entire test set mentions. \n  \nCEM evaluation results (best run per team only). \n  \n P  : precision;   R  : recall,   F  : F-score,   SD  : standard deviation in the bootstrap samples. \n  \nAdditionally, we also did a more granular evaluation of the recall for each of the individual chemical entity mention classes. The results for each class and the novel mentions are shown in additional materials table   (Additional file  ). The best results were obtained for the   systematic   class, corresponding to the names that follow the chemical nomenclature standards (IUPAC or IUPAC-like chemical names), with a top recall of 95.89%. Although correctly identifying mention boundaries of systematic names can be difficult, such kind of mentions do also show very strong word morphology and character n-grams characteristics distinct from other surrounding words. The second best recall result was obtained for trivial names, where one team reached a 94.25%. Trivial chemical names are better covered by lexical resources containing extensive lists of generic drug names and also drug brand names. They also do show some particularities that can be detected by machine learning methods like the usage of typical stems and affixes which denote characteristics of drugs (e.g. mode of action or the class a drug belongs to, e.g. -vir for antiviral drugs or -tinib for tyrosine kinase inhibitors). The best recall for other types of chemical entity classes was slightly worse, being 91.38% in case of chemical abbreviations and 90.06% for both identifiers and chemical families, followed by chemical formula with a recall of 89.37%. These types of chemical entities do often have a higher degree of ambiguity, especially some acronyms and short formula. The most problematic class was the chemical class   multiple  . Where the highest recall was of only 60.30%. In case of the CHEMDNER corpus those mentions account for less than one percent of the total (0.78%) number of chemical mentions. To determine the difficulty of each chemical mention class we examined how many runs correctly identified each of the Gold Standard chemical mentions and then looked at what chemical class it belonged to. Only 108 of the 25,351 test set chemical mentions were not detected by any of the teams, implying that over 99.99% of the mentions could be retrieved by at least one team. The Additional file   contains a figure that shows a boxplot with the number of runs that correctly identified each of the chemical entity mentions for each of the CEM classes. This figure indicates that trivial mentions on the whole, were the easiest ones for the participants followed by systematic chemical mentions. From the other CEM classes, abbreviations and formula are two types of mentions that do account for an important number of mentions in the test set and for which it is clear that overall a number of systems would require a better recognition strategy. One common characteristic of all participating teams was that they all used the provided CHEMDNER corpus either to train their system or to adapt and fine-tune previously implemented software. Only five teams also utilised other external corpora. These teams obtained on average a slightly worse F-score of 72.74% compared to the teams that only used the CHEMDNER corpus (F-score of 77.44%). Most of the participants (22 teams) used the official evaluation library to validate and improve their systems during the training and development phase. Those teams on average also obtained superior results (F-score of 77.36%) when compared to teams that did not rely on the BioCreative evaluation script (F-score of 71.99%). \n\n\n## Discussion \n  \nThe CHEMDNER task of BioCreative IV showed that the automatic recognition of chemical entities from PubMed abstracts is a feasible task by automated named entity recognition systems. The only mention class that still requires clearly a better detection performance is the class   multiple  , where individual entities do not correspond to a non-continuous string of text. A more fine-grained annotation of this particular class of mention together with the annotation of the actual dependencies of the various text strings that do correspond to a chemical could help to improve their detection. Another problematic case is short chemical formulas (e.g.: I, O, P, H), as some are highly ambiguous and do correspond in most of the cases to nonchemicals. Despite the good results for trivial and systematic mentions of chemicals, examining some of the frequent false negative cases not detected by many of the participating teams showed that there were also some common difficulties. Teams had problems in finding trivial names corresponding to dyes. Trivial names that showed unusual word morphology with embedded brackets were hard in terms of the correct mention boundary recognition. The only obvious issue with systematic names was encountered for very long names, those were challenging in terms of the correct mention boundary detection. Also some of the systems did apply a length cut-off when detecting chemicals, especially those that relied chemical dictionary lookup as the recognition approach. We carried out a survey on participating teams to better summarize the most relevant aspects of participating techniques. In this survey we covered aspects such as the used methodologies, exploited resources and software as well as the underlying features for the detection of chemical mentions. Most of the teams used some sort of lexical resources (lists of chemical names) derived from various databases or terminologies. In particular, ChEBI, PubChem and DrugBank were the most commonly used lexical resources. Some of the top scoring teams did additionally also some automated expansion of these original lists of chemicals. Additional file   provides a compendium of the main resources explored by the CHEMDNER participants. \n\nThe majority of teams did also explore existing chemical entity recognition software, mostly Oscar4 and ChemSpot. The output of external chemical entity taggers was frequently used as one more feature by participating systems. Some participants used only the text tokenization modules provided by these NER systems (for instance from Oscar4) as an alternative to more generic tokenization modules. Only few teams did use existing biomedical NLP/text mining software. For instance the top ranking team of the CEM task did integrate the AB3P (Abbreviation Plus P-Precision) tool for recognizing potential abbreviations of chemical names. As can be seen in Additional file  , teams also adapted a range of packages that implement machine learning algorithms (e.g. Mallet or CRF++) or general natural language processing software (e.g. OpenNLP) to build their systems. The participants used three general strategies to identify chemical entity mentions: (1) supervised machine learning approaches, (2) rule/knowledge-based approaches and (3) chemical dictionary look-up approaches. Most of the systems were hybrid systems using machine learning techniques based on conditional random fields (CRFs) that exploited also chemical dictionaries as one of the used lexical features. Figure   provides a summary of the participating systems based on the responses provided in the CHEMDNER survey. Analyzing the used techniques, indicated that CRFs were the method of choice for most teams, and that this machine learning technique can be considered as an efficient strategy for chemical NER. Only few teams explored the used of other machine learning techniques, mainly SVMs, which were used additionally to CRFs by six teams. It's worth noting that two systems that used mainly rule-based methods (together with some chemical gazetteers lookup) could also obtain competitive results, ranking third (team 179) and ninth (team 185) in the CEM task. \n  \n Overview of the methods used by participating teams  . \n  \nBuilding of these rule-based systems required a deep understanding of both the existing chemical nomenclature standards as well as of the CHEMDNER annotation guidelines. Surprisingly, two systems relied essentially on the use of lexical resources for chemical names (team 199 and team 222), exploiting a considerable number of different databases and terminologies they could obtain satisfactory results (rank 11 and 12 in the CEM task). The use of dictionary-lookup based approaches required efficient dictionary pruning and post-processing as well as strategies to do expansion of the original lists of chemicals. The top scoring team of the CEM task was a hybrid strategy that integrated all three general methods, that is a machine learning based approach based on various CRF models, patterns to find special types of mentions such as chemical formula and sequences of amino acids as well as chemical gazetteers. It also integrated an abbreviation detection method. Although this system did not explore more sophisticated chemical nomenclature rules and also made use of a limited chemical dictionary, the integration of those two additional methods served to improve the overall performance of this system when compared to other participants. We would expect that by combining CRFs-based models with sophisticated rule based approaches such as the one used by team 179 and extensive lexical resources such as explored by teams 199 and 222 would further improve chemical entity detection results. Analyzing the top-performing machine learning based systems also indicated that they did carefully examine various text tokenization modules and that the use of chemical domain-specific tokenization could slightly improve their results. They also used nomenclature rules as features and carried out extensive automatic post-processing of the results (e.g. checking if brackets are balanced within the chemical name). A more detailed examination of the various featured used by participating systems can be seen in Figure  . \n  \n Overview of used features by participating teams  . \n  \nTeams could submit up to five runs for each of the tasks. Examining at a general level, the differences between the returned runs showed that they corresponded to either different (or different combinations) of the used CRF models or were runs optimized for recall, precision or F-score. \n\nWe requested the submission of a confidence score and ranking of the predicted mentions/chemical entities, but we did not carry out a proper evaluation of the ranking this time, mainly because the used documents (abstracts) were too short to do a meaningful analysis. The rankings fro the CDI task were generated by participants using various alternatives: (1) simply counting the number of occurrences of mentions, (2) using manual rules to define the ranking based on the chemical entity class, (3) ranking the mention by examining if they were present in some chemical lexicon (e.g. ChEBI), (4) checking if the extracted mention was present in the training/development set, (5) use of confidence scores or marginal probabilities provided by the machine learning models. \n\nAlmost all systems relied on essentially the same pipeline for both the CEM and CDI task, returning the results of the CEM task (after filtering duplicate names and doing an entity ranking) as prediction for the CDI task. Therefore we thus think that doing a CDI task again would only be meaningful if the chemical entities have to be normalized to chemical structures or databases and if larger documents (full text articles of patents) are used. \n\n\n## Conclusion \n  \nThe CHEMDNER task was the first attempt to systematically examine independently the performance of chemical entity recognition methods. It could attract a considerable number of participants from academia and industry and resulted in a range of new applications for the recognition of chemical entity mentions. The best performing teams could reach a performance close to what could be expected by chemical database curators when manually labeling the text. Although 18 teams worked previously on this or a related topic, the CHEMDNER task could attract new research groups interested in the recognition of chemicals in text. Most teams (19) considered that, given a training data such as the CHEMDNER corpus provided for this task, the recognition of chemical entities is a task with a medium degree of difficulty and they would be interested in participating in a similar task again in the future. The CHEMDNER task was able to determine the state-of the art of chemical entity recognition systems and also promoted the improvement in terms of performance when compared to previously published methods. We expect that the tools resulting from this challenge constitute a valuable building block for text mining and information extraction technologies linking chemicals to other entities of interest such as genes and proteins, or to extract other relevant associations of chemical compounds (e.g. physical binding or drug target interactions, chemical entity metabolism, therapeutic, adverse effect, reactions and reactants, etc). We foresee that the CHEMDNER task will promote research in chemical entity recognition in general, but also provide useful insights for better processing of other document collections such as noisy text (in particular patents) and full text articles. \n\n\n## Competing interests \n  \nThe authors declare that they have no competing interests. \n\n\n## Authors' contributions \n  \nMK was responsible for the task definition and coordinated the corpus annotation and result evaluation. FL and MV helped define the task, the annotations, and the evaluation of the results. OR and JO were responsible for refining the annotation guidelines and supervised the annotation quality and results. AV supervised the entire task setting. All authors revised the manuscript. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 4331685, "text_md5": "6685b41ca66cbf6cb06ba2df56abbb3d", "field_positions": {"authors": [0, 121], "journal": [122, 134], "publication_year": [136, 140], "title": [151, 210], "keywords": [224, 344], "abstract": [357, 2135], "body": [2144, 38364]}, "batch": 1, "pmid": 25810766, "doi": "10.1186/1758-2946-7-S1-S1", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331685", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4331685"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331685\">4331685</a>", "list_title": "PMC4331685  CHEMDNER: The drugs and chemical names extraction challenge"}
{"text": "Wu, Chengkun and Xiao, Xinyi and Yang, Canqun and Chen, JinXiang and Yi, Jiacai and Qiu, Yanlong\nBMC Bioinformatics, 2021\n\n# Title\n\nMining microbe\u2013disease interactions from literature via a transfer learning model\n\n# Keywords\n\nMicrobe\u2013disease interactions\nNamed-entity recognition\nRelation extraction\nTransfer learning\n\n\n# Abstract\n \n## Background \n  \nInteractions of microbes and diseases are of great importance for biomedical research. However, large-scale of microbe\u2013disease interactions are hidden in the biomedical literature. The structured databases for microbe\u2013disease interactions are in limited amounts. In this paper, we aim to construct a large-scale database for microbe\u2013disease interactions automatically. We attained this goal via applying text mining methods based on a deep learning model with a moderate curation cost. We also built a user-friendly web interface that allows researchers to navigate and query required information. \n\n\n## Results \n  \nFirstly, we manually constructed a golden-standard corpus and a sliver-standard corpus (SSC) for microbe\u2013disease interactions for curation. Moreover, we proposed a text mining framework for microbe\u2013disease interaction extraction based on a pretrained model BERE. We applied named entity recognition tools to detect microbe and disease mentions from the free biomedical texts. After that, we fine-tuned the pretrained model BERE to recognize relations between targeted entities, which was originally built for drug\u2013target interactions or drug\u2013drug interactions. The introduction of SSC for model fine-tuning greatly improved detection performance for microbe\u2013disease interactions, with an average reduction in error of approximately 10%. The MDIDB website offers data browsing, custom searching for specific diseases or microbes, and batch downloading. \n\n\n## Conclusions \n  \nEvaluation results demonstrate that our method outperform the baseline model (rule-based PKDE4J) with an average  -score of 73.81%. For further validation, we randomly sampled nearly 1000 predicted interactions by our model, and manually checked the correctness of each interaction, which gives a 73% accuracy. The MDIDB webiste is freely avaliable throuth  \n\n \n\n# Body\n \n## Background \n  \nMicrobiota in the human body is of great significance to human health. Pathogenic microorganisms are the chief culprit for many human diseases [ ], such as the SARS outbreak in 2003 [ ,  ] and avian influenza (HPAI) [ ] in the past few years, as well as inflammatory bowel disease (IBD) caused by enteric human virome [ ,  ]. Studies have even shown that there is a close connection between mental illness and gut microbes [ ,  ]. Through the detection of gut microbes in patients with chronic heart failure (CHF), it was found that compared with normal individuals, CHF patients had higher levels of gram-negative bacteria and   Candida   in the intestine, and an increase of intestinal permeability, which promoted the process of CHF [ ]. The gut flora can also impact arthritis (AR). The work in [ ] applied 16S rDNA sequencing to sequence the gut microbiota of patients and healthy individuals and found that the abundance of the gut microbiota reduced significantly in patients with AR. Therefore, it is essential to efficiently explore relations between microbes and diseases, which is currently not feasible because most information is buried in the vast amount of unstructured biomedical literature. \n\nThe first human microbial-disease association database (HMDAD) was built to provide experimental data for microbial disease association research. The database only contains 39 disease entities and 292 microbial species, and the relationship between the two entities is established at the document level [ ]. Most studies on the prediction of microbial disease associations are based on this database like KATZHMDA [ ], NCPHMDA [ ], MDLPHMDA [ ], RNMFMDA [ ]. However, due to the limited types of diseases and microorganisms included in this database, a large amount of information in biomedical texts has not been thoroughly mined. MicroPhenoDB is a recent work of the relationships between disease phenotype, pathogenic microbes, and core genes. It was built by a manual review process, and a calculation method, which collects the IDSA guideline data, the manual curate data resource, and traceable literature with different weights to calculate the score between microbes and diseases [ ]. Most studies on the relationship between microorganisms and diseases need many human resources. Park et al.\u00a0[ ] proposed an ensemble parser model based on a hierarchical long short-term memory network. It firstly decided whether the two targeted entities interacted with each other, and then caught the relation trigger word. PubMed is a free database for biomedical and life sciences literature, with over 70 million abstracts and more than 7 million full-text articles. By March 2021, 64,510 records were retrieved from PubMed and 64,259 full-text records were retrieved from PMC by the \u2019microbe\u2019 query. As illustrated in Fig.\u00a0 , the amount of microbe-related literature is increasing rapidly in the recent 20\u00a0years, making it difficult for microbe researchers to identify, retrieve and assimilate all relevant publications. Hence, automated text mining is an essential tool to discover the valuable information hidden in this enormous amount of literature.   \nNumber of PubMed articles returned by the \u2018microbe\u2019 query \n  \n\nBiomedical named entity recognition(BNER) is a fundamental task for understanding biomedical literature, mainly presented as non-structural texts injected with many specialized terms. A number of successful NER tools have been developed for diseases [ ], genes/proteins [ ,  ] , species [ ], chemicals [ ], etc. In this work, we use DNorm [ ] to recognize disease entities, which is a machine learning based toolkit for disease NER and normalization. For microbes, there is no such tool available, and we have to build our own method. Biomedical relation extraction(BioRE) aims to capture relations between two entities from NER results automatically. The entity-relationship facilitates the acquisition of domain knowledge by researchers in the biomedical field, enables the automated processing of biomedical information, and promotes research tools in the biomedical field and the development of information in the medical field. Previous studies and datasets on BioRE already discussed about protein\u2013protein interactions (PPIs) [ ], drug\u2013drug interactions (DDI), drug\u2013target interactions (DTIs), etc. Still, the classification of the relation between microbe and disease has no clear definition. \n\nMachine learning and deep learning methods rely heavily on manually labeled data sets, and human annotation is costly and time-consuming. Transfer learning has been successfully utilized in many natural language processing fields such as text classification [ ], named entity recognition [ ]. It extracts knowledge from one or more source domains and applies it to the target domain. Giorgi and Bader\u00a0[ ] applied this idea on biomedical named entity recognition, a deep neural network(DNN) was trained on large silver-standard corpora with noise and then transferred to small gold-standard corpora. It indeed showed a significant improvement on 23 gold-standard corpora covering chemicals, disease, species, and genes/proteins. Inspired by the work of transfer learning for biomedical named entity recognition [ ], we introduced transfer learning into extracting microbe\u2013disease interactions from the biomedical literature. \n\nOur main contributions can be summarized as follows: (1) we utilized NER tools to locate microbe and disease entities from an extensive collection of related literature; (2) we manually created two microbe\u2013disease interaction corpus for the following training process, including a gold-standard and a silver-standard; (3) we applied transfer Learning to perform microbe\u2013disease relation extraction without the need for a large-scale curation; (4) we developed a user-friendly website to help biomedical researchers find valuable information about diseases and microbes. \n\n\n## Methods \n  \n### Data preparation \n  \nLiterature data used in this work was collected from PMC (MELINE abstracts) and PubMed (full-texts), by searching the keyword \u201cmicrobe\u201d, a list of PubMed IDs can be got (accessed on March 2021). We used Aspera ( ) as a tool to download the PubMed database on NCBI, then retrieved abstracts according to listed PubMed IDs. If the corresponding full-text is available in PMC, we then use Eutils, a tool provided by PMC, to obtain the XML file of the full-text. A total collection of 24,256 articles was built as our data sources. To locate microbe mentions in texts, we built a specialized dictionary of microbe names collected from Human Microbe\u2013Disease Association Database [ ] (HMDAD,  ), Virtual Metabolic Human [ ] (VMH,  ) and Disbiome [ ] ( ). The final microbe dictionary in included 3,775 microbes. Next, we retrieved the taxonomy id of each microbe name to prepare for the BioNER procedure. Figure\u00a0  shows the whole workflow of data preparation. \n\n#### Named entities recognition (NER) and relation extraction (RE) \n  \nIn this study, we considered the microbe\u2013disease relation at the sentence level. The sentence splitting is carried out with a Python natural language toolkit, called NLTK. The 24,256 articles were separated into sentences via NLTK. \n\nThere is no readily available NER tool for microbes. LINNAEUS is a dictionary-based species name identification system for biomedical literature, performs with 94% recall and 97% precision at the mention level [ ]. Using LINNAEUS and the microbe dictionary, we can track the microbial entities in the texts with the information of each entity\u2019s start and end position, which will be used as input data in the RE step (shown in Fig.\u00a0 b). DNorm is a well-established disease name normalization model with a 0.782 micro-averaged F-measure and 0.809 macro-averaged F-measure performance. Normalized disease mentions are identified with their MeSH ids. An example of DNorm result is shown in Fig.\u00a0 c. \n\nA successful RE requires at least one microbe mention and one disease mention in the input sentence. The sentence instance will be in the format like Fig.\u00a0 d, which is the input format of PKDE4J. \n\nOnce the sentences are correctly formatted, we removed those instances with more than 64 words as many longer sentences can lead to detection errors. We use a highly flexible and extensible relation extraction tool, PKDE4J, as the baseline method. It applies dependency tree-based rules to extract relationships among entities in sentences with two or more entities [ ]. PKDE4J is based on dependency parsing technologies, which define rules to find the syntactic and grammatical structures and trigger words from sentences. Figure\u00a0 e shows the output format of PKDE4J. We got 96,670 instances after the relation extraction of PKDE4J. We also used PKDE4J to generate the silver-standard corpus (SSC) (shown in Fig.\u00a0 f).   \nThe workflow of data preparation.   a   The initial full-text data.   b   The result of applying LINNAEUS to recognize microbe entities.   c   The result of applying DNorm to recognize disease entities.   d   The result of splitting texts into sentences and aligning the position of disease and microbe.   e   The result of relation extraction using PKDE4J.   f   The result of classification instance by the result of PKDE4J \n  \n\n\n\n### Data curation \n  \n#### Human annotated gold-standard corpus (GSC) \n  \nTo better evaluate the performance of our method, we curated gold-standard corpus with hand-labeled annotations. We employed PubTator Central (PTC,  ), a web-based system for automatic annotations of biomedical concepts in PubMed abstracts and PMC full-text texts, to help annotators mark entities with their MeSH ids and Taxonomy ids. Microbe\u2013disease relation types are defined as follows:   \n positive   This type is used to annotate microbe\u2013disease entity pairs with a positive correlation, such as microbe will cause or aggravate disease, microbe will increase when disease occurs. \n  \n negative   This type is used to annotate microbe disease entity pairs that have a negative correlation, such as microbe can be a treatment for a disease, or microbe will decrease when disease occurs. \n  \n relate   This type is used when a microbe disease entity pair appears in the instance and described they are related with each other without additional information \n  \n NA   This type is used when a microbe disease entity pair appears in the instance, but the relation of these two entities has not been described as positive, negative, or relate. For example, \u201cA diet of hydrolyzed protein increases can lead to growth inhibition of   Escherichia coli   and   Clostridium perfringens   in rats suffering from chronic enteropathy.\u201d (pmid: 32478040), the sentence described the relation between the protein and two microbes and has no description of the relation between   Clostridium perfringens   and chronic enteropathy, so we tag this instance with the \u201cNA\u201d type. \n  In terms of a comprehensive data set about micro-disease interaction, types \u201cpositive\u201d, \u201cnegative\u201d, \u201crelate\u201d and \u201cNA\u201d form a complete set of relations. Every instance is assigned with one unique relation type. We randomly extracted 1200 instances for annotation. Annotators search the pmid in PTC and then query the disease id and the microbe id in NCBI and Taxonomy separately to check whether the result of NER is correct. We removed the instance if the instance has no tag or has a wrong tag in PTC, which is 75 instances in 1200 total instances. Then the instances were classified into the above four relations we defined. Finally, we got a set of 1100 manually annotated instances, and we use it as the gold-standard corpus for transfer learning and performance evaluation. \n\n\n#### Silver-standard corpus (SSC) \n  \nThe cost of enlarging the size of GSC is very high as each sample needs to be carefully reviewed. Due to the high cost, the size of the GSC is very limited. To provide more training samples, we built a silver-standard corpus with automated tools rather than human annotation. This means SSC might contain many incorrect annotations (noise). \n\nTo do this, we applied PKDE4J on over 20,000 articles related to \u2018microbe\u2019. The results of PKDE4J include information on the relation between microbe and disease, \u2018RelationMentionType\u2019 and \u2018Trigger words\u2019 (shows in Fig.\u00a0 e), which can be used as auxiliary information for relation type annotation. For example, if one instance is tagged with the RelationMentionType \u2018increased\u2019, we assign the instance with a relation type \u2018positive\u2019. Results with RelationMentionType \u2018JUXTAPOSE\u2019 were removed. The \u2018Trigger word\u2019 tag was also utilized to define the relation type. We established a trigger word dictionary and used regular expressions to classify the instance. The trigger words with too few occurrences (less than five times) were not considered in the SSC. \n\nAt last, each instance will be classified in one relation type in positive, negative, relate, NA. Instances appeared in the GSC were removed from SSC. The resulting SSC dataset contains 12,959 samples, and it is used as a major training data source for the transfer learning procedure. Figure\u00a0  shows the distribution of relation types for both GSC and SSC.   \nThe distribution of relation types for GSC and SSC \n  \n\n\n\n### Transfer learning with BERE \n  \nMost machine learning application scenarios require a lot of labeled data for supervised learning. However, annotating data is a tedious and costly task. We address this problem via transfer learning. BERE is a deep learning framework to extract drug-related relations from literature automatically. This model uses latent tree learning and self-attention techniques to capture the syntactic information of the sentence. The input sentences firstly translate into the vector representations of words. Pre-trained word embedding is from  . Each word in sentence will be represented in a concatenation of a 200 dimensions word embedding and a randomly initialized 50 dimensions POS embedding. Then Bi-GRU and self-attention mechanisms are applied to encode short and long-range dependencies between words. Gumbel Tree-GRU can implicitly learn the syntactic features of sentences. And it embeds the contextual elements of two entities into the sentence representation. Lastly, a classifier will predict the relation between two entities. It shows great performance on the relation between drug\u2013drug interaction, and the authors applied the model on a distantly supervised drug\u2013target interaction dataset. A detailed description of BERE\u2019s architecture is explained in [ ]. \n\nIn the study of BERE, they use the DDI\u201913 dataset to demonstrate the performance of their model, and it turns out that the BERE model is better than six other baseline methods on the DDI\u201913 dataset. They then construct a distantly supervised Drug\u2013Target interaction (DTI) dataset, which inspired us to use BERE to build a disease\u2013microbe interaction dataset. In this work, we used the INS mode of BERE, which predicts each sentence instance into an individual class. \n\n#### Training and evaluation metrics \n  \nTo better verify the effectiveness of BERE on the MDI dataset with transfer learning, we compared the performance of BERE_TL and BERE_g. The SSC datasets were split into three disjoint subsets, 12,000 samples for training the model, and 1000 of those data as the validation set. The rest of the samples were used as a test set for the final evaluation. This split operation on SSC was applied twice to take the average result to reduce the prediction bias. We randomly separated the GSC as 800 for the train set, 100 for the valid set, and 200 for the test set. \n\nTo better demonstrate the role of transfer learning, we conducted fivefold cross-validation of the BERE_TL and BERE_g on GSC. We randomly split the GSC dataset into train set, validation set, and test set five times. Table   shows the result of the validation. We averaged the results of the five experiments. The typical evaluation indicators Precision, Recall, and  -score were used as evaluation metrics. The precision rate calculates the correct classified samples in all model samples, and the recall rate calculates the proportion of correct predicted correct positive samples.   is a measure of precision and recall. We also compute the average percent reduction in  -score as the same as [ ]:   \nThe procedures of mining information from literature \n  \n\n\n#### Web implementation \n  \nThe website of MDIDB is implemented in the framework of Django, with AJAX loading dynamic data from a database based on MySql. The visual front-end page is built on the basis of Bootstrap 4, and the chart is based on the visual plug-in echart. The website provided data access and operations in a user-friendly way. Users can browse the whole relevant microbe and disease list and the relevant statistical chart information of the corresponding word cloud chart and pie chart by clicking the related term. Simultaneously, the website provides a search function for users to retrieve the information they are interested in. The relevant result data set of the paper can also be obtained from the download page. \n\nThe whole system is based on NLP algorithms for text mining of massive biological literature. Figure\u00a0  shows the workflow of the entire text-mining system. After a series of post-processing, text mining results are stored in the database and operated by the backend server. Finally, we got a visual website containing 1198 diseases, 165 microorganisms, and 44,900 records of their relationship data. \n\n\n\n\n## Results \n  \nTo prove that the BERE model can lay a solid foundation for detecting microbe\u2013disease relations, we compared the performance of BERE on several datasets with the rule-based baseline PKDE4J(MDI). Table\u00a0  compares the micro-averaged performance metrics of each dataset. The learning rate was set to 0.0001, the dropout rate to 0.5. BERE_g(MDI) is generated by fine-tuning the original BERE model only on the GSC training set. Results of BERE(DDI) and BERE(DTI) come from the origin BERE paper. \n\nAs of yet, it is not clear whether the introduction of transfer learning on BERE can improve the performance of MDI detection. Thus we evaluated the performance on the MDI dataset with two modes: BERE_TL(MDI) introduces transfer learning on the GSC training set while BERE_g(MDI) directly applied the original BERE model. \n\nAs Table\u00a0  shows, we can see that compared with PKDE4J(MDI), BERE_g(MDI) achieves a higher score of precision, recall, and  -score on the same MDI dataset. Moreover, BERE_g(MDI) achieves a comparable performance with BERE(DDI) and BERE(DTI).   \nComparison of baseline performance on different datasets \n  \n\n### Quantifying the performance of transfer learning \n  \nTo highlight the effect of transfer learning, we compared the performances with or without transfer learning. The experiment was performed under five-fold cross-Validation, and the final result was computed by average. Table\u00a0  lists the results for the BERE_g(MDI) against BERE_TL(MDI). It is evident that transfer learning significantly improved precision, recall, and  -score. In addition, it brings an average reduction in error of 12% on GSC. Figure\u00a0  shows the precision\u2013recall curve of and the AUPRC result of BERE with transfer learning.   \nComparisons of the precision\u2013recall curves between BERE with or without transfer learning. The AUPRC and  -score for each method are on the top right contains \n    \nResults of fivefold cross-validation \n  \nThe best result of each performance index is boldfaced \n  \n\n\n### Error analysis \n  \nWe manually inspected some reported results of our model and we have the following observations: \n\nFirstly, sentences with too many compound clauses may give rise errors. To improve this, we will need better NLP tools for semantic parsing or syntactic analysis of texts. \n\nSecondly, some errors can be attributed to the NER tools. DNorm occasionally failed in cases of abbreviations and acronyms. For instance, \u2018WS\u2019 refers to wheat sensitivity in the article, but DNorm tagged it as an abbreviation of the disease \u2018Williams Syndrome\u2019. Pathologically related words can bring some misunderstanding too, \u2018syntrophic growth\u2019 was wrongly recognized as the disease \u2018Growth Disorders\u2019. To reduce such errors, we will need better NER tools. \n\nIn addition, some texts might not even constitute a proper sentence. We noticed one example \u201cGastric cancer H. pylori, Porphyromonas, Neisseria, Prevotella pallens, Streptococcus sinensis, Lactobacillus coleohominis.\u201d (PMID: 31236389), which was due to an improper representation of a table into text segments in the corresponding full-text XML document. \n\nWe selected 1000 predicted instances from the results of our model randomly and checked each instance manually. 731 out of 1000 were verified to be correct, and 268 were proved to be wrongly predicted, which gives an accuracy of 73.1%. 914 instances were not found in the aforementioned database MicroPheno, but our manual inspection found that 633 (69.2%) of them are correct and should be included. \n\nTo note, the recall of our method is around 74%, which means some useful information in literature might not be recovered. For instance, we know Bacillus cereus is a gram-positive bacteria that can produce toxin and causes diarrhea and we find some evidence by literature review [ \u2013 ]. However, this information was not included in our database. The reason is that our model only considers relation extraction at the sentence level. In some cases, useful information can only be mined across multiple sentences. We will leave that for future work.   \nQuery results in MDIDB \n    \nExample searching result of disease \u201cColonic Neoplasms\u201d and microbe \u201cBacillus cereus\u201d in MDIDB \n  \n\n\n### Searching on MDIDB website \n  \nThis section gives examples on how to access MDIDB and retrieve useful information from our database. \n\nTo demonstrate how to get related microbes by searching for disease names, we queried \u201cColonic Neoplasms\u201d, as illustrated in Fig.\u00a0 a. We obtained a list of microbe\u2013disease relation records about colonial neoplasms, and each record has one evidence to support the classification of entity relation. The statistical chart result is shown in Fig.\u00a0 a, b. \n\nWe can also search by microbe names. By searching microbe \u201cBacillus cereus\u201d (Fig.\u00a0 b), we got a list of related diseases, which includes Meningitis [ ], Diabetes Mellitus [ ], Dysentery, Endotoxemia [ ], shown in Fig.\u00a0 c, d. \n\nMDIDB can generate top-ten pie charts for different queries and present an informative word cloud for the most relevant microbes or diseases. For instance, the study [ ] shows probiotics Lactobacilli can bring less abdominal discomfort for patients with colon cancer. Keku et al.\u00a0[ ] discussed the relations between Fusobacterium species and colon cancer. Parisa et al.\u00a0[ ] had \u2019protective\u2019 anti-cancer properties for colon cancer. Fusobacterium nucleatum is a gram-negative obligate anaerobic bacteria and can activate Wnt/beta-catenin signaling to accelerating proliferation of colon cancer cells [ ,  ]. The relation of Clostridium and colon cancer was demonstrated in work [ ], Clostridium is associated with progression of colonic cancer [ ]. Moreover, Vacca et al.\u00a0[ ] proves that Lachnospiraceae is linked to colon cancer, Cueva et al.\u00a0[ ] found that Prevotella is associated with colon cancer, Bolourian and Mojtahedi\u00a0[ ] suggested that Streptomyces can suppress colon tumorigenesis, Boleij et al.\u00a0[ ] shows that some Streptococcus species are associated with colon cancer. Colorectal Neoplasms and Colonic Neoplasms have a similar statistic chart, and as we know, colon cancer and colorectal cancer are equivalent in some literature. \n\n\n\n## Discussion \n  \nExtracting structured knowledge from a large number of scientific literature can assist researchers retrieve interested information quickly. In this part, we compare and discuss several existing microbial disease databases and their extraction methods. Table   shows the difference between three databases in microbe and disease data. \n\nHMDAD ( ) [ ]: This is the first database of microbe and disease association. The data were collected by manual work, the scope of microbes, diseases, and even literature are limited. \n\nDisbiome ( ) [ ]: Didbiome provides a database of the association between the health situation of the host and the composition of its microbiota. It collects microbe\u2013disease associations by text mining from peer-reviewed publications. \n\nMicroPhenoDB ( ) [ ]: This database uses manual review and calculation methods to systematically integrate the associated data of pathogenic microorganisms, microbial core genes, and human disease phenotypes. The scoring model is optimized by assigning different weights to different research shreds of evidence to quantify the correlation between microorganisms and human diseases. \n\nThough MicroPhenoDB is rich in data, it takes a lot of time and effort to manually evaluate and audit the data. \n\nMDIDB includes a vast amount of text-mined information from a comprehensive collection of related literature. It also provides a structured way to present the classified relationship between microbial diseases and specific sentences in specific literature. 24,256 is the number of input articles that are processed by our methods, while 8458 is the number of articles with detected relations. \n\nOur system only contains 1065 microbial entities due to the lack of specification in the microbial dictionary. Besides, many abbreviated microorganisms can not be recognized in the NER stage, such as B. fragilis. For the current version, we only consider the microbe disease relationship at sentence level. In the future, we will add relation extraction across sentences.   \nDatabase contents of MDIDB compared with other databases \n  \nThe best result of each performance index is boldfaced \n  \n\n\n## Conclusion \n  \nInteractions of microbes and diseases are of great importance in the biomedical domain. Much valuable information is buried in the large-scale biomedical literature, which has not yet been effectively explored. In this work, we applied text mining to automatically detect the interaction between microbes and diseases from literature via a transfer learning framework. We manually annotated a gold-standard corpus. Then we utilized a state-of-art automated biomedical relation extraction model and fine-tuned it on the GSC. The introduction of an automatically generated corpus SSC greatly enlarged the number of training samples and led to satisfactory performance of 73.85%  -score. We conducted five-fold experiments to verify the effectiveness of our transfer learning method, and it provides approximately 10% reduction in error of   score. A total number of 44,900 interactions were extracted from over 20,000 articles. We randomly sampled 1000 results to analyze the accuracy of the predicted data, and 731 of 1000 were confirmed correct manually. \n\nExtraction results were utilized to construct a microbe\u2013disease interaction database with a web interface, which is freely available at  . Our framework allows large-scale analysis of microbe\u2013disease interactions with evidence of complex sentences. \n\n \n", "metadata": {"pmcid": 8430297, "text_md5": "11e490a487a8803ad1ce92d09a960140", "field_positions": {"authors": [0, 96], "journal": [97, 115], "publication_year": [117, 121], "title": [132, 213], "keywords": [227, 319], "abstract": [332, 2203], "body": [2212, 29459]}, "batch": 1, "pmid": 34507528, "doi": "10.1186/s12859-021-04346-7", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8430297", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8430297"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8430297\">8430297</a>", "list_title": "PMC8430297  Mining microbe\u2013disease interactions from literature via a transfer learning model"}
{"text": "Chowdhury, Shanta and Dong, Xishuang and Qian, Lijun and Li, Xiangfang and Guan, Yi and Yang, Jinfeng and Yu, Qiubin\nBMC Bioinformatics, 2018\n\n# Title\n\nA multitask bi-directional RNN model for named entity recognition on Chinese electronic medical records\n\n# Keywords\n\nRecurrent neural network\nMultitask learning\nWord embedding\nParts-of-speech tagging\nNamed entity recognition\nElectronic medical records\n\n\n# Abstract\n \n## Background \n  \nElectronic Medical Record (EMR) comprises patients\u2019 medical information gathered by medical stuff for providing better health care. Named Entity Recognition (NER) is a sub-field of information extraction aimed at identifying specific entity terms such as disease, test, symptom, genes etc. NER can be a relief for healthcare providers and medical specialists to extract useful information automatically and avoid unnecessary and unrelated information in EMR. However, limited resources of available EMR pose a great challenge for mining entity terms. Therefore, a multitask bi-directional RNN model is proposed here as a potential solution of data augmentation to enhance NER performance with limited data. \n\n\n## Methods \n  \nA multitask bi-directional RNN model is proposed for extracting entity terms from Chinese EMR. The proposed model can be divided into a shared layer and a task specific layer. Firstly, vector representation of each word is obtained as a concatenation of word embedding and character embedding. Then Bi-directional RNN is used to extract context information from sentence. After that, all these layers are shared by two different task layers, namely the parts-of-speech tagging task layer and the named entity recognition task layer. These two tasks layers are trained alternatively so that the knowledge learned from named entity recognition task can be enhanced by the knowledge gained from parts-of-speech tagging task. \n\n\n## Results \n  \nThe performance of our proposed model has been evaluated in terms of micro average F-score, macro average F-score and accuracy. It is observed that the proposed model outperforms the baseline model in all cases. For instance, experimental results conducted on the discharge summaries show that the micro average F-score and the macro average F-score are improved by 2.41% point and 4.16% point, respectively, and the overall accuracy is improved by 5.66% point. \n\n\n## Conclusions \n  \nIn this paper, a novel multitask bi-directional RNN model is proposed for improving the performance of named entity recognition in EMR. Evaluation results using real datasets demonstrate the effectiveness of the proposed model. \n\n \n\n# Body\n \n## Background \n  \nElectronic Medical Record (EMR) [ ], a digital version of storing patients\u2019 medical history in textual format, has shaped our medical domain in such a promising way that can gather all information into a place for healthcare providers. It comprises both structured and unstructured data that consists of patients\u2019 health condition and information such as symptoms, medication, disease, progress notes, and discharge summaries. EMR facilitates medical specialists and providers to track digital information and monitor them for patients\u2019 regular check-up. It can also provide healthcare suggestions to patients even they live in a remote area. Moreover, when a patient switches to a new healthcare provider, the provider can easily obtain patients\u2019 medical history and current health condition by studying patient\u2019s EMR. Therefore, information extraction [ ] from EMR is one of the most important tasks in medical domain. The intent of information extraction system is to identify and connect the related information and organize them in such a way that can help people to draw conclusions from it, and by avoiding the unnecessary and unrelated information. \n\nTo extract information like entity recognition from EMR is labor intensive and time consuming. Although there are many developed models for extraction of entity terms from textual documents, adopting these models for the purpose of medical entity recognition from EMR has been demonstrated as a challenging task, because most of the EMRs are hastily written and incompatible to preprocess [ ]. Moreover, incomplete syntax, numerous abbreviation, units after numerical values make the recognition task even more complicated [ ]. Standard Natural Language Processing (NLP) tools cannot perform efficiently when they are applied on EMR, since the entity terms of standard NLP is not designed for medical domain. Therefore, it is necessary to develop effective method to perform entity recognition from EMR. \n\nIn recent years, various deep learning based methods have been developed for Named Entity Recognition (NER) [ ] from EMR. Convolutional Neural Network (CNN) model is used for NER by using data mining to enhance the performance [ ]. Zao et al. [ ] proposed multiple label CNN based disease NER architecture by capturing correlation between adjacent labels. Dong et al. [ ] developed multiclass classification based CNN for mining medical entity types from Chinese EMR. \n\nMost recently, Recurrent Neural Network (RNN) such as Long Short-Term Memory (LSTM) is taking prominent place in NER due to its ability of dependency building in neighboring words. A hybrid LSTM-CNN is proposed in [ ]. The authors used CNN to extract the features and fed them to LSTM model for recognizing entity types from CoNLL2003 dataset. Wang et al. [ ] studied bi-directional LSTM architecture and concluded that this model is very effective for predicting sequential data. Moreover, the performance of the model is not based on language dependency. Simon et al. [ ] and Vinayak et al. [ ] used bi-directional RNN model on their Swedish EMR and Hindi dataset, respectively. In each case, the model shows better performance comparing to the state-of-the-art model. Similarly, the approach of using bi-directional RNN with LSTM cell has proven to perform well in extracting named entity recognition task [ ]. \n\nIn general, large corpus dataset is required to train deep learning models. However, there are limited number of corpus in many existing datasets that hinders the development of NER. Moreover, building labeled Chinese EMR data faces many challenges [ ], and most organizations do not want to share their data publicly as the data contains private information of patients. In order to address this challenge, a multitask bi-directional RNN model is proposed in this work for extracting entity terms from Chinese EMR. It is motivated by the observation that the performance of multitask learning model is much better comparing to individual learning approach when there is limited corpus dataset [ ]. The framework of the proposed multitask bi-directional RNN model for NER is given in Fig.\u00a0 .\n   \nFramework of the proposed multitask bi-directional RNN model for NER \n  \n\n\n## Methods \n  \nIn this work, a multitask bi-directional RNN model is proposed for extracting entity terms from Chinese EMR. The proposed model can be divided into two parts: shared layer and task specific layer, see Fig.\u00a0 . Specifically, vector representation of each word is a concatenation of word embedding and character embedding in the proposed model, see Fig.\u00a0 . Bi-directional RNN is used to extract context information from sentence. Then all these layers are shared by two different task layers, namely the parts-of-speech tagging task layer and the named entity recognition task layer. These two tasks layers are trained alternatively so that the knowledge learned from named entity recognition task can be enhanced by the knowledge gained from parts-of-speech tagging task.\n   \nVector Representation as concatenation of word embeddings and character embeddings. Vector representation of each word is presented as concatenation of word embeddings and character embeddings. The flow of word embedding is highlighted by red shaded box and character embedding is highlighted by white shaded box \n  \n\nRNN [ ] is an artificial neural network which can capture previous word information of a sequence in its memory. It computes each word of input sequence (  x  ,   x  , \u22ef,   x  ) and transforms it into a vector form (  y  ) by using the following equations: \n \n\n\n\nwhere   U  ,   U  ,   U   denote the weight matrices of input-hidden, hidden-hidden and hidden-output processes, respectively.   h   is the vector of hidden states that capture the information from current input   x   and the previous hidden state   h  . \n\nHere the bi-directional RNN is used to exploit both past and future context, where forward hidden states compute forward hidden sequence while backward hidden states compute backward hidden sequence. The output   y   is generated by integrating the two hidden states. In this work, we use a special form of bi-directional RNN, the bi-directional RNN with LSTM cell [ ]. LSTM is a special kind of RNN where hidden states are replaced by memory cells to capture long term dependent contextual phrase. The computation of LSTM is quite similar to RNN except for the hidden units, and it is given below: \n \n\n\n\n\n\n\n\n\n\nwhere   i  ,   g  ,   c  ,   o   and   \u03c3   are the input gate, forget gate, cell activation vector, output gate, and logistic sigmoid function of LSTM cell, respectively. These gates and activation functions soothe LSTM to avoid the limitation of vanishing gradients by storing long term dependencies terms of a sequence. \n\nThe shared layer contains two consecutive parts, illustrated by Figs.\u00a0  and  . In Fig.\u00a0 , each word is represented by a vector developed by Mikolov [ ]. The vector is built as a concatenation of word embeddings [ ] and character embeddings. Bi-directional RNN with LSTM cell is used to extract features at the character level and represent the features as character embeddings. Word embedding is achieved by word to vector representation. Character embeddings and word embeddings are then combined to represent each word in a vector representation. In Fig.\u00a0 , another bi-directional RNN with LSTM cell is used to extract context information from text sequence. Then the outputs (contextual word representations) are shared by two different bi-directional RNN with LSTM cell for two different tasks: parts-of-speech tagging and named entity recognition. These two task layers are trained alternatively so that knowledge from parts-of-tagging task can be used to improve the performance of named entity recognition task [ ]. The detailed settings of the proposed model is shown in Table\u00a0 .\n   \nContextual word representation from vector representation. To extract relevant context information from sentence, bi-directional RNN with LSTM cell is used to extract information from a vector associated with word embedding (red shaded box) and character embedding (white shaded box) to form contextual word representation (green shaded box) \n    \nThe proposed network architecture \n  \n\n\n## Results \n  \n### Dataset details \n  \nThe EMR dataset used in our experiment was collected from the departments of the Second Affiliated Hospital of Harbin Medical University, and the personal information of the patients have been discarded. An annotated/labeled corpus consisting of 500 discharge summaries and 492 progress notes has been manually created. The EMR data are written in Chinese with 55,485 sentences. The annotation was made by two Chinese physicians (A1 and A2) independently [ ,  ]. It is categorized into five entity types: disease, symptom, treatment, test, and disease group. An annotation example is shown in Fig.\u00a0 . The character n-grams are conducted by word segmentation and named entity recognition on Chinese sentences. In the domain of natural language processing (NLP) on Chinese, the first step is to segment the sentence into words containing n-gram characters since for Chinese the minimum semantic units are words, not individual characters. It can be accomplished by NLP tools like Stanford Word Segmenter [ ,  ]. Then for recognizing medical concepts from EMR, we define the named entity classes and use different labels to indicate these classes. For example, B/I/O labels denote the beginning word, inside word, and outside word of the named entities. Moreover, for named entity recognition on EMR, we attach the medical information to these three labels in order to denote different categories of named entities. For example, B_disease and B_treatment are denoting beginning words of disease and treatment named entities, respectively. The descriptions of entity types are given in Table\u00a0 .\n   \nTagging results on Chinese EMR [ ] \n    \nName of the entity types and their descriptions \n  \n\nThe categorized entity types are labeled in BIO format: B, starting of the medical entity type; I, inside of the medical entity type; O, apart from the entity type. The categorization of entities in BIO format is given in Table\u00a0 .\n   \nBIO format of entity types \n  \n\n\n### Experimental settings \n  \nIn this experiment, our proposed model is employed to extract medical information from EMR dataset. The key hyper parameters are: Number of hidden neurons for each hidden layer: 150, Minibatch size: 20, Number of epoch: 100, Optimizer: Adam optimizer, Learning rate: 0.01, Learning rate decay: 0.9. They are determined by trial and error. \n\n\n### Evaluation metric \n  \nDifferent metrics in terms of micro-average F score (MicroF), macro-average F score (MacroF) [ ] and accuracy have been used to evaluate the performance of our proposed model. Accuracy is calculated by dividing the number of predicted entities that is exactly matched with dataset entities over the total number of entities in the dataset. MicroF is calculated by MicroP and MicroR values whereas MacroF is affected by the average   F   values of each class: \n \n\nwhere   P   indicates precision measurement that defines the capability of a model to represent only related entities [ ] and   R   (recall) computes the aptness to refer all corresponding entities: \n \n\n\n\nwhereas   T   P   (True Positive) counts total number of entity matched with the entity in the labels.   F   P   (False Positive) measures the number of recognized label does not match the annotated corpus dataset.   F   N   (False Negative) counts the number of entity term that does not match the predicted label entity. Then, \n \n\n\n\n\n\nwhere   T   denotes the total number of categorized entities and   F  ,   P  ,   R   are   F  ,   P  ,   R   values in the   j   category of entities [ ]. \n\nMicroP, MicroR, and MicroF are defined as following. \n \n\n\n\n\n\n\n### Experimental results \n  \nOur experiments are implemented in different phases namely micro average, macro average and accuracy comparison. Precision, Recall and F-score are measured using our proposed multitask bi-directional RNN model and compared with the following classifiers: Naive Bayes (NB), Maximum Entropy (ME), Support Vector Machine (SVM), Conditional Random Field (CRF) [ ], and deep learning models including Convolutional Neural Network (CNN) [ ], single task bi-directional RNN (Bi-RNN) and transfer bi-directional RNN [ ], where NER can be defined as a multiclass classification problem for these classifiers [ ]. Among all the models, we have considered Bi-RNN model as baseline model. \n\nFirstly, performances are compared based on micro values and summarized in Tables\u00a0  and  . The results show that our proposed multitask bi-directional RNN model outperforms other models. For instance, the MicroF value of our proposed model is improved by 2.41% point and 4.67% point compared to the baseline model (Bi-RNN) and CNN, respectively in terms of results in Table\u00a0 . In addition, the MicroF value of our proposed model is improved by 3.07% point and 5.52% point compared to the baseline model (Bi-RNN) and CNN, respectively in terms of results in Table\u00a0 .\n   \nComparison results of MicroP, MicroR and MicroF measure on discharge summaries \n    \nComparison results of MicroP, MicroR and MicroF measure on progress notes \n  \n\nSince micro average only measures the effectiveness of model on a large number of entity, macro average is computed to evaluate the model\u2019s performance in the case of small number of entity terms [ ]. Table\u00a0  shows the comparison results of NER on discharge summaries. The macro average F-score is improved by 4.16% point compared to the baseline model. The F-measure ranged from 57.14% point to 88.61% point in different categorized entities when it is computed on our proposed model whereas the range is from 54.54% point to 84.68% point when it is computed from the baseline model. Table\u00a0  shows the comparison results of NER on progress note. The macro average F-score is improved by 13.82% compared to the baseline model. The F-measure ranged from 79.06% point to 94.56% point in different categorized entities when it is computed on our proposed model whereas the range is from 40.00% point to 89.52% point when it is computed from the baseline model.\n   \nComparison results of NER on discharge summaries \n    \nComparison results of NER on progress notes \n  \n\nThe comparison results of accuracy on discharge summaries and progress notes are given in Tables\u00a0  and  . It is observed that the overall accuracy is improved by 5.66% point and 9.41% point on discharge summary and progress note, respectively, compared to the baseline model. According to the evaluation results, our proposed model shows better performance on recognizing medical entity terms comparing with other models including CRF model. CRF uses the feature templates to extract features in order to build the NER model by introducing prior knowledge. On the other hand, the proposed model performs the NER task on Chinese EMRs without any prior knowledge.\n   \nComparison results (%accuracy) on discharge summaries \n    \nComparison results (%accuracy) on progress notes \n  \n\nIt is observed that the best accuracy is enlisted as 89.20% point in test terms and lowest performance is 36.00% point in recognizing disease terms for the case of discharge summary. The accuracy of recognizing disease terms is lowest comparing with other entities since there are very limited number of disease group (0.56% point) [ ] in sample which is not enough to train the model. Similar observations are gained for the case of progress note. \n\nIn addition, we examine how different features affect the model performance on the discharge summary data. We compare the proposed models built by word level features, character level features, and combined word level features and character level features. The comparison results are shown in Table\u00a0 . It is observed that combined features improve the model performance.\n   \nComparison results for character and word level feature \n  \n\n\n\n## Discussion \n  \nIn our proposed multitask model, we have been concentrating on improving the accuracy of named entity recognition task. Therefore, we have used different task layer (parts-of-speech tagging task) to enhance recognition performance which in turn improves the accuracy of named entity recognition task. More training time is needed for the proposed model since two task specific layers need to be trained, which involves two loss functions and two optimizers. We plan to use a joint loss function and joint optimizer to reduce the training time and improve the accuracy in our future research. \n\n\n## Conclusions \n  \nIn this paper, a novel multitask bi-directional RNN model is proposed for improving the performance of named entity recognition in EMR. Two different task layers, namely parts of speech tagging task layer and named entity recognition task layer are used in order to improve the information extraction method from EMR dataset by sharing the word embedding and character embedding layer. The feature sharing layer has a great impact on improving the accuracy of extracting entity information. Evaluation results using real datasets demonstrate the effectiveness of the proposed model. \n\n \n", "metadata": {"pmcid": 6309059, "text_md5": "a2053f6dc6fa72aadb86324c0b497072", "field_positions": {"authors": [0, 116], "journal": [117, 135], "publication_year": [137, 141], "title": [152, 255], "keywords": [269, 404], "abstract": [417, 2617], "body": [2626, 19923]}, "batch": 1, "pmid": 30591015, "doi": "10.1186/s12859-018-2467-9", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6309059", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6309059"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6309059\">6309059</a>", "list_title": "PMC6309059  A multitask bi-directional RNN model for named entity recognition on Chinese electronic medical records"}
{"text": "Popovski, Gorjan and Seljak, Barbara Korou\u0161i\u0107 and Eftimov, Tome\nDatabase (Oxford), 2019\n\n# Title\n\nFoodBase corpus: a new resource of annotated food entities\n\n# Keywords\n\n\n\n# Abstract\n \nThe existence of annotated text corpora is essential for the development of public health services and tools based on natural language processing (NLP) and text mining. Recently organized biomedical NLP shared tasks have provided annotated corpora related to different biomedical entities such as genes, phenotypes, drugs, diseases and chemical entities. These are needed to develop named-entity recognition (NER) models that are used for extracting entities from text and finding their relations. However, to the best of our knowledge, there are limited annotated corpora that provide information about food entities despite food and dietary management being an essential public health issue. Hence, we developed a new annotated corpus of food entities, named FoodBase. It was constructed using recipes extracted from Allrecipes, which is currently the largest food-focused social network. The recipes were selected from five categories: \u2018Appetizers and Snacks\u2019, \u2018Breakfast and Lunch\u2019, \u2018Dessert\u2019, \u2018Dinner\u2019 and \u2018Drinks\u2019. Semantic tags used for annotating food entities were selected from the Hansard corpus. To extract and annotate food entities, we applied a rule-based food NER method called FoodIE. Since FoodIE provides a weakly annotated corpus, by manually evaluating the obtained results on 1000 recipes, we created a gold standard of FoodBase. It consists of 12\u2009844 food entity annotations describing 2105 unique food entities. Additionally, we provided a weakly annotated corpus on an additional 21\u2009790 recipes. It consists of 274\u2009053 food entity annotations, 13\u2009079 of which are unique. The FoodBase corpus is necessary for developing corpus-based NER models for food science, as a new benchmark dataset for machine learning tasks such as multi-class classification, multi-label classification and hierarchical multi-label classification. FoodBase can be used for detecting semantic differences/similarities between food concepts, and after all we believe that it will open a new path for learning food embedding space that can be used in predictive studies. \n \n\n# Body\n \n## Introduction \n  \nIn biomedical text mining, automation of information extraction (IE) aimed to uncover relations of any type from scientific literature has become a very important task. One of the first steps in IE is performed by named-entity recognition (NER) that locates named entities in the text to be classified into pre-defined categories. Best-performance NER methods are usually corpus-based ( ), which require corpora of annotated entities of interest. Various annotated corpora have already been produced by shared tasks, such as BioNLP ( ) and BioCreative ( ), where the main aim is to challenge and encourage research teams on natural language processing (NLP) problems. These annotated corpora can be used for different research aims such as gene event extraction, cancer genetics, pathway curation, corpus annotation with gene regulation ontology, gene regulation networks in bacteria, bacteria biotopes, extracting the regulation of the seed development in plants, disease- and symptom-related entities, relations that exist between chemical/drug entities and disease entities, methods for annotations such as disease, phenotype, and adverse reactions in different text sources literature, family history information extraction, and clinical semantic textual similarity. \n\nHowever, in 2019, Lancer Planetary Health published that 2019 is the year of nutrition, where the focus should be on discovering relations between food systems, human health, and the environment. Contrary to the large number of available annotated corpora with entities from the biomedical domain, in the food domain there are a limited number of resources that could be used for research. \n\nToday, there are a vast number of recipes published on the internet, which carry valuable information about food and nutrition. However, to the best of our knowledge, there are only two existing corpora of annotated recipes: (i) the r-FG (recipe flow graph) corpus ( ) and (ii) the CURD (Carnegie Mellon University Recipe Database) corpus ( ). The r-FG corpus consists of 266 Japanese recipes annotated using eight tags related to   food  ,   tool  ,   duration  ,   quantity  ,   action by the chef  ,   action by foods  ,   state of foods   and   state of tools  . The CURD corpus consists of 300 annotated recipes and 350 unannotated ones, for which the Minimal Instruction Language for the Kitchen language (MILK) is used for annotation ( ). \n\nLet us mention the UCREL semantic analysis system (USAS), which is a framework for automated semantic analysis of text. It distinguishes between 21 major categories, one of which is also \u2018food and farming\u2019 (F) ( ). Further, it provides additional semantic tag information that is used in the Hansard corpus ( ). The Hansard corpus was recently created as part of the SAMUELS (Semantic Annotation and Mark-Up for Enhancing Lexical Searches) project ( ), with the aim to extract speeches (i.e. digitised debates) given in the British Parliament from 1803 to 2005. \n\nAs part of our previous work ( ), we developed drNER, which is a rule-based NER system used for IE from evidence-based dietary recommendations, where beside entities related to nutrition and dietary recommendations, food entities were also of our interest. However, drNER works with unstructured data. In drNER, food entities are extracted using the food semantic tags obtained by the UCREL semantic analysis on a token level combined with Boolean algebra rules in order to define phrases from text that are food entities. \n\nAlthough abovementioned recipe-annotated corpora exist, they are limited. The r-FG corpus is composed only of Japanese food recipes, and both the r-FG corpus and the CURD corpus use annotation schemes that are not detailed enough, providing only a general food entity; without differing between groups of dishes (e.g. soups, grain dishes, egg dishes, tea, coffee). Also, drNER provides only a general food entity because it was developed to distinguish between   food  ,   nutrient   and   quantity/unit  . The USAS can provide additional information about the selected food entity, but its limitation is that it works on a token level. A token, as defined as a problem in NLP, is a string of contiguous characters between pre-defined delimiters (e.g. white spaces, punctuation). Most commonly, a single token is a single word, number or abbreviation. For example, if we have \u2018grilled chicken\u2019 as one food entity that needs to be processed for its relations, the entities \u2018grilled\u2019 and \u2018chicken\u2019 will obtain separate semantic tags. For these reasons, we decided to create a FoodBase, which is a new corpus that can be used for automated food named-entity extraction and includes food entities annotated with the semantic tags from the Hansard corpus. \n\n\n## Methods and Materials \n  \nIn this section, we present how a resource of recipes to be used for IE was selected. Then, the Hansard corpus of semantic tags is described in more detail. We continue by presenting FoodIE, i.e. a rule-based NER ( ), that is used for structuring recipes. First, we briefly describe its basic steps and then we focus on its evaluation and the introduction of a new step that was added to FoodIE with the aim of the semantic annotation of the extracted food entities. \n\n### Recipe selection \n  \nTo start creating the FoodBase corpus with annotated food entities, we selected 1000 various recipes from Allrecipes ( ), which is the largest food-focused social network where everyone plays a part in helping cooks discover and share the home cooking. We selected this network because everyone can post recipes on Allrecipes, so we have variability in how users express themselves. The recipes were selected from five recipe categories: \u2018Appetizers/Snacks\u2019, \u2018Breakfast/Lunch\u2019, \u2018Dessert\u2019, \u2018Dinner\u2019 and \u2018Drinks\u2019, including 200 recipes for each recipe category. For each recipe, we collected information about the English recipe name, its ingredient list and the preparation instructions in English. The ingredient list consisted of English ingredient names and quantities in non-standard units and household measures provided in English (e.g. \u20181 large eggplant, halved lengthwise\u2019, \u20181 (8 ounce) package crumbled feta cheese\u2019). \n\n\n### Hansard corpus semantic tags \n  \nIn order to annotate food entities extracted from the selected recipes, we used semantic tags from the Hansard corpus ( ). In this corpus, semantic tags are ordered using a hierarchical structure, where food is addressed in the category \u2018Food and drink\u2019 (AG). The AG category is further split into three subcategories: \u2018Food\u2019 (AG:01), \u2018Production of food, farming\u2019 (AG:02) and \u2018Acquisition of animals for food, hunting\u2019 (AG:03). The \u2018Food\u2019 subcategory consists of 125 top level semantic tags, the \u2018Production of food, farming\u2019 consists of 36 top level semantic tags and the \u2018Acquisition of animals for food, hunting\u2019 consists of top level 13 semantic tags. In addition to the AG category, we decided to also use the categories \u2018Animals\u2019 (AE) and \u2018Plants\u2019 (AF), so that any missing information (semantic tag) for a food entity that is a recipe ingredient could be searched for in AE and AF, as part of nature animal or plant, respectively. The AE category consists of 15 semantic tags, while the AF category consists of 30 semantic tags. There are additional and more specific tags on a deeper hierarchical level within some of these tags, which are also utilised. More details about the Hansard corpus semantic tags can be found in Hansard ( ). \n\n\n### FoodIE: a rule-based food NER \n  \nTo enable NER that locates food entities, we have recently proposed a rule-based approach, called FoodIE, which works with unstructured textual data (i.e. recipe description) and consists of four steps ( ):\n   \n Food-related text pre-processing:   one of the main concerns of this step is to clean up the raw textual data, such as removing non-standard characters, excess white spaces and performing transliteration as to not confuse the taggers. \n  \n Text POS-tagging and post-processing the tag dataset:   this step consists of acquiring the textual data with Part of Speech tags, as well as ensembling both taggers\u2019 data to increase robustness. \n  \n Semantic annotation of food tokens in the text:   this is the main rule engine of FoodIE, which utilizes a small number of rules and performs semantic annotation of the tokens in the text, classifying it in one of four classes which are further used to perform NER. \n  \n Food name-entity recognition:   this step is concerned with chaining the semantically annotated tokens into food chunks that represent a single food concept. \n  \n\nFor the aims of creating the FoodBase corpus, we added an additional step to the end of the FoodIE pipeline:\n   \n Semantic annotation of the extracted food entities  : here, the Hansard semantic tags are grouped within each token for each food chunk, with the goal of representing the food concept in its entirety. \n  \n\nThe flowchart of the extended methodology is presented in  . More details about the first four steps have already been presented in our previous work ( ); however, in this paper, we will focus on the evaluation of FoodIE as this is the crucial step in building the annotated corpus. An example of running FoodIE on one recipe is explained in ( ), step by step. Then, we will describe the new step of semantic annotation of the extracted food entities. \n  \nFlowchart of the extended FoodIE methodology. \n  \n#### Evaluation of the extended FoodIE methodology \n  \nOnce the information about the recipes was selected from Allrecipes, we asked a person to manually extract food chunks from the description of each recipe. A food chunk is a contiguous collection of tokens which describe a single food concept. Then, we run the first two steps of FoodIE to obtain automatically extracted food chunks from the description of the same recipes. To avoid any kind of bias when comparing the food chunks extracted manually and automatically by FoodIE, another person was asked to cross reference the manually obtained chunks with the ones obtained by FoodIE. Using this method, true positives (TPs), false positives (FPs) and false negatives (FNs) were counted, while it was decided that the category true negative (TN) is not applicable to the nature of the problem and its evaluation. In our case, TP and FP mean outcomes where FoodIE correctly or incorrectly predicted the positive class, respectively. Similarly, a FN means an outcome where FoodIE incorrectly predicted the negative class. In addition to the results for TP, FP and FN, the results for \u2018Partial (Inconclusive)\u2019 are presented. This group of outcomes includes evaluations that could be either TP or FP/FN. For example, in the text segments \u2018empty passion fruit juice\u2019, \u2018cinnamon\u2019 and \u2018soda,\u2019 the actual food entity chunks are \u2018passion fruit juice\u2019, \u2018cinnamon sticks\u2019 and \u2018club soda\u2019, respectively. These occurrences are mostly due to the dual nature of words, meaning that a word is a synonym for both a noun and a verb or a synonym for an adjective and a verb. For such words, the FoodIE tagger sometimes incorrectly classifies the tokens. In these examples, \u2018empty\u2019 is tagged as an adjective, where in context it is a verb. The explanation is almost identical for the other two examples. For these reasons, when the evaluation metrics were being calculated, the \u2018Partial (Inconclusive)\u2019 category was omitted. Moreover, even if they are classified either as TP or as FP/FN, they would not significantly affect the results. We performed this kind of the evaluation in our previous study ( ) since there is no pre-existing method to evaluate such a text corpus. \n\n Checking the concept.   First, a subset of 200 recipes out of 1000 were processed and evaluated. From each category, we selected 40 recipes. More details about the predictions are presented in ( ). \n\nMost of the FNs are related to food concepts that are represented by their brand names (e.g. \u2018Snickers\u2019, \u2018Jim Beam\u2019). Some of them also occur when the semantic tagger incorrectly classifies some token with regard to the context in which they are mentioned (e.g. \u2018date\u2019 classified as a day of year, when it represents fruit). Furthermore, there are also examples with some specific foods related to some cultures (e.g. \u2018kefir\u2019). \n\nIn the case of FPs, most of the instances are related to concepts related to food, but not food concepts by themselves. In most cases, these are instruments or tools used in cooking. \n\n Second trial.   Once the effectiveness of the concept was evaluated on 200 recipes, the complete set of 1000 recipes was processed and evaluated, and predictions for them are presented in ( ). \n\nComparing the evaluation metrics for 200 and 1000 recipes presented in ( ), we can conclude that FoodIE behaves consistently. Evaluating the dataset with 200 recipes, which consists of 100 recipes that were analysed to build the rule engine and 100 new recipes that were not analysed beforehand, we obtained a precision of 0.9761, a recall of 0.9430 and a   F   score of 0.9593. Furthermore, by evaluating it on the dataset of 1000 new recipes, we obtained 0.9780 for precision, 0.9437 for recall and 0.9605 for the   F   score. From these results, we can conclude that FoodIE gives very promising and consistent results. \n\n\n#### Semantic annotation of the extracted food entities \n  \nOnce food entities were extracted using FoodIE, we annotated each of them using the semantic tags provided by the Hansard corpus. For this reason, annotations that are assigned to each food chunk are the semantic tags that belong to the tokens from which the chunk is constructed. As we explained before, these tags come only from three general Hansard corpus categories, i.e. \u2018Food and drink\u2019 (AG), \u2018Animals\u2019 (AE) and \u2018Plants\u2019 (AF). When a selected entity recognized as a food entity cannot be annotated with any semantic tag from the \u2018Food and drink category\u2019, a tag from either \u2018Animals\u2019 or \u2018Plants\u2019 is used. Moreover, when no semantic tag can be associated to the food entity, it is assigned to the top food level hierarchy, i.e. \u2018AG.01[Food]\u2019. \n\nExamples include the following:\n   \n\u2018grilled chicken\u2019 obtains the semantic tags AG.01.t.07[Cooking] /AG.01.d.06[Fowls] \n  \n\u2018tortilla chips\u2019 obtains AG.01.n.11[Bread] /AG.01.n.12[Pancake/tortilla/oatcake] \n  \n\u2018dry ranch salad dressing mix\u2019 obtains AG.01.h.02 [Vegetables]/AG.01.m [Substances for food preparation]/AG.01.n.09 [Prepared vegetables and dishes] \n  \n\u2018cauliflower\u2019 obtains AG.01.h.02.d [Cabbage/kale] \n  \n\n Manual evaluation  . Semantic annotations obtained by FoodIE were manually evaluated. Food entities reported as FPs were manually excluded from the corpus, while the food entities reported as FNs were included in the corpus. This was done in order to obtain a good benchmarking dataset, which contains all food entities that are present in the dataset of 1000 randomly selected recipes from five main dish categories. Furthermore, apart from excluding FPs and including FNs, the annotated semantic tags were double-checked. During this process, all the incorrect semantic tags were removed, while all the missing semantic tags were added to specific food entities. \n\n Annotation format.   We decided to annotate the extracted information using the BioC format ( ), which has been originally proposed by biomedical NLP and text mining tools. It is a simple XML-based format aimed for sharing text data and annotations, with the goals of simplicity, interoperability and broad use and reuse. In  , a selected recipe is presented in the BioC format. \n  \nAnnotated recipe from \u2018Appetizers and snacks\u2019 category presented in the BioC format. For the recipe presented in this figure, all the extracted food concepts are presented, along with their respective semantic tags and their location in the raw recipe text. \n  \n Post-processing of the annotated semantic tags.   While manually evaluating and correcting the semantic annotations generated by FoodIE, the food entities reported as FNs were incorporated into the FoodIE rule engine as a resource, with the goal to improve its performance. This means that the new version of the FoodIE rule engine is more robust, since it does not incorrectly produce the FNs that were manually added as a resource. In addition to this, there were some specific instances where the semantic tags themselves needed a modification in some way. For example, the semantic tag \u2018AG.01.af [Tea manufacture]\u2019 incorrectly appears every time the token \u2018mixture\u2019 is present in the food entity chunk, so it is removed from the list of semantic tags for that food chunk. Another example of this is the semantic tag \u2018 .03 [Brewing]\u2019 that incorrectly appears whenever the token \u2018mashed\u2019 is present. If the food entity does not contain any semantic meaning relevant to these semantic tags, the tag is removed. In addition to this, some semantic tags were very vague. The semantic tagger occasionally did not tag the food entity \u2018water\u2019 as such, but just provided the tag \u2018AG.01 [Food]\u2019. In such cases, the tag \u2018AG.01.z [Water]\u2019 was manually added for that food entity, while the original one was removed. Such omissions and inclusions, although rare (<5%), were performed as needed. \n\n Justification of using semantic tags from Hansard corpus.   Before we decided which semantic tags will be used to describe the extracted food entities, we tried several other knowledge resources for identifying food entities. We did this by using the set of 1000 recipes. To extract and describe the food entities, we used two approaches: FoodIE and NCBO annotator ( ). The NCBO Annotator is a web service that annotates text provided by the user by using relevant ontology concepts. It is available as part of the BioPortal software services ( ). The annotation workflow is based on an efficient syntactic concept recognition engine (which utilizes concept names and synonyms), as well as on a set of semantic expansion algorithms that leverage the semantic information found in ontologies. The methodology relies on ontologies to create annotations for textual data and presents them by using semantic web standards. It can be also used for named-entity extraction from food ontologies that are part of the BioPortal software services. FoodIE annotates the food entities using the semantic tags from the Hansard corpus, while the NCBO annotator was used for annotation in a combination with three food ontologies that are available in the BioPortal (i.e. FoodOn ( ), OntoFood and SNOMED CT ( )). We should mention here that SNOMED CT is also part of the Unified Medical Language Systems (UMLS) ( ). Each version of the NCBO annotator working with a different ontology was assumed to be as a different NER method (i.e. NCBO (SNOMED CT), NCBO (OF), NCBO (FoodOn)). Finally, a total of four different NER methods (FoodIE, NCBO (SNOMED CT), NCBO (OF) and NCBO (FoodOn)) that can be used for food information extraction were compared. \n\nTo evaluate the results, we selected three standard types of matches: true positives (TPs), false negatives (FNs) and false positives (FPs), as well as the aforementioned \u2018Partial (Inconclusive)\u2019 match type. The results from counting the instances of each match type are presented in  . It is important to note that not all ontologies provided annotations for each recipe. More specifically, out of 1000 recipes, SNOMED CT missed 6, OntoFood missed 71, and FoodON missed 5. Next, we are going to explain the results for every match. \n  \nResults from comparing different NER methods in the food domain \n  \nLooking at the comparison results in  , we can see that the number of TPs is substantially larger when using FoodIE (11461) when compared to the three other ontologies with the NCBO annotator, i.e. SNOMED CT (5100), OF (2279) and FoodON (5725). The number of TPs should be maximized. \n\nMoving on to the FPs, FoodIE again provides the best results of the four, while FoodON provides significantly more FPs than the other three NER methods. Respectively, they provide FoodIE (258), SNOMED CT (472), OF (378) and FoodON (1502). The number of FPs should be minimized. \n\nThe last of the standard types of matches is FN, where FoodIE once again behaves superiorly to the other three NER methods. The numbers here are FoodIE (684), SNOMED CT (5327), OF (9026) and FoodON (4968). The number of FNs should be minimized. \n\nThe last type of match we take into account is the partial match type. It is not clear whether this type of match should be maximized or minimized in and of itself, as it heavily depends on the number of other types of matches (especially TPs and FNs). For example, ideally all the food concepts would be matched as TPs and none as FNs. However, if a TP match is not encountered for a specific food concept instance, the second-best occurrence would be to \u2018partially\u2019 match it. The worst-case scenario is when the food concept is matched as a FN. \n\nBy performing the analysis, we can conclude that FoodIE, using the Hansard corpus, provides the most promising results since it can extract a larger number of food concepts as opposed to the NCBO annotator in combination with the selected ontologies. Moreover, the results imply that the three food ontologies (i.e. SNOMED CT, FoodOn, OntoFood) do not represent the food domain exhaustively, as many food concepts are not extracted using the NCBO annotator running on these ontologies. This indicates that they do not exist as entities in the food ontologies themselves. \n\n Food ontologies alignment.   To align food concepts in different food ontologies, we have created a resource, named FoodOntoMap, that consists of food concepts extracted from recipes. For each food concept, semantic tags from four food ontologies are assigned. With this, we create a resource that provides a link between different food ontologies, which can further be reused to develop applications for understanding the relation between food systems, human health and the environment. \n  \nDescriptive statistics for the number of words per recipe, the number of food entities per recipe and the number of semantic tags per food entity \n  \nThe results from FoodOntoMap are four different datasets and one data set mapping. Each dataset consists of an artificial ID for each unique food concept that is extracted by using each approach, the name of the extracted food concept and the semantic tags assigned to it. Each dataset corresponds to one of the four semantic resources: Hansard corpus, FoodOn, OntoFood and SNOMED CT. At the end, there is one data set mapping, called FoodOntoMap, where for each concept that appears at least in two datasets, the mapping between them is provided by listing the artificial ID of the concepts from each of the datasets in which it is encountered. The datasets consist of 13\u2009205, 1069, 111 and 582 unique food concepts, obtained using Hansard corpus, FoodOn, OntoFood and SNOMED CT, respectively. The FoodOntoMap mapping consists of 1459 food concepts that are found in at least two of the food semantic resources. \n\nThe motivation for building such a resource in the food domain comes from the existence of the UMLS, which is extensively used in the biomedical domain. For example, the MRCONSO.RRF table that is a part of the UMLS is used in a lot of semantic web applications since it can map the medical concepts to a variety of different biomedical standards and vocabularies. \n\n\n\n\n## Results and Discussion \n  \n### FoodBase corpus overview \n  \nAfter applying FoodIE for semantic annotation of 1000 randomly selected recipes with semantic tags from the Hansard corpus and performing post-processing of the annotated semantic tags, the initial FoodBase corpus was generated. It consists of 12\u2009844 food entities extracted from the selected recipes for dishes from five main groups, with 2105 unique food entities in total. \n\nBecause the evaluation of extended FoodIE gave very promising and consistent results, we used it again for extracting and annotating food entities for a new, more extensive subset of 21\u2009790 recipes from the same five groups of dishes, i.e. \u2018Appetizers/Snacks\u2019, \u2018Breakfast/Lunch\u2019, \u2018Dessert\u2019, \u2018Dinner\u2019 and \u2018Drinks\u2019. The outcome was the next version of the FoodBase corpus that includes much more recipes and corresponding food entities (274\u2009053 total food entities, with 13\u2009079 unique food entities). However, this version has not been processed manually to exclude the FPs and to include the FNs. To distinguish between the two versions of the FoodBase corpus, we call the manually post-processed version containing 1000 recipes \u2018curated\u2019 and the one consisting of 21\u2009790 recipes as annotated by FoodIE, \u2018un-curated\u2019. \n\nThe descriptive statistics (i.e. mean, median, mode and standard deviation) for the number of words per recipe, the number of entities extracted per recipe and the number of semantic tags assigned per food entity for both versions are presented in  . The distribution of the number of words per recipe for the curated and un-curated version of FoodBase is presented in  , while the distribution of the number of food entities extracted per recipe is presented in  . Additionally, the distribution of the number of semantic tags assigned per food entity for both versions is presented in  . \n  \nDistribution of the number of words per recipe. It is apparent that both distributions have a similar trend. However, the distribution of the un-curated version is much smoother because it includes more recipes. (a) Curated dataset. (b) Un-curated dataset. \n    \nDistribution of the number of extracted food entities per recipe. It is apparent that both distributions have a similar trend. However, the distribution of the un-curated version is much smoother because it consists of more recipes. (a) Curated dataset. (b) Un-curated dataset. \n    \nDistribution of the number of assigned semantic tags per food entity. Analysing it, it follows that both distributions have a similar trend, which is a power law distribution. \n  \nLooking at the descriptive statistics provided for the number of words per recipe, it is apparent that there are no big differences between the descriptive statistics of both versions. The biggest difference appears for the mode. The mode for the curated version is 121.00, while that for the un-curated version is 91.00. If we look more closely at the distribution provided for the curated version in  , we can see that these two values for frequency are close and that they differ by no more than 15. Moreover, it follows that the distributions have a similar trend. However, the distribution of the un-curated version is much smoother because it includes more recipes. The same conclusion is also true for the descriptive statistics provided for the number of extracted food entities per recipe. The only difference that is apparent is for the modes. It is 5.00 for the curated version and 10.00 for the un-curated version. However, if we look at the distribution of the curated version ( ), we can see that the difference between their frequency is less than 10. From  , it is obvious that the distributions have a similar trend, the only difference is that the distribution of the un-curated version is much smoother, which is reasonable, since it includes more recipes. In  , the distribution of the number of assigned semantic tags per food entity is presented for both FoodBase versions, separately. Analysing it, it follows that both distributions have a similar trend, which is a power law distribution. \n\nAdditionally, the statistics are presented for each category \u2018Appetizers/Snacks\u2019, \u2018Breakfast/Lunch\u2019, \u2018Dessert\u2019, \u2018Dinner\u2019 and \u2018Drinks\u2019 separately in  . It is evident that in both versions of FoodBase the average number of extracted entities, as well as the standard deviation, is the largest in the \u2018Dinner\u2019 category. There is no big difference between the average number of extracted entities from the \u2018Appetizers/Snacks\u2019, \u2018Breakfast/Lunch\u2019 and \u2018Dessert\u2019, and additionally, they have similar standard deviations. The \u2018Drinks\u2019 category has the smallest average number and standard deviation of extracted entities. If we compare the descriptive statistics for each category between both versions, we can see that there are not big deviations between their values. \n  \nDescriptive statistics for the number of food entities per recipe and the number of semantic tags per food entity, for each category separately \n  \nThe number of food entities per the 10 most frequent semantic tags for both FoodBase versions is presented in  . The most frequent tag for both versions is the \u2018AG.01 [Food]\u2019, which is the top level in the food category hierarchy in the Hansard corpus. This result comes from the fact that if there is no semantic tag assigned to the entity, but it is recognized as food, then it is automatically assigned to this semantic tag. If we compare the other nine most frequent semantic tags, we can see that the semantic tags \u2018AG.01.l.02 [Sweetener (syrup/honey/chocolate)]\u2019, \u2018AG.01.m [Substance for food preparation]\u2019, \u2018AG.01.n [Dishes and prepared food]\u2019, \u2018AG.01.e [Dairy products]\u2019, \u2018AG.01.n.11 [Bread]\u2019 and \u2018AG.01.g [Eggs]\u2019 appear in both versions of FoodBase. The initial FoodBase based on 1000 recipes also includes \u2018AG.01.l.03 [Spice]\u2019, \u2018AG.01.w [Setting table]\u2019 and \u2018AG.01.h.02.e [Onion/leek.garlic]\u2019, while the next FoodBase version based on 21\u2009790 recipes includes \u2018AG.01.k [Flour]\u2019, \u2018AG.01.j [Meal]\u2019 and \u2018AG.01.e.01 [Butter]\u2019. \n  \nNumber of food-named entities per 10 most frequent semantic tags. From the 10 most frequent semantic tags in both the curated and un-curated version, 7 are identical across both versions. The three that differ are due to the difference in the number of recipes in both versions. (a) Curated dataset.   (  b) Un-curated dataset. \n    \nAvailability of all data and tools related to the FoodBase resource \n  \nLooking at the 10 most frequent tags for both versions, we can see that 7 out of 10 semantic tags are the same. There is a difference between the frequencies of the semantic tags in both versions, since the un-curated version consists of more recipes. However, the first idea of building such a corpus is that it can be generally used for the bi-classification problem (food vs. not food concept), where the semantic tags are not crucial. Further, using some sampling techniques, different subsets can be generated from the un-curated version with regard to the semantic tags that will be of interest (e.g. top 5 or 10 most frequent) in order to produce more representative data sets for training corpus-based NERs. \n\nIn general, the difference between the curated and un-curated versions is that the un-curated version consists of FPs, which in most of the cases are related to objects that are not food concepts but are concepts closely related to food entities or an instrument for food or cooking. Also, the FNs are not included, and they are related to food entities that are branded food products, or some rare foods that are typical for some cultures. \n\nThe availability of data and tools used to create FoodBase corpus is provided in  . \n\n\n\n## Conclusions \n  \nOur motivation to start building the FoodBase corpus has been to provide the scientific community with a fundamental resource required for learning corpus-based methods that can be used for food-named entity recognition. FoodBase is presented in two versions: curated and un-curated. The curated version is manually evaluated, consisting of 1000 recipes, while the un-curated version consists of 21\u2009790 recipes. For this reason, we can consider FoodBase as a whole as a \u2018silver standard\u2019. It can be also used as a benchmark dataset for several ML tasks, such as multi-class classification ( ), multi-label classification ( ) and hierarchical multi-label classification ( ). Multi-class classification is applied when a food entity may be annotated with several semantic tags (e.g. \u2018Food\u2019 (AG:01); \u2018Production of food, farming\u2019 (AG:02); and \u2018Acquisition of animals for food, hunting\u2019 (AG:03)). Multi-label classification is performed when an output is a more complex structure such as a vector of tags with some dependencies among them (i.e. the food entity can belong to multiple semantic tags simultaneously). Hierarchical multi-label classification is needed when the classes are hierarchically structured and food entities can be assigned to multiple paths of the class hierarchy at the same time (e.g. the food hierarchy from the Hansard corpus). As part of future work, we are working on presenting benchmarking results obtained from corpus-based food-named entity recognition using three datasets of different scale and quality (e.g. 200 recipes\u2014manually annotated, 1000 recipes\u2014manually annotated and 21\u2009790 recipes\u2014automatically annotated), in order to explore the utility of having such a corpus by applying sensitivity analysis. \n\nThe FoodBase corpus will enable a further development of more accurate food NERs to be used for the extraction of food entities not only from recipes as presented in this paper but also from scientific literature. Consequently, the exploration and the extraction of relations between food entities and other biomedical entities such as drug, disease and gene entities will be supported. \n\nMoreover, the FoodBase corpus is a step towards food normalization where semantic, instead of lexical, similarity can also be included. Furthermore, the semantic tags will be able to be used for building food embedding space needed for predictive studies. \n\n \n", "metadata": {"pmcid": 6827550, "text_md5": "a5da07468200e208a55a98e156e02594", "field_positions": {"authors": [0, 63], "journal": [64, 81], "publication_year": [83, 87], "title": [98, 156], "keywords": [170, 170], "abstract": [183, 2256], "body": [2265, 35823]}, "batch": 1, "pmid": 31682732, "doi": "10.1093/database/baz121", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6827550", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6827550"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6827550\">6827550</a>", "list_title": "PMC6827550  FoodBase corpus: a new resource of annotated food entities"}
{"text": "Foufi, Vasiliki and Timakum, Tatsawan and Gaudet-Blavignac, Christophe and Lovis, Christian and Song, Min\nJ Med Internet Res, 2019\n\n# Title\n\nMining of Textual Health Information from Reddit: Analysis of Chronic Diseases With Extracted Entities and Their Relations\n\n# Keywords\n\nsocial media\nchronic disease\ndata mining\n\n\n# Abstract\n \n## Background \n  \nSocial media platforms constitute a rich data source for natural language processing tasks such as named entity recognition, relation extraction, and sentiment analysis. In particular, social media platforms about health provide a different insight into patient\u2019s experiences with diseases and treatment than those found in the scientific literature. \n\n\n## Objective \n  \nThis paper aimed to report a study of entities related to chronic diseases and their relation in user-generated text posts. The major focus of our research is the study of biomedical entities found in health social media platforms and their relations and the way people suffering from chronic diseases express themselves. \n\n\n## Methods \n  \nWe collected a corpus of 17,624 text posts from disease-specific subreddits of the social news and discussion website Reddit. For entity and relation extraction from this corpus, we employed the PKDE4J tool developed by Song et al (2015). PKDE4J is a text mining system that integrates dictionary-based entity extraction and rule-based relation extraction in a highly flexible and extensible framework. \n\n\n## Results \n  \nUsing PKDE4J, we extracted 2 types of entities and relations: biomedical entities and relations and subject-predicate-object entity relations. In total, 82,138 entities and 30,341 relation pairs were extracted from the Reddit dataset. The most highly mentioned entities were those related to oncological disease (2884 occurrences of cancer) and asthma (2180 occurrences). The relation pair anatomy-disease was the most frequent (5550 occurrences), the highest frequent entities in this pair being cancer and lymph. The manual validation of the extracted entities showed a very good performance of the system at the entity extraction task (3682/5151, 71.48% extracted entities were correctly labeled). \n\n\n## Conclusions \n  \nThis study showed that people are eager to share their personal experience with chronic diseases on social media platforms despite possible privacy and security issues. The results reported in this paper are promising and demonstrate the need for more in-depth studies on the way patients with chronic diseases express themselves on social media platforms. \n\n \n\n# Body\n \n## Introduction \n  \n### Background \n  \nPeople are often concerned about their health status and a range of medical issues, especially when it comes to complex or chronic diseases that can take a long time to treat or monitor. Patients often desire easy access to information about diseases and symptoms to understand their condition and to facilitate self-management of diseases without total reliance upon interaction with a physician [ ]. Patients with chronic diseases in particular use social media to seek and provide social, emotional, and practical support [ ]. Therefore, social media information can influence patients\u2019 decisions to manage their chronic condition [ ]. \n\nSocial media platforms may support patients in their search for medical products or provide suggestions to promote healthy behavior and can improve health education as they allow people to write about their experiences with diseases, drugs, symptoms, and treatments. In recent years, social media platforms have grown quickly, with the public, patients, and health professionals sharing their experiences, looking for information, and interacting with others. \n\nCurrently, more than 74% of internet users connect to social media, and 42% of the internet users take advantage of social media for health information. Moreover, 32% of social media users in the United States share about their health care experiences and family\u2019s struggle stories and 29% search for health information via social media platforms to observe other patients\u2019 experiences with their diseases [ ]. Furthermore, 51% of those who live with a chronic disease have used the internet for information about health topics such as details of a specific disease, medical procedures, drugs, medical devices, or health insurances [ ]. \n\nWith its growing number of users, social media has become a powerful tool that can promote information sharing about health care, provide feedback from users, and foster support systems [ ]. In addition, the existence of social media platforms enables researchers to learn and discover the health experiences and feeling of patients and potentially discover new knowledge in health science. For example, user conversation content from health-related online forums, such as blogs, Twitter, and Facebook, has already been analyzed to find the clusters of breast cancer symptom [ ], examine smoking [ ], and understand the user discourse and describe social media interactions about obesity prevention [ ]. In particular, Reddit has been used as a data source for similar studies [ - ]. \n\nThe interactions between individuals on social media and the information they share constitute an important new source of data that can be used, on one hand, to understand the impact of drugs, diseases, and medical treatments on patients outside controlled clinical settings and, on the other hand, to comprehend health-related behavior. \n\nDiscovering public knowledge in social media text constitutes a challenge for researchers and health care providers. To achieve this goal, various text mining approaches, such as topic modeling, information extraction, and visualization, exist. \n\n\n### Biomedical Entity and Relation Extraction \n  \nIn the era of biomedical text mining, bioentities and their relations have arisen as a challenge to discover new knowledge. To mine the huge amounts of unstructured data, automatic information extraction tools have been conceived and developed based on several approaches. There are multiple systems developed for the identification and analysis of relations between diseases, drugs, and genes, such as Extraction of Drugs, Genes and Relations, a natural language system that extracts information about drugs and genes relevant to cancer from the biomedical literature [ ]. Extraction of drug-disease treatment pairs from the published literature was also carried out [ , ]. To extract health social media information, adverse drug reactions and drug indications from a Spanish health forum were examined [ ] using MeaningCloud [ ], a multilingual text analysis engine based on a distant-supervision method to detect relations between drugs and side effects and used them to classify the relation instances. \n\nPKDE4J2.0 is a system that extracts bioentities and their relations with the aim to discover biomedical scientific knowledge. It is based on a dictionary to automatically tag bioentities according to their types and a set of predefined rules used for relation extraction. PKDE4J2.0 can be applied for knowledge search, knowledge network construction, and knowledge inference [ ]. PKDE4J1.1 was used to investigate drug-disease interactions in article abstracts from PubMed Central for making drug-symptom-disease triples [ ]. This tool was also applied in biomedical literature to extract biomedical verbs to present a relation type between 2 entities [ ] and on full-text papers to extract biological entities from diseases and genes and construct a knowledge network [ ]. \n\n\n### Health Information Extraction From Social Media Platforms \n  \nA large number of patients, caregivers, and health professionals use social media platforms to discuss mental health issues. They also constitute an important data source for researchers. Machine learning and statistical methods were used to discriminate online messages between depression and control communities using mood, psycholinguistic processes, and content topics extracted from the posts generated by members of these communities [ ]. Users are interested in searching for treatment-related information, communicating with physicians to share their feelings about treatment effectiveness and side effects, discussing questions in health communities, and gaining knowledge about their conditions [ ]. User-generated content from these platforms contains valuable information [ ]. Their posts reflect what users think and feel about their medical experiences and often attract the attention of other patients, caregivers, and doctors. \n\nLu et al [ ] mined data from online health communities and used text clustering integrating medical domain\u2013specific knowledge to investigate patient needs and interests. Their results show that compared with existing methods, the addition of medical domain\u2013specific features into their feature sets achieved significantly better clustering than was achieved without the addition of those features. Moreover, there were significant differences in hot topics on different kinds of disease discussion platforms. Health-related posts on social media were analyzed to investigate the polarity of opinions online, performing sentiment analysis [ ]. Medical terms, including those related to conditions, symptoms, treatments, effectiveness, and side effects, were extracted to generate a virtual document addressing each question raised by members of the community. Then latent Dirichlet allocation (LDA) was modified by adding a weighting scheme known as conditional LDA to cluster virtual documents with similar distributions of medical terms into a conditional topic (C-topic). Finally, the clustered C-topics were analyzed according to sentiment polarities and physiological and psychological sentiments. Identification of topics of patients' discussions on (1) Facebook about breast cancer and (2) cancerdusein.org was performed [ ]. These topics were assigned to functional and symptomatic dimensions by applying LDA topic modeling and identified relations between the topics and the questionnaires. \n\nAmong others, Denecke [ ] reported that \u201cuser-generated content on the web has become a new source of useful information to be added to the conventional methods of collecting clinical data.\u201d \n\nIn terms of biomedical information extraction, previous studies relied on formal research and individual case studies to identify biomedical information. These approaches include observations of changes in patients [ ], meta-analysis of data from relevant databases [ ], and surveys of cancer patients [ ]. However, the scientific literature is generally limited to subscribers, and electronic medical records are not publicly available for reasons of patient privacy [ ]. Moreover, these sources do not provide a complete understanding of how patients suffering from a chronic disease feel and how they express these feelings. \n\nUsing data from conversations between patients on social media platforms provides valuable information for researchers, physicians, and health care providers. This data source is different from, and complementary to, that obtained from conventional experimental methods. \n\n\n### Research Objectives \n  \nTherefore, a social media platform (Reddit) was chosen as the data source for this research that aimed to answer the following questions: \n  \nWhich biomedical entities are prominent in the health social media platforms? \n  \nWhat types of entities are related in the corpus? \n  \nHow do people express themselves about chronic diseases on social media platforms? \n  \n\n\n## Methods \n  \n### Data Collection \n  \nThe data used for this research were extracted from disease-specific subreddits of the social news and discussion website Reddit [ ]. Forums such as Reddit tend to have sharp contrast when compared with similar offline groups; for instance, people are likely to discuss problems that they do not feel comfortable to discuss face to face [ ]. As of 2013, Reddit\u2019s official statistics included 56 billion page views, 731 million unique visitors, 40,855,032 posts, and 404,603,286 comments [ ]. In particular, the subreddit about cancer numbers 22,429 subscribers and 75 posts per day [ ]. These numbers demonstrate the external validity of Reddit. Another reason for having chosen Reddit as a data source is that the language of text posts is more structured than in other social media platforms such as Twitter. \n\nReddit\u2019s core functionality is the sharing of text-based posts with others who may or may not be members of the site. The subforum function allows the creation of designated spaces for users to congregate and interact with each other over a shared interest. Those subforums are called   subreddits  . A finite set of 19 subreddits related to chronic diseases was empirically selected for analysis. The choice of the specific subreddits was based on medical expertise and on the impact of these diseases on the quality of everyday life of patients. \n\nAs the main goal was the detection of relations between entities and of the way people suffering from chronic diseases express themselves in social media and not the study of characteristics of specific chronic diseases, the posts from the 19 subreddits were merged in a single dataset. \n\nAll of these subreddits host public content. In this research, no populational study has been performed. The study focuses on the expression of feelings and not on the identity of people sharing their experiences. From each post, only the title of the post and the body or textual content was extracted without additional information related to their authors. \n\nThe study was submitted to the Swiss Ethical Committee who concluded to a decision of nonconsideration provided that the collected data are not identifiable. \n\n\n### Lexicosemantic Resources \n  \nLexicosemantic resources were constructed and incorporated into the tool. These resources included a list of stop words and biomedical dictionaries of diseases, drugs, anatomy, procedures, symptoms, side effects, and findings created from clinical health care terminologies such as the Systematized Nomenclature of Human and Veterinary Medicine - Clinical Terms [ ], the National Library of Medicine's controlled vocabulary thesaurus [ ], the Gene Ontology knowledgebase [ ], the Kyoto Encyclopedia of Genes and Genomes database [ ], and the DrugBank database [ ]. Semantic relations properties were attributed to 4558 biomedical verbs extracted from the Unified Medical Language System [ ]. \n\nThe dictionaries were enriched with lemmas extracted from the corpus; for instance,   chemo  ,   AML   (acute myeloid leukemia),   take care, support  , and   fight  . \n\n\n### Description of the Tool \n  \nIn this research, the PKDE4J version 2.0 tool [ ] was used. This text mining system consists of 2 modules: entity extraction and relation extraction. \n\n#### Entity Extraction Module \n  \nThis module integrates dictionary-based entity extraction and rule-based relation extraction into a highly flexible and extensible framework. The Stanford CoreNLP pipeline [ ] was modified to make it suitable for advanced dictionary-based entity extraction. The entity extraction module consists of 4 major submodules: preprocessing, dictionary loading, entity annotation, and postprocessing. PKDE4J can analyze entities and relations from both structured and unstructured text. \n\n\n#### Relation Extraction Module \n  \nThe relation extraction workflow identifies directed qualified relations starting from sentences from which 2 or more entities have been extracted by the entity extraction module. The relation extraction module takes a list of verbs and nominalization terms that are employed to identify relations of interest. After extracting entities from a sentence, further relation extraction algorithms are executed to construct rules for the extraction of relations of entities. A set of 20 dependency parsing\u2013based rules is at the core of the relation extraction module and provides an ontologically enriched structure for sentences by annotating edges with dependency types. To extract relations, the system identifies a verb, which may be located between entities and contains relational characteristics, then, it checks the bioverb list to determine the relation between the entities ( ) [ ]. \n  \nThe workflow of the PKDE4J text mining system. \n  \n\n\n### Visualization \n  \nThe Gephi platform [ ] was used to visualize the network of chronic diseases in the corpus. To build a graph, the   k-   shortest paths routing algorithm was applied. The graph visualization tool was then used to map the chronic disease entities. A PageRank of terms was computed to rank the important entities in the network; therefore, entities ranked highly by PageRank have the highest impact. \n\n\n### Validation of Entity Extraction \n  \nTo evaluate the performance of the tool on entity extraction, 1000 posts randomly selected from the entire corpus were manually validated. The entities were evaluated as correct or incorrect based on the following specific guidelines. \n\n#### Findings and Symptoms \n  \nThis category refers to a phenomenon that is experienced by a person or described by a clinician and cannot be considered as a disease in the context, for example, \u201cThis news makes me feel anxiety.\u201d \n\n\n#### Disease Names \n  \nThis category refers to an abnormal condition of a human, animal, or plant that causes discomfort or dysfunction [ ]. As also mentioned in the previous category, the context helps to distinguish between a disease and a symptom or finding. For example, in the sentence \u201cAfter trying which dosage is good, my insomnia is thankfully gone again,\u201d   insomnia   refers to a disease, whereas in the sentence \u201cI have had symptoms of insomnia within the last months,\u201d   insomnia   describes a symptom/finding. \n\n\n#### Side Effects \n  \nThis category includes a symptom/finding or a disease that is caused by a treatment in the context. For example: \n \nSince beginning treatment have woken with bouts of nausea... \n \n\n#### Procedure \n  \nProcedure refers to any intervention carried on someone related to physical mental or social health. For example: \n \n...treatment which would include surgery and radiation/chemotherapy according to his oncologist \n \n\n\n\n## Results \n  \n### Data Collection \n  \nA dataset of 17,624 text posts was semiautomatically collected using crawlers accessing public streams.   shows the subreddits used for this research, the number of posts per subreddit, and the proportion of corpus representation of each subreddit: \n  \nSources used for the data collection. \n  \nAfter sorting the data corpus, duplicate posts and those with no relevant meaning, such as advertising posts and posts containing only a hyperlink, were removed. The final corpus comprises 17,580 posts (2,137,115 tokens). \n\n\n### Biomedical Entity and Relation Extraction \n  \nThe PKDE4J system performed named entity extraction and 2 types of relation extraction: relations between biomedical entities and between subjects, predicates, and objects on the sentence level. The system\u2019s output is a corpus annotated with entities and information about their relation. \n\nThe entities are either simple terms or complex structures referring to diseases, anatomy, procedures, findings, symptoms, side effects, or drugs. In total, PKDE4J extracted 82,138 entities from the Reddit dataset, as shown in  . The entity names and entity types were allocated to the 7 categories of the biomedical dictionaries. The 10 most frequent entity names followed by the number of occurrences in the corpus are displayed in  . It should be noted that the terms are given in the text in the form found in the corpus. Therefore, abbreviated terms have not been expanded. \n\nAs displayed in the table, 29,669 disease entities were extracted representing 1341 unique diseases; 19,956 anatomy entities, of which 369 are distinct anatomical terms; 11,549 procedures of 296 different types; 6256 symptoms entities describing 65 symptoms; 5351 entities representing side effects of 321 different types; and 35 different drug names (616 in total). The most highly represented diseases are oncological (  cancer  ,   breast cancer  ,   tumor  ,   leukemia  , and   lymphoma  ) or relate to asthma. The anatomy category contains a range of anatomical terms. Specifically,   blood   is the most frequent term. Other widely used anatomical terms are   back  ,   brain  ,   hand  ,   hair  ,   breast  ,   chest  ,   heart  , and   neck  . The procedures category comprises terms referring to chemical treatments (  chemo  ), surgery, laboratory test (  blood test  ), social interventions (  advice   and   listening  ), and others. \n\nThe most frequent symptom mentioned in the corpus is   pain   (472 occurrences).   Fatigue  ,   inflammation  ,   nausea  , and   cough   are some of the symptoms commonly reported by patients or relatives in the dataset. In the side-effect category, the most frequent entities are   anxiety  ,   stress  ,   swelling  ,   crying  , and   fear   followed by   disability   and   worry  . The most commonly reported drug is   prednisone   followed by   morphine  ,   salbutamol  , and   tramadol  . \n\n\n### Validation of Entity Extraction \n  \nAmong the 5151 extracted entities, 3682 were correctly labeled by the system, whereas 1469 were attributed with incorrect labels. The performance of the system was 71.48%. \n\nNext, an error analysis was performed on the incorrectly labeled entities. Errors were classified into 3 categories: \n  \nLexical errors (488/1469, 33.21%): the term   breast   is an anatomical term, but in the post, the compound term   breast cancer   appears. However, the system failed to extract the entire entity. \n  \nDictionary errors (550/1469, 37.44%), for example,   air   and   aspergillus   were falsely listed as an anatomical term and as a drug name, respectively. \n  \nAmbiguous concepts (431/1469, 29.33%): the term   bleeding   could be either a disease name or a symptom. \n    \nEntity extraction results. \n    \nTen most frequent entities by type. \n    \n\n### Relation Extraction \n  \nThe system extracted 2 entities (entity 1 and entity 2) found in the same sentence and linked with a relation and then it attributed the type of entities. For instance, entity 1,   Borderline   (disease) co-occurs with   High blood pressure   (symptom) in the sentence. In total, 30,341 relation pairs were extracted, as shown in  . \n\nOf the 30,341 relation pairs, the most frequent entity relation pairs and their number of co-occurrences are shown in  . \n\nThe relations between anatomy and disease entity types are the most frequent (5550 pairs). The pair disease-disease co-occurs 4668 times, and the pair anatomy-anatomy appears 3595 times. \n\n contains the 5 most frequent entities per relation pair. \n  \nExample of entity relation extraction. \n    \nMost frequent entities per relation pair. \n    \nThe 5 most frequent entities per relation pair. \n  \nThe most frequent entities in the pair disease-anatomy is   Cancer|Lymph  , in the pair disease-disease is   Cancer|ALL  , and in the pair anatomy-anatomy is   Back|Hair  . \n\nTo summarize, once the most frequent entities were extracted, the results were processed according to the shortest path between each entity pair to produce the graph shown in  . Among 2561 nodes and 13,405 edges, this entity network shows that   pain   highly co-occurs with other entities in the network (biggest node, weighted at 0.022461), followed by   cancer   (PageRank score at 0.018057) and   surgery   (PageRank score at 0.015443). \n\nThe node   pain   has connections with other nodes, including   fatigue  ,   inflammation  ,   stomach  ,   joints  ,   cancer  , and   chemo  . The node   cancer   is strongly linked to   chemo  ,   surgery  ,   treatment  ,   ALL (acute lymphoblastic leukemia)  ,   anxiety  , and   blood  . The nodes of   surgery  ,   chemo  , and   treatment   are linked to diseases and body parts. Finally, the entity nodes relating to mental health, such as   anxiety   and   depression  , also appear in the network and associate with other bioentity types.   shows the most frequent entities and the corresponding PageRank scores: \n  \nBiomedical entity network. \n    \nThe most frequent entities and the corresponding PageRank scores. \n  \n\n### Subject-Predicate-Object Entity Relation Extraction \n  \nThe system extracted 69,263 subject or object entities. The top 10 entities are shown in  . In total, 41,068 relations were extracted and the results were classified into 2 types of subjects: subject pronoun (  I, you, he, she, it, we  , and   they  ) and subject noun (  treatment  ). The relation pairs are divided into 19,645 pairs of subject pronoun-object entities and 21,423 pairs of subject noun-object entities. \n\n shows 2 examples of the subject-predicate-object relation extraction: the subject (for example   I  ,   he  ,   anyone  ,   it  , and   asthma  ), the predicate (verbs such as   have  ,   get  , and   increase  ), the object (terms such as   eczema  ,   allergies  ,   childhood asthma  ,   my cough  , and   allergy shots  ), and the sentence of the corresponding post. \n\n#### Subject Pronoun-Predicate-Object \n  \nThe subject pronoun-predicate-object relation extraction demonstrates that the most frequent subject pronoun is   I   (11,691 times, including   I\u2019ve  ,   I\u2019m  , and   I\u2019d  ). Some examples are shown in  . \n\n\n#### Subject Noun-Predicate-Object \n  \n shows some examples of subject noun-predicate-object relation extraction. Among the 21,423 relation pairs, the most frequent subject nouns are diseases such as   asthma   (272 occurrences), including phrases such as   asthma anxiety  ,   asthma attacks  ,   my asthma  ,   my asthma and allergies  ,   my asthma flare  , and   cancer   (226 occurrences). \n  \nThe top 10 occurrences of subjects and objects. \n    \nExamples of subject-predicate-object relation extraction results. \n    \nExample of subject pronoun-predicate-object relation extraction. \n    \nExample of subject noun-predicate-object relation extraction. \n  \n demonstrates the most used subject and object entities. The results show that the most frequent subject is the pronoun   I   (PageRank score: 0.100619). The pronoun   It  ,   She  ,   He  ,   They  , and   You   are also frequently used. Diseases, body parts, treatments, and symptoms are widely used as the subjects and/or objects as well as the possessive pronoun   my   (  my mother  ,   my eyes  , and   my dad  ).   presents the most frequent subject and object entities and the corresponding PageRank scores. \n\n\n\n### Social Media Language \n  \nExpressions that constitute specific terms developed on social media, such as   pm   (private message),   FWIW   (For What it's Worth) were identified in the corpus. \u201cA common feature of microblog texts is the use of symbols in posts, such as the love-heart dingbat symbol\u201d [ ]. Emoticons such as  , and text-based emoticons such as   LOL   (laughing out loud), :), :-(, =), and :( are also frequent. \n\nIn addition, the corpus contains informal phrases such as \u201cRooting for you!\u201d and \u201cI\u2019m still chugging along\u201d; adjectives such as   loopy  ,   drippy  ,   dicey  , and   zonked  ; and verbs such as   puke   that substitute for their equivalents in standard language. \n\nEntities found in the Reddit corpus present numerous morphosyntactic variants. For example, the term   chemotherapy   was rarely found, but the short form   chemo   was frequently used. The disease name   Hodgkin\u2019s Lymphoma   is as   Hodgkin Lymphoma  ,   Hodgkins Lymphoma  ,   Hodgkin disease  , and   HL  . Similarly, the entity name   Mixed Cellularity Classical Hodgkin Lymphoma   is found as   Mixed Cellularity Hodgkin Lymphoma  ,   Mixed Cellularity Hodgkins Lymphoma  , and   MCCHL.   Moreover, there are many abbreviated forms of entity names, such as   ALL  ,   AML  ,   BRCA2   (breast cancer type 2),   CLL   (chronic lymphocytic leukemia),   CML   (chronic myeloid leukemia),   COPD   (chronic obstructive pulmonary disease),   DCIS   (ductal carcinoma in situ), and   GERD   (gastroesophageal reflux disease). When these forms were included in the disease dictionary, the system managed to detect them. Some examples are presented in  . \n  \nSubject and object entity network. \n    \nThe most frequent entities and the corresponding PageRank scores. \n    \nExamples of disease entities in their abbreviated form. \n  \n\n\n## Discussion \n  \n### Principal Findings \n  \nIn this paper, we collected user-generated chronic disease\u2013related data from Reddit and extracted information pertinent to biomedical entities and their relations to examine the characteristics of the language used by users in this social media platform. Initially, the corpus was created by semiautomatically extracting posts from specific subforums of Reddit. Next, lexicosemantic resources from various sources were created. To perform the information extraction tasks\u2014entity extraction and relation extraction\u2014the PKDE4J text mining system was used. The system extracted 82,138 biomedical entities and 30,341 relations. These results indicate that the corpus contains a large amount of information. \n\n\n### Performance of the Tool \n  \nAs described in the Results section, the system achieved a high performance in the named entity extraction task and the attribution of entity types (3682/5151, 71.48% extracted entities were correctly labeled). As already mentioned, the language used in Reddit is structured enough, with a satisfactory number of full sentences so the system managed to extract entities and their relations. The error analysis showed that the system failed to detect a number of entities or falsely attributed the entity type, because of lexical errors, to dictionaries\u2019 errors or to ambiguous concepts. \n\n\n### Entity Extraction \n  \nEntities prominent in the corpus refer to diseases, anatomical terms, procedures, findings, and symptoms. While interpreting the entities extracted from the corpus, it must be taken into account that the corpus was constructed by selecting subreddits created to share information about specific diseases. Therefore, it is expected that entities related to these diseases are the most likely to be represented. For instance, parts of the body affected by specific cancers, such as   breast   or   blood  , occur very frequently. \n\nThe most frequent disease entities in this corpus are oncologic diseases such as   cancer  ,   ALL  , and   breast cancer  . Frequently mentioned nononcologic diseases are   asthma  ,   depression  , and the generic entity   disease  . The entity   thyroid stimulating hormone   (TSH) is frequently mentioned, but it should be further classified in findings. \n\nThe most frequent anatomy entity is   blood  . This is explained primarily because of the numerous posts speaking about   leukemia   and   lymphoma  . Moreover, people often report the results of blood tests, a situation that increases the number of entities identified. \n\nTerms tagged as   procedures   extracted from the corpus are mainly linked to oncologic diseases. About 2000 occurrences of   chemo   and   chemotherapy   were extracted.   Chemotherapy   is a significant procedure with numerous side effects. The fact that patients mention it at a high frequency shows that it is a treatment with a strong impact on quality of life and raises a lot of questions and worries for the patients involved. Social intervention procedures such as   listening   and   advice   are also frequent (see  ). This observation indicates that apart from technical information about treatment and surgeries, people also speak about the support they got during their disease or search for it in the community. \n\nIn medicine, symptoms can be difficult to differentiate from findings. This difference often resides in the context of the phenomenon. In the corpus, entities belonging to those categories as well as the side-effects category can be analyzed together to gain a better understanding of the results. Most frequent entities from these categories are closely related to the patient experiences and feelings. Concepts related to the feeling of fear are the most frequently present in this merged category: 7 out of 30 entities express feelings of fear or related with fear using the words   anxiety  ,   stress  ,   confused  ,   scared  ,   terrified  ,   fear  , and   worry  . This is coherent with studies on cancer survivors that state the fear of cancer recurrence as almost universal among this population [ ]. It appears that people with chronic conditions use social media to share feelings they have experienced. The chronic diseases selected in this corpus frequently imply severe impact on lifestyle and decrease life expectancy. Therefore, it is logical that   fear   and   anxiety   are prominent entities in the corpus. \n\nHealth-related quality of life in chronically ill patients is a known field in medical research since numerous years. Questionnaires such as the European Organisation for Research and Treatment of Cancer Quality of Life-C15-Palliative [ ] or, more recently, the Functional Assessment of Cancer Therapy-General 7 [ ] and Patient-Reported Outcomes Measurement Information System [ ] are used routinely to assess it in those populations. When looking at the top concerns raised by patients suffering from cancer [ ], it is interesting to note that they are in line with the top entities extracted from the corpus. More specifically, the most frequent nondisease entity extracted,   pain  , is a key item in multiple quality-of-life assessment questionnaires. This shows that the experiences that the patients share on social media platforms are coherent with what has been proven to have an impact on their life. \n\nOverall, entities extracted from the corpus are coherent with similar studies conducted on health-related social media [ ] and with validated evaluation of the quality of life of patients suffering from chronic diseases. \n\n\n### Relation Extraction \n  \nThe relation extraction performed on this corpus shows that the most highly represented relation type identified is the   disease-anatomy   relation (5550 occurrences). The pairs most frequently representing a disease and its localization are   cancer-lymph  ,   asthma-lungs  , and   tumor-blood  . This suggests that people using social media platforms to speak about their chronic diseases are willing to explain which disease they suffer from as well as the location of the disease. This propensity is probably linked to the fact that such subreddits are used to share life experiences and to find people with similar backgrounds. This commonality can be reassuring and informative for a person suffering from the same disease. To find these people and knowledge, it is valuable to share the nature of the disease and its anatomical location. \n\nThe second most frequent entity pair is   disease-disease   (4668 occurrences). Pairs of entities such as   cancer-ALL   (acute lymphoblastic leukemia) and   asthma-allergy   are frequent. This co-occurrence of diseases might be related to the fact that chronic diseases often lead to complications and to other diseases. For example,   asthma-allergy   was perceived from the sentence \u201chave allergy induced asthma.\u201d \n\nThe third most frequent entity pair is   anatomy-anatomy   (3595 occurrences). When looking at specific occurrences of this pair, the pairs   neck-lymph  ,   brain-lungs  , and   lungs-lymph   are frequent. Another entity pair,   head-hair   is related to people speaking about the side effects of chemotherapy. \n\nRelations linking   diseases   to   procedures   are present at a high frequency in the corpus (2540 occurrences). When looking specifically in this category, it is clear that the most frequently identified disease in the corpus,   cancer  , is also most highly represented in those relations. \n\nThe relations extracted from the corpus demonstrate that patients with chronic diseases are willing to share detailed information about their health condition in a structured manner, describing thoroughly the disease, its location, the symptoms it caused, and the effect of treatment. \n\n\n### Subject-Predicate-Object Entity Relation Extraction \n  \nThe language patterns of subject-predicate-object relations demonstrate important characteristics of health social media language. As is apparent in the outputs, subject pronouns and object pronouns were frequently mentioned and were used mostly in the singular first-person pronoun, such as   I  ,   me  , and   my  . These patterns are related to the way individuals share personal or family experiences (\u201cI-had-a bad cold or sinus infection,\u201d \u201cAllergens-explains-my severe asthma,\u201d \u201cIt-is making-my heartburn,\u201d and \u201cAnyone-develop-eczema\u201d) and feelings (\u201cI have a history of testicular cancer in family so Im pretty scared bht im hoping its nothing\u201d). Also, patients or relatives, after having described their problem, treatment, and possible effects, often ask for advice, as shown in the following sentence: \n \nI noticed the depression is occurring simultaneously with the increased asthma symptoms and was wondering if there is a correlation and if anyone else has experienced this? \n \n\n### Social Media Language \n  \nData derived from clinical narratives and research papers differ significantly from social media content. The language and style used by the authors as well as the content are different. From a linguistic point of view, medical blogs usually consist of syntactically correct sentences but can contain verbless clauses or sentences without subjects [ ]. Abbreviations, enumerations, and citations of conversations, medical terms, and opinion-related words are used frequently in medical blog posts and websites. As stated in the study by Korkontzelos et al [ ], \u201cin social media, users rarely use technical terms.\u201d Moreover, emoticons are very often used to convey emotion or to give contextual information to correctly understand a message (such as irony or sarcasm). The corpus processed in this research confirmed these observations. \n\n\n### Limitations \n  \nThere are 2 major limitations of the PKDE4J tool with regard to the objectives of this paper. First, PKDE4J was initially developed for the processing of well-structured biomedical texts and not for social media text. This issue has a relatively less impact on this paper given that the entity extraction task is based on dictionaries. However, for tasks such as part-of-speech and sentence parsing needed for the extraction of relations, the informality of social media text poses a challenge. Second, the lack of terms from the dictionaries as well as lexical and semantic ambiguities lowered the performance of the system. For instance, abbreviations and acronyms can have multiple interpretations, and this can lead to ambiguities. In the current version of the system, these types of ambiguities are not handled. Consequently, all occurrences of   ALL   found in the corpus were extracted, even those that do not refer to the disease   Acute Lymphocytic Leukemia  . Also, the lexical unit   back   has sometimes been falsely recognized as a body part. \n\n\n### Conclusions \n  \nData from social media platforms devoted to health can provide valuable information about the experiences of the patients involved. In this paper, we reported the application of an information extraction approach using the PKDE4J tool to detect, extract, and visualize chronic disease entities and relations and to identify characteristics of the social media language in a corpus collected from Reddit. \n\nIn the Results section, we showed which disease entities are frequently mentioned and which are the most frequent relation pairs. Relation extraction demonstrated that the most frequent relation pair is the   disease-anatomy   pair and the subject-object relation pattern in the social media language is the use of the first-person pronoun provided that people share personal experiences. \n\nAlthough data privacy and information sharing is becoming a major concern in research and legal frameworks, such as the General Data Protection Regulation law, have begun to set boundaries for the storage and sharing of information generated by users, it is interesting that despite those concerns, users are willing to share private health information in open social networks. \n\nFurther research should focus on the enrichment of dictionaries and adaptation of rules to common usages of social media language and the processing of emoticons for the sentiment analysis task. Finally, the identification of the type of semantic relations and the evaluation on the relation extraction results should be performed to assess the performance of the system in this task. \n\n\n \n", "metadata": {"pmcid": 6595941, "text_md5": "dc27faa905d9fe0d7fa8c23db85eda3e", "field_positions": {"authors": [0, 105], "journal": [106, 124], "publication_year": [126, 130], "title": [141, 263], "keywords": [277, 318], "abstract": [331, 2566], "body": [2575, 40776]}, "batch": 1, "pmid": 31199327, "doi": "10.2196/12876", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6595941", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6595941"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6595941\">6595941</a>", "list_title": "PMC6595941  Mining of Textual Health Information from Reddit: Analysis of Chronic Diseases With Extracted Entities and Their Relations"}
{"text": "Cho, Hyejin and Kim, Baeksoo and Choi, Wonjun and Lee, Doheon and Lee, Hyunju\nSci Data, 2022\n\n# Title\n\nPlant\u00a0phenotype relationship corpus for biomedical relationships between plants and phenotypes\n\n# Keywords\n\nData publication and archiving\nData acquisition\n\n\n# Abstract\n \nMedicinal plants have demonstrated therapeutic potential for applicability for a wide range of observable characteristics in the human body, known as \u201cphenotype,\u201d and have been considered favorably in clinical treatment. With an ever increasing interest in plants, many researchers have attempted to extract meaningful information by identifying relationships between plants and phenotypes from the existing literature. Although natural language processing (NLP) aims to extract useful information from unstructured textual data, there is no appropriate corpus available to train and evaluate the NLP model for plants and phenotypes. Therefore, in the present study, we have presented the plant-phenotype relationship (PPR) corpus, a high-quality resource that supports the development of various NLP fields; it includes information derived from 600 PubMed abstracts corresponding to 5,668 plant and 11,282 phenotype entities, and demonstrates a total of 9,709 relationships. We have also described benchmark results through named entity recognition and relation extraction systems to verify the quality of our data and to show the significant performance of NLP tasks in the PPR test set. \n  \n  \n \n\n# Body\n \n## Background & Summary \n  \nWith rapid technological advancements, data from various fields have been accumulated regardless of the domain, and the accumulated text collection contains innovative information that has not been structured thus far . In particular, the biomedical domain contains useful information, such as those pertaining to new drug resources, and many studies continue to be conducted for the collection of meaningful information for the identification and exploration of interesting patterns based on such data sources . However, most data remain unorganized because the volume of new data continues to increase faster than our ability to process such data to construct and analyze meaningful information; thus, manual conversion of data into structured forms is impossible . Therefore, essential goals of natural language processing (NLP) include the extraction of meaningful data and the construction of significant information from unstructured text in an efficient and accurate manner . Currently, the use of deep learning-based techniques has recently ushered impressive improvements in the accuracy of many applications . Additionally, NLP models can be improved by implementing a deep learning approach because many researchers have demonstrated the achievement of state-of-the-art results across many fundamental NLP tasks and the highest scores . \n\nIn pharmaceutical development, plants are known for their therapeutic potential over thousands of years, indicating the possibility of obtaining a wide range of natural products from trees, shrubs, herbs, and crops with pharmaceutical capabilities . Particularly, medicinal plants and plant-derived medicines are widely used as therapeutic agents in traditional cultures and their efficacy can be verified by numerous clinical studies and medical records based on accumulated experience. Although natural compounds within plants may cause grave side effects, many modern pharmacological drugs are derived from medicinal plants as vital resources . Owing to such characteristics, many researchers have examined the therapeutic effects of various botanicals by elucidating the mechanisms of action of these plants. As interest in the development of new drugs from medicinal plants has increased, scientific investigations into the beneficial effects of plants include the extraction of various sources of information such as those pertaining to disease prevention and health promotion in addition to clinical treatment; thus, the definition of the entity type that can encompass such sources is necessary. Herein, the term \u201cphenotype\u201d refers to a wide range of characteristics observable in a human. To extract novel information on plants and phenotypes that have not been organized based on the accumulated literature, the NLP technique can be applied using deep learning techniques. In other words, for improvements in deep learning techniques, a free large-scale and well-constructed corpus may play an important role and it reflects the performance of deep learning models. \n\nThese issues highlight the necessity of creating a large-scale and high-quality dataset for NLP tasks. All biomedical NLP communities share the features of remarkable efforts into the performance of high-quality manual annotations of biomedical literature. In the biomedical domain, widely used datasets include 2010 i2b2/VA  and ShARe/CLEF  for clinical texts, NCBI disease corpus  for diseases, BioCreative II Gene Mention corpus (BC2GM)  and JNLPBA  for gene and proteins, BioCreative V Chemicals Disease Relationship (BC5CDR)  for diseases and chemicals, CHEMDNER  for drug and chemicals, LINNAEUS  and Species-800  for species, and the Plant corpus  for plant entities. Moreover, several biomedical relation corpora exist for elucidating various relationships between biomedical entities such as AIMed , BioInfer , CHEMPROT , DDI , EU-ADR , GAD , CoMAGC , Plant-Disease , and Plant-Chemical . Most text corpora contain a detailed markup of several types of entities and relationships in a limited number of abstracts or articles but cannot reflect the relationships between plants and phenotypes mentioned in biomedical publications. \n\nHere, we present the plant-phenotype relationship (PPR) corpus, a resource established to support the development and evaluation of various tasks and to extract new information in the biomedical domain. Using the proposed guidelines, we manually annotated 600 abstracts from the PubMed database. In the PPR corpus, 5668 plant and 11,282 phenotype mentions were annotated, and 9709 relationships between them including \u201cIncrease,\u201d \u201cDecrease,\u201d \u201cAssociation,\u201d and \u201cNegative\u201d were annotated. As the PPR corpus is split into three types\u2014train, development, and test sets\u2014we suggest that our corpus will be invaluable for advancing and evaluating text-mining techniques for biomedical NLP tasks. The PPR corpus is publicly available for various studies in the biomedical domain. \n\n\n## Methods \n  \n### Selection of candidate abstracts \n  \nTo construct the corpus, annotators independently annotated the mentions of plants and phenotypes and their relationships in the given candidate abstracts. In this section, we describe the process of selecting and annotating candidate abstracts. \n\nWe first automatically extracted data on 13,408,586 scientific abstracts from the PubMed database using PubTator  with automated annotations from state-of-the-art text mining systems for biomedical entities. PubTator is used along with DNorm  and SR4GN  to pre-tag disease and species names in the articles, respectively. However, these tools are not sufficient for the detection of plant and phenotypic entities. For the annotation of entity names, an entity mention with its offset of the location in texts must be identified. In the case of plants, we utilized LingPipe , a dictionary-based named entity recognition tool, with a plant name dictionary derived from the NCBI Taxonomy database  to pre-annotate plant names in the abstracts. The NCBI Taxonomy dictionary contains 151,250 concepts and 315,173 terms in English, Chinese, and Latin. To improve the accuracy, we removed stopwords from the dictionary, which frequently appear in the texts but are not related to plants, such as anemia (Taxonomy ID: 12939), lens (Taxonomy ID: 3863), laser (Taxonomy ID: 62990), NAME (Taxonomy ID: 55581), and thymus (Taxonomy ID: 49990). For phenotypes, we applied the deep learning named entity recognition (NER) model  trained by using the NCBI disease corpus to extract data on disease terms . We also used MetaMap , a configurable application for mapping biomedical text to the UMLS Metathesaurus, to retrieve information about additional clinical terms. Therefore, we combined the results of both NER models into a pre-annotated phenotype. \n\nUsing the pre-annotated mentions of plants and phenotypes, a total of 704,372 co-occurrence sentences from 469,567 pre-annotated abstracts were obtained, in which at least one plant name and phenotype name co-occurred. We then randomly selected 600 candidate abstracts from the pre-annotated abstracts. In spite of pre-processing which helped to remove stopwords for plant names and to include disease NER results, inappropriate entities remained in the candidate abstracts. Therefore, heuristic post-processing with additional annotations is necessary to precisely define the type of plant and phenotype desired. During the annotation task, annotators independently annotated the mentions of plants, phenotypes, and their relationships in the candidate abstracts. Figure\u00a0  depicts the workflow of our corpus construction, and details of the annotation guidelines are described in the next section.   \nThe pipeline of the PPR corpus construction. \n  \n\n\n### Annotation guidelines \n  \nAnnotation guidelines were established to improve inter-annotator agreement for the manual annotation task. The workflow of the PPR corpus construction involved the following two main annotation steps: a mention-level annotation and a relation-level annotation. Here, we describe how annotators annotated mentions and relations from the candidate abstracts. The guidelines include the annotation process of plant and phenotype mentions as the mention-level annotation and highlight the relationships between them as relation-level annotations. Thus, guidelines were categorized for entity annotation and relationship annotation. In the annotation step, the brat rapid annotation tool (BRAT) , an intuitive web-based tool for text annotation supported by natural language processing (NLP) technology, was used to maximize the annotation efficiency. \n\nAt this stage, the annotators checked whether the pre-annotated mentions in the abstracts were correctly annotated. If the pre-annotation was incorrectly added due to NER errors, the annotators corrected them to the best of their knowledge. Moreover, most NER systems have been developed based on the sequence labeling method, in which each token is classified as a single label. For this reason, the entity annotations only focus on the longest entity, without the inner nested entities. To maximize the accuracy of the annotations, the annotators formulated guidelines for annotating mentions through the discussion. The annotators followed the guidelines for the annotation task. \n\n#### Annotation of plant mentions \n  \nAs previously mentioned, plant mentions were first pre-annotated using LingPipe with the NCBI Taxonomy dictionary. Details regarding the guidelines for the annotation of plant mentions are as follows.   \nAnnotators manually annotate plant names based on the pre-annotated mentions. In this step, the NCBI taxonomy dictionary was used to annotate the exact plant names. \n  \nAnnotators should annotate all synonyms in the dictionary. \n  \nIf the candidate plant mention contains both plant name with terms describing specific parts or extracts and its abbreviation, annotators annotate both the plant name excluding part or extract names and the abbreviation term as well. (e.g.,   several extracts of    Tripterygium wilfordii Hook F (TWHF)   ) \n  \nAnnotators do not annotate the words that represent part names of plants, for example, roots, stems, and leaves. (e.g.,    persimmon    leaf extract, the stem bark of    Catalpa ovata   ) \n  \nAnnotators do not annotate the terms describing the processing methods of plants to extract their active compounds, for example, extraction methods and cooking methods. (e.g.,   Korean red    ginseng    extract, water extracts of    Tochu   ) \n  \nThe plant-based products should not be annotated. (e.g., annotators do not annotate \u201c  chocolate  \u201d made of   cocoa   and \u201c  cigarette  \u201d made of   tobacco  .) \n  \nThe plant derived substances should not be annotated. (e.g., annotators do not annotate \u201c  caffeine  ,\u201d \u201c  rg3  ,\u201d and \u201c  lycopene  \u201d as a plant name.) \n  \nDo not annotate pre-annotated mentions derived from plant names as plants if they do not mean plant themselves. (e.g., \u201c  tobacco mosaic virus  \u201d is not a plant.) \n  \nThe scientific name of plants basically consists of a genus name and a specific epithet name, which refers to the species within the genus. The genus name is always mentioned first, followed by the mention of a specific epithet name. Annotators should consider words after a specific epithet\u2019s name (e.g., \u201c  activities of    Phryma leptostachya var. asiatica    Hara extract  \u201d). \n  \n\n\n#### Annotation of phenotype mentions \n  \nMetaMap with UMLS semantic types is widely utilized in various biomedical NLP tasks for all integrated concepts. As mentioned before, the term \u201cphenotype\u201d refers to any observable characteristic in the human body, including diseases. In this step, we annotated disease names as phenotypes using the deep learning NER model; we also used the results derived with the use of MetaMap that belong to the following UMLS semantic types: T019, T020, T033, T034, T037, T038, T039, T041, T046, T047, T048, T049, T050, T184, T190, and T191. The other names not included in the standard were not used for the annotation. Here, we divided the phenotypes into three categories as follows:   \n Positive phenotype (POS)  : phenotypes with effects that positively affect humans (e.g.,   recovery, anti-cancer  , and   anti-inflammatory  ). \n  \n Negative phenotype (NEG)  : phenotypes that are known to be harmful to human health and those that need to be medically healed to suppress the negative effects on humans (e.g.,   inflammation, breast cancer  , and   cervical carcinoma  ). \n  \n Neutral phenotype (NEU)  : phenotypes for which there exists an uncertainty regarding their distinguishment into positives or negatives (e.g.,   pregnancy, sweating, blood pressure  , and   fat weight  ). \n  \n\nAfter annotating the phenotype mentioned in the article, annotators should consider the category (POS, NEG, or NEU) to which the selected mention belongs according to the definitions presented above. Details regarding the guidelines for the annotation considered for phenotype mentions are described below.   \nAnnotators manually annotate mentions that satisfy only one of the three categories as the phenotype. \n  \nAnnotators should consider words in the form of noun phrases for deciding the scope of entity annotation (e.g., \u201c  acute phase of inflammation  \u201d and \u201c  injured the muscle  \u201d could be associated with \u201c  acute inflammation   [UMLS ID: C0333361]\u201d and \u201c  muscle injury   [UMLS ID: C0410256],\u201d respectively). \n  \nAnnotators should annotate the only function of an organism, organ, or tissue as a phenotype mention, not the organism, organ, or tissue itself (e.g., \u201c   liver disorders   \u201d in \u201c  to treat liver disorders  \u201d is the phenotype, but \u201c  liver  \u201d in \u201c  nuclear extracts of the liver  \u201d is not the phenotype). \n  \nAnnotators should consider the mention with quantitative concepts like weight, length, or concentration as the phenotype. Although a mention itself is not a phenotype in principle, when this mention appears together with a quantitative concept, it is regarded as a neutral phenotype (e.g., \u201c  body weight  \u201d in \u201c  increase in    body weight   ,\u201d and \u201c   high blood pressure   \u201d are all deemed phenotypes.) \n  \nIf there is a noun phrase containing a phenotype mention, and if it is significantly related to the phenotype, then annotators should annotate all terms in the phrase as the phenotype mention (e.g., the annotator annotates \u201c   oral squamous cell carcinoma   \u201d rather than \u201c  carcinoma  \u201d itself). \n  \nDo not annotate general terms such as \u201c  phenotype  ,\u201d \u201c  syndrome  ,\u201d \u201c  deficiency  ,\u201d and \u201c  complications  .\u201d \n  \nDo not include terms indicating species names like \u201c  human   and   mouse  \u201d (e.g., \u201cto suppress various   human    tumors   \u201d). \n  \nIf the pre-annotated mention contains the name of a cell line, annotators should annotate it after separating the phenotype mention and cell line name (e.g., in \u201cMDA-MB231   human breast cancer cell  \u201d, two mentions \u201c   MDA-MB231   \u201d and \u201c   breast cancer   \u201d should be separately annotated). \n  \nAnnotators should annotate \u201cany phenotype-induced symptom\u201d as a phenotype, but not use \u201cany chemical-induced symptom\u201d as a phenotype. In other words, annotators should only consider the symptom term present in the mention as a phenotype excluding \u201cchemical-induced.\u201d (e.g.,    diabetes-induced cardiomyopathy    includes whole words as the phenotype, but in case of \u201c  ethyl phenylpropiolate-induced ear edema  \u201d, \u201c  ear edema  \u201d is only the phenotype mention.) \n  \nDo not annotate simple substances such as \u201c  glucose  \u201d and \u201c  lipid  .\u201d \n  \nThe mention of the phenotype model and cell line is considered a negative phenotype. This includes the treatment or assay involving the phenotypes. \n  \nBacteria and viruses should be considered as a negative phenotype. \n  \n\n\n#### Annotation of PPRs \n  \nAnnotators should determine one class label for denoting the relationships between the annotated plants and phenotypes. The class labels for the PPR have been divided into four classes as follows:   \n Increase relationship\u00a0(Increase)  : A plant-derived compound contextually increases a specific phenotype (e.g., \u201c   Anti-cancer    effect of    Annona Muricata Linn    leaves crude extract   (   AMCE   )   on breast cancer cell line  . [PubMed ID: 27558166]\u201d). \n  \n Decrease relationship\u00a0(Decrease)  : A plant-derived compound contextually decreases a specific phenotype (e.g., \u201c  The flowers of    Prunus persica Batsch    have been used for    skin disorders    in East Asia from ancient times  . [PubMed ID: 11917253]\u201d). \n  \n Association relationship\u00a0(Association)  : A plant-derived compound is contextually related to a specific phenotype. However, it is difficult to define either an increase or decrease (e.g., \u201c  To the best of our knowledge, this is the first description of    acute hepatitis    associated with   T. capitatum administration  . [PubMed ID: 12072605]\u201d). \n  \n Negative relationship (No relation)  : Although a pair of plant and phenotype mentions is observed in the same sentence, there is no relationship between the two mentions. Particularly, the title is always considered as a negative relationship, even if a plant-derived compound is contextually related to a specific phenotype (e.g., \u201c  Differential effects of    Viscum album    extract IscadorQu on cell cycle progression and apoptosis in   cancer   cells. [PubMed ID: 15547686]\u201d, \u201c  Anti-obesity   action of oolong    tea   . [PubMed ID: 10094584 (Title)]\u201d). Note that in the released corpus, a negative relationship is not specified because any co-occurrence of plant and phenotype entities without specific relationship types (increase, decrease, and association) can be considered a negative relationship. \n  \n\n\n\n### Inter-annotator agreement (IAA) measurement \n  \nAs the corpus has been manually constructed by the annotators, the quality of corpus data, which is one of the most important issues in the annotation process of the corpus, relies on the knowledge of the annotators. As previously mentioned, the construction of the PPR corpus was organized in the mention-level annotation and relation-level annotation between mentions. Therefore, the inter-annotator agreement (IAA) was independently calculated at each annotation level. In this study, three different IAA measures were calculated to assess the accuracy of the corpus. First, a simple index measurement, defined as the proportion of agreement between the two annotators, is calculated as follows: where   N   represents the total number of annotation units. Note that we used the simple index as \u201cStrict matches\u00a0(Strict)\u201d for full-word matches and \u201cPartial matches\u00a0(Partial)\u201d for overlap matches. Second, the G-index was used to measure the overall inequality of the annotator\u2019s work and is calculated as follows: where   P   represents a simple index,   P   = 1/  k  , and   k   denotes the number of relation classes. Lastly, Cohen\u2019s kappa (  \u03ba  )\u00a0index  is the most frequently used index for calculating the overall agreement scores between two annotators. The kappa value is calculated as follows: where   P   represents the hypothetical probability of an agreement by chance. A kappa value of = 1 indicates complete agreement, and kappa = 0 indicates no agreement between the two annotators. According to the study conducted by Viera   et al  . , kappa values ranging from 0.61 to 0.80 denote \u201csubstantial\u201d agreement and those presenting with values 0.81 or above indicate \u201calmost perfect\u201d agreement. \n\n#### Disagreements \n  \nIn the annotation results, we found several fully disagreed cases and partially agreed cases between two annotators. According to our analysis, most of fully disagreed cases occurred in the following cases: (i) one of annotators did not recognize abbreviation for plant mention as plant (ex., SEG\u2009=\u2009semi-evergreen); (ii) one of annotators mistakenly identified the word, \u201cextract,\u201d which is a substance taken from a plant, as plant name; and (iii) one of annotators annotated terms related to cells, chemical levels, genes as phenotype mentions even though they were not included in the phenotype range (ex., human lung epithelial cells, nitric oxide level, and COX-2). Most of partially agreed cases appeared in the following cases: (i) one of annotators included the words corresponding to plant parts in the plant mention although they should not be included (ex., P. guajava leaf, Persicariae Rhizoma) and (ii) annotators mistakenly regarded chemical-induced disease/symptom or plant-induced disease/symptom as phenotype (ex., Tripterygium wilfordii-induced liver injury, colitis-associated colon cancer, and circulating tumor-related leukocytes). \n\n\n\n\n## Data Records \n  \nThe PPR corpus is the first corpus annotated with information on plant and phenotype entities and their relationships derived from PubMed abstracts. The PPR corpus consists of data from 600 non-redundant abstracts randomly extracted from the PubMed database, which contains 16,937 mention annotations (with 5,858 unique mentions) and 9,709 relation annotations (with 8,135 unique relations). To facilitate benchmarking experiments, the set of articles must be categorized into train, development, and test sets during corpus construction. We fixed the number of abstracts in development and test sets to 100 by referring to the NCBI disease corpus , as the separated corpus is useful for the development of new algorithms, the avoidance of overfitting, and the accurate evaluation of new models. Therefore, the PPR corpus was divided into 400, 100, and 100 articles as the train, development, and test sets, respectively. \n\nTable\u00a0  shows the overall statistics of entity and relation annotations in the PPR corpus, and the three data sets exhibit similar aspects of the number of mentions and relations, which renders increased utility to the corpus for training models. In addition, we compared overlapping of entities and relations among training, development, and test sets. Here, the overlap between entities means exactly the same in annotated mentions and their types. For the relations, the overlap means that both entities and their relationship have identical annotation. For plant entities, 242 out of 934 (25.91%) mentions in the development set and 180 out of 968 (18.60%) mentions in the test set overlapped with plant entities in the train set. For phenotype entities, 1,249 out of 1,803 (69.27%) mentions in the development set and 1,174 out of 1,812 (64.79%) mentions in the test set overlapped with phenotype entities in the train set. Since we used biomedical literature to build the PPR corpus and specifically defined observable characteristics in a human as a phenotype in this study, the phenotype names overlaps relatively more than plant names. For relationships, 45 out of 1,563 (2.88%) relations in the development set and 25 out of 1,569 (1.59%) relations in the test set overlapped with the relation of the train set. Because the PPR corpus contains various kinds of plant names and plant-phenotype relationships with little redundancies, NLP models trained by the PPR corpus may show robust performances any other data set.   \nOverall corpus statistics. \n  \nThe PPR corpus consists of training, development, and test sets for plant and phenotype name recognition and relation extraction tasks. \n  \n\nThe PPR corpus contains information regarding all sentences of abstracts and annotations of entities and relationships at the sentence level. Figure\u00a0  shows an example of our PPR corpus. All data fields are formatted in tab-delimited text files. For sentences, the fields consist of PubMed ID (e.g., 10072339) with a sentence number separating the underlined and plain sentence. For entities, the fields are as follows: PubMed ID (e.g., 10072339), start index (e.g., 0), end index (e.g., 17), text (e.g., \u201c  Facial dermatitis  \u201d), and entity type (e.g., \u201cNegative_phenotype\u201d). The fields for relationships are as follows: PubMed ID (e.g., 10072339), relationship type (e.g., \u201cIncrease\u201d), front entity information (e.g., \u201c0 17   Facial dermatitis   Negative_phenotype\u201d), and rear entity information (e.g., \u201c81 87   potato   Plant\u201d).   \nExample of the PPR corpus. The first line is a sentence obtained from the first sentence of an abstract (PubMed ID: 10072339), followed by annotated named entities and their relationships. The named entity information includes PubMed ID, start and end positions, annotated mention, and entity type. The relationship information consists of PubMed ID, relation type, and information related to the two entities. \n  \n\nThe PPR corpus is publicly available at two locations:   \nFigshare, an open scientific data repository . \n  \nA git repository of the project is accessible at  . \n  \n\n\n## Technical Validation \n  \n### Inter-annotator agreements \n  \nIn this annotation task, the construction of the PPR corpus included six phases comprising the inclusion of 100 abstracts per phase, and two annotators who were experienced in biomedical text-mining participated in the experiments. Table\u00a0  describes the overall IAA results of each phase between two annotators for the construction of the PPR corpus. The average \u201cstrict matches\u201d IAA scores for plant and phenotype mentions were 91.5% and 66.4%, respectively. When the \u201cpartial matches\u201d IAA scores were calculated, two mention-level annotations yielded IAA scores higher than those obtained using \u201cstrict matches.\u201d The average \u201cPartial\u201d IAA scores were 94.8% of the plant mentions and were 80.9% of the phenotype mentions, representing higher agreement with the consideration of plant mentions compared to phenotype mentions. The lowest agreement score for each entity is represented in Phase 1 of the phenotype entity annotation and Phase 6 of the plant entity annotation. The annotator disagreements of the phenotype entity in Phase 1 occurred when a few mentions were not present in the annotation guidelines. The conflicts were primarily attributable to the differences in the mention boundaries. We further assessed the IAA scores of the relation-level annotations based on the three criteria considered. Additionally, for relation-level annotations, the average IAA scores according to the simple index, G-index, and Cohen\u2019s kappa were estimated to be 92.2%, 90.7%, and 86.9%, respectively.   \nOverall inter-annotator agreement (IAA) results of each phase. \n  \nThe PPR corpus was annotated by two annotators at the mention- and relation-levels. Therefore, IAA was independently calculated at each annotation level, and three different IAA measures were calculated to assess the accuracy of the corpus. \n  \n\n#### IAA for entities \n  \nTable\u00a0  shows the overall statistics of the PPR corpus in comparison with the previously published corpora for biomedical NER tasks, grouped by entity types. All mentions in the PPR corpus are annotated as two types of entities, plant and phenotype names, in which 5,668 and 11,282 mentions are mapped, respectively. For plant type, although species corpora (LINNAEUS  and Species-800 ) contain the names of all organisms, including botanical terminology, the PPR corpus demonstrates the presence of a greater number of plant name annotations. To obtain insights into the diversity of plant names within the corpus, the PPR corpus is constructed based on the information derived from 600 abstracts, which is approximately three times greater than that of the plant corpus . An IAA score of the entity-level was obtained by correctly identifying the mentions classified by each entity type. Although the IAA score of plant mentions in the PPR corpus is inferior to that in the plant corpus  (91.5% vs 98.5%) because of the complicated annotation guideline of the PPR corpus, it also suggests a high level of agreement (\u201calmost perfect\u201d agreement).   \nStatistics of the biomedical NER corpora for the annotated entities. \n  \n\nFollowing the definition in the annotation guideline, it is observed that the phenotype refers to all events, such as positive, negative, and neutral effects on humans. Therefore, the proposed concept of phenotype includes disease and clinical terms. The NCBI disease corpus  comprises 6,892 disease mentions, and the BC5CDR corpus  is composed of 12,850 disease mentions, in which 8.7 mentions and 8.6 mentions per abstract are mapped, respectively. In contrast, clinical reports have a relatively considerable number of clinical term annotations in the corpora. For instance, the 2010 i2b2/VA corpus  comprises 88.2 mentions per report and the ShARe/CLEF  corpus includes 37.5 clinical terms per report. The PPR corpus contains 18.8 phenotype mentions per abstract, which includes more entity information than the published abstract-based disease corpora. The IAA score of phenotype mentions shows low performance compared with others because there were a few changes in the guidelines. However, the final PPR corpus was constructed using a sufficient disagreement resolution process to ensure accuracy. \n\n\n#### IAA for relations \n  \nTable\u00a0  represents the overall statistics of the PPR corpus, together with previously published corpora for biomedical RE tasks, grouped by relation types. The PPR corpus consists of 9,709 annotated PPRs in 600 PubMed abstracts and includes 16.2 relations per abstract. Although CHEMPROT  presents with a similar number of relations with the PPR corpus, it exhibits the presence of two relationships per abstract. Thus, the information in each abstract may not be sufficient. The IAA score of the plant-disease corpus  is similar to that of the PPR corpus; however, its ratio of relations per abstract is 6.6, which is relatively lower than that of the PPR corpus. The plant-chemical corpus  is a sentence unit corpus with only one relation type \u201ccontain\u201d used for describing that a plant contains chemicals. Other corpora related to diseases, disorders, and clinical terms, which are also a component of the phenotype, are shown in Table\u00a0 , including BC5CDR , EU-ADR , GAD , CoMAGC , and plant-disease . The PPR corpus includes a sufficient number of relations compared to these corpora, and the IAA result of the PPR corpus showed \u201calmost perfect\u201d agreement (kappa\u2009=\u20090.869). Moreover, a ratio of relations per abstract of the PPR corpus is as high as 16.2 relations while that of the plant-disease corpus is 6.6.   \nStatistics of the biomedical RE corpora for the annotated relationships. \n  \n\n\n\n### The evaluation techniques \n  \nAs an application of the PPR corpus, we used NER and performed relation extraction (RE) tasks to the corpus. NER is one of the most widely known text mining-related tasks, which involves recognition of numerous domain-specific entities in the biomedical text, and RE is another commonly studied NLP task to classify relationships between the recognized named entities in text. For the NLP tasks with the best performance, most researchers previously used various combinations of hidden layers such as deep neural networks and conditional random fields architectures . Recently established deep learning methods, especially contextualized language models such as BERT , have resulted in significant improvements in many NLP tasks, including NER and RE. Therefore, we considered fine-tuning BERT-based models such as BERT , BioBERT , BlueBERT , SciBERT , and PubMedBERT .   \n BERT  : BERT is a contextual language representation model using pre-training deep bidirectional representations from unlabeled text. Instead of conducting traditional left-to-right language modeling, BERT is trained on two tasks: a masked language model (MLM) by predicting randomly masked tokens and a next sentence prediction (NSP) by predicting whether two sentences follow each other. BERT demonstrates a simple architecture based on the transformer and shows powerful performance in various NLP tasks, while illustrating the potential of the fine-tuning approach. \n  \n BioBERT  : BioBERT is a domain-specific language representation model designed for biomedical text and is initialized with the checkpoint of BERT, followed by training of the BERT model using PubMed abstracts and PubMed Central full-text articles. BioBERT achieves SOTA performance in various biomedical NLP tasks with minimal task-specific fine-tuning, while requiring only minimal architectural modifications. \n  \n BlueBERT  : Similar to BioBERT, BlueBERT is recognized as another variant of BERT, which is initialized with BERT and is further pre-trained using information available in PubMed abstracts and clinical notes derived from MIMIC-III. The standard approach of utilization of BERT-based models, such as BioBERT, is initialized with application of the BERT model, followed by continuous conduction of the pre-training process with MLM and NSP using their respective corpora. \n  \n SciBERT  : SciBERT is a variant of BERT-based models and demonstrates the same architecture as that exhibited by BERT. While BERT was pre-trained using general-domain corpora, SciBERT was pre-trained using information available in a greater number of scientific papers that consist of complete textual content based on computer science and biomedical domains. Previously, the vocabulary was considered the same as the original BERT model generated using information available in Wikipedia and BookCorpus. A major disadvantage of this approach is that vocabulary is not representative of the target biomedical domain. Therefore, they constructed a new in-domain vocabulary for their scientific text corpora, called SciVocab, to overcome the problem of the continual pre-training approach. \n  \n PubMedBERT  : PubMedBERT is another pre-trained language model exhibiting the same architecture as that demonstrated by BERT. Unlike those observed in the mixed-domain pre-training models, the weights of the PubMedBERT model were not initialized with those of BERT during pre-training. They constructed an in-domain vocabulary of the target biomedical domain and pre-trained it using information available in PubMed abstracts and additional data in full-text PubMed Central articles. The PubMedBERT model was pre-trained using information available in PubMed abstracts and additional data in full-text PubMed Central articles, which comprised data derived from 14 million PubMed abstracts with 3 billion words and contained 21 GB of textual data in total. \n  \n\nUsing the PPR corpus, we used five types of BERT-based models for biomedical NER and RE tasks between plant and phenotype entities and compared the performance of pre-trained models. For fair comparison, all parameters of the BERT-based models are set to the default values described in BERT . Figure\u00a0  illustrates the fine-tuning techniques of NER and RE using the BERT-based models. In the NER task, the NER process involves the classification of entities with proper boundaries and types based on informal texts. To recognize the entity in texts, each token in the input sentence is assigned the BERT-based classifier, and the label of each token is determined through the probabilities calculated by using the SoftMax function. In the biomedical domain, entity names are usually extremely complex and specific; hence, the vocabulary of models must contain all tokens in sentences. The anonymization process prevents the RE model from exhibiting bias toward specific words or tokens. For simplicity, we anonymized target entities in a sentence by replacing the plant entity with \u201c@Plant$\u201d and the phenotype entity with \u201c@Phenotype$.\u201d As the RE task is usually considered as a classification issue at the sentence or sequence level, the achievement of a representation of a certain number of dimensions for the input sequence is necessary. We presented the sentence using the \u201c[CLS]\u201d token of BERT\u2019s last hidden layer despite common practice . Particularly, the embeddings of the \u201c[CLS]\u201d token typically act as pooling token embeddings representing the whole sequence for downstream tasks. For the RE task, the label of the input sentence is decided based on the probabilities of relation classes using the SoftMax function.   \nBERT-based fine-tuning model architectures. The input sentence is \u201cThe   tumor  \u00a0specific cytotoxicity of dihydronitidine from   Toddalia asiatica Lam   (PubMed ID: 16465544).\u201d In this case, \u201ctumor\u201d is annotated with the negative phenotype, and \u201cToddalia asiatica Lam\u201d is the plant mention. Figure (  A  ) represents the BERT-based NER model, and Figure (  B  ) shows the BERT-based RE model. \n  \n\nOur evaluation metrics were micro-F1, macro-F1, and weighted-F1 scores. In such cases, precision(p) was defined as the number of true positives divided by the number of predictions, recall(r) was defined as the number of true positives divided by the number of annotations, and F1-score(f) was defined as the harmonic mean of precision and recall. Micro-F1 score is used to enumerate the global true positives, false positives, and false negatives, whereas macro-F1 is used to convey the average unweighted class scores. Since macro-F1 is often used to assign equal weights to both frequent and infrequent classes, we must consider the entity type distribution. Weighted-F1 scores are used to denote the average \u201cweighted\u201d class scores to consider class imbalance in the PPR corpus. \n\n\n### Named entity recognition \n  \nFor named entity recognition, we evaluate performance via strict matching, which helps to evaluate both the boundary and entity types of mentions. Table\u00a0  shows the comparison of NER performance for precisions, recalls, and F-scores of the five BERT-based models which consist of two types of evaluation experiments. The first approach is to use a divided corpus where the training set is used to train a model, the development set is used to optimize the model during training time, and the test set is used to evaluate the performance of the model. Since we always use the same data for evaluation under all cases, this method can be used to evaluate the benchmark system. The second approach is the well-known   k  -fold cross-validation method.   k  -fold cross-validation is an objective approach for examining the accuracy of statistical prediction methods. Thus, there is no need to artificially separate the corpus into train and test sets. In this study, we used the method of 5-fold cross-validation\u00a0(  k  \u2009=\u20095). In the PPR test set, although BioBERT demonstrated the obtainment of a relatively lower macro recall than that obtained by using PubMedBERT (86.80% vs. 87.07%), BioBERT exhibited the achievement of the best F1 scores for all types of evaluations. In the 5-fold cross-validation method, the fine-tuned BioBERT outperformed all other models.   \nEvaluation of the BERT fine-tuned models to recognize plant and phenotype mentions based on the conduction of two types of evaluation experiments. \n  \n\nTable\u00a0  represents performance comparison of BERT-based models based on the target entity types used to divide plant and phenotype names: ALL, PLT, PHE, PLT/PHE, and POS/NEG/NEU. Originally, the PPR corpus consisted of four types of plants and three subtypes of phenotypes (negative, positive, and neutral), as \u201cALL\u201d in Table\u00a0 . The model of \u201cPLT\u201d considers only plant mentions. The \u201cPOS/NEG/NEU\u201d model recognizes only phenotype mentions such as positive, negative, and neutral phenotypes, respectively, whereas the models of \u201cPHE\u201d recognize the only phenotype mentions regardless of the subtypes of phenotype mentions. The model of \u201cPLT/PHE\u201d is used simultaneously with two types of named entities as plant and phenotype mentions regardless of the subtypes. Similar to Table\u00a0 , BioBERT in Table\u00a0  shows outperformed performance with respect to all cases except for the macro recall. Moreover, BioBERT outperformed other types of ALL, PHE, PLT/PHE, and POS/NEG/NEU.   \nPerformance comparison of BERT-based models based on target entity types used to divide plant and phenotype names: a total of four types for plants and three subtypes of phenotypes (ALL), only plant (PLT), the only phenotype (PHE), two types of named entities as plant and phenotype mentions regardless of the subtypes (PLT/PHE), and only phenotype mentions such as positive, negative, neutral phenotypes (POS/NEG/NEU). \n  \n\n\n### Relation information extraction \n  \nFor relation information extraction, we also performed experiments to elucidate RE performance using five different BERT-based models. Table\u00a0  demonstrates the results of each RE model shown in the same manner using the NER evaluation. In the PPR test set, the fine-tuned PubMedBERT demonstrated the obtainment of the best micro-, macro-, and weighted F-scores compared with the other models. After PubMedBERT, BioBERT showed the second-best performance. For a more detailed comparison, we also performed 5-fold cross-validation in the RE task. In the 5-fold cross-validation, although BlueBERT demonstrated the achievement of a higher micro F-score (87.50%) than the others, PubMedBERT showed the achievement of best macro and weighted F-scores of 68.66% and 87.34%, respectively. Since the PPR corpus contains data on a relatively small number of association relations, the macro F-score seems to be lower than the micro-and weighted F-scores. In conclusion, although BioBERT showed the best performance in the NER task, PubMedBERT generally showed the best performance in the RE task.   \nEvaluation of BERT fine-tuned models to extract information on the relationships between plant and phenotype mentions based on the conduction of two types of evaluation experiments. \n  \n\n\n\n## Usage Notes \n  \nThe PPR corpus is made available under the Creative Commons Attribution 4.0 International Public License (CC-BY). The codes to run this corpus is available at  . Compared to other plant-disease corpora, the PPR corpus defined positive and negative phenotypes, which is a wide range of characteristics observable in a human, so that it can explain a wider range about medical information than the plant-disease corpus. Negative phenotype is known to be harmful to human health and need to be medically healed to suppress the negative effects on humans. Thus, disease name in our corpus is a part of the negative phenotypes. \n\n \n", "metadata": {"pmcid": 9135735, "text_md5": "3cc3bcaa62932fa711a69caf9e80f77b", "field_positions": {"authors": [0, 77], "journal": [78, 86], "publication_year": [88, 92], "title": [103, 197], "keywords": [211, 259], "abstract": [272, 1472], "body": [1481, 43657]}, "batch": 1, "pmid": 35618736, "doi": "10.1038/s41597-022-01350-1", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9135735", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9135735"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9135735\">9135735</a>", "list_title": "PMC9135735  Plant\u00a0phenotype relationship corpus for biomedical relationships between plants and phenotypes"}
{"text": "Armengol-Estap\u00e9, Jordi and Soares, Felipe and Marimon, Montserrat and Krallinger, Martin\nGenomics Inform, 2019\n\n# Title\n\nPharmacoNER Tagger: a deep learning-based tool for automatically finding chemicals and drugs in Spanish medical texts\n\n# Keywords\n\nmachine learning\nnatural language processing\nneural networks (computer)\n\n\n# Abstract\n \nAutomatically detecting mentions of pharmaceutical drugs and chemical substances is key for the subsequent extraction of relations of chemicals with other biomedical entities such as genes, proteins, diseases, adverse reactions or symptoms. The identification of drug mentions is also a prior step for complex event types such as drug dosage recognition, duration of medical treatments or drug repurposing. Formally, this task is known as named entity recognition (NER), meaning automatically identifying mentions of predefined entities of interest in running text. In the domain of medical texts, for chemical entity recognition (CER), techniques based on hand-crafted rules and graph-based models can provide adequate performance. In the recent years, the field of natural language processing has mainly pivoted to deep learning and state-of-the-art results for most tasks involving natural language are usually obtained with artificial neural networks. Competitive resources for drug name recognition in English medical texts are already available and heavily used, while for other languages such as Spanish these tools, although clearly needed were missing. In this work, we adapt an existing neural NER system, NeuroNER, to the particular domain of Spanish clinical case texts, and extend the neural network to be able to take into account additional features apart from the plain text. NeuroNER can be considered a competitive baseline system for Spanish drug and CER promoted by the Spanish national plan for the advancement of language technologies (Plan TL). \n \n\n# Body\n \n Availability:   PharmacoNER Tagger can be accessed at  . \n\n## Introduction \n  \nChemical compounds and drugs represent a key biomedical entity of common interest for a range of scientific disciplines, including medicine and clinical research, pharmacology as well as basic biomedical research. Due to the growing amount of clinical texts, medical and biomedical literature and medicinal chemistry patents, a systematic approach to recognize such entities in order to semantically enrich these documents and enable further relation extraction task is needed. Text mining and information extraction efforts are gradually being adopted to empower the transformation of unstructured running texts to more structured data representations that can be directly consumed by content analytics and information retrieval infrastructures [ ]. \n\nSemantically labeling drug mentions in medical documents is a necessary step to allow a range of downstream text mining applications, like drug-resistance, drug dosage recognition, duration of medical treatments, drug repurposing, detection of medication-related allergies, drug-drug interactions or disease-drug relations. Drug names show some inherent characteristics such as the constraints imposed by the International Nonproprietary Names (INN) system by exploiting certain suffixes and prefixes to group them according to certain attributes. For instance, the suffix \u2018-caine\u2019 is usually used for local anesthetics. These properties are not only useful for healthcare professionals and pharmacologists to memorize drug names, their attributes and fining them in the clinical literature, they are also constituting valuable attributes for automatically labeling them through language technology tools. \n\nNamed entity recognition (NER) [ ] is the task of automatically extracting and identifying mentions of entities of interest in running text, typically through their mention offsets or by classifying individual tokens whether they belong to entity mentions of not. Among many other uses, such as entity-aware machine translation, NER can also be used for the automated information extraction and anonymization [ ] of medical texts. Early techniques involved the use of hand-crafted rules. The most successful approach consisted in using Conditional Random Fields (CRFs), a graph-based model for modeling sequences that still relied on features defined by humans. In the last years, Natural Language Processing (NLP) techniques have been improved with the use of word embeddings, that is to say, projections of words to vector spaces such that among other findings, the distance can be used as a semantic similarity measure [ ]. Deep learning (e.g., artificial neural networks [ANNs] with a high number of hidden layers) has proved to be extremely useful in solving NER problems. In this article, we introduce PharmacoNER tagger, NER software based on NeuroNER [ ] adapted to the domain of Spanish medical texts and improved with non-domain-specific features. \n\n### The problem \n  \nMedical documents mentioning drugs and chemical compounds offer a relevant source of information regarding the patients\u2019 treatment. For instance, doctors and researchers may be interested in finding health records of patients treated with a particular kind of compound in order to manually analyze them or use them in biomedical NLP pipelines. Nevertheless, the task of manually detecting these entities would be expensive and time-consuming, which prompts for the development of NER systems for automating this task. In the context of chemical compounds, NER is usually referred to as chemical entity recognition (CER). \n\n\n### Challenges in NER for medical texts and chemical compounds \n  \nNER itself has proven to be a challenging task because of the ambiguity and complexity of human languages. In the particular case of CER, additional difficulties arise. \n\nFirst of all, not all chemical names present distinctive patterns as far as name segments or chemical word morphology are concerned. CER systems are particularly sensitive with regard to both spelling errors and the tokenization strategy of choice since chemical documents usually exhibit hyphenated text segments or variable use of parentheses, brackets, dots, and other punctuation signs. In addition, chemical documents tend to be loaded with acronyms and abbreviations, which are one of the main sources of false positives. \n\nAnother characteristic that makes CER difficult is the fact that the detection of mention boundaries is especially cumbersome when long chemicals or modifiers are present. A more in-depth description of difficulties in tagging chemicals can be found at [ ]. \n\n\n### Other approaches \n  \nIn the past, most NER systems were rule-based or leveraged graph-based models such as CRFs, while nowadays the task is usually approached with deep learning techniques. In the case of CER systems, we can highlight some relevant works. \n\nChemicalTagger [ ] parsed the text with a formal grammar and domain-specific regular expressions (regex) and used the parse tree in combination with the Part-of-Speech (POS) tags obtained with an English tagger in order to extract chemical entities. They reported achieving machine-annotator agreements of 88.9% for phrase recognition and 91.9% for phrase-type identification. The test corpus was assembled by compiling 50 paragraphs from the experimental sections of polymer synthesis related papers. \n\nCheNER [ ], a tool for the identification of chemical entities and their classes in the biomedical literature, is based on a combination of CRFs, regex and dictionary matching. An F-Score of about 73% was reported, with minor differences depending on the particular experiment. For evaluation, two corpora containing 3,500 documents with approximately 29,500 annotated chemical entities divided into several classes were used. \n\nOn the other hand, MetaMap [ ], a tool for recognizing Unified Medical Language System (UMLS) concepts in the text, mapped entities to UMLS concepts by the means of rules and a parser. \n\nIn CHEMDNER [ ], a drug and chemical names extraction competition, the top scoring teams obtained F-scores of 87.39% and 88.2% depending on the particular task, employing CRFs and domain-specific rules. These scores were obtained in the CHEMDNER corpus, a dataset with chemical compounds manually annotated by domain experts. \n\nWith regard to drug recognition, a vast survey on approaches and resources was presented in Liu et al.\u2019s study [ ], including classical machine learning techniques and hybrid approaches. The authors suggested using deep learning techniques for drug recognition as future work. \n\nA system for drug name recognition and classification in biomedical texts was introduced in Segura-Bedmar et al.\u2019s study [ ]. By combining information obtained by the UMLS MetaMap Transfer (MMTx) program and nomenclature rules recommended by the World Health Organization (WHO) INNs Program, a broader coverage than previous approaches based on standalone MMTx was achieved. \n\nFor an extensive review of text mining techniques for drugs and chemical compounds, readers can refer to Vazquez et al.\u2019s study [ ]. \n\nNevertheless, the aforementioned works used techniques that have some important limitations [ ]. Dictionary-based methods have problems dealing with name variability since chemical naming exhibits high variation. Rule construction, apart from requiring domain knowledge and an extensive manual workload, has proven to be difficult to scale to new rules and to transfer to slightly different domains. On the other hand, statistical models such as CRFs and classic machine learning algorithms are not powerful enough for detecting some patterns and require a certain amount of feature engineering. All these previous approaches focused on data in English, despite the fact that there is a considerable amount of textual data and repositories (e.g., MEDES, Scielo, Ibecs, or Cuiden), as well as a large medical end users community interested in tools for processing data in Spanish. Regarding other languages, this work [ ] presented a system based on CRFs that was capable of recognizing entities in French biomedical documents. \n\n\n### Our proposal \n  \nOur proposal involves the adaption of a state-of-the-art NER (i.e., NeuroNER) based on deep learning to the particular domain of the Spanish medical texts in order to identify identities as proteins or other components. Remarkably, the system we base our work on has a CRF layer apart from deep learning components. The original system takes tokens as its input, while our proposal involves using additional features in the neural network. \n\n\n### Key innovations \n  \nOur contribution consists in extending an existing neural NER system with additional features that had worked with classic CERs. In particular, POS tags, gazetteer features, and affixes features have been added to the network and were as a pre-preprocessing step. Different experiments have been conducted in order to determine whether these features improved the performance of the neural network. The extension of the neural network is domain and language-agnostic, while the pre-processing is domain-dependent and specific tools for Spanish medical texts have been used. By the neural network itself being language-agnostic we mean that our system could be adapted to other languages by building gazetteer and affix dictionaries and using a POS tagger trained for the required language. \n\n\n\n## NeuroNER \n  \nNeuroNER, an open-source program for named-entity recognition, achieved state-of-the-art performance by having a neural architecture containing three layers: (1) character-enhanced token-embedding layer, (2) a label prediction layer, and (3) a label sequence optimization layer. \n\nRecurrent neural networks (RNNs) are ANNs such that the computational graphs have cycles, which are used for dealing with sequences [ ]. Long short term memory (LSTM) [ ] are gated units widely used in RNN implementations. A Bidirectional LSTM (Bi-LSTM) is an LSTM unit such that two inputs layers with opposite directions are present, which allows the network to get information from both the past and future states simultaneously. NeuroNER makes use of Bi-LSTM for the character-enhanced token-embedding layer, while character embeddings are previously passed through their own LSTM. \n\nIn  , we show a representation of the ANN in the original NeuroNER implementation. We must notice that this is just a snippet of the structure of one token. The full ANN has connections for the RNN between tokens from the character-enhanced token embedding upwards. For details about this model, readers can refer to Dernoncourt et al. [ ]. \n\nIn addition, NeuroNER has the ability to load pre-trained token embeddings, which may increase final performance and decrease training time. For more details about NeuroNER, please refer to the following related works: Dernoncourt et al. [ , ]. \n\n\n## Proposed Method \n  \nThe following features have been added to both the neural network and the dataset parser of NeuroNER: \n\n\u2012 POS features: with our modification, PharmacoNER tagger can parse annotations (both in BRAT or CONLL formats) of POS tags, encode them with a one-hot encoding for each token and include this in the token embedding layer of the network. \n\n\u2012 Gazetteer features: our modified version of NeuroNER is able to parse a text file with terms related to the target entities, build a dictionary and assign a positive value for the words that are found in this dictionary. This feature is then concatenated with the character-enhanced token embedding layer. A file with the dictionary has to be supplied by the user. \n\n\u2012 Affixes features: PharmacoNER tagger can use information regarding affixes in a similar manner as the gazetteer features, but using regular expressions to detect whether a particular word has an affix related to the target entities. A file of affixes shall be supplied by the user. \n\nNotice that the aforementioned features are domain and language independent. The final software can be found in Github ( ), with information about installation and samples. In  , the adaptation of the neural network is also depicted, showing the differences from  . \n\n\n## Evaluation \n  \n### Data \n  \nFor evaluating PharmacoNER tagger, we used the SPACCC dataset, a manually annotated corpus of 1,000 clinical cases written in Spanish and annotated with mentions of chemical compounds, drugs, genes, and proteins. In particular, the set of labels consists of Normalizables (4,398 labels), No Normalizables (50 labels), Proteins (3,009 labels), and Unclear (167 labels). Note that Normalizables and No Normalizables refer to chemical entities. For instance, the word \u201ctriglic\u00e9ridos\u201d (triglycerides) is annotated as Normalizable, while \u201creceptores de progesterona\u201d (progesterone receptors) is labelled as Proteins. One can notice that the dataset is heavily imbalanced. The dataset has not been released yet due to an ongoing shared task ( ). \n\n\n### Embeddings \n  \nEight different pre-trained word embeddings have been tested, from the Spanish Billion Word Corpus (SBWC) and data crawled from SciELO ( ) and health-related categories of Wikipedia. SBWC data is general-domain, while SciELO and Wikipedia are in-domain. \n\n\u2012 General domain (Universidad de Chile Spanish Word Embeddings,  ): \n\nFastText embeddings from SBWC \n\nGloVe embeddings from SBWC \n\n- Domain-specific (PlanTL Embeddings,  ): \n\nFastText Scielo \n\nFastText medical Wikipedia \n\nFastText Scielo + Health Wikipedia \n\nWord2Vec Scielo \n\nWord2Vec medical Wikipedia \n\nWord2Vec Scielo + Health Wikipedia \n\n\n### Sampling \n  \nThe SPACCC dataset was split with the following proportions: 80% for training, 10% for validation, and 10% for test. \n\nSince the dataset is imbalanced, a stratified splitting has been applied, in order to approximately maintain the same proportions of labels in the three sets. \n\n\n### SPACCC POS tagger and freeling \n  \nFreeLing [ ] is an open source language analysis tool suite, released under the Affero GNU General Public License of the Free Software Foundation. FreeLing was used for building the SPACCC POS Tagger ( ), which is a POS tagger trained with Spanish medical texts. SPACCC POS Tagger was successfully applied to the provided data as a pre-processing step. \n\n\n### Gazetteer \n  \nThe gazetteer dictionary was built with a pharmaceutical \u201cnomenclator\u201d maintained by the Spanish government ( ), which consists of drug names and active components of medicines. For instance, indomethacin is listed as an active ingredient, and therefore the appearances of this particular word are marked as belonging to the dictionary. \n\n\n### List of affixes \n  \nThe list of affixes was retrieved from a biomedical website ( ) and translated into Spanish. The final format is a tab-separated file, with the type of affix (e.g., suffix, prefix, or root), the affix in English, the affix in Spanish, an example in English and the Drug class. For PharmacoNER tagger, only the columns type, and affix in Spanish are used. \n\n\n### Metrics \n  \nThe CONLL evaluation script is used for evaluating the results. The following overall metrics are computed: accuracy, precision, recall, F1 score. For details about the metrics, please refer to Krallinger et al.\u2019s study [ ]. For the sake of comparison, we will utilize the F1 score as the primary performance metric. \n\n\n### Results with the best configuration \n  \nTo train our system, we tested several configurations with various levels, which are now described: \n\n\u2012 Pre-trained embeddings: Total of eight models, from general domain and in-domain. \n\n\u2012 POS: Yes (using POS) and No (without POS information) \n\n\u2012 Gazetteer: Yes (with gazetteer) and No (without gazetteer) \n\n\u2012 Affixes: Yes (with affix information) and No (without affix information). \n\nWe trained our systems until convergence, that is until no improvement was identified in the development set for at least 10 epochs. In addition, given that the number of samples from the classes No Normalizables and Unclear are too small, we decided to discard those classes. This led our NER system to have the goal of identifying Proteins and Normalizable chemicals. First of all, to narrow down the search space, we turned off all additional features and just tested the word embeddings. Once the best embedding was found, we fixed it and tested the other configurations. \n\nIn  , we present the result of our system regarding the metrics mentioned in Section 4.7. The final setting is: POS with Gazetteer turned one. One can notice that there is not a great difference between the validation and the test set in all metrics, thus meaning that no overfitting occurred. \n\n\n\n## Conclusion \n  \nIn this article we have introduced PharmacoNER tagger, a neural NER based on an existing state-of-the-art system, NeuroNER. We extended and especially adapted to the particular domain of chemical compounds in Spanish medical texts. The results in the validation set of the experiments with the SPACCC dataset have showed that the best configuration consisted of the FastText Scielo + Health Wikipedia embeddings, the POS, and the gazetteer, which have proven to be an additional source of information that can be leveraged by neural networks. In the test set, the aforementioned best configuration obtained an F1 score of 89.06. The extension to the network is domain-agnostic and could be used in other fields, but the pre-processing steps have been specifically designed for our domain. \n\nCER involves additional challenges to the ones already present in generic NER. Our work shows that some of the sources of additional information typically used in previous CER systems based on non-neural techniques, such as affixes, can be leveraged as well by state-of-the-art neural NER systems. \n\nWe foresee that this resource will be a valuable contribution not only to semantically enhance medical texts in Spanish for pharmacological and drug-related information, but it also highlights a useful approach on how to implement medical NER taggers for languages other than English. Moreover, PharmacoNER tagger will be a useful competitive baseline system and design principle for the upcoming PharmacoNER BioNLP 2019 shared task. \n\n \n", "metadata": {"pmcid": 6808625, "text_md5": "1b742a48c35859b225cded3dc2427508", "field_positions": {"authors": [0, 88], "journal": [89, 104], "publication_year": [106, 110], "title": [121, 238], "keywords": [252, 324], "abstract": [337, 1909], "body": [1918, 20189]}, "batch": 1, "pmid": 31307130, "doi": "10.5808/GI.2019.17.2.e15", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6808625", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6808625"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6808625\">6808625</a>", "list_title": "PMC6808625  PharmacoNER Tagger: a deep learning-based tool for automatically finding chemicals and drugs in Spanish medical texts"}
{"text": "Hong, Zhi and Pauloski, J. Gregory and Ward, Logan and Chard, Kyle and Blaiszik, Ben and Foster, Ian\nFront Mol Biosci, 2021\n\n# Title\n\nModels and Processes to Extract Drug-like Molecules From Natural Language Text\n\n# Keywords\n\ncoronavirus disease-19\ncoronavirus disease-19 open research dataset challenge\nnamed entity recognition\nlong-short term memory\ndata mining\n\n\n# Abstract\n \nResearchers worldwide are seeking to repurpose existing drugs or discover new drugs to counter the disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). A promising source of candidates for such studies is molecules that have been reported in the scientific literature to be drug-like in the context of viral research. However, this literature is too large for human review and features unusual vocabularies for which existing named entity recognition (NER) models are ineffective. We report here on a project that leverages both human and artificial intelligence to detect references to such molecules in free text. We present 1) a iterative model-in-the-loop method that makes judicious use of scarce human expertise in generating training data for a NER model, and 2) the application and evaluation of this method to the problem of identifying drug-like molecules in the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198,875 papers. We show that by repeatedly presenting human labelers only with samples for which an evolving NER model is uncertain, our human-machine hybrid pipeline requires only modest amounts of non-expert human labeling time (tens of hours to label 1778 samples) to generate an NER model with an F-1 score of 80.5%\u2014on par with that of non-expert humans\u2014and when applied to CORD\u201919, identifies 10,912 putative drug-like molecules. This enriched the computational screening team\u2019s targets by 3,591 molecules, of which 18 ranked in the top 0.1% of all 6.6\u00a0million molecules screened for docking against the 3CLPro protein. \n \n\n# Body\n \n## 1 Introduction \n  \nThe Coronavirus Disease (COVID-19) pandemic, caused by transmissible infection of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has resulted in tens of millions of diagnosed cases and over 1,450,000 deaths worldwide ( ); straining healthcare systems, and disrupting key aspects of society and the wider economy. It is thus important to identify effective treatments rapidly via discovery of new drugs and repurposing of existing drugs. Here, we leverage advances in natural language processing to enable automatic identification of drug candidates being studied in the scientific literature. \n\nThe magnitude of the pandemic has resulted in an enormous number of academic publications related to COVID-19 research since early 2020. Many of these articles are collated in the COVID-19 Open Research Dataset Challenge (CORD-19) collection ( ;  ). With 198,875 articles at the time of writing, that collection is far too large for humans to read. Thus, tools are needed to automate the process of extracting relevant data, such as drug names, testing protocols, and protein targets. Such tools can save domain experts significant time and effort. \n\nExtracting named entities from scientific texts has been studied for more than 2\u00a0decades. Prior work relied on matching tokens (words) in text to entries in existing databases or ontologies ( ;  ). However, for our task, existing drug databases like DrugBank cover both   too much  , in that they include entities of many types (for example, \u201crabbit\u201d is in DrugBank as an allergen), and   too little  : because the creation of such databases is time-consuming, they cannot keep up with the new entities in the latest COVID\u201919-related publications (\u223c200\u00a0k papers in the CORD\u201919 corpus), which is exactly what we want to extract here. \n\nMachine learning (ML) and deep learning (DL) methods have also been used for identifying named entities from free text. While less dependent on a comprehensive ontology, they need labeled data for training. Traditional training data collection employs a brute-force approach, collecting a large corpus and labeling each and every word indiscriminatingly ( ). The time and human resources involved is enormous. We, in contrast, had access to just a few assistants who worked on labeling only during free time in their day job. Thus we needed a more careful approach of selective labeling to maximize their effectiveness. \n\nTowards this goal, we describe here how we have tackled two important problems: creating labelled training data via judicious use of scarce human expertise, and applying a named entity recognition (NER) model to automatically identify drug-like molecules in text. We are looking not only for novel drugs under development, but also any small molecule drug that has been used to treat patients with COVID\u201919 or similar infectious diseases like SARS and MERS. In the absence of expert-labeled data for the growing COVID literature, we employ an iterative model-in-the-loop collection process inspired by our previous work ( , ) and demonstrate that it can build a high quality training set without input from domain scientists. We first assemble a small bootstrap set of human-verified examples to train a model for identifying similar examples. We then iteratively apply the model, use human reviewers to verify the predictions for which the model is least confident, and retrain the model until the improvement in performance is less than a threshold. (The human reviewers were administrative staff without scientific backgrounds, with time available for this task due to the pandemic.) \n\nHaving collected adequate training data via this model-guided human annotation process, we then use the resulting labeled data to re-train a NER model originally developed to identify polymer names in materials science publications ( ) and apply this trained model to CORD-19. We show that the labeled data produced by our approach are of sufficiently high quality than when used to train NER models, which achieves a best F-1 score of 80.5%\u2014roughly equivalent to that achieved by non-expert humans. \n\nThe labeled data, model, and model results are all available online, as described in  . \n\n\n## 2 Materials and Methods \n  \nWe aim to develop and apply new computational methods to mine the scientific literature to identify small molecules that have been investigated or found useful as antiviral therapeutics. For example, processing the following sentence should allow us to determine that the drug   sofosbuvir   has been found effective against the Zika virus: \u201cSofosbuvir, an FDA-approved nucleotide polymerase inhibitor, can efficiently inhibit replication and infection of several ZIKV strains, including African and American isolates.\u201d ( ). \n\nThis problem of identifying drug-like molecules in text can be divided into two linked problems: 1) identifying references to small therapeutic molecules (\u201cdrugs\u201d) and 2) determining what the text says about those molecules. In this work, we consider potential solutions to the first problem. \n\nA simple way to identify entities in text that belong to a specialized class (e.g., drug-like molecules) is to refer to a curated list of valid names, if such is available. In the case of drugs, we might think to use DrugBank ( ) or the FDA Drug Database ( ), both of which in fact list   sofosbuvir  . However, such databases are not in themselves an adequate solution to our problem, for at least two reasons. First, they are rarely complete. The tens of thousands of entity names in DrugBank and the FDA Drug Database together are just a tiny fraction of the billions of molecules that could potentially be used as drugs. Second, such databases may be overly general: DrugBank, for example, includes the terms \u201crabbit\u201d and \u201ccalcium,\u201d neither of which have value as antiviral therapeutics. In general, the use of any such list to identify entities will lead to both false negatives and false positives. We need instead to employ the approach that a human reader might follow in this situation, namely to scan text for words that appear in contexts in which a drug name is likely to appear. In the following, we explain how we combine human and artificial intelligence for this purpose. \n\n### 2.1 Automated Drug Entity Extraction From Literature \n  \nFinding strings in text that refer to drug-like molecules is an example of Named Entity Recognition (NER) ( ), an important NLP task. Both grammatical and statistical (e.g., neural network-based) methods have been applied to NER; the former can be more accurate, but require much effort from trained linguists to develop. Statistical methods use supervised training on labeled examples to learn the contexts in which entities of interest (e.g., drug-like molecules) are likely to occur, and then classify previously unseen words as such entities if they appear in similar contexts. For instance, a training set may contain the sentence \u201cRibavirin was administered once daily by the i. p. route\u201d ( ), with   ribavirin   labelled as   Drug  . With sufficient training data, the model may learn to assign the label   Drug   to   arbidol   in the sentence \u201cArbidol was administered once daily per os using a stomach probe\u201d ( ). This learning approach can lead to general models capable of finding previously unseen candidate molecules in natural language text. \n\nThe development of effective statistical NER models is complicated by the many contexts in which names can occur. For example, while the contexts just given for   ribavirin   and   arbidol   are similar, both are quite different from that quoted for   sofosbuvir   earlier. Furthermore, authors may use different wordings and sentence structures: e.g., \u201cgiven by i. p. injection once daily\u201d rather than \u201cadministered once daily by the i. p. route.\u201d Thus, statistical NER methods need to do more than learn template word sequences: they need to learn more abstract representations of the context(s) in which words appear. Modern NLP and NER systems do just that ( ). \n\n#### 2.1.1 SpaCy and Keras-Long-Short Term Memory Models \n  \nWe consider two NER models in this paper, SpaCy and a Keras long-short term memory (LSTM) model. Both models are publicly available on DLHub ( ) and GitHub, as described in  . \n\nSpaCy ( ;  ) is an open source NLP library that provides a pre-trained entity recognizer that can recognize 18 types of entities, including PERSON, ORGANIZATION, LOCATION, and PRODUCT. Its model calculates a probability distribution of a word over the entity types, and outputs the type with the highest probability as the predicted type for that word. When pre-trained on the OntoNotes five dataset of over 1.5\u00a0million labeled words ( ), the SpaCy entity recognizer can identify supported entities with 85.85% accuracy. However, it does not include drug names as a supported entity class, and thus we would need to retrain the SpaCy model on a drug-specific training corpus. Unfortunately, there is no publicly available corpus of labeled text for drug-like molecules in context. Thus, we need to use other methods to retrain this model (or other NER models), as we describe in  . \n\nWhile SpaCy is easy to use, it lacks flexibility: its end-to-end encapsulation does not expose many tunable parameters. Thus we also explore the use of a Keras-LSTM model that we developed in previous work for identification of polymers in materials science literature ( ). This model is based on the Bidirectional LSTM network with a conditional random field (CRF) layer added on top. It takes training data labeled according to the \u201cIOB\u201d schema. The first word in an entity is given the label \u201cB\u201d (Beginning), the following words in the same entity are labeled \u201cI\u201d (Inside), and non-entity words are labeled \u201cO\u201d (outside). During prediction, the Bi-LSTM network tries to assign one of \u201cIOB\u201d to each word in the input sentence, but it has no awareness of the validity of the label sequence. The CRF layer is used on top of Bi-LSTM to lower the probability of invalid label sequences (e.g., \u201cOIO\u201d). \n\nWe compare the performance of SpaCy and Keras-LSTM models under various conditions in  . \n\n\n#### 2.1.2 Model-In-The-Loop Annotation Workflow \n  \nWe address the lack of labeled training data by using   (and see  ) to assemble a set of human- and machine-labeled data from CORD-19 ( ). In describing this process, we refer to paragraphs labeled automatically via a heuristic or model as   silver   and to silver paragraphs for which labels have been corrected by human reviewers as   gold  . We use the Prodigy machine learning annotation tool to manage the review process: reviewers are presented with a silver paragraph, with putative drug entities highlighted; they click on false negative and false positive words to add or remove the highlights and thus produce a gold paragraph. Prodigy saves the corrected labels in standard NER training data format. \n  \nModel-in-the-loop Annotation Workflow \n    \nOverview of the training data collection workflow, showing the three phases described in the text and with the parameter values used in this study. Each phase pulls paragraphs from the CORD-19 dataset (blue dashed line) according to the   Select   criteria listed (yellow shaded box). Phases B and C repeatedly update the weights for the NER model (green arrows) that they use to identify and label uncertain paragraphs; human review (yellow and gold arrows) corrects those silver paragraphs to yield gold paragraphs. Total human review work is \u223c278 + 600+960 = 1838 paragraphs. \n  \nOur algorithm involves three main phases, as follows. In the first   bootstrap   phase, we assemble an initial test set of gold paragraphs for use in subsequent data acquisition. We create a first set of silver paragraphs by using a simple heuristic: we select   N   paragraphs from CORD-19 that contain one or more words in DrugBank with an Anatomical Therapeutic Chemical Classification System (ATC) code, label those words as drugs, and ask human reviewers to correct both false positives and false negatives in our silver paragraphs, creating gold paragraphs. In the subsequent   build test set   phase, we repeatedly use all gold paragraphs obtained so far to train an NER model; use that model to identify and label additional silver paragraphs, and engage human reviewers to correct false positives and false negatives, creating additional gold paragraphs. We repeat this process until we have   N   initial gold paragraphs. \n\nIn the third   build labeled set   phase, we repeatedly use an NER model trained on all human-validated labels obtained to date, with the   N   gold paragraphs from the bootstrap phase used as a test set, to identify and label promising paragraphs in CORD-19 for additional human review. To maximize the utility of this human effort, we present the reviewers only with paragraphs that contain one or more   uncertain words  , i.e., words that the NER model identifies as drug/non-drug with a confidence in the range (min, max). We continue this process of model retraining, paragraph selection and labeling, and human review until the F-1 score improves by less than   \u03f5  . \n\nThe behavior of this algorithm is influenced by six parameters:   N  ,   N  ,   N  ,   \u03f5   min, and max.   N   and   N   are the number of paragraphs that are assigned to human reviewers in the first and subsequent steps, respectively.   N   is the number of examples in the test set.   \u03f5   is a threshold that determines when to stop collecting data. The min and max determine the confidence range from which words are selected for human review. In the experimental studies described below, we used   N   = 278,   N   = 120,   N   = 500,   \u03f5   = 0, min = 0.45, and max = 0.55. \n\nThe NER model used in the model-in-the-loop annotation workflow to score words might also be viewed as a parameter. In the work reported here, we use SpaCy exclusively for that purpose, as it integrates natively with the Prodigy annotation tool and trains more rapidly. However, as we show below, the Keras-LSTM model is ultimately somewhat more accurate when trained on all of the labeled data generated, and thus is preferred when processing the entire CORD-19 dataset: see   and  . \n\nThis semi-automated method saves time and effort for human reviewers because they are only asked to verify labels that have already been identified by our model to be uncertain, and thus worth processing. Furthermore, as we show below, we find that we do not need to engage biomedical professionals to label drugs in text: untrained people, armed with contextual information (and online search engines), can spot drug names in text with accuracy comparable to that of experts. \n\nWe provide further details on the three phases of the algorithm in the following, with numbers in the list referring to line numbers in  . \n\n\n#### A) Bootstrap \n  \n  \n1. We start with the 2020\u201303-20 release version of the CORD-19 corpus, which contains 44,220 papers ( ). We create  , a random permutation of its paragraphs from which we will repeatedly fetch paragraphs via next ( ). \n  \n2. We bootstrap the labeling process by identifying as   the 2,675 items in the DrugBank ontology with a Anatomical Therapeutic Chemical Classification System (ATC) code attached (eliminating many, but not all, drug-like molecule entities). \n  \n3. We create an initial set of silver paragraphs,  , by selecting   N   paragraphs from   that include a word from  . \n  \n4. We engage human reviewers to remove false positives and label false negatives in  , yielding an initial set of gold paragraphs,  . \n  \n\n\n#### B) Build test set \n  \n  \n5. We expand the test set that we will use to evaluate the model created in the next phase, until we have   N   validated examples. \n  \n6. We train the NER model on 60% of the data collected to date and evaluate it on the remaining 40%, to create a new trained model,  , with improved knowledge of the types of entities that we seek. \n  \n7. We use the probabilities over entities returned by the model to select, as our   N   new silver paragraphs,  , paragraphs that contain at least one uncertain word (see above). \n  \n8. We engage human reviewers to convert these new silver paragraphs,  , to gold,  . \n  \n9. We add the new gold paragraphs,  , to the bootstrap set  . \n  \n\n11\u201312. Having assembled at least   N   validated examples, we select the first   N   as the test set,  , and use any remaining examples to initialize the new gold set,  . \n\n\n#### C) Build labeled set \n  \n  \n13. We assemble a training set  , using the test set   assembled in the previous phases for testing. This process continues until the F-1 score stops improving see  . \n  \n\n14\u201317. Same as Steps 6\u20139, except that we train on   and test on  . Human reviewers are engaged to review new silver paragraphs and produce new gold paragraphs, which are then added to   instead of  . \n\n\n\n### 2.2 Data-Performance Tradeoffs in Named-Entity Recognition Models \n  \nAs noted in  , our model-in-the-loop annotation workflow requires repeated retraining of a SpaCy model. Thus we conducted experiments to understand how SpaCy prediction performance is influenced by model size, quantity of training data, and amount of training performed. \n\nAs the training data produced by the model-in-the-loop evaluation workflow are to be used to train an NER model that we will apply to the entire CORD-19 dataset, we also evaluate the Keras-LSTM model from the perspectives of big data accuracy and training time. \n\n#### 2.2.1 Model Size \n  \nWe first need to decide which SpaCy model to use for model-in-the-loop annotation. Model size is a primary factor that affects training time and prediction performance. In general, larger models tend to perform better, but require both more data and more time to train effectively. As our model-in-the-loop annotation strategy requires frequent model retraining, and furthermore will (initially at least) have little data, we hypothesize that a smaller model may be adequate for our purposes. \n\nTo explore this hypothesis, we study the performance achieved by the SpaCy medium and large models ( ) on our initial training set of 278 labeled paragraphs. We show in   the performance achieved by the two models as a function of number of training epochs. Focusing on the harmonic mean of precision and recall, the F-1 score (a good measure a model\u2019s ability to recognize both true positives and true negatives), we see that the two models achieve similar prediction performance, with the largest difference in F-1 score being around 2%. As the large model takes over eight times longer to train per epoch, we select the medium model for model-in-the-loop data collection. \n  \nPrecision, recall, and F-1 scores of medium and large SpaCy models trained on 278 examples. \n  \n\n#### 2.2.2 Word Embedding Models \n  \nThe Keras LSTM model requires external word vectors since, unlike SpaCy, it does not include a word embedding model. To explore the affect of different word embedding models we trained both BERT ( ), a top-performing language model developed by Google, and FastText ( ), a model shown to have outperformed traditional Word2Vec models such as CBOW and Skipgram in our previous work ( ). While Google has released pre-trained BERT models, and researchers often build upon these models by \u201cfine-tuning\u201d them with additional training on small external datasets, it is not suitable to our problem as the vocabulary used in the CORD-19 is very different than the datasets used to train these models.  Rather than use a pre-trained BERT model, we trained a BERT model on the CORD-19 corpus using a distributed neural network training framework from our previous work ( ). As the CORD-19 corpus is approximately 20% of the size of the training data used by Google used to train BERT, we reduced the word embedding size proportionally from 768 to 128 to avoid over-fitting. For FastText word embeddings we set the size to the default 120. \n\nWe used word embeddings derived from both models to train the Keras LSTM model on the same training and testing data collected in  . The model using FastText embeddings achived a slightly higher F-1 score (80.5%) than the model trained with BERT embeddings (78.7%). This result is likely due to the limited training data and embedding size. In short, the humongous BERT model requires an equally humongous amount of data to achieve the best performance, and without such it will not necessarily outperform other much smaller and less computationally intensive word embedding models. In the remainder of the paper, we use the FastText word embedding model. \n\n\n#### 2.2.3 Amount of Training Data \n  \nAs data labeling is expensive in both human time and model training time, it is valuable to explore the tradeoff between time spent collecting data and prediction performance. To this end, we manually labeled a set of 500 paragraphs selected at random from CORD-19 ( ) as a test set. Then, we used that test set to evaluate the results of training the SpaCy and Keras-LSTM models of   on increasing numbers of the paragraphs produced by our human-in-the-loop annotation process.   shows their F-1 score curves as we scale from 0 to 1,000 training samples. With only 100 training examples, SpaCy and Keras-LSTM achieve F-1 scores of 57 and 66%, respectively. SpaCy performs better than Keras-LSTM with fewer training examples (i.e., less than 300), after which Keras-LSTM overtakes it and maintains a steady 2\u20133% advantage as the number of examples increases. This result motivates our choice of Keras-LSTM for the CORD-19 studies in  . \n  \nTraining curves for the SpaCy and Keras models for different number of examples collected. \n  \nWe stopped collecting training data after 1,000 examples. We see in   that the performance of the SpaCy and Keras-LSTM models is essentially the same with 1,000 training examples as with 700 examples, with the F-1 score even declining when the number of available examples increases to 800 or 900. At 1,000 examples the F-1 score is greatest for both models. We conclude that the 1,000 training examples, along with the other 500 withheld as the test set, are best-suited to train our models. There are 4,244 and 1861 entities in the training and test set, respectively. \n\n\n#### 2.2.4 Training Epochs \n  \nPrediction performance is also influenced by the number of epochs spent in training. The cost of training is particularly important in a model-in-the-loop setup, as human reviewers cannot work while an model is offline for training. \n\n shows the progression of the loss, precision, recall, and F-1 values of the SpaCy model during 100 epochs of training with the initial 278 examples. We can see that the best F-1 score is achieved within 10\u201320 epochs. Increasing the number of epochs does not result in any further improvement. Indeed, F-1 score does not tell us all about the model\u2019s performance. Sometimes training for more epochs could lead to lower loss values while other metrics (such as precision, recall, or F-1) no longer improve. That would still be desirable because it means the model is now more \u201cconfident,\u201d in a sense, about its predictions. However, that is not the case here. As shown in  , after around 40 epochs the loss begins to oscillate instead of continuing downwards, suggesting that in this case training for 100 epochs does not result in a better model than only training for 20 epochs. \n  \nLoss, precision, recall, and F-1 of SpaCy model during training for 100 epochs on 278 paragraphs. \n  \n shows the progression of accuracy and loss value for the Keras-LSTM model with the initial 278 examples. In  , we see that validation accuracy improves as training accuracy increases during the first 50 epochs. After around epoch 50, the training and validation accuracy curves diverge: the training accuracy continues to increase but the validation accuracy plateaus. This trend is suggestive of overfitting, which is corroborated by  . After about 50 epochs, the validation loss curve turns upwards. Hence we choose to limit the training epochs to 64. After each epoch, if a lower validation loss is achieved, the current model state is saved. After 64 epochs, we test the model with the lowest validation loss on the withheld test set. \n  \nEvolution of the training and validation accuracy   (A)   and loss   (B)   as the Keras-LSTM model is trained for from 1 to 256 epochs. \n  \n\n\n\n## 3 Results \n  \nWe present the results of experiments in which we first evaluate the performance of our models from various perspectives and then apply the models to the CORD-19 dataset. \n\n### 3.1 Evaluating Model and Human Performance \n  \nWe conducted experiments to compare the performance of the SpaCy and Keras-LSTM NER models; compare the performance of the models against humans; determine how training data influences model performance; and analyze human and model errors. \n\n#### 3.1.1 Performance of SpaCy and Keras Named-Entity Recognition Models \n  \nWe used the collected data of   to train both the SpaCy and Keras-LSTM NER models of   to recognize and extract drug-like molecules in text. The SpaCy model used is the medium English language model en_core_web_md ( ). For the Keras model, the input embedding size is 120, the LSTM and Fully-connected hidden layers have a size of 32, and the dropout rate is 0.1. The model is trained for 64 epochs with a batch size of 64. We find that the trained SpaCy model achieved a best F-1 score of 77.3%, while the trained Keras-LSTM model achieved a best F-1 score of 80.5%, somewhat outperforming SpaCy. \n\nAs shown in  , the SpaCy model performs better than the Keras-LSTM model when trained with small amounts of training data\u2014perhaps because of the different mechanisms employed by the two methods to generate numerical representations for words. SpaCy\u2019s built-in language model, pre-trained on a general corpus of blog posts, news, comments, etc., gives it some knowledge about commonly used words in English, which are likely also to appear in a scientific corpus. On the other hand, the Keras-LSTM model uses custom word embeddings trained solely on an input corpus, which provides it with better understanding of multi-sense words, especially those that have quite different meanings in a scientific corpus. However, without enough raw data to draw contextual information from, custom word embeddings can not accurately reflect the meaning of words. \n\n\n#### 3.1.2 Comparison Against Human Performance \n  \nRecognizing drug-like molecules is a difficult task even for humans, especially non-medical professionals (such as our non-expert annotators). To assess the accuracy of the annotators, we asked three people to examine 96 paragraphs, with their associated labels, selected at random from the labeled examples. Two of these reviewers had been involved in creating the labeled dataset; the third had not. For each paragraph, each reviewer decided independently whether each drug molecule entity was labeled correctly (a true positive), was labeled as a drug when it was not (a false positive), or was not labeled (a false negative). If all three reviewers agreed in their opinions on a paragraph (the case for 88 of the 96 paragraphs), we accepted their opinions; if they disagreed (the case for eight paragraphs), we engaged an expert. \n\nThis process revealed a total of 257 drug molecule entities in the 96 paragraphs, of which the annotators labeled 201 correctly (true positives), labeled 49 incorrectly (false positives), and missed 34 (false negatives). The numbers of true positives and false negatives do not sum up to the total number of drug molecules because in some cases an annotator labeled not to a drug entity but the entity plus extra preceding or succeeding word or punctuation mark (e.g., \u201csofosbuvir,\u201d instead of \u201csofosbuvir\u201d) and we count such occurrences as false positives rather than false negatives. In this evaluation, the non-expert annotators achieved an F-1 score of 82.9%, which is comparable to the 80.5% achieved by our automated models, as shown in  . In other words, our models have performance on par with that of non-expert humans. \n\n\n#### 3.1.3 Effects of Training Data Quality on Model Performance \n  \nWe described in the previous section how review of 96 paragraphs labeled by the non-expert annotators revealed an error rate of about 20%. This raises the question of whether model performance could be improved with better training data. To examine this question, we compare the performance of our models when trained on original vs. corrected data. As we only have 96 corrected paragraphs, we restrict our training sets to those 96 paragraphs in each case. \n\nWe sorted the 96 paragraphs in both datasets so that they are considered in the same order. Then, we split each dataset into five subsets for K-fold cross validation (  K   = 5), with the first four subsets having 19 paragraphs each and the last subset having 20. Since   K   is set to five, the SpaCy and Keras models are trained five times. In the   i  th round, each model is trained on four subsets (excluding the   i  th) of each dataset. The   i  th subset of the corrected dataset is used as the test set. The   i  th subset of the original dataset is not used in the   i  th round. \n\nWe present the K-fold cross validation results in  ,  . The models performed reasonably well when trained on the original dataset, with an average F-1 score only 2% less than that achieved with the corrected labels. Given that the expert input required for validation is hard to come by, we believe that using non-expert reviewers is an acceptable tradeoff and probably the only practical way to gather large amounts of training data. \n  \nK-fold (  K   = 5) validation of the SpaCy model on 96 paragraphs with original vs. corrected labels. The first five rows are the results of each fold; the last row is the average F-1 score of the five folds. \n    \nK-Fold (K = 5) validation of the Keras-LSTM model on 96 paragraphs with original vs. corrected labels. The first five rows are the results of each fold; the last row is the average F-1 score of the five folds. \n  \n\n\n### 3.2 Applying the Trained Models \n  \nAfter training the models with the labeled examples, we applied the trained models to the entire CORD-19 corpus (2020\u201310-04 version with 198,875 articles) to identify potential drug-like molecules. Processing a single article takes only a few seconds; we adapted our models to use data parallelism to enable rapid processing of these many articles. \n\nWe ran the SpaCy model on two Intel Skylake 6,148 processors with a total of 40 CPU cores; this run took around 80 core-hours and extracted 38,472 entities. We ran the Keras model on four NVidia Tesla V100 GPUs; this run took around 40 GPU-hours and extracted 121,680 entities. We recorded for each entity the number of the times that it has been recognized by each model, and used those numbers as a voting mechanism to further determine which entities are the most likely to be actual drugs. In our experiments, \u201cbalanced\u201d entities (i.e., those whose numbers of detection by the two models are within a factor of 10 of each other) are most likely to appear in the DrugBank list. As shown in  , we sorted all extracted entities in descending order by their total number of detections by both models and compared the top 100 entities to DrugBank. We found that only 77% were exact matches to drug names or aliases, or 86% if we included partial matches (i.e., the extracted entity is a word within a multi-word drug name or alias in DrugBank). In comparison, among the top 100 \u201cbalanced\u201d entities, 88% were exact matches to DrugBank, or 91% with partial matches. \n  \nPercentage of detected entities that are also found in DrugBank, when running either on all words found by our model or on just the balanced subset, and with \u201cfound\u201d defined as either a full or partial match. \n  \nAlthough DrugBank provides a reference metric to evaluate the results, it is not an exhaustive list of known drugs. For instance, remdesivir, a drug that has been proposed as a potential cure for COVID-19, is not in DrugBank. We manually checked via Google searches the top 50 \u201cbalanced\u201d and top 50 \u201cimbalanced\u201d entities not matched to DrugBank, and found that 70% in the \u201cbalanced\u201d list are actual drugs, but only 26% in the \u201cimbalanced\u201d list. Looking at the false positives in these top 50 lists, the \u201cbalanced\u201d false positives are often understandable. For example, in the sentence \u201cELISA plate was coated with \u2026 and then treated for 1\u00a0h at 37.8\u00b0C with dithiothreitol \u2026 \u201d, the model mistook the redox reagent   dithiothreitol   for a drug entity, probably due to its context \u201ctreated with.\u201d On the other hand, we found no such plausible explanations for the false positives in the \u201cimbalanced\u201d list, where most false positives are chemical elements (e.g., silver, sodium), amino acids (e.g., cysteine, glutamine), or proteins (e.g., lactoferrin, cystatin). \n\nFinally, we compared our extraction results to the drugs being used in clinical trials, as listed on the United States National Library of Medicine website ( . We queried the website with \u201ccovid\u201d as the keyword and manually screened the returned drugs in the \u201cInterventions\u201d column to remove stopwords (e.g., tablet, injection, capsule) and dosage information (e.g., 2.5\u00a0mg, 2.5%) and only keep the drug names. Then we compared the top 50 most frequently appeared drugs to the automatically extracted drugs from literature. The \u201cbalanced\u201d entities extracted by both models matched to 64% of the top 50 drugs in clinical trial, whereas the \u201cimbalanced\u201d entities only matched to 6% in the same list. \n\nThe results discussed here are available in the repository described in  . \n\n\n### 3.3 Validating the Utility of Identified Molecules \n  \nWe are interested to evaluate the relevance to COVID research of the molecules that we extracted from CORD19. To that end, we compared our extracted molecule list against ZINC and Drugbank sets that a group of scientists at Argonne National Laboratory had used for computational screening. We found that our list contained an additional 3,591 molecules not found in their screening sets (filtered by their canonical SMILE strings). Applying their methods to screen those 3,591 molecules for docking against a main coronavirus protease, 3CLPro, revealed that 18 had docking scores in the top 0.1% of the 6.6\u00a0M ZINC molecules that they had screened previously ( )\u2014significantly more than the four that we would expect by chance. \n\nAs reported by  , those researchers have leveraged the outputs of our models in a computational screening pipeline that leverages HPC resources at scale, coupled with multiple artificial intelligence and simulation-based approaches (including leads from NLP), to identify high-quality therapeutic compounds to screen experimentally. A further manuscript, detailing the end-to-end process from data collection to simulation, incorporation of these results, cellular assays, and identification of high performing therapeutic compounds, is in preparation. \n\n\n\n## 4 Discussion: Analysis of Human and Model Errors \n  \nFinally, we explore the contexts in which human reviewers and models make mistakes. Specifically, we study the tokens that appear most frequently near to incorrectly labeled entities. To investigate the effects of immediate and long-distance context, we control, as   window size  , the maximum distance between a token and a entity for that token to be considered as \u201ccontext\u201d for that entity. \n\nOne difficulty with this analysis is that the most frequent tokens identified in this way were mostly stop words or punctuation marks. For instance, when the window size is set to three, the 10 most frequent tokens around mislabeled words are, in descending order, \u201ccomma (,),\u201d \u201cand,\u201d \u201cmg,\u201d \u201cperiod (.),\u201d \u201cright parenthesis ()),\u201d \u201cwith,\u201d \u201cof,\u201d \u201cleft parenthesis ((),\u201d \u201cis,\u201d and \u201cor.\u201d Only \u201cmg\u201d is neither a stop word nor punctuation mark. \n\nThose tokens provide little insight as to why human reviewers might have made mistakes, and furthermore are unlikely to have influenced reviewer decisions. Thus we exclude stopwords and punctuation marks when providing, in  , lists of the 10 most frequent tokens within varying window sizes of words that were   incorrectly   identified as molecules by human reviewers. \n  \nThe 10 most frequent tokens, excluding stopwords and punctuation marks, within various window sizes around entities incorrectly labeled by human reviewers. \n  \nWe see that there are indeed several deceptive contextual words. With a window size of one, the 10 most frequent tokens include \u201coral,\u201d \u201cdose,\u201d and \u201cintravenous.\u201d It is understandable that an untrained reviewer might label as drugs words that immediately precede or follow such context words. Similar patterns can be seen for window sizes of three and five. Without background knowledge to draw from, non-experts are more likely to rely on their experience gained from labeling previous paragraphs. One may hypothesize that after the reviewers have seen a few dozen to a few hundred paragraphs, those deceptive contextual words must have left a deep impression, so that when those words re-appear they are likely to label the strange unknown word close to them as a drug. \n\nTo investigate this hypothesis, we also explored the most frequent words around drug entities that are   correctly   labeled by human reviewers: see  . Interestingly, we found overlaps between the lists in  ,  : in all, three, four, and two overlaps for window sizes of one, three, and five, respectively, when treating all numerical values as identical. This finding supports our hypothesis that those frequent words around real drug entities may confuse human reviewers when they appear around non-drug entities. \n  \nThe 10 most frequent tokens, excluding stop words and punctuation marks, within various window sizes around entities correctly labeled by human reviewers. \n  \nWe repeat this comparison of context words around human and model errors while considering stopwords and punctuation marks.  ,   show the 20 most frequent tokens in each case. We see that 20\u201325% of the tokens in  , but only 5\u201310% of those in  , are   not   stop words or punctuation marks. As the model only learns its word embeddings from the input text, if a token often co-occurs with drug entities in the training corpus the model will treat it as an indication of drug entities near its presence, regardless of whether or not it is a stopword. This apparently leads the model to make incorrect inferences. Humans, on the other hand, are unlikely to think that stopword such as \u201cthe\u201d is indicative of drug entities, no matter how frequently they appear together. \n  \nThe 20 most frequent tokens, including stop words and punctuation marks, within various window sizes around entities incorrectly labeled by human reviewers. Words that are neither stop words nor punctuation words are in boldface. \n    \nThe 20 most frequent tokens, including stop words and punctuation marks, within various window sizes around entities incorrectly labeled by the Keras model. Words that are neither stop words nor punctuation words are in boldface. \n  \n\n## 5 Data Availability and Formats \n  \nWe have made our annotated training data, trained models, and the results of applying the models to the CORD-19 corpus publicly available online. ( ). \n\nIn order to facilitate training of various models, we published the training data in two formats\u2014an unsegmented version in line-delimited JSON (JSONL) format, and a segmented version in Comma Separated Value (CSV) format. The JSONL format contains the most comprehensive information that we have collected on the paragraphs in the dataset. We choose JSONL format rather than a JSON list because it allows for the retrieval of objects without having to parse the entire file. A JSON object in the JSONL file has the following structure:   \n\u2022 text: The original paragraph stored as a string without any modification. \n  \n\u2022 tokens: The list of tokens from text after tokenization. \n  \n\u2022 text: The text of the token as a string. \n  \n\u2022 start: The index of the first character of the token in text. \n  \n\u2022 end: The index of the first character after the token in text. \n  \n\u2022 id: Zero-based numbering of the token. \n  \n\u2022 spans: The list of spans (sequences of tokens) that are labeled as named entities (drugs) \n  \n\u2022 start: The index of the first character of the span in text. \n  \n\u2022 end: The index of the first character after the span in text. \n  \n\u2022 token_start: The index of the first token of the span in text. \n  \n\u2022 token_end: The index of the last token of the span in text. \n  \n\u2022 label: The label of the span (\u201cdrug\u201d) \n  \n\nAnother commonly adopted labeling scheme for NER datasets is the \u201cIOB\u201d labeling scheme, in which the original text is first tokenized and each token is assigned a label \u201cI,\u201d \u201cO,\u201d or \u201cB.\u201d The label \u201cB (eginning)\u201d means the corresponding token is the first in a named entity. A label \u201cI (nside)\u201d is given to every token in a named entity except for the first token. All other tokens gets the label \u201cO (utside)\u201d which means they are not part of any named entity. The aforementioned JSONL data are converted according to the IOB scheme and stored in Comma Separated Value (CSV) files with one training example per line. Each line consists of two columns: a first of tokens that made up of the original texts, and a second of the corresponding IOB labels for those tokens. In addition to a different labeling scheme, the samples in the CSV files are segmented, meaning that each sentence is treated as a training sample instead of an entire paragraph. This structure aligns with that used in standard NER training sets such as CoNLL03 ( ). \n\nThe trained SpaCy and Keras models and the results of applying the models to the 198,875 articles in the CORD-19 corpus are also available in this GitHub repo. Additionally, the pre-trained SpaCy model is provided as a cloud service via DLHub ( ;  ). (The Keras model could not be hosted there due to compatibility issues with DLHub.) This cloud service allows researchers to apply the model to any texts they provide with as few as four lines of code. \n\n\n## 6 Conclusion and Future Directions \n  \nWe have presented a human-machine hybrid pipeline for collecting training data for named entity recognition (NER) models. We applied this pipeline to create a NER model for identifying drug-like molecules in COVID-19-related research papers. Our pipeline facilitated efficient use of valuable human resources by presenting human labellers only with samples that were most likely to confuse the NER model. We explored various trade-offs, including model size, number of training samples, and training epochs, to find the right balance between model performance and time-to-result. In total, human reviewers working with our pipeline validated labels for 278 bootstrap samples, 1,000 training samples, and 500 test samples. As this work was performed in conjunction with other tasks, we cannot accurately quantify the total effort taken to collect and annotate the above training and test samples, but it was likely around 100 person-hours. \n\nNER models trained with these training data achieved a best F-1 score of 80.5% when evaluated on our collected test set. Our models correctly identified 64% of the top 50 drugs that are in clinical trials for COVID-19, and when applied to 198,875 articles in the CORD-19 collection, identified 10,912 molecules with potential therapeutic effects against the SARS-CoV-2 coronavirus. The code, model, and extraction results are publicly available. Our work provided an additional 3591 SMILES strings to scientists at Argonne National Laboratory to be used in computational screening pipelines, of which 18 ranked in the top 0.1% of the molecules screened.   have leveraged the outputs of our models in a computational screening pipeline that leverages HPC resources at scale to identify high-quality therapeutic compounds to screen experimentally. A further manuscript detailing the end-to-end process of identifying high performing therapeutic compounds is in preparation. \n\n \n", "metadata": {"pmcid": 8435623, "text_md5": "4828f50a6be63b46d3576ff8d7539386", "field_positions": {"authors": [0, 100], "journal": [101, 117], "publication_year": [119, 123], "title": [134, 212], "keywords": [226, 364], "abstract": [377, 1967], "body": [1976, 46178]}, "batch": 1, "pmid": 34527701, "doi": "10.3389/fmolb.2021.636077", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8435623", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8435623"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8435623\">8435623</a>", "list_title": "PMC8435623  Models and Processes to Extract Drug-like Molecules From Natural Language Text"}
{"text": "Chen, Qingyu and Lee, Kyubum and Yan, Shankai and Kim, Sun and Wei, Chih-Hsuan and Lu, Zhiyong\nPLoS Comput Biol, 2020\n\n# Title\n\nBioConceptVec: Creating and evaluating literature-based biomedical concept embeddings on a large scale\n\n# Keywords\n\n\n\n# Abstract\n \nA massive number of biological entities, such as genes and mutations, are mentioned in the biomedical literature. The capturing of the semantic relatedness of biological entities is vital to many biological applications, such as protein-protein interaction prediction and literature-based discovery. Concept embeddings\u2014which involve the learning of vector representations of concepts using machine learning models\u2014have been employed to capture the semantics of concepts. To develop concept embeddings, named-entity recognition (NER) tools are first used to identify and normalize concepts from the literature, and then different machine learning models are used to train the embeddings. Despite multiple attempts, existing biomedical concept embeddings generally suffer from suboptimal NER tools, small-scale evaluation, and limited availability. In response, we employed high-performance machine learning-based NER tools for concept recognition and trained our concept embeddings, BioConceptVec, via four different machine learning models on ~30 million PubMed abstracts. BioConceptVec covers over 400,000 biomedical concepts mentioned in the literature and is of the largest among the publicly available biomedical concept embeddings to date. To evaluate the validity and utility of BioConceptVec, we respectively performed two intrinsic evaluations (identifying related concepts based on drug-gene and gene-gene interactions) and two extrinsic evaluations (protein-protein interaction prediction and drug-drug interaction extraction), collectively using over 25 million instances from nine independent datasets (17 million instances from six intrinsic evaluation tasks and 8 million instances from three extrinsic evaluation tasks), which is, by far, the most comprehensive to our best knowledge. The intrinsic evaluation results demonstrate that BioConceptVec consistently has, by a large margin, better performance than existing concept embeddings in identifying similar and related concepts. More importantly, the extrinsic evaluation results demonstrate that using BioConceptVec with advanced deep learning models can significantly improve performance in downstream bioinformatics studies and biomedical text-mining applications. Our BioConceptVec embeddings and benchmarking datasets are publicly available at  . \n   Author summary  \nCapturing the semantics of related biological concepts, such as genes and mutations, is of significant importance to many research tasks in computational biology such as protein-protein interaction detection, gene-drug association prediction, and biomedical literature-based discovery. Here, we propose to leverage state-of-the-art text mining tools and machine learning models to learn the semantics via vector representations (aka. embeddings) of over 400,000 biological concepts mentioned in the entire PubMed abstracts. Our learned embeddings, namely BioConceptVec, can capture related concepts based on their surrounding contextual information in the literature, which is beyond exact term match or co-occurrence-based methods. BioConceptVec has been thoroughly evaluated in multiple bioinformatics tasks consisting of over 25 million instances from nine different biological datasets. The evaluation results demonstrate that BioConceptVec has better performance than existing methods in all tasks. Finally, BioConceptVec is made freely available to the research community and general public. \n \n\n# Body\n \n## Introduction \n  \nIn the biomedical domain, one primary application of text mining is to extract knowledge within the biomedical literature automatically [ ]. Specifically, identifying important concepts (mentioned in the literature, such as gene/proteins, diseases, and mutations, is critical to biocuration [ ], literature-based knowledge discovery [ ], and many downstream applications [ \u2013 ]. Previous studies have used different words such as concepts, entities, names, and mentions to refer to the same topic in the biomedical domain. Here, we use   bio-concepts   for consistency. Similar to the use of word embeddings, capturing the representation of bio-concepts plays a vital role in biomedical applications such as biomedical relation extraction [ ] and document classification [ ]. Existing studies use the term   concept embeddings  , which is a special kind of word embedding [ \u2013 ]. According to the literature, a concept embedding may contain only the concept vectors [ ], or it may contain vectors of both concepts and common words [ ]. Named entity recognition (NER) tools or concept dictionaries are often used to identify and normalize concepts in a consistent format [ ]. \n\nSince 2014, word embedding models have revolutionized how to represent text. In these models, each word is represented as a high dimensional vector [ ,  ,  ,  ]. The vector representations are learned on large-scale free text corpora via unsupervised learning. Primary methods include training the embeddings based on (1) averaged surrounding context words, such as the continuous bag-of-words (cbow) model in word2vec [ ], (2) weighted context words, such as the skip-gram model in word2vec, (3) global co-occurrence statistics, such as GloVe [ ], and (4) word n-grams, such as fastText [ ]. The use of vector representations can capture related words from different lexicons, such as cancer and tumor. This overcomes the limitations of traditional bag-of-words approaches that rely on exact term matching [ ]. To date, text-mining applications have rapidly adopted word embeddings. For instance, the use of embeddings have shown promising performance in biomedical applications such as biomedical document classification [ ], sentence retrieval [ ], and question answering [ ]. \n\nIt is known that biomedical concepts have a high degree of ambiguity [ ]. The same words can be used to describe different types of concepts in free text; for example, AP2 can be the name of a gene ( ), a chemical ( ), or a cell-line ( ). Conversely, the same concepts can have different names; for example, the HER2 gene has at least 10 different synonyms mentioned in text ( ). In addition, a bio-concept can span multiple words; for example,   serum and glucocorticoid-induced protein kinase   is the name of a gene (SGK1,  ). Therefore, accurate NER is essential prior to training concept embeddings. \n\nWe present a detailed summary of the existing bio-concept embeddings in  . These studies have used various corpora (mainly electronic health records (EHR), combined with medical claims, biomedical corpora, or Wikipedia) and several training methods (mainly word2vec, while some used GloVe and fastText) to train concept embeddings. Overall, the primary method paradigm is consistent among these studies and generally involves two steps. In the first step, NER tools are applied to identify and normalize target concepts and to replace the mentions in the text as a preprocessing to the corpora. In the second (embedded training) step, embedding training occurs, whereby standard word embedding training methods, such as word2vec, are employed. Note that we consider concept embeddings trained on knowledge bases, such as gene2vec [ ], as different work because knowledge bases are distinct from free-text collections. For example, knowledge bases contain concepts already curated either manually or semi-automatically; therefore, training concept embeddings via knowledge bases does not require NER tools. In addition, the relationships between concepts in knowledge bases already have been organized in a structured format, such as ontologies. Free text, however, is unstructured, and training embeddings on free text occurs purely in an unsupervised way. Also note that individual knowledge bases contain only specific types of concepts by design. By contrast, a wide spectrum of concept types are described in the literature. \n   An overview of biomedical concept embeddings trained on large-scale free-text corpora.  \nRepository: the scope of concepts. Corpora: the training collection. Note that for EHR (electronic health records) and Claims (medical claims), the size is the number of patients, whereas for Wikipedia, PubMed (abstracts), and PMC (full-text articles), the size is the number of documents. #Concepts: the number of distinct concepts in the embedding. Method: the method for training embeddings. PCA: principle component analysis. PMI: pointwise mutual information. Intrinsic evaluation: a focus on applications that directly use the similarity between the vectors produced by word embeddings, such as word-pair similarity and relatedness. Extrinsic evaluation: a focus on downstream applications that use only word embeddings as an intermediate component. For example, the last study evaluated the effectiveness of concept embeddings for heart-failure prediction. Availability: whether the studies made the embeddings publicly available (we accessed on 04/20/2019). \n    \nDespite these recent efforts, past studies share some limitations. As shown in  , existing studies used NER tools to recognize and normalize Unified Medical Language System (UMLS) concepts [ ]. A long series of evaluation studies demonstrate that the effectiveness of these NER tools fluctuates dramatically for different types of UMLS concepts [ \u2013 ]. For example, Hassanzadeh et al. evaluated the NER tools used by the studies in   and found that the F1-score ranged from 5% to 75% for different types of UMLS concepts [ ]. Likewise, Re\u00e1tegui et al. found that the F1-score of the NER tools varied from 44% to 96% for different types of diseases [ ]. Importantly, errors produced in the NER step may diminish the effectiveness of bio-concept embeddings. For example, low precision, such as a non-concept word wrongly identified as a bio-concept by NER tools, will bias the context or nearby words of the true bio-concepts when training embeddings. Similarly, low recall, such as true bio-concepts that are not identified by NER tools, will reduce the number of training instances and decrease the concept coverage of bio-concept embeddings. \n\nSecond, almost no studies had evaluated the effectiveness of concept embeddings in extrinsic evaluations. The evaluation of word embeddings can be broadly categorized into two types (i.e., intrinsic and extrinsic) [ ]. Intrinsic evaluations are commonly accomplished via an unsupervised setting or using weakly supervised labels, whereas extrinsic evaluations are often performed via a supervised setting in downstream applications. As shown in  , only one study [ ] performed extrinsic evaluations for heart failure, predicting whether a patient would be diagnosed as having heart failure based on the associated clinical notes. The study used a basic long short-term memory (LSTM) model with randomly initialized embedding as the baseline and replaced the randomly initialized embedding with the proposed concept embedding to compare the performance. Although the results demonstrated that the proposed concept embedding has better performance, the study (1) did not compare the results with those of other existing concept embeddings and (2) did not compare the results with those of the state-of-the-art model that had achieved the highest performance on that task [ ]. \n\nFurther, importantly, the existing concept embeddings are designed primarily for concepts and applications in the clinical domain, whereas concept embeddings for the biological domain remain to be developed. As shown in  , existing studies used UMLS concepts and mainly used EHR data as the training corpora. Correspondingly, the evaluation focuses on clinical applications, i.e., the evaluation datasets are generated from EHR data. For example, most of the studies evaluated the two datasets, UMNSRS (Medical Residents Relatedness Set)-Similarity [ ] and UMNSRS-Relatedness [ ], each consisting of ~600 pairs of clinical concepts derived from EHR data and annotated by physicians. Similarly, the above extrinsic evaluation of heart-failure prediction is also based on a patient\u2019s clinical notes [ ]. Developing embeddings for biological concepts and applications is also important. \n\nIn response, we propose BioConceptVec, a collection of concept embeddings on primary biological concepts mentioned in the biomedical literature.   shows an overview of our study. Specifically, the study has three primary contributions: \n  \nTo our knowledge, we are the first study to use machine learning-based NER tools to recognize and normalize biological concepts for training bio-concept embeddings. Specifically, we employed PubTator, a state-of-the-art NER system with concept annotations for the entire PubMed abstracts [ ]. It contains over 400,000 concepts, which is the largest among the publicly available concept embeddings. For example, our evaluation of the human gene coverage shows that BioConceptVec covers 33% more gene concepts than the existing concept embeddings. \n  \nWe conducted large-scale intrinsic and extrinsic evaluations to quantify the validity and utility of BioConceptVec. The intrinsic evaluations contain ~18 million instances from six datasets. BioConceptVec has significantly higher performance (up to 10% improvement) than the existing concept embeddings and is consistent across multiple datasets. The extrinsic evaluations cover two downstream applications: protein-protein interaction (PPI) prediction, consisting of ~8 million PPIs from the STRING database [ ], and drug-drug interaction (DDI) classification, consisting of ~5,000 DDIs from a community-recognized gold standard dataset. The extrinsic evaluation results demonstrate that the deep learning models that use BioConceptVec can significantly improve the state-of-the-art performance, achieving an AUC of 0.95 for predicting PPIs and an F1-score of 0.80 for extracting DDIs. \n  \nWe make all of the embeddings and evaluation datasets publicly available. The embeddings and datasets can be downloaded via  . We also provide a Jupyter notebook that contains code examples for users to get started. \n     An overview of our study.  \nBioConceptVec was trained on PubMed abstracts, which consists of ~30 million documents. (1) We employed PubTator, which contains four NER tools, to annotate and normalize the concepts. (2) We trained four concept embeddings on the normalized corpus. (3) We conducted both intrinsic evaluations on drug-gene interactions and gene-gene interactions, and extrinsic evaluations on protein-protein interaction prediction and drug-drug interaction extraction to evaluate the effectiveness of BioConceptVec. \n  \n\n## Materials and methods \n  \n### Training corpus and method \n  \n#### NER step: Using PubTator to annotate biological concepts \n  \nWe trained concept embeddings on the ~30 million abstracts in the entire PubMed. We followed the preprocessing pipeline from [ ] (the code is publicly available via  ). As noted, the first step of bio-concept embedding development is to use NER tools to identify the target concept mentions (e.g., \u201cestrogen receptor\u201d) and to further normalize the mentions to the concept identifiers (e.g., \u201cNCBI Gene: 2099\u201d). As an example, shown in  , a targeted concept (i.e., MLN4924) is identified and normalized to a chemical concept: MESH:C539933. Due to the requirement of high-quality concept normalization for the concept embeddings, we applied PubTator to annotate the full PubMed abstracts. PubTator [ ] is a PubMed-scale resource that utilizes four NER tools (i.e., TaggerOne [ ], GNormPlus [ ], tmVar [ ], and SR4GN [ ]) with a recent deep learning-based module for disambiguating conflict mentions [ ] (when the mentions are annotated by two or more concept taggers) to recognize six key biological concepts (i.e., genes, mutations, diseases, chemicals, cell lines, and species).   provides a summary of the state-of-the-art performance of the NER tools in PubTator on various benchmarking datasets. \n   Identified bio-concept in-text and the normalized versions (one instance per type) in PubTator.    \n\n#### Embedding training step: Using word2vec, GloVe, and fastText to produce BioConceptVec \n  \nWe trained concept embeddings on the full collection of PubMed abstracts after concept recognition via PubTator, i.e., identified named entities are replaced with bio-entity types and IDs (e.g., Disease_MESH_D008288) before training. To our knowledge, there is no agreement on which embedding model is the most effective in biomedical domains. For example, Wang et al. [ ] showed that fastText achieved the highest performance in biomedical event trigger detection versus other word embeddings [ ], whereas Jin et al. [ ] found that word2vec has better performance in biomedical sentence classification [ ]. In this study, we therefore trained four different word embeddings, cbow, skip-gram, GloVe, and fastText such that future studies can choose our concept embeddings according to their specific requirements. \n\nIn general, the methods to train word embeddings can be categorized into two groups: window-based and matrix factorization-based [ ]. The major distinction between these two categories is that window-based methods aim to learn the semantics of words based on local context, i.e., words within a pre-defined window size, whereas matrix factorization-based methods aim to learn the semantics of words based on global statistics of words in corpora. word2vec and fastText belong to the first category while GloVe belongs to the second category. word2vec has two versions: cbow, training a model using context words as input to predict a target word, and skip-gram: reversely using a target word to predict context words [ ]. fastText is an extension of word2vec, using character n-grams to represent a word [ ]. In contrast, GloVe is dramatically different from word2vec and fastText. It builds a matrix based on global co-occurrences between the words and then applies matrix factorization. \n\nAs mentioned, fastText represents each word as a set of character n-grams. In the case of bio-concept embeddings, however, each bio-concept should be considered a unit. Thus, when training with fastText, we disabled the n-grams representation for bio-concepts (in contrast, for the words that are not bio-concepts, we still used the default n-grams representation in fastText). \n\nThe values of hyperparameters for training embeddings are summarized in  . Our choice of hyperparameters is based on similar studies in the past and other related work in the general domain. \n   The values of hyperparameters used for training BioConceptVec.  \n Default values  : the default values are identical to the values selected by baseline embeddings. We used the default values to train BioConceptVec (cbow), BioConceptVec (skip-gram), BioConceptVec (GloVe) and BioConceptVec (fastText).   Other values  : we also adopted other commonly-used hyperparameter values to test the effectiveness of BioConceptVec (cbow) under different parameter settings. \n    \n\n#### Hyperparameters and other methods for comparison \n  \nTo directly compare with the existing concept embeddings, we used the exact hyperparameter values from Yu et al. [ ] as the default setting. As shown in  , of the three publicly available concept embeddings, it is the only concept embedding trained on PubMed. The other two were trained on EHR data. We measured the concept overlap in terms of genes and found that concept embeddings trained on EHR data contain a significantly fewer number of genes than do embeddings trained on PubMed. Thus, we did not compare with those two EHR-driven methods. \n\nYu et al. [ ] used cbow to train the concept embeddings and their hyperparameters are summarized in  . Hence, under the same parameter settings, we firstly trained a common cbow word embedding on PubMed abstracts, as a baseline. Common word embeddings do not contain vectors for normalized bio-concepts. The words in a bio-concept name, however, often exist in common word embeddings. For example, the TOR3A gene ( ) does not exist in a common word embedding, but the words of its name   torsin family 3 member A   all exist. Thus, we averaged the word vectors based on the bio-concept name to represent the concept vector. Averaged vectors are used as a strong baseline for many embedding-related tasks, such as sentence similarity [ ] and sentiment analysis [ ]. We refer to the averaged word embedding baseline as BioAvgWord (cbow). As such, we are able to directly compare BioConceptVec (cbow) with the two baselines: BioAvgWord (cbow) and the concept embedding provided by Yu et al. \n\nIn addition, we trained and assessed BioConceptVec (cbow) under different parameters but keeping the same values for minimal word occurrences (so that embeddings share the same vocabulary), learning rate and training epochs (so that embeddings share the same optimization procedure). For each of the other hyperparameters, we selected two representative values that were used in the previous studies on embeddings [ ,  ], as shown in   (other values). Note that we do not select larger values for the negative samples and down-sampling threshold because the training epoch is set to be 10 \u2013it would require more epochs to stabilize the loss when there are more samples. \n\nFurthermore, different studies show that performance can vary by different embedding methods [ ,  ]. Thus, we also train BioConceptVec using skip-gram, GloVe and fastText, using the same default setups. We make all of the four versions of BioConceptVec (cbow, skip-gram, GloVe and fastText) publicly available so that users can experiment and choose between the models for their tasks. \n\nTo ensure a fair comparison, the evaluation datasets described below contain only concepts shared among these baseline methods and BioConceptVec. We also measured the coverage of concepts using human genes as an example. \n\n\n\n### Intrinsic evaluations \n  \n#### Identifying related genes based on drug-gene and gene-gene interactions \n  \nWe posit that concept embeddings should give higher similarity to related concepts than to unrelated concepts. The intrinsic evaluations in our study quantify the effectiveness of concept embeddings in terms of identifying related genes. We concentrate on genes because genes are a central focus of biological studies; the interactions between genes (or genes and other biological concepts) are essential for understanding the structures and functions of a cell [ ,  ]. In addition, biological studies over the decades have collected related genes from different perspectives, such as those based on expression signatures, pathways, and gene ontologies (GO). These collected related genes can be used as a gold standard for our intrinsic evaluations. In contrast, other biological concepts, such as diseases and mutations, are somewhat difficult to define in regard to the notion of relatedness systematically. We considered related gene pairs based on drug-gene interactions and gene-gene interactions, as explained below. \n\n\n#### Evaluation dataset construction and evaluation metrics \n  \nWe adopted six datasets for creating evaluation datasets. The detailed statistics of these datasets are summarized in  . The relatedness of genes was modeled from two broad categories. The first was based on the relationships between genes and other bio-concepts, and the second was based on the relationships among genes. \n   The statistics of datasets in intrinsic evaluation tasks.  \nThere are six datasets in total. #groups: the number of groups in a dataset. Each group has a related set and an unrelated set of genes based on drug-gene interactions provided by CTD or gene sets provided by MSIGDB. #distinct concepts: the total number of distinct genes in a dataset. Avg #concepts per group: the average of number of genes in a group; note that one gene may be in multiple groups. #pairs: the total number of pairs in a dataset. Avg #pairs per group: the average of the number of pairs per group. \n    \nFor the first category, we used the Comparative Toxicogenomics Database (CTD) [ ], which captures drug-gene interactions. For each drug, we consider the genes that interact with the same drug as a related set and randomly select the same number of genes that do not interact with the drug as an unrelated set. A related and unrelated set together form a group. Ideally, concept embeddings should have significantly higher similarity for the related sets than the unrelated sets for each group. \n\nFor the second category, we used five gene sets (C1\u2013C5) of MSigDB [ ]. MSigDB captures related genes using different perspectives, and each gene set is generated from a distinct perspective. For example, MSigDB C1 is generated based on human chromosomes, and MSigDB C5 is generated based on GO. The strategy of creating related and unrelated sets is the same as above. For example, in terms of MSigDB C5, the genes that share the same GO term are considered a related set, and the same number of genes that do not share that GO term are randomly generated as an unrelated set. \n\nWe computed the similarity of a set by averaging the cosine similarity of all of the pairs in the set, using concept embeddings. Cosine similarity is the most popular similarity measure used by embeddings [ ]. Importantly, different embeddings may report different cosine similarities for same pairs, and the range of cosine similarities also may be different, which is strictly inevitable [ ]. To reduce the biases, for each embedding, we first applied   Z  -score standardization to the cosine similarities of all of the pairs and then used Min-Max normalization to transform the range to [0, 1]. \n\nWe used the similarity score difference between related sets and unrelated sets at group level as the final evaluation metric. As noted, a more effective concept embedding should have a greater similarity score difference between the related set and the unrelated set for a group. For computational efficiency, we restricted the maximum number of genes in a set to be 100, i.e., a group has, at most, 200 genes in total. Note that MSigDB has other gene sets, such as C6 and C7. We did not use them because the number is fewer than 100 in shared genes. Collectively, our intrinsic evaluation datasets contain over 13,000 genes and over 17 million instances across six datasets. \n\n\n\n### Extrinsic evaluations \n  \nWe further evaluated the utility of BioConceptVec in two downstream applications: protein-protein interaction (PPI) prediction on the STRING database [ ] and drug-drug interaction (DDI) classification on biomedical literature [ ]. \n\n#### Protein-protein interaction prediction on the STRING database \n  \nAnalyzing functional interactions between proteins, which facilitates the understanding of the cellular processing and characterization, is a routine task in molecular systems biology [ ]. The STRING database is one of the most comprehensive data resources that integrate, score, and analyze publicly available PPIs [ ]. To date, it consists of over 3 billion PPIs from ~25 million proteins ( ). The PPIs in the STRING database are scored by accumulating a wide range of evidence, such as measurements from biological experiments, co-expressions, and gene co-occurrences. \n\nExisting studies have used STRING for training and testing machine learning models for PPI prediction [ ,  ]. In a recent study, for example, Smaili et al. constructed two PPI datasets for human proteins: (1) PPIs based on combined scores, i.e., the score calculated from multiple sources (including results from the biomedical literature and many others, such as gene co-expressions, biological experiments and pathways), which we refer to as the   combined-score  , and (2) PPIs that have the experimental score over 700, i.e., the score is based only on biological experiments and is greater than 700, which we refer to as the   experimental-700  . The study considered these PPIs as positive instances and randomly generated the same number of negative instances. Smaili et al. split the datasets into the training and testing datasets, accounting for 70% and 30% of the total number of PPIs, respectively. They further developed a deep learning model by taking the vector representations of the two proteins as inputs and predicting whether the proteins have interactions. The deep learning model was an artificial neural network (ANN) that had two hidden layers [ ]. Using the same model, the study tested different vector representations and reported Area Under the Curve (AUC) accordingly. \n\nWe followed this study [ ] for creating the datasets and implementing the reported ANN model.   provides a summary of the statistics of the datasets. The combined-score dataset covers all of the 13,802 proteins that are shared by concept embeddings and STRING databases. In comparison, the previous study sampled only 1,800 proteins. We also implemented a 2-layer ANN. The details of the hyperparameters are summarized in  . In keeping with the previous study, the model and hyperparameters are identical when testing different concept embeddings. The Precision, Recall, F1-score, and AUC are reported. \n   Statistics of the datasets for PPI prediction.  \n#Concepts: the number of concepts in the dataset. #Training: the number of training instances; same applies to #Validation and #Testing. \n    \n\n#### Drug-drug interaction extraction on biomedical literature \n  \nWe also examined the usefulness of concept embeddings in a text-mining task. Specifically we evaluated the performance of concept embeddings on the SemEval 2013: Task 9 DDI extraction corpus [ ] for DDI classification. This dataset consists of over 1,000 documents from the DrugBank database [ ] and PubMed abstracts and ~5,000 DDIs manually annotated by two senior pharmacists, serving as a gold standard dataset for relation extraction by the community [ ]. \n\nIn this task, the input is a sentence that contains a pair of drugs. If the pair of drugs represents a true DDI, the model needs to output the DDI type; otherwise, the model needs to indicate the pair is not a true DDI [ ]. The annotators classified a DDI into one of four types: advice, effect, mechanism, and int (the interaction occurs, but its type cannot be classified) [ ]. We used the official training and testing datasets. The statistics of the datasets are summarized in  . This is a multi-class classification problem (i.e., 5 classes: 4 DDI types and a negative class indicating a pair is not a DDI), and the organizers used the F1-score to measure the multi-class performance of true DDIs (i.e., without considering the negative cases). We followed the same evaluation procedure. \n   Statistics of the DDI extraction datasets.  \nMechanism, Effect, Advice, Int are four types of DDIs. Negative means that the instance does not contain a DDI. \n    \nWe implemented a simple averaged sentence embedding neural network model (SEN) for DDI classification.   illustrates the architecture of SEN. For an input sentence, it first uses word embedding to map the vectors of each word in the sentence (Embedding Layer in  ). We used the recent context-based word embedding ELMo in the Embedding Layer [ ], which was shown to be superior to common word embeddings in relation extraction tasks [ ]. Then it averages all of the word vectors to obtain the sentence vectors (Averaged Layer), followed by dense layers (the hidden layers used in the ANN above). Finally, it outputs class probabilities. The details of the hyperparameters of SEN are summarized in  . SEN has been used widely as a baseline model in sentence-related applications [ ]. We hypothesized that adding the vector representations of the drugs mentioned in the sentences will increase the classification performance. We used PubTator to map the drug mentions into concept identifiers. Thus, similar to PPI prediction, we used the same model and tested different concept embeddings. The Precision, Recall, and F-1 score are reported. \n   The architecture of the model used for DDI extraction.    \n\n\n\n## Results and discussions \n  \n### Number of shared human genes in BioConceptVec and other public embeddings \n  \n shows the number of human genes with computed embeddings in each method. We compared all of the publicly available concept embeddings shown in  . There are two embeddings provided by Choi et al. ( ). We used the version from   stanford_cuis_svd_300  .  txt  .  gz   because it contains more concepts and also more human genes than the other one.   illustrates that BioConceptVec contains more human genes than other publicly available concept embeddings. Specifically, it covers about 3,000 more human genes than does the second highest embedding method (Yu et al). In total, these four embeddings cover 18,881 human genes, ~98% of which can be found in BioConceptVec. We manually examined the genes that were missing in BioConceptVec and found that most of them only occurred once. We also found that these genes occur more frequently in PMC full-text articles; we plan to integrate both PubMed abstracts and PMC full-text articles for training concept embeddings in the future. \n   Gene coverage results in terms of human genes.  \nThe number of human genes in different embeddings is shown individually. In total, these four embeddings consist of 18,881 human genes. Note that the embeddings from Beam et al. and Choi et al. were mainly trained on EHR. The results mainly aim to demonstrate that biomedical literature and EHR contain significantly different concepts. \n  \nNotably, the embeddings from Beam et al. and Choi et al, were primarily trained on EHR, and these embeddings are designed mainly for clinical applications. Hence, they only cover a small number of gene and protein concepts. This comparison thus further illustrates that the biomedical literature contains significantly different bio-concepts from clinical notes. \n\n\n### Intrinsic evaluation results \n  \n and   show the intrinsic evaluation results on the six evaluation datasets. As noted, the average group similarity difference (%) is used as the evaluation metric. A more effective concept embedding should have higher similarity difference between the positive set and the negative set of a group. Using the same embedding training method and the same hyperparameters, the results in   show that the performance of BioConceptVec (cbow) is consistently higher (an average of 4 percentage points) than that of Yu et al. on the six datasets. The differences are even more remarked when compared to the average word embedding (an average of 7 percentage points). In addition, the results also show that BioConceptVec (cbow) achieves consistently better performance than that of baseline approaches with different hyperparameters. Collectively, these results suggest the positive impact of our selected NER methods. \n   The intrinsic evaluation results in terms of average group similarity difference (%) for the six evaluation datasets.  \n Direct comparison   shows the results of BioConceptVec (cbow) using identical hyperparameters as the baselines. The baselines were also trained using cbow.   Different hyperparameters   shows the results of BioConceptVec (cbow) using different hyperparameters (provided in  ): w, v, s, and n stand for window size, vector dimension, sampling threshold, and negative samples, respectively. \n     The intrinsic evaluation results for BioConceptVec from different embedding methods (cbow, skip-gram, Glove, and fastText).  \nThe embeddings were trained using the same default parameters. Direct comparison: the results of baseline embeddings and BioConceptVec trained using cbow. \n  \nIn  , we report the effect of different embedding methods. As shown, there is no one-size-fits-all method that always achieves the best performance across all of the datasets. For instance, BioConceptVec (cbow) had the best performance on the CTD dataset, whereas BioConceptVec (GloVe) had the highest score on the MSigDB C1 dataset. This is consistent with the findings in the previous literature on embedding comparison [ ,  ]. Hence, it is necessary to make embeddings trained with different methods publicly available. \n\n\n### Extrinsic evaluation results \n  \n#### Protein-protein interaction predictions on STRING database \n  \n illustrates the classification results of PPI predictions on the STRING database. The direct comparison results show that BioConceptVec (cbow) has better performance than the baseline approaches\u2013achieving the highest F1 score and AUC on both datasets. The results of BioConceptVec (cbow) with different hyperparameters is summarized in  , which further demonstrate that its performance was consistent overall. When comparing BioConceptVec trained using different methods, BioConceptVec (fastText) had the best overall performance for this task, although the performance of BioConceptVec (cbow) and BioConceptVec (skip) are very close. Note that we were unable to directly compare with the previous study [ ] because the proposed embedding is not publicly available. Also as noted, the performance of the study was measured on ~1,800 proteins, whereas our datasets contain ~13,000 proteins. \n   Classification results of PPI predictions on the STRING database.  \nCombined-scores: PPIs that have combined scores are considered positive cases. Experimental-700: PPIs that have experimental scores over 700 are considered positive cases. Direct comparison: the results of embeddings using the same method (cbow) and same hyperparameters. Different embedding methods: the results of BioConceptVec (skip-gram), BioConceptVec (GloVe) and BioConceptVec (fastText). The highest results of each section are marked as bold. \n    \n\n#### Drug-drug interaction extraction results \n  \n demonstrates the evaluation results on DDI extraction. We ran the model 5 times with different random seeds and then calculated the average performance [ ]. The state-of-the-art (SOTA) model by Zhang and colleagues achieved an F1-score of 0.73 on this dataset [ ]. Their model uses an LSTM as an encoder with an attention mechanism and outperforms other feature-based, kernel-based, and neural networks-based methods. We found that, compared with the SOTA model, the SEN model had a slightly better classification performance on advice, effect, and mechanism relation types but had a dramatically lower performance on int relation where a DDI cannot be classified into a specific type. \n   Classification results of DDI classification.  \nSOTA: state-of-the-art. P: Precision. R: Recall. The SOTA results are extracted from [ ]. Direct comparison: the results of embeddings using the same method (cbow) and same hyperparameters. Different embedding methods: the results of BioConceptVec (skip-gram), BioConceptVec (GloVe) and BioConceptVec (fastText). The highest results of each section are marked as bold. \n    \nWe also measured the performance of SEN by adding concept vectors. The direct comparison results show that BioConceptVec has better performance than the baseline approaches. Adding BioConceptVec improves the F1-score significantly and BioConceptVec (cbow) appears to be the most effective in this task. The results of BioConceptVec (cbow) using different hyperparameters are summarized in  . It also shows that the performance is consistent. \n\nWe further qualitatively analyzed the errors by comparing the results of the SEN model with and without BioConceptVec. We found that the SEN model failed to classify challenging cases in which the definitions of relation types are somewhat similar. For example, the sentence, \u201cZidovudine competitively inhibits the intracellular phosphorylation of stavudine,\u201d contains the relation \u201czidovudine-stavudine.\u201d The annotator classified it as the effect type, but the SEN model wrongly classified it as the mechanism type. According to the annotation guidelines, both effect and mechanism types can describe pharmacological effects. The effect type, however, focuses on the change of the effect, whereas the mechanism type focuses on the underlying reason for the change. For this case, inhibiting the intracellular phosphorylation describes the change rather than the mechanism. There are ~20 similar erroneous cases for which the SEN model only mixed the effect type with the mechanism type. Adding BioConceptVec (cbow) to the SEN model correctly classified all of them. This is likely due to the fact that BioConceptVec provides additional information learnt from the entire PubMed abstracts, making the classification of the two related types easier as a result. Collectively, the results confirm the hypothesis that adding concept representatives improves the performance of downstream deep learning models and suggests that BioConceptVec has the potential to facilitate the development of deep learning models in the biomedical domain. \n\nIn this work, we propose BioConceptVec, concept embeddings that focus on primary biological concepts mentioned in the biomedical literature. We employed SOTA biological NER tools and trained four concept embeddings on the full collection of ~30 million PubMed abstracts. We evaluated the effectiveness of BioConceptVec in intrinsic and extrinsic settings, consisting of ~25 million instances in total. The results demonstrate that BioConceptVec consistently achieves the best performance in multiple datasets and in a range of applications. We hope that it can facilitate the development of deep learning models in biomedical research. In the future, we plan to leverage both PubMed abstracts and PMC full-text articles for training BioConceptVec. \n\nThis study focused on the evaluation on human genes because there are rich resources readily available for serving as a gold standard. We plan to evaluate BioConceptVec embeddings on different concept types in the future. Also, the quality of our concept embeddings is dependent on the accuracy of the NER tools. Improving NER tools such as PubTator would help enhance the quality of BioConceptVec. Finally, in this work, we did not apply retro-fitting, which is a fine-tuning step to further optimize the embeddings based on specific tasks with gold standard labels. For example, one of the most common retro-fitting procedures is to optimize the performance of the generated embeddings on identifying synonyms and acronyms. We did not employ it because such datasets are very limited for biomedical concepts. We plan to develop related datasets and apply the approach to further enhance BioConceptVec. \n\n\n\n\n## Supporting information \n  \n \n", "metadata": {"pmcid": 7237030, "text_md5": "b46798dd2f0701a5d10bbd1d60fbf134", "field_positions": {"authors": [0, 94], "journal": [95, 111], "publication_year": [113, 117], "title": [128, 230], "keywords": [244, 244], "abstract": [257, 3701], "body": [3710, 42764]}, "batch": 1, "pmid": 32324731, "doi": "10.1371/journal.pcbi.1007617", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7237030", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7237030"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7237030\">7237030</a>", "list_title": "PMC7237030  BioConceptVec: Creating and evaluating literature-based biomedical concept embeddings on a large scale"}
{"text": "Bravo, \u00c0. and Cases, M. and Queralt-Rosinach, N. and Sanz, F. and Furlong, L. I.\nBiomed Res Int, 2014\n\n# Title\n\nA Knowledge-Driven Approach to Extract Disease-Related Biomarkers from the Literature\n\n# Keywords\n\n\n\n# Abstract\n \nThe biomedical literature represents a rich source of biomarker information. However, both the size of literature databases and their lack of standardization hamper the automatic exploitation of the information contained in these resources. Text mining approaches have proven to be useful for the exploitation of information contained in the scientific publications. Here, we show that a knowledge-driven text mining approach can exploit a large literature database to extract a dataset of biomarkers related to diseases covering all therapeutic areas. Our methodology takes advantage of the annotation of MEDLINE publications pertaining to biomarkers with MeSH terms, narrowing the search to specific publications and, therefore, minimizing the false positive ratio. It is based on a dictionary-based named entity recognition system and a relation extraction module. The application of this methodology resulted in the identification of 131,012 disease-biomarker associations between 2,803 genes and 2,751 diseases, and represents a valuable knowledge base for those interested in disease-related biomarkers. Additionally, we present a bibliometric analysis of the journals reporting biomarker related information during the last 40 years. \n \n\n# Body\n \n## 1. Introduction \n  \n\nThe Biomarkers Definition Working Group (formed by the US National Institutes of Health (NIH) and the US Food and Drug Administration (FDA), academia, and industry) defined biomarker as \u201c  a characteristic that is objectively measured and evaluated as an indicator of normal biologic processes, pathogenic processes, or pharmacologic responses to a therapeutic intervention  \u201d [ ]. With the advent of the genomics era, in April 2008, the FDA published in one of its \u201cGuidance for Industry\u201d documentations the specific definition of a genomic biomarker as \u201c  a measurable DNA and/or RNA characteristic that is an indicator of normal biologic processes, pathogenic processes, and/or response to therapeutic or other interventions  \u201d [ ]. More recently, Anderson and Kodukula [ ] provided some definitions of different types of biomarkers (e.g., surrogate, clinical endpoint, diagnostic, prognostic, predictive, pharmacodynamic, efficacy, and toxicity/safety [ \u2013 ]) within their review of the role of biomarkers in pharmacology and drug discovery. All these definitions specify the requirements to be held by a biomarker, the different types that exist, their potential role in disease diagnosis and progression or in the therapeutic response control, and their utility for the assessment of new chemical entities as potential lead therapeutics [ ]. \n\nThousands of biomolecules are being investigated as potential biomarkers, but most of them do not advance effectively for diagnostic, prognostic, or therapeutic goals for different reasons (for a detailed discussion on this topic see [ \u2013 ]). The results of the research on potential biomarkers are widely reported on the biomedical literature. The MEDLINE database [ ] has currently indexed more than 23\u2009M articles, and since 1989 the MeSH term \u201cBiological Markers\u201d is applied to annotate those articles that provide data on \u201c  measurable and quantifiable biological parameters (e.g., specific enzyme concentration, specific hormone concentration, specific gene phenotype distribution in a population, presence of biological substances) which serve as indices for health- and physiology-related assessments, such as disease risk, psychiatric disorders, environmental exposure and its effects, disease diagnosis, metabolic processes, substance abuse, pregnancy, cell line development, epidemiologic studies, etc  .\u201d [ ], and later on, in 2008, the MeSH term \u201cBiomarkers, Pharmacological\u201d was introduced to specifically annotate the \u201c  measurable biological parameters that serve for drug development, safety and dosing (DRUG MONITORING)  \u201d [ ]. In particular, genomic biomarkers are frequently reported in the literature together with disease-related information. Thus, the MEDLINE database contains valuable knowledge for those interested in gathering information on biomarkers. In order to identify, extract, and analyse this information from the literature, automatic processing of the texts is required [ ]. There are only few reports on text mining approaches in the biomarkers field [ \u2013 ]. Here, we present a knowledge-driven text mining approach for the extraction of disease-related biomarker information from the literature. Our approach, firstly, takes advantage of biomarker-specific MeSH terms annotations to retrieve a specific and comprehensive pool of publications from MEDLINE, secondly, applies our named entity recognition method (BioNER) to \u2009(1) identify genes and diseases as entities of interest, (2) filter ambiguous entities, (3) cluster equivalent terms to a certain concept, (4) characterize those genes as potential biomarkers based on terminology used, and, finally, (5) find associations between genes and diseases in single sentences, and ranks the associations based on their frequency in the literature. This approach, that allows the unique identification of genomic biomarkers and their associated diseases, was applied to the MEDLINE database resulting in a comprehensive knowledge base on disease-related biomarkers, which is publicly available at  . In addition, we provide an analysis of the results obtained and present an evaluation of the trend of biomarker research reporting as a topic in the scientific literature. \n\n\n## 2. Material and Methods  \n  \nWe developed a text mining workflow aimed at extracting information on disease-related biomarkers from scientific publications. Briefly, after document selection, the text mining approach comprises as a first step the recognition and normalization of the   disease   and   biomarker   entities in biomedical publications by means of the biomedical named entity recognition (BioNER) system and, secondly, the identification of relationships between the aforementioned entities by their cooccurrence in sentences. For example, the following sentence (taken from PMID: 17397492), \u201c  CK20    is an important biomarker that can be used to identify  \u2009\n   TCC    in urine cytology smears,  \u201d contains the cooccurrence of the entities\u2009   CK20   (gene) and\u2009   TCC   (disease). \n\nThe different steps addressed in the text mining workflow are illustrated in   and detailed below. \n\n### 2.1. Document Selection \n  \nTo obtain a set of publications focused on biomarkers, we formulated the following PubMed query: (\u201cBiological Markers\u201d [MeSH Terms]) AND (has abstract [text]) AND (English [lang]) AND (\u201c0001/01/01\u201d [PDAT]: \u201c2013/06/30\u201d [PDAT]) AND \u201chumans\u201d [MeSH Terms], that resulted in 375,331 publications (September 30, 2013). \n\n\n### 2.2. Development of Gene and Disease Dictionaries for Biomarker-Specific Information \n  \n Gene Dictionary  . In order to collect the terms referring to human genes and proteins, we have integrated data from three biological databases: NCBI-Gene [ ], HGNC [ ], and UniProt [ ,  ], followed by a semiautomatic curation process. These databases are cross-referenced between each other, providing a way to collect and integrate the terminology for a specific gene/protein entity from the different sources in a single entity.   shows an example of terminology integration for the Lipocalin-2 gene. Note that we do not make a distinction between gene and protein mentions in the text, because in general both types of entities share their terminology. Thus, for the sake of simplicity, we refer to genes and proteins as genes. \n\n Disease Dictionary  . The Unified Medical Language System (UMLS) [ ] database was used to create the disease dictionary. The UMLS Metathesaurus is a large, multipurpose, and multilingual thesaurus that contains millions of biomedical and health-related concepts, their synonymous names, and their known relationships. We selected all the concepts in English from the freely distributed vocabularies corresponding to the following semantic types: Congenital Abnormality (T019), Acquired Abnormality (T020), Disease or Syndrome (T047), Mental or Behavioral Dysfunction (T048), Experimental Model of Disease (T050), Sign or Symptom (T184), Anatomical Abnormality (T190), and Neoplastic Process (T191). \n\nBoth dictionaries were curated and extended semiautomatically using different rules to facilitate the matching task. Each dictionary has its own distinctive features; for example, the gene dictionary has a high prevalence of acronyms referring to genes (i.e., A2MP1, NOTCH1, and SF3B1), whereas long terms are prevalent in the disease dictionary (i.e., Alzheimer's disease, acute lymphoblastic leukemia, primary eosinophilic endomyocardial restrictive cardiomyopathy, and rheumatic tricuspid stenosis and insufficiency). In our curation process we defined the following rules with specific adjustments depending on the dictionary.   \nTo reduce ambiguity in the dictionary, the terms with a length smaller than three characters are removed. \n  \nA specific number of characters are replaced by their general form; that is, the characters \u201c\u00e0, \u00f6, \u00e7, \u00fb\u201d are replaced by \u201ca, o, c, u\u201d (i.e.,   Sj\u00f6gren-Larsson syndrome   by   Sjogren-Larsson syndrome  ). \n  \nNew variants are generated for gene symbols (i.e.,   IL2  ,   IL 2  ,   IL   (  2),   or   IL-2   is the same acronym referring to   interleukin 2  ). \n  \nTerms containing digits (Arabic numbers) can be written with roman numbers. New terms are generated by replacing Arabic with Roman numbers (  Adenylosuccinate lyase deficiency type 4   by   Adenylosuccinate lyase deficiency type IV  ). \n  \nTerms can contain Greek letters (such as HP1-alpha, HP1-beta, and HP1-gamma) or symbols (as HP1-  \u03b1  , HP1-  \u03b2  , and HP1-  \u03b3  ); both cases are considered. \n  \nPrefix and suffix labels not used in natural language are removed from the terms (i.e.,   [X]Gastric neurosis   or   Leber Aongenital Amaurosis [Disease/Finding]   by   Gastric neurosis   and   Leber Congenital Amaurosis  ). \n  \nAll terms are transformed into lowercase characters (i.e.,   FALDH deficiency   by   faldh deficiency  ). \n  \nAll punctuation marks are removed to improve the fuzzy matching (i.e.,   hnf-3-gamma   by   hnf 3 gamma  ). \n  \n\nParticularly, the disease dictionary was also processed with Casper [ ], a rule-based software that suppresses undesired terms from the UMLS Metathesaurus and generates additional synonyms and spelling variations, and, afterwards, manually curated in order to remove very general terms such DISEASE or SYNDROME. \n\nAs a final step, to select a set of putative biomarker genes from our   gene dictionary  , we conducted a text mining search for the genes that are mentioned together with biomarker terms in the same sentence (biomarker rule filtering) and then all entries of this set of genes were extracted from the dictionary as putative biomarkers-related terms. The rationale of this approach is that genes mentioned together with terms such as \u201cmarker\u201d are very likely biomarkers themselves. The biomarker terms were collected from the concept \u201cbiological markers\u201d present in the MeSH terminology [ ]. This step retrieved a total of 3,533 genes which were mentioned together with at least one biomarker term, and they were collected from our   gene dictionary   to create the   biomarker-specific gene dictionary  . A similar procedure was applied to obtain a subset of the   disease dictionary   relevant to the biomarkers topic, the   biomarker-specific disease dictionary  . This filter allowed the selection of 3,122 diseases cooccurring with biomarker terms. \n\n shows the number of concepts, the ambiguity, and the variability for all the dictionaries to illustrate the effect of the curation and rules applied. The ambiguity quantifies how a term can refer to different concepts, while the variability reflects the average number of unique terms for each concept. The best curation process is the one that improves the variability minimizing the ambiguity of the dictionaries. In the case of the   gene dictionary  , the number of terms between the raw and curated dictionaries increases in 19% with a slight effect in the ambiguity. In the case of the   disease dictionary  , there are no major changes in ambiguity and variability after dictionary curation. Overall, the curation process keeps the ambiguity and improves the variability. \n\n\n### 2.3. BioNER \n  \nThe BioNER system applies the biomarker-gene and disease-biomarker dictionaries using fuzzy- and pattern-matching methods to find and uniquely identify entity mentions in the literature [ \u2013 ]. Firstly, our BioNER receives the dictionary type to extract mentions and a list of document identifiers (obtained in the document selection step). Each publication is recovered from a document repository and the abstracts are split into sentences, and a set of patterns is created from the selected dictionary (  biomarker-specific gene or disease   dictionaries), after removing a list of stop words. For each sentence, the BioNER extracts the longest term from the patterns without overlap. Then, each mention is normalized to its unique identifier using the dictionary. \n\n\n### 2.4. Relation Extraction \n  \nIn this study, we applied a relation extraction (RE) method based on cooccurrence findings, which assumes that a biomarker and a disease are associated if they are mentioned together in the same sentence. From 164,300 abstracts, 686,172 cooccurrences were found between 2,803 biomarkers and 2,751 diseases, resulting in 131,012 disease-biomarker different associations. Certainly, the title and the body of the abstract show different writing styles, in terms of both syntax and semantics. Generally, the title or the last part of the abstract tends to express more concisely the final message of the publication, whereas the rest of the abstract contains background information and more hypothetical discourses as contextual information of the study. In order to account for these differences and make a distinction depending on where the cooccurrence is detected in the text, the system separates each abstract into 3 parts: title, abstract body, and conclusions. Then, the associations are scored based on the frequency of each association in the literature represented by a variant of the Inverse Document Frequency model [ ] as follows:\n \nwhere the score for the association between disease D and biomarker B (Score , ( )) is obtained as the product between the inverse document frequency of the association between D and B (  idf  (DB,   A  ), ( )) and the normalized frequency of the association between D and B overall the documents (  af  (DB,   A  ), ( )). \n\nThe   idf   provides an indication of the popularity of the association across the corpus of documents under study, and it is obtained by dividing the total number of abstracts (|  A  |) by the number of abstracts containing the association between D and B (|{  a   \u2208   A   : DB \u2208   a  }|) and taking the logarithm of that quotient ( ). The function   af  (DB,   A  ) in ( ) is the frequency of the association between B and D in the   i  th abstract (  A  ) and it is defined with a quotient between   f  (DB,   A  ), which is the number of times that the association between B and D occurs in   A   (multiplied by 2 if DB occurs in title or conclusion of   A  , or 1 if DB occurs in the body), and the maximum frequency of any association in   A   (max\u2061{  f  (  XY  ,   A  ) :   XY   \u2208   A  }). \n\n\n### 2.5. Analysis and Validation \n  \nIn order to validate the disease-biomarker associations identified by text mining, we compared them to the biomarker information contained in the DisGeNET database, release 2.0 (July, 2012). DisGeNET is a database that integrates knowledge on the genes associated with human diseases from various expert curated databases and the literature [ ,  ]. For this study we used the set of associations labelled as \u201cbiomarker\u201d according to the DisGeNET gene-disease association ontology [ ,  ]. We collected a list of 12,887 genes associated with 6,135 different disease terms stored in DisGeNET. \n\n\n\n## 3. Results and Discussion  \n  \nIn this paper, we present a new methodology to extract disease-biomarker associations from the literature. One of the major challenges that any text mining application faces is the variability of terms referring to the same concept; and then, consequently, the identification of entities in a nonambiguous manner (i.e., gene, protein, and disease). In this respect, biomedical terms gathered in domain-specific lexicons such as dictionaries, ontologies, and terms classifications (i.e., MeSH disease tree [ ]) serve to organize synonymous terms into a central concept, facilitating both entity recognition and the hierarchical exploration of the results [ ]. Another challenge in biomedical text mining is the identification of relationships between two entities [ ]. Thus, our methodology faces both challenges by (1) the identification of   biomarker   and   disease   entities by means of the BioNER system and (2) the extraction of relationships between these entities by cooccurrence in sentences. An analysis of the associations between disease and biomarker is presented according to their mention frequency in MEDLINE, and they are evaluated by manual inspection and by comparison with the biomarker information integrated in the DisGeNET database. \n\nThe application of our text mining approach on a set of 375,331 publications pertaining to biomarkers (see  ) resulted in 686,172 disease-gene cooccurrences found in 164,300 abstracts. These cooccurrences represented associations between 2,803 genes and 2,751 diseases, giving rise to 131,012 unique disease-gene associations, which should be considered as potential disease-biomarker associations due to both the   document selection strategy   and the   biomarker rule filtering   addressed (see   for details and find examples of sentences including disease and biomarker concepts in  ). It is important to remark that the biomarker and disease mentions found in the text are linked to their corresponding identifiers in standard vocabularies (NCBI Gene for biomarkers and UMLS for diseases). This normalization of the entities extracted from the publications enables the unique identification of these entities and opens the possibility of integration of the extracted information with data from other standardized resources. \n\n### 3.1. Distribution of Biomarker Information in the Biomedical Literature \n  \nFrom the approximately 23\u2009M publications contained in the MEDLINE database, 375,331 are related to biomarkers and therefore have been annotated with the MeSH terms \u201cbiological markers\u201d and \u201cBiomarkers, Pharmacological\u201d by PubMed curators. From these publications, 164,300 contain information on genes and proteins as biomarkers of a given disease in the abstract. The distribution of cooccurrences encountered in the title, the body of the abstract, and the conclusions section was 10, 85, and 5%, respectively. The evolution in reporting disease-biomarker related information throughout the years is presented in  . The document set under study represents publications in the field of biomarkers that contain information on genes and proteins from 3,983 different journals. Both the number of journals that publish disease-biomarkers-related data and the number of published articles show a progressive increase from the early 1980s. Only 5 of the journals include   marker   or   biomarker   in their journal name (  Int. J Biol. Markers   (336 abstracts),   Dis. Markers   (187),   Cancer Epidemiol. Biomarkers Prev.   (413),   Biomarkers   (11), and   Genet Test Mol. Biomarkers   (35)) and contribute to the disease-biomarker association list of this present study with a total of 2,253 disease-biomarker associations, which means only a 2% of the total list of associations identified in this present study. \n\nA further analysis of the provenance of the cooccurrences, in terms of journals that report them, was carried out and results for the top 10 journals are represented in  . Concretely, these 10 journals report 13% (94,760) of the cooccurrences identified in the 12% (20,341) of the abstracts of the working set. Interestingly, the total number of cooccurrences is proportional to the number of disease-biomarker associations recorded from each of the top 10 journals. Note that the publication start year of each journal points out that not necessarily those journals reporting most disease-biomarker associations in our working set started their publication earlier than others (i.e., Plos ONE). Most of the articles published in these top 10 journals describe basic laboratory, translational, and clinical investigations, and some of them have a special focus on specific therapeutic areas: hematology (  Blood  ), immunology (  J Immunol.  ), and oncology (  Cancer Res., Clin. Cancer Res., Cancer  ,   Int. J Cancer  ). In fact, over 300 journals of the list include the \u201cclinical\u201d word in their name, over 200 include the word \u201ccancer\u201d or the prefix \u201conco\u201d, and around 140 include the prefix \u201cimmun\u201d; which are by far the main fields where biomarkers are being investigated. \n\nTwenty-one percent of the disease-biomarker associations were identified in the top 10 journals (56% of diseases and 68% of biomarkers collected in this study, resp.; see  ). Over 50% of the associations are retrieved from the first 100 journals (81% of diseases and 87% of biomarkers), and over 80% are from the first 500 journals (95% of diseases and 97% of biomarkers); and till we consider the first 1,000 journals we do not reach more than 90% of the total associations (98% of diseases and 99% of biomarkers). \n\nThis analysis shows that the number of journals and articles that report biomarkers information has increased during the last years, and this fact (i) expands the publication bias (few journals are specialized in biomarkers research and development, while most of journals include in their scope the biomarker topic or at least publish special issues devoted to biomarkers research), (ii) makes difficult the retrieval and exploitation of this information, and (iii) highlights the need of an improvement in the biomarker related data reporting [ ] to ensure better quality of automatic extraction by means of mining techniques. \n\n\n### 3.2. Analysis and Validation of the Disease-Biomarker Associations \n  \nThe 131,012 disease-biomarker associations were scored based on their mention frequency in MEDLINE (see   for details of the associations distribution based on the score described in  ). The top 10 associations with higher score are shown in  , where very well-studied disease biomarkers can be found (for instance, TP53 and ERBB2 for cancer and CD4 for immunodeficiencies). \n\n shows the analysis of the associations based on the Score  (a) and the number of publications (b). The percentage of associations reported by different numbers of publications (from 1-2 publications to more than 2,000) in the corpus under study (131,012 associations, light grey bars) is represented. The caption shows the data for the associations reported by more than 100 publications, which represent a small percentage of all the associations. Note that most of these associations (more than 90%) have been reported in publications from the last three years. \n\nIn general, associations with high score are supported by a high number of publications ( ), and globally around 80% of the associations are supported by only 1 or 2 publications and have a low score. From this set, 35% corresponds to studies published in the last 3 years ( ). The novelty of these associations could explain the low number of supporting articles. Thus, it is likely that the remaining 65% of the associations supported by very few publications represent studies that could no longer be reproduced or are focused on very specific genes or diseases that are not of widespread interest, as in the case of the most prevalent diseases such as some types of neoplasms. It is noteworthy that, for most of the associations supported by more than 10 articles, at least one of these articles has been published in the last 3 years ( ). \n\nA further analysis of the results allows us to identify both the set of biomarkers associated with a large number of diseases (see  ) and the set of biomarkers associated with few (1 or 2) diseases. This information can be an indication of the \u201cspecificity\u201d of a biomarker with respect to diseases. For example, a biomarker associated with many different diseases would be less specific than the other that has been studied in relation to a single disease, and,   vice versa  , the same consideration can be done for the diseases. The distribution of the number of associated diseases (for biomarkers) and biomarkers (for diseases) is depicted in  . For example, the genes PANK2, ANK3, and RNF7 appear as very specific biomarkers as they are associated with a single disease. On the other hand, several genes related to immune responses have been reported in associations with hundreds of diseases, such as IL6, TNF, and CD4 (  and  ). \n\nWith respect to diseases, the results show that cancer is the therapeutic area that has more associated biomarkers (  and  ). For instance, leukemia is associated with 782 biomarkers, and some of them (NOTCH1, SF3B1, and BIRC3) have been found in recent literature reporting [ ]. In contrast, few biomarkers have been identified for diseases like lupus vulgaris and Bowen's disease. \n\nThe disease-biomarker associations were also assessed according to the disease classes of the MeSH disease classification [ ], indicating that neoplasm, nervous system diseases, and immune system diseases are by far the ones more investigated in the biomarkers research field (see  ). \n\nIn average, 11% of the disease-biomarker associations identified by our text mining approach were found in DisGeNET. Since DisGeNET contains information on the genetic determinants of human diseases and is not specially focused on biomarkers as defined in the present study, it is not surprising that only a small fraction of the information extracted from the literature is contained in DisGeNET database (July 2012 release). In addition, lag time in the population of the source databases by human curators may account for this difference. The dataset provided by the text mining approach here presented constitutes a large and valuable source of information on disease-related biomarkers, which can be used to populate specialized databases and to guide further research on biomarker validation. However, it is important to note that, based on the relation extraction approach used in this study, a proportion of the disease-biomarker associations found by this approach could be false positives. Future work will take in consideration the syntactic structure of the sentences in which a biomarker and a disease cooccur for the relation extraction process, with the aim of improving the precision of the approach. Search of semantic patterns reported in the abstracts' sentences will be checked in parallel to new data available from current and new disease-related biomarkers databases, with the aim of providing comprehensive and up-to-date knowledge to those biomedical researchers working in the disease-related biomarker field. \n\n\n### 3.3. Related Work \n  \nOnly few studies have proposed text mining approaches for extraction of biomarker related data [ \u2013 ]. For example, Younesi et al. presented a methodology for the retrieval of documents about biomarkers and showed as use cases the identification of markers for Alzheimer disease and multiple sclerosis [ ]. Hui and Chunmei propose a finite state machine to identify pathways and diseases related to biomarkers [ ]. We show in this study that a knowledge-driven approach is able to systematically exploit biomarker-specific information from large literature databases (e.g., MEDLINE) providing a comprehensive resource of biomarkers associated with diseases covering all the therapeutic areas. \n\n\n\n## 4. Conclusions and Future Directions \n  \nThe biomedical literature represents a rich resource for the identification of biomarker related information. However, both the size of the literature databases and the lack of standardization make difficult the automatic exploitation of the information contained in these resources. Text mining approaches have proven to be useful for the extraction of relations between entities, especially for the identification of interactions between proteins [ ]. Here, we show that a knowledge-driven text mining approach can exploit a large literature database to extract a dataset of biomarkers related to diseases covering all therapeutic areas. \n\nA bibliometric analysis of the journals reporting biomarker related information during the last 40 years highlighted the disparity among journals of different disciplines which expands the publication bias, hampers the information retrieval and its exploitation, and, even, evidences the need of a standardization of the biomarker related data reporting to improve the quality of automatic extraction by means of mining techniques and gain confidence with their outcomes. \n\nOur methodology focused on the extraction of disease-biomarker associations reported in the literature. This knowledge-driven approach takes advantage of the annotation of MEDLINE publications pertaining to biomarkers with MeSH terms, narrowing the search for specific publications and therefore minimizing the false positive ratio. The application of this methodology resulted in the identification of 131,012 disease-biomarker associations between 2,803 genes and 2,751 diseases and represents a valuable knowledge base for those interested in disease-related biomarkers. The results of this present study are available at  . \n\nFuture work in this area will focus on the identification of the type of association between the disease and the biomarker (for instance, distinguishing between the different levels of certainty that can be used to express an association or to specify the type of molecular change of the gene or protein associated with the disease). In addition, other types of molecules that can act as disease biomarkers could be identified as well. \n\n \n", "metadata": {"pmcid": 4009255, "text_md5": "a49ed873d9d6771572b41a6c68bd09f9", "field_positions": {"authors": [0, 80], "journal": [81, 95], "publication_year": [97, 101], "title": [112, 197], "keywords": [211, 211], "abstract": [224, 1469], "body": [1478, 30547]}, "batch": 1, "pmid": 24839601, "doi": "10.1155/2014/253128", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4009255", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4009255"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4009255\">4009255</a>", "list_title": "PMC4009255  A Knowledge-Driven Approach to Extract Disease-Related Biomarkers from the Literature"}
{"text": "Gu, Wenhao and Yang, Xiao and Yang, Minhao and Han, Kun and Pan, Wenying and Zhu, Zexuan\nBioinform Adv, 2022\n\n# Title\n\nMarkerGenie: an NLP-enabled text-mining system for biomedical entity relation extraction\n\n# Keywords\n\n\n\n# Abstract\n \n## Motivation \n  \nNatural language processing (NLP) tasks aim to convert unstructured text data (e.g. articles or dialogues) to structured information. In recent years, we have witnessed fundamental advances of NLP technique, which has been widely used in many applications such as financial text mining, news recommendation and machine translation. However, its application in the biomedical space remains challenging due to a lack of labeled data, ambiguities and inconsistencies of biological terminology. In biomedical marker discovery studies, tools that rely on NLP models to automatically and accurately extract relations of biomedical entities are valuable as they can provide a more thorough survey of all available literature, hence providing a less biased result compared to manual curation. In addition, the fast speed of machine reader helps quickly orient research and development. \n\n\n## Results \n  \nTo address the aforementioned needs, we developed automatic training data labeling, rule-based biological terminology cleaning and a more accurate NLP model for binary associative and multi-relation prediction into the   MarkerGenie   program. We demonstrated the effectiveness of the proposed methods in identifying relations between biomedical entities on various benchmark datasets and case studies. \n\n\n## Availability and implementation \n  \nMarkerGenie is available at  . Data for model training and evaluation, term lists of biomedical entities, details of the case studies and all trained models are provided at  . \n\n\n## Supplementary information \n  \n are available at   Bioinformatics Advances   online. \n\n \n\n# Body\n \n## 1 Introduction \n  \nRelations of biomedical entities (bioentities) are critical to biomedical studies and are hidden in a large number of biomedical articles. In this work, the main goal is to rapidly and accurately identify   associative   relations between a pair of biomedical entities present in the literature. We consider two entities to be   associative   in a context when they are described to be correlated directly, causal or non-causal. Most biomedical entity relations such as a biomarker and a disease are associative. Determining such a relation is typically an important first step to guide additional wet-lab or clinical studies to verify the   diagnostic, predictive, prognostic, predisposing   and   treatment   relation. Without being exhaustive, a bioentity may refer to a disease, a gene, a metabolite or a microbial taxa. \n\nMany biomedical text-mining methods have been used to identify associations of diseases and biomarkers. These methods can digest research articles more efficiently and comprehensively compared to human researchers and can help prioritize the targets in diagnoses and drug target discovery. In the following, we provide a brief overview of the current status of biomarker relation database curation and text-mining methods. \n\nBioentity relation databases are typically manually curated and serve as the ground truth for the research community. For example,   manually extracted 292 microbe\u2013microbe, 39 disease\u2013disease and 483 microbe\u2013disease associations from microbiome-related articles.   established a disease\u2013microbiome database by querying PUBMED database using criteria ([(\u2019microbiota\u2019 OR \u2019microbiome) and (\u2019health\u2019 OR \u2019disease\u2019)] and [microbiome alterations]). Then, the disease, microbiome terms and their relations were extracted manually.   created a database that relates human metabolism with genetics, microbial metabolism, nutrition and diseases. \n\nTo automate and expand the scope of entity relation extraction, a few methods, including PolySearch2 ( ), BEST ( ), GenCLiP 3 ( ), STRING ( ), IBDDB ( ) and DrugShot ( ), have been introduced. With a common assumption that the strength of entity association is positively correlated with their co-occurring frequency in the same context, these methods first identified frequently co-occurring entities of interest, then refined the entity relation with different scoring and filtering criteria. However, there are some limitations. A pair of entities with low co-occurring frequency can be reliable but would be missed. For example, recently discovered relations would have few mentions in the literature. Meanwhile, high co-occurrence counts can include many false positives that require   ad hoc   and complex rules to eliminate. \n\nThese limitations have been addressed by supervised machine learning (ML)-based methods ( ;  ;  ). To curate CIViC database ( ), published literature was parsed and sentences containing a pair of target entities were identified via exact string matching. A support vector machine-based classifier was then trained using 800 labeled sentences.   proposed a novel neural network architecture for identifying protein\u2013protein interactions (PPIs) from biomedical text using a tree long-short-term memory (LSTM) network with structured attention to traverse the dependency tree of a sentence through a child sum tree LSTM. Meanwhile, structural information was learned through a parent selection mechanism by modeling non-projective dependency trees. The main challenge for the application of ML methods is the lack of labeled training data. Although distant supervision ( ) can be used to acquire additional training data with positive labels, negative training data cannot be generated and this method requires a high-quality knowledge database that is typically hard to curate. \n\nIn this article, we treat finding relevant biomedical entities as a sentence-level binary/multiple relation classification task. During entity extraction, we introduced rule-based strategies to reduce false positive extractions as the existing bioentity terminologies still contain a large number of ambiguities and sometimes, errors. To address the lack of training data and the labor-intensive manual labeling process, we proposed an automated training data generation using co-occurrence frequency matrix and demonstrated its practical use. We then developed a new model, SBGT (SciBERT+Gumbel Tree-GRU), for relation classification that uses SciBERT ( ) to encode the context features of words and Gumbel Tree-GRU ( ) to encode the syntactic structures of sentences. \n\nWe provide MarkerGenie as an online text-mining tool. The current release includes the following entities: diseases, microbiomes, genes and metabolites. The corpus currently includes the free-text and tables of articles in PubMed and PubMed Central. The overview of MarkerGenie is given in   that includes four main components: user query processing, article retrieving and sentence filtering, model-based classification and results reporting. The implementation details of MarkerGenie are provided in  . \n  \nMarkerGenie online workflow. MarkerGenie is a text-mining system for identifying biomarker relations with diseases. Given a query disease term, MarkerGenie first identifies relevant disease terms through fuzzy matching. Then, it retrieves articles according to the synonyms of the disease, and select the sentences that contain both the disease and biomarkers through entity extraction. Afterwards, the filtered sentences are classified by NLP models. Finally, the system returns the biomarkers related to the disease extracted from the literature in detail that including the source sentences, tables and articles. To improve speed, result caching was used \n  \n\n## 2 Materials and methods \n  \n### 2.1 SBGT model \n  \nIn the proposed SBGT model, we used SciBERT ( ) to extract the contextual features of words given the input sentence. SciBERT can improve the handling of unseen and rare words by using subword tokenizer in between words and characters. It had been experimentally shown to outperform BERT-Base and Bio-BERT in relation extraction of biomedical text ( ). Then, we used the Gumbel Tree-GRU ( ) to encode the syntactic structure. The encoded vectors were concatenated and fed into a fully connected layer for prediction. As shown in  , given a sentence, SciBERT extracts the contextual features of each word. Each word is encoded as a 1*768 vector. Then Gumbel Tree-GRU is used to organize those words into a vector to represent the sentence. Afterward, the vector as well as the contextual features of Entity1 and Entity2 are concatenated to indicate their relation. Finally, a fully connected layer is applied to predict the probability of the relation falling within each category. \n\n\n### 2.2 Unsupervised training data generation for binary relation classification \n  \nTo generate the training data, a co-occurrence frequency matrix of the bioentities from sentences was first constructed from free-text in PubMed and PubMed Central. We chose entity pairs with the most co-occurrence counts and used two thresholds, \u2018minimum co-occurrences   t  \u2019 and \u2018truncating quantity   t  \u2019, to generate the positive data. Particularly, sentences containing a pair of entities co-occurring   times were considered. At most   t   of these sentences were retained to prevent the bias toward high frequency entity pairs. The default values of   t   = 10 and   t   = 50 were empirically set and used in all current experiments. To generate negative data, sentences containing entity pairs with the frequency of one in the matrix were included except the ones that contain a single disease term and biomarker term; because we found that the latter was more likely to be a positive case. Note that the negative sample means no direct association between two biomedical entities in a sentence. Same as co-occurrence-based methods, some rarer and possibly more relevant biomedical associations may be missed by ignoring low-occurring data. However, the samples generated here were used as the labeled data to train a model rather than used as the final result. The associative relation between a pair of bioentities is extracted by the trained model regardless of their co-occurrence frequency in the actual prediction stage. An example of the positive and negative data generation process is given in  . The complete training data was generated subject to a 6:4 ratio for positive and negative instances. The ratio is consistent with the fraction of positive and negative instances observed in the literature. The generated training data were further divided by an 8:2 ratio into training and validation sets for model parameter optimization on F1-score. The model performance was measured on independent datasets. \n  \nExample of unsupervised training data generation. The heatmap of the co-occurrence of 20 diseases and 20 microbes in the literature is shown in the left part of this figure. Gastroenteritis and Vibrio parahaemolyticus co-occur most frequently: more than 400 times, greater than a predefined threshold, so they are considered related and the corresponding sentences in the articles were selected as positive samples. On the contrary, the low co-occurrence couples, e.g. Liver Cirrhosis and Alkaline xylosoxidans, tend to be irrelevant and the corresponding sentences formed the negative samples \n  \n\n### 2.3 Entity extraction \n  \nEntity extraction is a pre-requisite step of relation extraction. The entities of interest were curated in term lists in advance. Currently, the following entity term lists have been curated\u2014disease, microbiome, metabolite and gene as detailed in  . First, spaCy ( ) was used for sentence splitting and tokenization. Then, similar to CIViCmine, exact string matching was applied to the tokenized sentences to extract entities. To achieve this, we first constructed a trie on all synonyms and then located the most extended term in sentences by traversing the trie. This strategy has a run time complexity of O(  n  ) for a length-  n   sentence. \n\nTo improve the accuracy of entity recognition, rule-based filtering was further applied. If a disease term had a prefix of a letter followed by a dot, like \u2018s. pneumonia\u2019, the term was disregarded; we also removed term with a length less than four characters unless it was determined to be an abbreviation, conforming the pattern of \u2018synonym + (entity)\u2019. \n\n\n### 2.4 Relation extraction from tables \n  \nDifferent from the classification-based relation extraction used for text, we used rule-based methods on tables. A table was first extracted and stored as a tuple (caption, table \u2013 head, table \u2013 body: list of data rows). The bioentity relations generally appear in two different patterns in a table as illustrated by the disease\u2013microbiome relation extraction example: when a disease term and a collective term of the microbiome (e.g. \u2018microbiome\u2019, \u2018bacteria\u2019) co-occurred in the caption of a table and the microbiome terms were present in the body of the table, all microbiome terms in the table body were considered to be related to the disease ( ). When a disease term and a specific microbiome term co-occurred in a row or caption of the table, they were considered to be related ( ). \n  \nIllustration of relation extraction from tables. (  A  ) CRC and a collective terms of the microbiome (\u2018bacterium\u2019) co-occur in the caption of the table, so all microbes in the table body are considered to be related to CRC. (  B  ) CRC and a specific microbiome term (\u2018Fusobacterium nucleatum\u2019) co-occur in a row of the table. They are considered as related \n  \n\n### 2.5 Granular relation classification between a disease and bioentities \n  \nWhen a disease and a bioentity were determined by binary classification to be associative, MarkerGenie can further predict them to be one of the five granular relation types\u2014  Predictive, Prognostic, Diagnostic, Predisposing   or Treatment   if there is a potential specific relation between them judged by CIViCmine\u2019s search terms ( ). The training sentences of this classification task were first generated via distant supervision method using knowledge databases of CBD ( ), MarkerDB ( ) and Oncomx ( ). Then we used a term list (e.g. \u2018risk\u2019 and \u2018survival\u2019) provided by CIViCmine to screen sentences that potentially contain one of the five specific relations. In addition, we expanded the term list by using pre-trained word vectors to include synonyms to increase the size of training data. \n  \nThe workflow of MarkerGenie for classifying the granular relation types between a disease and bio-entities. When a disease and a bio-entity are determined by binary classification to be associative, MarkerGenie judges if there is a potential specific relation between them by using CIViCmine\u2019s search terms. Then, the trained model is applied to predict the granular relation types (Predictive, Prognostic, Diagnostic, Predisposing or Treatment) of the filtered sentences \n  \n\n\n## 3. Results \n  \nIn this section, we first demonstrate the improved accuracy of SBGT model by applying it on the curated benchmark datasets that were used by previous methods\u2014the binary relation classification of PPI ( ) and the multi-relation classification of drug\u2013drug interaction (DDI\u201913) ( ). Next, we demonstrate the validity of automatic training data generation by applying MarkerGeine to disease\u2013biomarker binary associative relation classification. This task does not require any prior knowledge or curated databases. However, when curated databases are available, MarkerGenie would generate training data via distant supervision strategy and produce multi-relation classification. This was demonstrated on disease\u2013gene multi-relation extraction task as carried out in CIViCmine ( ). Finally, we demonstrate how MarkerGenie can aid biomarker discovery with a few case studies. \n\n### 3.1 Binary relation classification \n  \nThe SBGT model was first validated on the PPI corpora ( ), which was used as a benchmark dataset by prior methods. The dataset information and hyper-parameters of SBGT are summarized in  . To ensure the generalization of the learned model, we replaced the pair of proteins in each sentence with \u2018PROTEIN1\u2019 and \u2018PROTEIN2\u2019. In addition, all sentences were truncated or padded to a maximum length of 100. The performance of SBGT was compared with seven other state-of-the-art models\u2014sdpCNN ( ), sdpLSTM ( ), Bert ( ), BioBERT ( ), DRCNN ( ), Bi-LSTM ( ) and BioKGLM ( ). The evaluation scheme and parameters of the compared algorithms were all set per the original papers. The F1-scores of these methods are given in  , where SBGT achieved 3.2% improvement over the runner up. Since some of the methods were evaluated with macro F1-score in the corresponding references, we also included this metric in  , where SBGT showed consistent superiority to the compared models, including DCNN ( ), Att-sdpLSTM ( ), tLSTM ( ) and DRCNN. \n  \nPPI and DDI\u201913 dataset information and hyper-parameters of SBGT \n      \nComparison of SBGT and other methods on PPI dataset in terms of F1 score and macro-F1 score \n  \n\n### 3.2 Multi-relation classification \n  \nWe applied SBGT to the DDI\u201913 dataset ( ) where the goal was to determine specific relations (defined as {NA, ADVICE, EFFECT, MECHANISM, INT}) given two drugs. Like binary classification, we replaced the pair of drugs in each sentence with \u2018<ent1 >\u2019 and \u2018<ent2 >\u2019 and all sentences were truncated or padded to a maximum length of 100. On this dataset, SBGT were trained with the hyper-parameters shown in  . SBGT was compared with the seven other state-of-the-art models, including SCNN ( ), CNN-bioWE ( ), MCCNN ( ), Joint AB-LSTM ( ), RvNN ( ), Position-aware LSTM ( ) and BERE ( ) in terms of precision, recall and F1-score. As shown in  , SBGT attained the best trade-off of precision and recall. In terms of F1 score, SBGT obtained a score of 77.1% that is \u223c3% higher than that of the second best model. \n  \nComparison of SBGT and other methods on DDI\u201913 dataset in terms of Precision, Recall and F1 score \n  \n\n### 3.3 Disease\u2013biomarker associative binary classification with automatic training data generation \n  \nWe selected three major biomarker types\u2014microbiome, metabolite and gene\u2014to study their   associative   relations with diseases from publicly available articles of PubMed and PubMed Central. The labeled training data for these tasks are scarce or even missing though some have been manually curated ( ;  ). We introduced an unsupervised method that can automatically generate the labeled training data in Section 2. \n\nAdmittedly, the automatic label generation can include many false positive instances\u2014upon manual inspection, around 15\u201320% of the positive samples are incorrectly labeled. Yet, we can obtain a large amount of data within a few hours\u2019 run time. We have obtained around 6000 disease\u2013microbiome and 10\u00a0000 disease\u2013metabolite or disease\u2013gene training samples. The data of this size would be more suitable for deep learning strategies compared to the typical curated data size in the hundreds scale ( ). Though trained using noisy data increased the model bias, the overall model performance improved along with the size of training samples on the test data. As the example of disease\u2013microbiome shown in  , the F1 value generally increased as more data became available. \n  \nAn illustration of the impact of automatically acquired training data on model performance. The SBGT model was trained on different sizes of disease\u2013microbiome training dataset and F1 scores were obtained on the independent 477 test samples. Each experiment was repeated three times and the F1 score was the average \n  \nTo evaluate the performance of MarkerGenie on the above tasks, we manually curated 477 disease\u2013microbiome samples and 610 annotated disease\u2013metabolite samples. For disease\u2013gene prediction, 382 labeled disease\u2013gene samples were directly obtained from  . MarkerGenie predicted disease\u2013microbiome, disease\u2013metabolite and disease\u2013gene relations with precisions of 83.28%, 85.26% and 82.01%, respectively ( , the corresponding F1 scores and precision\u2013recall curves are shown in  ). Empirically, around 60\u201370% instances of disease\u2013biomarker pairs co-occurring in the same sentence have a true positive relation, MarkerGenie therefore removed over 10\u201320% of the false positive instances. For these three tasks, MarkerGenie recalled 84.92%, 89.73% and 88.43% of the relations, respectively. We note that, the reported performance from   on this disease\u2013gene relation dataset obtained from the same study had both higher precision (\u223c5%) and recall (\u223c2%), yet its generalizability cannot be independently evaluated. Also note that,   used the rule-based method that factors in the prior knowledge of validated disease\u2013gene relations, which is generally unknown to the model. \n  \nPerformance of MarkerGenie on disease\u2013biomarker relation identification. (  A  ) Precision and recall of disease\u2013biomarkers\u2019 associative binary classification. (  B  ) F1 scores of disease\u2013biomarkers\u2019 associative binary classification. (  C  ) Precision-recall curves, due to the high threshold at the beginning, there are few samples marked as positive examples, so the upper left part of the curve fluctuates greatly. (  D\u2013G  ) Precision-recall curves of MarkerGenie and CIViCmine on four specific relation extraction, i.e. predictive, prognostic, diagnostic and predisposing \n  \n\n### 3.4 Granular relation extraction via distant supervision \n  \nFollowing binary associative relation prediction, MarkerGenie can rely on disease\u2013biomarker relation knowledge-bases to automatically generate training data via distant supervision, then yield more deterministic relation predictions. In this part, the performance of MarkerGenie was verified with the 250 test samples from   that contains four granular relation types,   Diagnostic, Predictive, Predisposing   and   Prognostic   between cancers and genes. MarkerGenie was compared with CIViCmine ( ) in terms of precision and recall, where the precision\u2013recall curves of the two methods are shown in  . MarkerGenie obtained better precision and recall than CIViCmine. \n\nIn the following, we demonstrate how MarkerGenie can be applied to biomarker discoveries with different case studies. \n\n#### 3.4.1 Identification of colorectal cancer-related microbes \n  \nColorectal cancer (CRC) is the third most common cancer worldwide and one of the primary causes of cancer-related deaths ( ). The association between CRC and the human gut microbiome is a focus of the current CRC research ( ;  ;  ). In this study, we used MarkerGenie to find the microbes related to CRC from the literature and manually verified the results. \n\nIn searching for microbes related to CRC, MarkerGenie returned a total of 2257 sentences that included 264 microbes. Among these 2257 sentences, 2118 were correctly predicted, whereas 98 were wrongly predicted and 41 were difficult to judge via manual inspection. The overall sentence\u2019s binary classification precision is 93.8%. For microbes, 247 out of 264 microbes are associated with CRC. In  , an example list of microbes and the corresponding sentences is shown in A. The top 10 microbes with the highest occurrences are shown in B, among these, eight of them have been previously shown to be significantly associated with CRC in the meta-analysis study ( ). The remaining two microbes \u2018Helicobacter pylori\u2019 and \u2018Human papillomavirus\u2019 also have been shown to be strongly related to CRC in more recent work ( ;  ). These results should provide a good reference to researchers studying CRC and the microbiome. \n  \nStatistics of microbes related to CRC returned by MarkerGenie. (  A  ) Example of microbes and the corresponding sentences. (  B  ) The top 10 microbes returned by MarkerGenie \n  \nAs discussed earlier, upon a positive prediction of binary associative relation between CRC and a microbe, we can further generate more deterministic relation classification via distant supervision (see Section 2.5). Here, MarkerGenie produced 185 predisposing, 181 predictive, 154 prognostic, 67 treatment and 33 diagnostic relations. \n\n\n#### 3.4.2 Identification of breast cancer-related genes \n  \nIdentifying relevant genes is valuable for the early diagnosis, prevention and treatment of breast cancer ( ;  ;  ). We used MarkerGenie to search and rank the importance of the genes associated with breast cancer. Similar to BEST ( ), we presented the top 10 genes found in MarkerGenie along with the ones identified by BEST, Polysearch2 and CIViCmine in  . Eight of them were identified in at least one of the other methods and reported to be associated with breast cancer in CIViC knowledge-base ( ) or NCBI\u2019s GENE database ( ). The remaining two genes \u2018  ITK  \u2019 and \u2018  NAC  \u2019 were false positives upon inspection. Specifically, the term \u2018NAC\u2019 refers to a type of therapy for breast cancer. For \u2018ITK\u2019, the term identified in association with breast cancer is \u2018EMT\u2019, which is an alias of \u2018  ITK  \u2019 gene. However, \u2018EMT\u2019 refers to \u2018epithelial-mesenchymal transition\u2019 that is a process linked to breast cancer. Both false positives are valid entries in the gene list but had different meanings in the text. To further improve accuracy, ambiguities of terms in the list need to be resolved. \n  \nTop 10 genes retrieved with the query \u2018Breast cancer\u2019 by different systems \n  \n\n#### 3.4.3 Disease\u2013miRNA association inference \n  \nThe output of MarkerGenie can also be directly used for other applications such as association prediction. We select the disease\u2013miRNA association inference as a suitable application as it involves three-way interactions\u2014disease\u2013disease, disease\u2013miRNA and miRNA\u2013miRNA. The details of the inference method and experimental results are provided in  . Based on miRNA\u2013miRNA functional similarity, disease\u2013disease semantic similarity and the disease\u2013miRNA associations identified by MarkerGenie, we can infer unknown disease\u2013miRNA associations as accurately as the methods based on curated databases like HMDD ( ). MarkerGenie can serve as a surrogate for the laboriously curated databases. \n\n\n\n\n## 4 Conclusions \n  \nIn this work, we proposed a text-mining system, MarkerGenie, to identify bioentity relations from texts and tables of publications in PubMed and PubMed Central. The identification problem was formulated as a relation classification task. A new unsupervised training data generation method and new classification model SBGT were introduced and tested with benchmark datasets and real-world case studies. The experimental results demonstrated the effectiveness of the system. There are further rooms for improvement, including cross-sentence relations extraction, improving negative samples selection, and better ways to handle ambiguities of short entity terms such as gene symbols. It is also favorable to recognize the context (e.g. conditions of experiments and biology relevance) in which the biomarkers are identified and to improve the entity extraction with text-mining methods (e.g. PubTator and NER models). \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 9710573, "text_md5": "3b31af4a8858bc7975b034c2a5e04f8e", "field_positions": {"authors": [0, 88], "journal": [89, 102], "publication_year": [104, 108], "title": [119, 207], "keywords": [221, 221], "abstract": [234, 1864], "body": [1873, 27009]}, "batch": 1, "pmid": 36699388, "doi": "10.1093/bioadv/vbac035", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9710573", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9710573"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9710573\">9710573</a>", "list_title": "PMC9710573  MarkerGenie: an NLP-enabled text-mining system for biomedical entity relation extraction"}
{"text": "Larmande, Pierre and Liu, Yusha and Yao, Xinzhi and Xia, Jingbo\nGenomics Inform, 2021\n\n# Title\n\nOryzaGP 2021 update: a rice gene and protein dataset for named-entity recognition\n\n# Keywords\n\nbiological dataset\ngene mention\nnamed entity recognition\nnatural language processing\nOryza species\n\n\n# Abstract\n \nDue to the rapid evolution of high-throughput technologies, a tremendous amount of data is being produced in the biological domain, which poses a challenging task for information extraction and natural language understanding. Biological named entity recognition (NER) and named entity normalisation (NEN) are two common tasks aiming at identifying and linking biologically important entities such as genes or gene products mentioned in the literature to biological databases. In this paper, we present an updated version of OryzaGP, a gene and protein dataset for rice species created to help natural language processing (NLP) tools in processing NER and NEN tasks. To create the dataset, we selected more than 15,000 abstracts associated with articles previously curated for rice genes. We developed four dictionaries of gene and protein names associated with database identifiers. We used these dictionaries to annotate the dataset. We also annotated the dataset using pre-trained NLP models. Finally, we analysed the annotation results and discussed how to improve OryzaGP. \n \n\n# Body\n \n## Introduction \n  \nThe past few decades have seen a deluge of information in agronomy. However, a substantial proportion of this information is available in unstructured scientific documents, such as journal articles, reviews, abstracts, and reports. Despite advances in data sciences, innovations in agronomy are still often text-based. One of the challenges is to extract the biological entities and their relationships contained in text fields and scientific papers. Many of these text fields contain molecular mechanisms and phenotypes of interest that are often described by complex expressions associating biological entities linked by specialised semantic relationships (e.g., \"  Ehd1 and   Hd3a   can also be down-regulated by the photoperiodic flowering genes Ghd7 and Hd1  \" source PMID: 20566706). To address this issue, the objective is to develop computational tools to extract biological entities and their relationships in order to extract relevant information\u2014here, the entities   Ehd1  ,   Hd3a  ,   Ghd7  , and   Hd1   and the down-regulated relationship. The biomedical field has long experience in developing NLP approaches. The Biocreative [ ] and BioNLP conferences [ ] have demonstrated numerous advances in this area achieved through the development of datasets and tools. However, little research has been done on this issue in plant science and, more precisely, in the rice sector. For these reasons, we developed a dedicated dataset for rice named OryzaGP. The first release of OryzaGP was initially published in 2019 during BLAH5. The first version originally gathered relatively few PubMed abstracts and focused on named entity recognition (NER) by providing only entities tagged with gene or protein labels. In this new version, we updated the number of PubMed abstracts and provided both NER and the results of named entity normalisation (NEN) when available. Moreover, we tried to merge several database identifiers coming from different resources under the same name. The next section will describe the procedure of building the OryzaGP dataset and how it was annotated. \n\n\n## Methods \n  \nSimilarly to the first version, we started by downloading the Oryzabase reference datasets from the Oryzabase [ ] web application. Oryzabase provides a manually curated dataset for new rice-related PubMed entries. We filtered out a list of PubMed identifiers that we used to create the OryzaGP_2021 project on PubAnnotation [ ]. PubAnnotation [ ] is a repository of text annotations related to literature in the life sciences, such as PubMed or PMC articles. It also provides features to create, manage, and access annotations through APIs. \n\nAnnotations were conducted through two applications: PubDictionary and HunFlair [ ]. PubDictionary is a repository of public dictionaries for the life sciences. It was developed as a model annotation service for PubAnnotation and provides the RESTFul API for dictionary-based text annotation. HunFlair is a NER tagger covering five biomedical entity types. It is integrated into the Flair NLP framework, and it uses a character-level language model pre-trained on roughly 24 million biomedical abstracts and 3 million full texts. \n\nIn order to use PubDictionary to annotate OryzaGP, we created several dictionaries of gene/protein entities. We first downloaded the Oryzabase gene dataset, which contains several gene mentions associated with database identifiers. We created the Oryzabase dictionary containing labels, gene names, symbols, synonyms and Oryzabase identifier URIs. Next, we repeated the same process to create the RAPDB [ ], MSU [ ], and UniProt [ ] dictionaries. Additionally, we refined the RAPDB and UniProt dictionaries by adding new entries extracted from the RAPDB gene datasets. All these dictionaries were uploaded to PubDictionary and used to create several annotators.   shows the size (i.e., the number of entries) of these dictionaries. Finally, we utilized PubAnnotation to run several annotations on OryzaGP using these dictionaries. We merged these annotations in a single project ( ). \n\nHunFair, which comes with models for genes, proteins, chemicals, diseases, species and cell lines, is an advanced NER tagger for biomedical texts. Compared with other biomedical NER tools, such as GNormPlus [ ] and HUNER [ ], HunFlair showed better performance on the BioNLP 2013 CG [ ] and Plant-Disease corpus [ ]. In the OryzaGP project, we imported the HunFlair pre-trained model directly to annotate the abstracts in OryzaGP. HunFlair annotated each abstract with genes, proteins, chemicals, diseases, and species, and converted the JSON results into a format that met the requirements of the PubAnnotation platform. All annotations created by HunFlair were prefixed with   hunflair:NA   plus the entity type (e.g., gene, disease, cell line, chemical, and species). \n\n\n## Results \n  \nCompared to the first version of OryzaGP, this updated version was significantly improved.   compares basic statistics on both versions. The number of articles was increased from 10,000 to 15,000, and consequently the number of sentences and words increased as well. The number of annotations also increased. In the first version, the annotations were produced with an improved Bi-LSTM-CRF model from [ , ] previous research [ , ]. Around 29,000 annotations were found. In this current version, we used multiple annotators to achieve this goal (see the Materials and Methods section) and obtained about 1 million annotations ( ). The annotations were merged into the single project.   shows an example of merged annotations with the TextAE editor from PubAnnotation. We can see tagged entities with a class label and other entities tagged with database identifiers (i.e., NEN). We obtained NEN results in 64% of cases, which means that nearly two-thirds of the annotations are linked with a database identifier. \n\nTo our knowledge, OryzaGP is the first dataset created for genes and proteins in rice species. It can help to better train NLP tools to recognize rice-related biological entities. Moreover, this new version contains a large number of normalized genes and proteins. However, manual checking of these annotations revealed some false positives. For this iteration of the project, it was not possible to develop a strategy to automatically evaluate the rate of false-positive and false-negative annotations. This remains a task for future work. \n\n### Future work \n  \nOur future work will first focus on identifying false positives and negatives to improve annotations. We manually observed that false positives often occurred with gene and protein full names. Some annotations did not match the whole sequence of words. Our hypothesis is that there often exist co-occurrences of full names and symbols in the same sentence or abstract. Thus, we will analyze and classify these co-occurrences. \n\nNext, we plan to normalise the annotations done by HunFlair. Some are already merged with NEN, but some are not. We plan to analyse these annotations, especially those standing for gene symbols, and set up a strategy to normalise them. \n\nFinally, we are interested in adding new annotation types such as plant organs or plant traits. Thus, we will create dictionaries and train NLP tools to achieve this goal. \n\n\n \n", "metadata": {"pmcid": 8510865, "text_md5": "6483ad2bcec7fdff5f6299075c378a12", "field_positions": {"authors": [0, 63], "journal": [64, 79], "publication_year": [81, 85], "title": [96, 177], "keywords": [191, 290], "abstract": [303, 1384], "body": [1393, 8687]}, "batch": 1, "pmid": 34638174, "doi": "10.5808/gi.21015", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8510865", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8510865"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8510865\">8510865</a>", "list_title": "PMC8510865  OryzaGP 2021 update: a rice gene and protein dataset for named-entity recognition"}
{"text": "Srivastava, Prashant and Bej, Saptarshi and Yordanova, Kristina and Wolkenhauer, Olaf\nBiomolecules, 2021\n\n# Title\n\nSelf-Attention-Based Models for the Extraction of Molecular Interactions from Biological Texts\n\n# Keywords\n\ntext mining\nself-attention models\nbiological literature mining\nrelationship extraction\nnatural language processing\n\n\n# Abstract\n \nFor any molecule, network, or process of interest, keeping up with new publications on these is becoming increasingly difficult. For many cellular processes, the amount molecules and their interactions that need to be considered can be very large. Automated mining of publications can support large-scale molecular interaction maps and database curation. Text mining and Natural-Language-Processing (NLP)-based techniques are finding their applications in mining the biological literature, handling problems such as Named Entity Recognition (NER) and Relationship Extraction (RE). Both rule-based and Machine-Learning (ML)-based NLP approaches have been popular in this context, with multiple research and review articles examining the scope of such models in Biological Literature Mining (BLM). In this review article, we explore self-attention-based models, a special type of Neural-Network (NN)-based architecture that has recently revitalized the field of NLP, applied to biological texts. We cover self-attention models operating either at the sentence level or an abstract level, in the context of molecular interaction extraction, published from 2019 onwards. We conducted a comparative study of the models in terms of their architecture. Moreover, we also discuss some limitations in the field of BLM that identifies opportunities for the extraction of molecular interactions from biological text. \n \n\n# Body\n \n## 1. Why Text Mining? \n  \nText mining techniques used for extracting information from text have been popularly used since 1992. Famous applications of text mining include IBM\u2019s Watson program, which performed spectacularly when competing against humans on the nightly game show Jeopardy [ ]. Such techniques have played a significant role over the years in extracting and organizing information from biological texts. For example, the popular STRING database [ ] uses automated text mining of the scientific literature to integrate all known and predicted associations among proteins, including both physical interactions and functional associations [ ]. \n\nBiological systems are complex in nature. Years of research have produced a large volume of publications on the key molecular players involved in numerous cellular processes, disease phenotypes, and diseases. For example, a PubMed search for the molecule \u201cp53\u201d produces more than a hundred thousand hits; a PubMed search for the disease \u201ccolorectal cancer\u201d produces more than two-hundred thousand hits. For cell-level and tissue-level processes such as \u201capoptosis\u201d and \u201cmetastasis\u201d, there are more than four-hundred thousand hits. \n\nIn the five-year span of 2016\u20132020, the average number of PubMed article hits per year for \u201cp53\u201d, \u201ccolorectal cancer\u201d, \u201capoptosis\u201d, and \u201cmetastasis\u201d were 4974, 13,548, 29,812, and 22,305, respectively. One obvious application for text mining is the search for information from the literature, as part of research projects. Since there are various databases and disease map projects that map out molecular interactions relevant to chosen diseases, the maintenance of such repositories requires substantial effort. A motivation for text mining is then also to assist the updating of data in repositories with new information from publications. \n\nModeling biological systems can have diverse motivations: investigating molecular interactions and their nature to understand regulatory mechanisms, investigating associations among molecules and diseases or broader disease phenotypes, investigating the consequences of genetic mutations and perturbations to cellular processes. Clearly, molecules such as genes, proteins, and drugs play a crucial role in such investigations. Rather than attempting to describe complex biological processes as a function of a handful of molecules, systems biologists increasingly appreciate the complexity of these systems, trying to visualize these processes as functions achieved through interactions among numerous molecular entities. These molecular entities (genes, proteins, or drugs) interact in harmony inside biological systems for each phenotype to be realized, be it a cellular process (e.g., cell signaling, metabolism, apoptosis), a disease phenotype (e.g., acute inflammation, metastasis), or even a disease (e.g., cancer, gaucher disease). However, a comprehensive systemic understanding requires extracting and integrating knowledge acquired from existing and new publications. In many cases, this results in large-scale models that require much manual effort from the modelers, who laboriously hand-pick knowledge from hundreds of publications [ ]. \n\nText mining and Natural-Language-Processing (NLP)-based techniques are finding their applications in reducing the efforts of biologists to mine the biological literature for tasks such as the creation of large-scale models and keeping databases updated. This recent field of research focused on automatic knowledge extraction and mining from biomedical literature is known as Biomedical Literature Mining (BLM). In this review article, we focus on recent developments in BLM for the extraction of molecular interactions from biological texts. \n\nBLM consists of several types of tasks, combinations of which can be realized as complex workflows to achieve the goal of knowledge extraction. An elementary task, for instance, is Named Entity Recognition (NER), which aims to identify biomedical concepts from given text corpora. State-of-the-art models can perform this task with high accuracy. This upstream task is usually followed by Relationship Extraction (RE). Approaches for the RE task can be broadly categorized as rule-based approaches and Machine Learning (ML) approaches. Rule-based approaches depend on predefined rules based on inherent textual patterns in biomedical texts. The success of such approaches depends on the quality of the designed rules. In ML approaches, RE is usually posed as a classification problem. However, the design of the classification problem can vary with the motivation of the modeler. For example, there can be a binary classification problem that aims to model merely whether there exists an interacting pair of proteins in a document. More complicated multiclass classification problems investigate the nature of interactions among entities [ ]. A general workflow for BLM is provided in  . \n\nML-based approaches that are used for RE have several broad categorizations such as feature-based approaches, kernel-based approaches, and neural-network-based approaches. Feature-based approaches involve the extraction of expert annotated lexical and syntactic features and use the same for modeling. Kernel-based approaches aim to map syntactic trees to higher-dimensional feature spaces by the proper choice of kernels. Neural-Network (NN)-based approaches can learn latent feature representations from labeled data. Neural network architectures such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRUs) have been widely explored in this domain. Recent advances in the field of NLP are due to the introduction of a new class, known as self-attention-based models. These models account for long-range dependencies in text data and can learn contextual associations in text data better than previous neural-network-based models [ ]. Using transformer architectures as the basis, several context-specific pretrained models such as BERT and BioBERT have been built, aimed at facilitating learning from biomedical texts [ , , ]. \n\nRule-based text mining approaches have been reviewed comprehensively by Zhao et al. [ ]. Several deep-learning-based approaches were covered by Zhang et al. [ ], covering publications until 2019. Self-attention-based models entered the stage around 2017; pretrained networks for the extraction of molecular interactions at an abstract or sentence level, from biomedical texts, such as BERT and BioBERT, were published after 2018. In this review, we therefore focus on self-attention-based models, both novel architectures and pretrained networks, published from 2019 onwards. For mining recent works on self-attention-based models, we searched for published articles and preprints that were publicly available. We searched for well-known databases for publications such as   arXiv,   bioRxiv,   PubMed,   Semantic Scholar, etc. In addition to these, we also searched for publication databases on recent issues of several related journals and conferences. We only surveyed research articles that involved a self-attention mechanism for biological literature mining. We first give a brief description of the philosophy behind self-attention. Next, we discuss the architectural aspects and compare the performances of some recent models proposed in the context of BLM. Finally, we discuss the pros and cons of using such models in the context of BLM before concluding our article. \n\nNote that, given that there are many abbreviations for the terminologies relevant to this article, we provide some important abbreviations in  , for the convenience of the readers. \n\n\n## 2. Evolution of Deep Learning Models for NLP \n  \nSequence-to-sequence models typically receive a sequence as the input and generate a sequence as the output. Input and output sequences can be numerical, time-dependent data, or string data. The Recurrent Neural Network (RNN) is a deep-learning-based model designed for learning from sequence data. At every learning step, RNNs take elements of a sequence as the input, generate an output for that time step, and update a hidden state that can be associated with the \u201cmemory\u201d of the network. For text-based data, RNNs once used to be the state-of-the-art models. However, RNNs proved to be less effective to learn from longer sequences, that is to create associations among elements of long sequences. This means that if there is a long sequence of text (a long sentence) and there is an association between two words, one located at the beginning of the sentence and the other towards the end, RNNs are unlikely to capture that information. LSTMs and GRUs were designed to mitigate this \u201cmemory\u201d problem. The extremely popular LSTM model, for example, is designed to retain or forget information that is stored in the hidden state sequentially. Transformers, in contrast to the previous models, receive the whole sequence as the input rather than taking elements of a sequence sequentially as inputs. To allow the model to recognize the sequential nature of the data, it employs the concept of positional encoding. The attention mechanism is then used to learn associations among elements of the sequence, which in turn are used to make decisions. Taking the entire sequence as the input helps this model learn relatively long-range associations among elements of a long sequence, which makes it apt for text data and thus applicable to NLP. Since the introduction of the attention model by Bahdanau et al. for machine translation in 2015, it has found applications in a wide range of NN-based architectures [ ], while it received more recognition after the introduction of transformer models in 2017 [ ]. However, apart from NLP, the attention mechanism has been applied in computer vision, time series analysis, and reinforcement learning [ , , ]. In the NLP domain, attention models have helped improve machine translation, question-answering problems, text classification, representation learning, and sentiment analysis [ , , , , ]. In what follows, we discuss some interesting aspects of the self-attention-based models. We briefly visualize the evolution of sequence-to-sequence models in  . \n\n### 2.1. Self-Attention and Its Advantages \n  \nA typical sequence-to-sequence model consists of an encoder\u2013decoder architecture [ ]. The traditional encoder\u2013decoder framework used in RNN, LSTM, or GRU has two main limitations, as mentioned in Chaudhari et al. [ ]:   \nThe encoder compresses all input information into a vector of fixed length, which is passed to the decoder, causing significant information loss [ ]; \n  \nSuch models are unable to model the alignment between input and output vectors. \u201cIntuitively, in sequence-to-sequence tasks, each output token is expected to be more influenced by some specific parts of the input sequence. However, decoder lacks any mechanism to selectively focus on relevant input tokens while generating each output token\u201d [ ]. \n  \n\nThe attention model tackles this issue by enabling the decoder to access the whole encoded sequence. The attention mechanism assigns attention weights over the input sequence, which captures the importance of each token in a sequence and prioritizes them for generating output tokens at each step. \n\nThe concept of self-attention came into prominence after the introduction of the transformer model. \u201cIntra-attention, also known as self-attention, is an attention mechanism relating different positions of a single sequence to compute a representation of the sequence\u201d [ ]. Vaswani et al. demonstrated that the transformer architecture has a shorter training time and higher accuracy for machine translation without any recurrent component [ ]. Transformers have become a state-of-the-art approach for NLP tasks, and they have been adopted for a variety of NLP problems such as the Generative Pretraining Transformer (GPT, GPT-2) for language modeling, the universal transformer for question answering, and Bidirectional Encoder Representations from Transformer (BERT) for language representation [ , , ]. The transformer model has two key aspects:   \n Positional encoding:   Given an input sentence in a transformer model, the model first creates a vectorized representation of the sentence   S  , such that each word in the sentence is represented by a vector of a user-defined dimension. The vectorized version of the sentence   S   is then integrated with positional encoding. Recall that, unlike sequence-to-sequence models such as RNNs and LSTMs, which would feed the sequence elements (words in a sentence) as the input sequentially, self-attention-based models feed the entire sequence (sentence) as the input at a time. This requires a mechanism that can account for the sequential structure of the input sequence/sentence. This is achieved through positional encoding. The formal expression for positional encoding is given by a pair of equations:\n \n\nIn Equations ( ) and ( ), the expression   pos   is used to denote the position of a word in a sentence and   d   denotes the dimension of user-defined dimensions for the word embeddings, that is each word is essentially perceived by the model as a   d  -dimensional vector. The index   i   runs over the dimensions of these word embeddings and take values in the range  . Note that Equations ( ) and ( ) propose two different functions over the vector, depending on whether one is calculating an odd index or even index of the word-embedding vector. The dependence of the positional encoding functions on  , given that these functions are periodic functions by design, ensures that several frequencies are captured over several dimensions of the word-embedding vectors. \u201cThe wavelengths form a geometric progression from   to 10,000 \u00d7  \u201d [ ]. Intuitively, proximal words in a sentence are likely to have a similar   P   value in a lower frequency, but can still be differentiated in the higher frequencies. For far apart words in a sentence, the case is just the opposite. Equations ( ) and ( ) also ensure the robustness and uniformity of the positional encoding function   P  , over all sentences, independent of their length [ ]; \n  \n The self-attention mechanism:   Once the positional encoding is integrated with the word embedding of an input sentence   S  , the resultant vector   W   is fed into the mechanism of self-attention. There is a popular analogy used by many data scientists to explain the concepts of Query (  Q  ), Key (  K  ), and Value (  V  ), which are central to the idea of self-attention. When we search for a particular video on YouTube, we submit a query to the search engine, which then maps our query to a set of keys (video title and descriptions), associated with existing videos in the database. The algorithm then presents to us the best possible values as the search result we see. For a self-attention mechanism [ ],\n \n\nA dot product between   Q   and   K   in the form   can measure the attention between pairwise words in a sentence, to generate attention weights. The attention weights are used to generate a weighted mean over the word vectors in   V  , to obtain relevant information from the input as per the given task. As these vectors are learned through the training procedure of the model, the framework can help the model retrieve relevant information from an input, for a given task. The equation governing the process is given by [ ]:\n \n\nIn practice, however, a multiheaded attention mechanism is used. The idea of multiple heads is again often compared to the use of different filters in CNNs, where each filter learns latent features from the input. Similarly, in multiheaded attention, different heads learn different latent features from the input. The information from all heads is later integrated by a concatenation operation. To account for multiple heads, Equation ( ) is violated of course, and the dimension of the positionally encoded word vector   W   is distributed over the multiple heads. Equation ( ) is also adjusted accordingly by replacing the denominator of   by  , where   is the dimension of the keys considering multiple heads. Several other concepts such as layer normalization and masking are also used in transformer models, which we will not discuss in detail here. A representation of the transformer architecture and attention map over a sentence is provided in   [ ]. \n  \n\n\n### 2.2. Pretrained Models \n  \nPretraining models has been in existence for a long time. The idea behind pretrained language models is to create a black box that can understand a language and can be used for specific tasks in that language. These language models are usually pretrained on very large datasets to generate embeddings, which are used in various NLP models. These learned word embeddings are generalized and do not represent any task-specific information. Hence, to utilize them properly, they are fine-tuned on task-specific datasets. Using these pretrained language representations can help decrease the model size and achieve state-of-the-art performance. \n\nBERT was introduced in 2019 by Devlin et al., which is a bidirectional pretrained transformer network, trained on unlabeled texts. BERT aims to generate a language representation by utilizing the encoder network of the transformer model. BERT can be used in a variety of NLP tasks such as question-answering, text classification, language inference, sentiment analysis, etc. The pretrained BERT model can be fine-tuned with one additional output layer to create NLP models without requiring task-specific architecture engineering [ ]. \n\nThe BERT\u2019s authors presented two BERT models, BERT  and BERT . BERT  consists of 12 transformer blocks, 12 self-attention heads, hidden units of size 768, and a total of 110M trainable parameters, whereas BERT  has 24 transformer blocks, 24 self-attention heads, with a hidden unit size of 1024 and a total of 340M parameters. BERT can take as the input both a single sentence and a pair of sentences as one token sequence, allowing it to handle a variety of NLP tasks. The first token of every sequence is a classification token ( ). To separate sentence pairs, a token ( ) is used. Moreover, a learned embedding is added to every token, indicating that it belongs to a sentence. The input representation is obtained by adding token embeddings, sentence embeddings, and positional embeddings. \n\nDevlin et al. used two pretraining strategies for BERT: the first is the Masked Language Model (MLM), and the second is Next Sentence Prediction (NSP). The masked language model randomly chooses 15% of the input tokens and masks them by replacing the chosen tokens with the   token. These masked tokens are then predicted by BERT based on the context of other nonmasked tokens. The MLM task enables bidirectional transformer pretraining, which allows the model to learn the context of a word based on both its left and right surrounding words [ ]. In the next sentence prediction task, the model receives pairs of sentences as an input and predicts whether the first sentence is followed by the second sentence. When choosing sentences   A   and   B   for the NSP pretraining task, 50% of the pretraining examples are chosen such that   A   is followed by   B   and labeled as  . The other 50% of pretraining examples are chosen such that   A   is not followed by   B   and labeled as  . For the pretraining corpus, the authors used BooksCorpus having 0.8 B words and text passages of English Wikipedia having 2.5 B words [ ]. WordPiece embedding was used to create a vocabulary of 30,000 words [ ]. BERT obtained state-of-the-art performance on eleven NLP tasks including the General Language Understanding Evaluation (GLUE) benchmark, the Stanford Question Answering Dataset (SQUAD), and the Situations With Adversarial Generations (SWAG) dataset [ , , ]. \n\nSince BERT\u2019s release, several BERT-based models have been released for domain-specific tasks, for example ALBERT, BERTweet, CamenBERT, RoBERTa, SciBERT, and BioBERT. BioBERT, presented by Lee and Yoon et al., is a pretrained language model for biomedical text mining [ , , , , , ]. During pretraining, BioBERT was initialized with weights from BERT and then trained on biomedical domain corpora. The biomedical corpora consisted of PubMed abstracts having 4.5B words and PMC full articles having 13.5B words. To ensure BERT\u2019s compatibility with BioBERT, the original vocabulary of BERT was used. WordPiece tokenization was applied for words that were not present in BERT\u2019s vocabulary (for example, immunoglobulin was tokenized as I ##mm ##uno ##g ##lo ##bul ##in) [ ]. BioBERT outperformed BERT and other state-of-the-art models in three biomedical NLP tasks: NER, RE, and QA. BioBERT achieved state-of-the-art performance, requiring only minimal architectural modification. Since its introduction, BioBERT has been used in various NLP tasks [ ]. \n\n\n\n## 3. Applications of Self-Attention-Based Models in BLM \n  \n### 3.1. Commonly Used Datasets \n  \nInteractions among genes, proteins, chemicals, and drugs is a well-explored field. These types of studies have been one of the cornerstones of systems biology, as they help visualize complex biological processes at a higher level of complexity. As a result, there are quite a few well-maintained and organized databases in these directions. As we observed in our review, the most popular ones used for self-attention-based models are the BioGRID, IntAct, DrugBank, and ChemProt datasets. In addition, many other PPI-based databases such as STRING, MINT, BIND, TRRUST, and AIR are publicly available [ , , , , , ]. These databases contain annotations for numerous proteins and interactions. Interestingly, however, these datasets are all annotated differently. For example, for the BioGRID database, there are fifteen types of annotated interactions: direct interaction, synthetic lethality, physical association, association, colocalization, dosage lethality, dosage rescue, phenotypic enhancement, phenotypic suppression, synthetic growth defect, synthetic rescue, dosage growth defect, negative genetic interaction, synthetic haploinsufficiency, and positive genetic interaction [ ]. In contrast, for datasets such as TRRUST or AIR, there are only three types of mentioned interactions: activation (positive), repression (negative), and unknown (undefined). Some works also prefer to curate customized datasets for their studies [ , ]. Elangovan et al. considered the IntAct database as the basis of their training data creation. Their annotation was based on chemical characterizations of the interactions [ , ]. They designed their study as a classification problem on eight classes: acetylation, methylation, demethylation, phosphorylation, dephosphorylation, ubiquitination deubiquitination, and negative. Su et al. and Giles et al., on the other hand, used two and five types of annotations respectively for PPIs. Moreover, as Giles et al. explored in their study, even for human-annotated data, ambiguities persist [ ]. \n\n\n### 3.2. Architectural Comparison of Some Recent Attention-Based Models \n  \nA summary of all discussed models is provided in  . We now discuss the architectural aspects of the models in detail. \n\nElangovan et al. (2020) [ ]: The motivation of the work by Elangovan et al. lied in the fact that in popular PPI databases such as IntAct, despite containing a large amount of information on PPIs, only 4% of these interactions are functionally annotated. The functional annotations of two interacting proteins can however be found in relevant publications. Given relevant text data (e.g., abstracts of publications), Elangovan et al. focused on extracting functional annotations of interacting proteins [ ]. \n\nFor this particular work, the authors selected PPIs from the IntAct dataset having seven types of functional annotations, namely: phosphorylation, dephosphorylation, methylation, demethylation, ubiquitination, deubiquitination, and acetylation. The task addressed in the article was, therefore, to determine the type of PPIs, rather than solely to determine whether two proteins interact. PPIs for which the type of interaction is explicitly mentioned in the abstract of a relevant article were termed as   typed interactions   [ ]. \n\nAssuming that the type of the interaction of a PPI can appear anywhere in the abstract, possibly across multiple sentences, the authors used an abstract-level annotation of the PPIs. Due to this coarse-grained annotation method, where the data are labeled as per the co-existence of the PPI and the interaction type word in an abstract and not by precise causation between the two entities, the model was described by the authors as a \u201cweakly supervised\u201d one [ ]. \n\nThe authors were also careful to state their assumption that the annotated PPI be described in the abstract of the article, although in practice, this information may prevail in any part of the text. It was further assumed that if, for an annotated PPI in the IntAct database, the type of interaction does not appear in the abstract, then it is annotated somewhere in the full text. Such data instances motivated the authors to define negative samples in the training data. Given a protein pair  , if there is no associated interaction word in the abstract against the IntAct annotation(s) of the pair, then the protein pair and the IntAct annotation form a negative sample. Note that this implies that a negative sample does not necessarily mean that the protein pair does not interact with each other, but merely that the abstract of the relevant article does not mention this interaction. This rather strong assumption also makes the data noisy, as mentioned by the authors. This, on the other hand, implies that   untyped interactions  , or interactions whose type is not known, would also be a subset of the negative samples [ ]. \n\nThe model used by the authors for this paper was a fine-tuned version of the BioBERT model. The fine-tuning process enabled BioBERT to adapt to the typed PPI classification task. The authors referred to this model as PPI-BioBERT in the article. To further improve the probability estimate of each prediction, the authors used an ensemble of 10 PPI-BioBERT models for decision-making [ ]. \n\nGiles et al. (2020) [ ]: While conventional string matching is used to search for co-occurrences of entities (gene or protein names) in a sentence, this results in the inclusion of large amounts of noise in the results. For instance, as the authors of this particular research work pointed out, in the case of the PPI detection problem, about 75% of the sentences containing co-occurring names of possibly interacting proteins do not describe any causal relationship among them. With this motivation, the authors investigated the possibility of using fine-tuned BioBERT to analyze these co-occurrences and thereby to accurately determine the functional association among the co-occurring proteins in a given sentence [ ]. \n\nAn interesting experiment conducted by the authors during the data preparation was the investigation of interannotator agreement. Three independent expert curators curated PPIs from 925 sentences identified by NER tagging within papers drawn from MEDLINE. Surprisingly, concordance among all three curators was observed in only 48.8% of the cases, which demonstrated the complexity of the problem [ ]. \n\nMoreover, the authors experimented with the need for a semisupervised preprocessing step for training data curation. This experiment was necessary due to an inherent class imbalance between positive protein interactions and the coincidental mention of proteins. The authors repeated the data curation step after filtering the sentences such that only those that contained two genes identified to have a strong likelihood of interacting, signified by a high combined StringDB score, were retained. Even with high reliability scores from StringDB, no improvement in the rate of identification of positive interactions was found. However, for some other cases, such as the drug\u2013drug interaction problem, this step proved to be more effective. The authors concluded that this type of preprocessing approach can assist in cases of balanced training data curation in specific problems. \n\nAs far as predictive models are concerned, the authors compared some rule-based approaches with a fine-tuned version of BioBERT [ ]. \n\nSu et al. (2020 and 2021) [ , ]: We now discuss two research papers that are related to each other and share two common authors. The first paper investigated the scope of the BERT and BioBERT model in general BLM problems. The second paper improved on the result of the first one by improving the performance of the pretrained BERT model by using a pretraining step involving contrastive learning. Both papers used very similar study designs. The effectiveness of the models was demonstrated by applying them to three types of RE tasks from the biomedical domain: chemical\u2013protein (ChemProt, using the BioGRID database), drug\u2013drug (DDIs, using the DrugBank database), and protein\u2013protein interactions (PPIs, using the IntAct database). The PPI classification task is considered a binary classification, indicating that the authors refrained from a more function-oriented classification, as explored by Elangovan et al., whereas the ChemProt and BioGRID classification tasks are multilabel classification tasks with five and four annotated interaction types in the respective databases [ , ]. \n\nIn the first paper, Su et al. (2020) proposed some new fine-tuning mechanisms for the BERT model. They pointed out that the RE problems are posed as classification problems and pretrained models such as BERT rely on a specific [CLS] token from the last layer to make decisions. \u201cThe [CLS] token is used to predict the next sentence (NSP task) during the pretraining, which usually involves two or more sentences, but the inputs of our relation extraction tasks only contain one sentence. This indicates that the [CLS] output might ignore important information about the entities and their interaction because it is not trained to capture this kind of information [ ]\u201d. As a solution to this, the authors proposed to add a new module that could summarize all outputs from the last layer and concatenate that information with the [CLS] output as an extra fine-tuning step. The authors experimented with the choice of the new module used to summarize information using LSTM and additive attention [ ]. \n\nIn the second paper, Su et al. (2021) proposed a contrastive-learning-based approach to improve the performance of the pretrained models. The term contrastive learning is used for a family of methods to construct a discriminative model comparing pairs of inputs. The training process for such models is designed such that similar input instances have \u201cpositive\u201d labels, whereas dissimilar input instances are labeled as \u201cnegative\u201d instances. The goal is to learn a text representation by maximizing the agreement between inputs from positive pairs via a contrastive loss in the latent space, and the learned representation can then be used for relation extraction. The authors pointed out the lack of exploitation of the potential of such contrastive models for text data in general and RE problems from biomedical natural language processing specifically. The reason behind this, as explained by the authors, is that it is more challenging to design a general and efficient data augmentation method to construct positive and negative pairs necessary to train such models [ ]. \n\nMoreover, in Su et al. (2021), the authors proposed a new metric, \u201cprediction shift\u201d, to measure the sensitivity degree to which the small changes of the inputs will make the model change its prediction, thereby arguing that the proposed model is more robust compared to simply using BERT for the classification of interaction words [ ]. \n\nTo generate a positive pair of samples compatible with the training design of the contrastive model, the authors resorted to simplistic data augmentation techniques. The goal was to slightly alter the original sentence using methods such as synonym replacement, the random swap of words, or the random deletion of words. Given a sentence   s  , two entity mentions (chemical or gene names)   and   in   s   and a relation type   r   also mentioned in   s  , the authors hypothesized that the Shortest Dependency Path (SDP) between the two entity mentions (  and  ) in the sentence   s   captures the required information to assert the relationship of the two entities. Keeping the SDP fixed, the authors therefore altered the rest of the word tokens in the text to generate augmented data, to ultimately generate positive samples. The hypothesis related to SDP is not novel in itself and has been explained in related research articles: \u201cIf entities   and   are arguments of the same predicate, then the shortest path between them will pass through the predicate, which may be connected directly to the two entities, or indirectly through prepositions.\u201d Given a training batch of   N   sentences, the authors created an alternative \u201cview\u201d of each sentence (making a pool of   sentences), and then for every sentence   s  , they considered   as a positive pair. The other   sentences were considered to be a negative sample, each compared to the sentence   s   [ ]. \n\nThe general architecture of the model is fairly similar to the general structure of Siamese neural networks. Training samples (sentences) are fed into the neural network in pairs (labeled positive or negative), and each input sentence in the pair goes through two independent channels of an identical architecture. The final output is then generated by combining the outputs from these two independent channels, which are used to calculate the loss, which is optimized to be less for similar sentences (positive pairs). Each independent channel has a neural network encoder used to create encoding for the input sentences corresponding to the channel and a projection head (a multilayered perceptron) to transform the encoding to a desired dimension, which is known to improve the representation quality during training [ ]. \n\nWang et al. (2020) [ ]: RE among proteins is affected by mutations, implying that interactions among proteins may vary from one study to another depending on these mutations, as well as the context of the study. To this end, the Biocreative VI challenge consists of two subtasks:   \nIdentifying documents describing mutations affecting PPI; \n  \nExtracting relevant PPI through RE. \n  \n\nThe first task, also referred to as document triage by the authors, clearly improves the practicality of using NLU-based models for RE in the context of PPI. The second task can extract interacting protein pairs from documents containing a triage. The term \u201ctriage\u201d refers to a tuple of a source protein, a target protein, and their relevant interactions. Although RE is the main task addressed in this research article, the authors argued that the introduction of auxiliary tasks, such as document triage classification (whether a document describes genetic mutations affecting protein\u2013protein interactions) and the gene recognition task (NER), significantly improves the RE task [ ]. \n\nThe experiments for triage and RE tasks were performed on the BioCreative VI Track 4 corpus, containing 4082 articles in the training set, of which 1729 were relevant to PPIs involving mutations. Standard preprocessing approaches such as replacing mentions of gene names by predefined strings were employed [ ]. \n\nThe architecture of the model is compatible with the multitask (main and auxiliary tasks) learning strategy as proposed by the authors. For creating meaningful vector representation of the input text, the authors used the BERT and BioBERT models. The BERT layer was shared as an embedding layer for all downstream layers. For the main RC task and auxiliary document triage task, a downstream text CNN model was added to the model. Independent BiLSTM layers were used as a downstream layer for the gene recognition auxiliary task. The authors argued that the introduction of the auxiliary learning tasks improved the classification performance of the main RE task [ ]. \n\nZhou et al. (2019) [ ]: In this research work, the authors proposed the Knowledge-aware Attention Network (KAN) for PPI extraction. The motivation of this work, published in 2019, was the fact that pre-existing methods needed extensive feature engineering and could not make full use of the prior knowledge available in the form of knowledge bases [ ]. \n\nExperiments with the model were conducted on the BioCreative VI Track 4 PPI extraction task corpus. PPI relation triplets were extracted from two knowledge bases, IntAct and BioGRID, both of which contain 45 relation types. A total of 1,518,592 triples and 84,819 protein entities were obtained for knowledge representation training, i.e., they were fed as prior knowledge to the model during training. As other approaches, the KAN model has elaborate preprocessing protocols. Some assumptions adopted during the preprocessing seem to be rather strict. For example, the authors wrote: \u201cTo reduce the number of inappropriate instances, the sentence distance between a protein pair should be less than three.\u201d In addition to this, other general protocols such as replacing gene/protein names and context-specific words (interactions) by predefined strings were also employed [ ]. \n\nAs far as the model architecture is concerned, KAN is innovative. A schematic representation of the model is shown in  . KAN has two architectural components that are identical in structure, one for processing information relevant to a source protein and the other for processing information relevant to the corresponding target protein, given a source\u2013target protein pair in a sentence. The information on the positions of the source and target proteins is encoded along with the sentence while the input is fed into the model. This is performed by modifying the general idea of positional embedding that is employed in the self-attention-based model in general. In this case, position encoding is encoded with respect to the positions of the source and target proteins in a given sentence. Respective positional encodings for the source and target proteins are fed into the respective architectural components along with the encoded sentence. Next, these inputs are passed through a diagonal-disabled multiheaded attention layer in each architectural component. Generally, self-attention-based processes are represented as some mathematical operations among a Query (  Q  ), Key (  K  ), and Value (  V  ) vector. The same vector (vectorized form of the word sequence in a sentence added to the positional encoding) is considered for   Q  ,   K  , and   V  . The multiplication of   K   and   V   produces a square attention matrix, which is then multiplied by   Q  . In the KAN model, however, the authors used a different form of   Q  . As the model aims to exploit the entity relation triplets recorded in triplets as prior knowledge, TransE (a typical knowledge representation approach, which represents the relation between two entities as a translation in a representation space) was used to create vector representations of these triplets. The vector representations of the source and target (  and  ) proteins were used as a part of   Q   in the respective architectural components. After passing the outputs of the attention layers through a feed-forward network and a multidimensional attention layer in each architectural component, the outputs from two architectural components were concatenated to obtain the final feature representation. At this stage, the vector representation of the relation between the source and the target proteins ( ) was also concatenated with the vector representation of the proteins to take advantage of prior knowledge. The results from this layer were then passed through a softmax activation to obtain the final outputs for the classification task. The authors also experimented with several variations of the KAN model [ ]. \n\n\n### 3.3. Performance Comparison among the Discussed Models \n  \nThe models we discussed in the previous section were designed to perform diverse tasks varying from document triage finding to RE problems (PPI, DDI, ChemProt, etc.) to detection of \u201ctyped\u201d PPIs. Moreover, they operate on different datasets and have different preprocessing approaches involved. In addition to these factors, they are also often evaluated on different performance measures. It is therefore difficult, if not impossible, to come up with a fair way of comparing their performances. However, one can still observe patterns in the results, which can be of significance. \n\nThe results of Wang et al. and Zhou et al. are comparable. Both of them addressed the same dataset, that is the BioCreative VI dataset. An evaluation criterion called \u201cexact match evaluation\u201d was also similar in both cases. It is defined as: \u201cA predicted relation only counts when the GeneIDs are the same as human-annotated GeneIDs.\u201d In this regard, Wang et al.\u2019s model with an F1 score of 43.14 clearly outperformed the KAN model by Zhou et al., with an F1 score of 38.23, which confirms that learning auxiliary tasks along with the principal task could play a role in improving model performances. It is also noteworthy that the preprocessing protocols of Wang et al. were comparatively simpler. Although these two papers dealt with the extraction of interacting protein pairs from documents, they did not emphasize specifying the type of interaction. Knowing the type of interaction can be extremely useful while creating a large-scale disease map such as the Atlas of Inflammation Resolution [ ] or the Parkinson\u2019s Disease Map [ ]. \n\nElangovan et al., on the other hand, addressed the type of interaction among protein pairs. Their \u201ctyped\u201d classification problem deals with seven different kinds of interactions: phosphorylation, dephosphorylation, methylation, demethylation, ubiquitination, deubiquitination, and acetylation. Thus, their model is a multiclass classification model PPI-BioBERT, which produced an F1 score of 35.4%. Interestingly, an ensemble of 10 PPI-BioBERT models improved the F1 score to 54%, which shows that there is a scope for ensemble-based models in RE-based problems [ ]. \n\nSu et al. (2020 and 2021) explored the same three BLM-based problems, PPI, DDI, and ChemProt. The first article written in 2020 compared several variations of the BioBERT model (such as using LSTM and attention layers and utilizing the classification token [CLS] in their model). Using their models, they achieved F1 scores of 82.8, 80.7, and 76.8 for the PPI, DDI, and ChemProt tasks, respectively. However, from the results, it is difficult to conclude which kind of architecture (using LSTM or attention layers) in particular is beneficial. Perhaps that is why, in the follow-up article in 2021, the authors used none of the ideas of attention or LSTM layers. Rather, they focused on contrastive learning. The best results were produced by a model that used contrastive learning in addition to adding information from external knowledge bases. In this case, they achieved F1 scores of 82.7, 82.4, and 76.9 for the PPI, DDI, and ChemProt tasks, respectively, which was not a significant improvement on the previous work [ , ]. \n\n\n\n## 4. Discussion \n  \nThe classification approach popularly used for RE hinders the transfer of knowledge across databases and corresponding datasets. This is due to the different annotations used in different databases. Knowledge transfer and integration across databases, in accordance with the classification approach, therefore requires the amalgamation of corresponding datasets with different annotations. This can create a multilabel classification problem with multiple classes, making a model difficult to train. We observed that the F1 score is naturally less in classification problems with multiple classes. For example, Su et al. had a binary classification task for PPI and thus had a superior F1 score compared to Elangovan et al., who considered multiple classes in their classification problem [ , , ]. Furthermore, F1 scores are low for models that use exact match evaluation for measuring their performance. Intrinsic imbalance persistent in such datasets along with ambiguous manual annotations across datasets make it even harder to effectively train classification models. This reduces the practical usability of the classification models as a modeler cannot customize these models as per his/her modeling needs, and he/she have to rely on pre-annotated databases to train his/her models. \n\nMoreover, modeling directed interactions requires knowledge on the source entity, the target entity, and the relationship between them. Usually, this used to be designed as a relationship-triplet-finding problem. Most classification models for RE do not consider the sense of directionality that is associated with the related entities. Only a few models, such as the KAN, consider taking the source and target entities as a part of the input and try to predict the corresponding interaction. However, still, the KAN is not trained in such a way that it can differentiate between a source and a target entity in the case of a directed interaction. \n\nEven if such a classification model exists that can differentiate between a source and target entity of a directed interaction, while practically using such a model to extract relationships from new data, a modeler has to know from a relevant text which entity is the source and which entity is the target. Without knowing this information, directed interactions cannot be modeled. While NER-based models can identify the entity names from new literature, the issue of annotating source and target entities was not addressed in the discussed approaches, in general. This again hinders the practical applicability of such models in knowledge extraction. \n\nWhat makes the practical use of many of these models difficult are the diverse preprocessing protocols and strict assumptions adopted by the models. Almost every model that we discussed replaces protein or chemical names by specific strings (e.g., Su et al., Wang et al., Elangoven et al., Zhou et al.) [ , , , , ]. The model by Zhou et al., for example, adopted elaborate protocols while curating training data such as [ ]:   \nReducing the number of inappropriate instances; the sentence distance between a protein pair was assumed to be less than three; \n  \nSelecting the words among a protein pair and three expansion words on both sides as the context word sequence with respect to the protein pair; \n  \nRemoving protein names from the input string; \n  \nReplacing numeric entries by predefined strings; \n  \n\nFor attention-based models, even though there have been some attempts at making the models explainable by observing the attention matrices, such attempts are rare in the case of BLM. For example, Su et al. (2020) and Zhou et al. made some limited efforts to explain the behavior of their models [ , ]. \n\nIn the field of computer vision, the concept of explainable models is quite popular. Being able to explain decisions made by a model can be important in the case of BLM as well. A byproduct of explainability could be, for example, a knowledge graph, which is a compact way of summarizing much information, as well as discovering new information [ ]. Biological information can be represented in its most general form as knowledge graphs. A model that can be used to curate and represent from new literature entities such as genes, proteins, phenotypes, etc., and their relationships in the form of knowledge graphs can address some issues of the classification approach discussed before. The nodes of the knowledge graph represent the entities, and the edges are the annotations of the directed or undirected relations among the entities. Customized edge annotations, as per the interest of the modeler, can be fed into a model, making the model adaptable to the need of the modeler. Given the positional information on a word representing an edge annotation (e.g., activation, repression, phosphorylation) in a sentence, self-attention-based models can be used to predict the positions of the source and target node entities (e.g., source gene, target gene) for that particular edge annotation. In case a modeler is not interested in modeling interentity relationships in particular and is simply interested in modeling whether there is an association between two entities (gene\u2013phenotype association), such a model can account for this by learning the position of the target entity, given the position of the source entity or vice versa. \n\nA knowledge-graph-based model, as discussed above, could be used in a pipeline with other NLP tasks to develop an end-to-end approach for customized knowledge extraction and knowledge discovery. For example, NER and document triage can be used as preceding tasks in a pipeline. The discovery of relationships among new entities can be achieved through models operating on knowledge graphs generated by the model. For example, Liu et al. proposed a model for the discovery of new relationships among compounds and diseases from knowledge graphs using a reinforcement learning approach on knowledge graphs [ ]. \n\n\n## 5. Conclusions \n  \nClearly, attention-based models, both novel architectures and pretrained networks, are being explored widely in the domain of BLM. Complex algorithms have been constructed to handle a wide variety of tasks such as NER, RE, document classification, and triage mining. Some publications have proposed coherent workflows attempting to make the algorithms more practically usable. However, challenges such as diversely annotated datasets, the transfer of knowledge for trained models across datasets, the lack of explainability, complex preprocessing protocols, and the large amount of computational power required to tune pretrained models reveal the scope of further research in this domain with the goal of a more generalistic and practically useful approach. \n\n \n", "metadata": {"pmcid": 8615611, "text_md5": "531aded9d139b9cd9425edb8cca73ddb", "field_positions": {"authors": [0, 85], "journal": [86, 98], "publication_year": [100, 104], "title": [115, 209], "keywords": [223, 338], "abstract": [351, 1761], "body": [1770, 52079]}, "batch": 1, "pmid": 34827589, "doi": "10.3390/biom11111591", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8615611", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8615611"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8615611\">8615611</a>", "list_title": "PMC8615611  Self-Attention-Based Models for the Extraction of Molecular Interactions from Biological Texts"}
{"text": "Bachman, John A. and Gyori, Benjamin M. and Sorger, Peter K.\nBMC Bioinformatics, 2018\n\n# Title\n\nFamPlex: a resource for entity recognition and relationship resolution of human protein families and complexes in biomedical text mining\n\n# Keywords\n\nText mining\nProtein families\nGrounding\nNamed entity linking\nNamed entity recognition\nBiocuration\nEvent extraction\nNatural language processing\n\n\n# Abstract\n \n## Background \n  \nFor automated reading of scientific publications to extract useful information about molecular mechanisms it is critical that genes, proteins and other entities be correctly associated with uniform identifiers, a process known as named entity linking or \u201cgrounding.\u201d Correct grounding is essential for resolving relationships among mined information, curated interaction databases, and biological datasets. The accuracy of this process is largely dependent on the availability of machine-readable resources associating synonyms and abbreviations commonly found in biomedical literature with uniform identifiers. \n\n\n## Results \n  \nIn a task involving automated reading of \u223c215,000 articles using the REACH event extraction software we found that grounding was disproportionately inaccurate for multi-protein families (e.g., \u201cAKT\u201d) and complexes with multiple subunits (e.g.\u201cNF-   \u03ba  B\u201d). To address this problem we constructed FamPlex, a manually curated resource defining protein families and complexes as they are commonly encountered in biomedical text. In FamPlex the gene-level constituents of families and complexes are defined in a flexible format allowing for multi-level, hierarchical membership. To create FamPlex, text strings corresponding to entities were identified empirically from literature and linked manually to uniform identifiers; these identifiers were also mapped to equivalent entries in multiple related databases. FamPlex also includes curated prefix and suffix patterns that improve named entity recognition and event extraction. Evaluation of REACH extractions on a test corpus of \u223c54,000 articles showed that FamPlex significantly increased grounding accuracy for families and complexes (from 15 to 71%). The hierarchical organization of entities in FamPlex also made it possible to integrate otherwise unconnected mechanistic information across families, subfamilies, and individual proteins. Applications of FamPlex to the TRIPS/DRUM reading system and the Biocreative VI Bioentity Normalization Task dataset demonstrated the utility of FamPlex in other settings. \n\n\n## Conclusion \n  \nFamPlex is an effective resource for improving named entity recognition, grounding, and relationship resolution in automated reading of biomedical text. The content in FamPlex is available in both tabular and Open Biomedical Ontology formats at  \nunder the Creative Commons CC0 license and has been integrated into the TRIPS/DRUM and REACH reading systems. \n\n \n\n# Body\n \n## Background \n  \nA critical challenge in contemporary molecular biology is integrating detailed mechanistic information about specific genes and proteins with genome-scale information about the interaction networks in which these genes participate. Networks of molecular mechanisms are powerful tools for interpreting large-scale data in the context of prior knowledge [ \u2013 ]. The construction of biological networks benefits from exchange formats such as BioPAX [ ] that allow disparate databases to be aggregated into uniform, machine-readable resources such as Pathway Commons [ ]. However, a significant fraction of the information available in the literature has not been recorded in pathway databases. Text mining has the potential to address this gap by augmenting curated network resources with molecular mechanisms automatically extracted from the literature. However, current systems are not yet able to extract mechanisms with a quality equal to that of human curators [ ]. \n\nOne challenge in using text-mined information for biological data analysis is that molecular mechanisms are often described in the literature in terms of aggregate entities such as multi-protein families (e.g., \u201cRAS\u201d, \u201cAKT\u201d) and multi-subunit complexes (e.g., \u201cNF-   \u03ba  B\u201d, \u201cAP-1\u201d) rather than the specific genes or proteins measured in large-scale experiments. For example, a Pubmed search for \u201cNF-kappaB\u201d yields over 65,000 citations; this transcription factor is not a single molecular entity but rather a class of heterodimers involving combinations of at least five different genes in two families (  RELA, RELB, REL, NFKB1,   and   NFKB2  ). This poses two challenges for machine reading. First, the text string \u201cNF-   \u03ba  B\u201d must be normalized to a standard identifier, a task known variously as named entity linking (NEL), named entity normalization (NEN), named entity disambiguation (NED), or simply \u201cgrounding.\u201d [ ]. Second, the mapping of \u201cNF-   \u03ba  B\u201d to its constituents must be established so that the activities of NF-   \u03ba  B can be linked to the properties of the genes from which it is comprised. Such \u201cstatic relations\u201d must be resolved either by explicit curation or algorithmically [ \u2013 ]. \n\nSuccess in the first task, grounding, is essential for practical applications of text mining [ ,  ]. Entities without associated identifiers cannot be used for downstream assembly and interpretation tasks, and systematic   misidentification   of entities clutters extracted networks with errors that skew data analysis. Relevant approaches to grounding have been studied extensively in the context of the general problem of biomedical entity normalization [ ,  \u2013 ], and generally involve two steps. First, a named entity as encountered in text is normalized, for example by stemming [ ], removal of affixes [ ], or expansion of abbreviations [ ]. Effective preprocessing depends on an explicit or implicit representation of how specific entities (e.g., diseases vs. chemicals vs. genes) variously appear in text (see 2.2.4 in [ ]). \n\nThe normalized string is then matched to names and synonyms in existing taxonomies [ ]. Difficulties in grounding protein families and complexes are encountered in this latter step because there is no standard ontology for these entities as they are commonly described in the scientific literature. Relevant identifiers can be found in protein family databases (InterPro, Pfam, NextProt) and curated interaction databases (Reactome, SIGNOR, OpenBEL) allowing complexes and families to be resolved into their constituent genes. However, such databases generally lack lexical synonyms corresponding to the many ways in which entities are referenced in text, limiting their value for literature mining. Conversely, general biomedical vocabularies and thesauri such as NCIT and MeSH contain entries and lexical synonyms for families and complexes but often lack the ontological resolution of these terms into child concepts (e.g. entries C94701 in NCIT and D055372 in MeSH for the holo-enzyme AMPK, neither of which define its constituents). In combination, these diverse databases provide substantial information about families and complexes, but integration of this information is difficult because they rarely contain cross-references for related concepts among themselves. Prior work has addressed aspects of normalization for protein families, for example by automatically identifying families and their constituents directly from the literature [ ,  ] or by combining information in gene family databases with patterns in the names and synonyms of genes [ ,  ]. However, the problem of identifying, normalizing, and linking information about protein families and complexes is less well-understood than that of gene normalization [ ,  ,  ], and draws on a smaller base of taxonomic resources. \n\nIn this paper we describe FamPlex, a curated lexical and ontological resource that improves grounding and relationship resolution for families and complexes encountered in the mining and curation of biomedical text. FamPlex contains a set of identifiers for protein families and complexes along with mappings that link: (i) text strings and FamPlex identifiers, (ii) FamPlex identifiers and identifiers representing protein families and complexes in other resources, and (iii) FamPlex families/complexes and their constituent members. FamPlex also contains a list of prefixes and suffixes frequently appended to protein names for use in named entity recognition (NER) and entity normalization. The FamPlex resource consists of a set of comma-separated value (CSV) files listing entities and relations, along with Python scripts for checking consistency and identifying equivalent identifiers in other databases. FamPlex is hosted on GitHub at   and is made available under the Creative Commons CC0 license. It is also available in the Open Biomedical Ontology (OBO) format and can be accessed via the NCBO BioPortal [ ] at  , or through  . \n\n\n## Construction and content \n  \nDevelopment of FamPlex was motivated by an empirical analysis of grounding accuracy in events extracted by the REACH biomedical literature mining software [ ,  ]. As described in detail below, we found that grounding of protein families was disproportionately inaccurate and that a relatively small proportion of frequently misgrounded entities accounted for the bulk of all grounding errors. An examination of existing resources highlighted the fragmented nature of information on protein families and complexes and the general lack of suitability of these resources for literature mining. FamPlex was conceived as a \u201cbridging\u201d resource to link available information about families, complexes, and other frequently misgrounded entities across a diverse set of existing bioinformatics databases. \n\nAt the core of FamPlex is a set of identifiers representing protein families and complexes (Fig.\u00a0 ). FamPlex represents the hierarchical relationships of these high-level entities to each other and to individual genes, along with corresponding synonyms in text and cross-references to other databases where available. Entities and mappings are recorded in a set of CSV files.\n   \nFamPlex links named entities to protein families and complexes and their constituents.   a   Structure of FamPlex content. The affixes in gene_prefixes.csv can be used to improve recognition of molecular entity names, which can be linked to database identifiers using the lexical synonyms in grounding_map.csv. FamPlex itself contains identifiers representing families and complexes which are mapped to corresponding identifiers in other databases in equivalences.csv. Hierarchical relationships among families, complexes, and genes are listed in relations.csv.   b   Workflow for curation and evaluation. A gene list was used to define a corpus of articles that was divided into two subsets, \u201ctraining\u201d and \u201ctest\u201d. The \u201ctraining\u201d corpus was processed with REACH and results were evaluated and used to guide curation. The \u201ctest\u201d corpus was processed after incorporation of FamPlex and results were compared against the baseline from the training corpus \n  \n\n### Selection of corpus for curation and evaluation \n  \nTo empirically guide curation of entities and synonyms based on the frequency of their appearance in literature we selected a corpus of articles focused on the proteins, protein families, complexes, and molecular events relevant to pathway biocuration (Fig.\u00a0 ). Specifically, we combined the 3752 signaling proteins in Reactome [ ] with the members of protein families and complexes defined in OpenBEL resource files [ ]. From this gene list a corpus of 269,489 papers was assembled by retrieving papers curated for each gene from the Entrez Gene database [ ]. Abstracts were obtained from MEDLINE and full texts were downloaded either from the Pubmed Central Open Access subset (in XML or text format), the Pubmed Central Author Manuscript Collection, or via the Elsevier text and data mining API (Table\u00a0 ).\n   \nComposition of article corpus by source \n  \n\n\n### Event extraction from text using REACH and INDRA \n  \nThe corpus of \u223c270,000 papers was processed with the REACH event extraction software [ ], yielding a set of sentences, named entities, and extracted relations (Fig.\u00a0 ). REACH is built on widely-used methods for syntactic parsing and named-entity recognition: it uses the Stanford CoreNLP parser [ ] for syntactic parsing and draws information on biology-specific named entities from Uniprot, InterPro, Pfam, HMDB, ChEBI, Gene Ontology, MeSH, Cellosaurus, ATCC, and CellOntology. As a final step we used the INDRA software [ ] to convert events extracted by REACH into INDRA Statements, a format suitable for analyzing and assembling sets of mechanisms into networks and executable models of various kinds. \n\n\n### Characterizing patterns of grounding errors \n  \nThe set of entities and events extracted by REACH was used to characterize patterns of grounding errors and prioritize entities and their lexical synonyms for subsequent curation (Fig.\u00a0 ). Prior to curation, the corpus of articles was divided into two sets: a \u201ctraining\u201d set and a \u201ctest\u201d set consisting of 80% (215,360) and 20% (53,840) of the articles, respectively (the total of 269,200 articles for the combined training and test sets differs from the full corpus of 269,489 reported in Table\u00a0  due to the fact that content could not be retrieved for 267 articles and reading failed for an additional 22). The \u201ctraining\u201d set of articles was processed with REACH in the absence of FamPlex to evaluate baseline grounding accuracy and guide curation. Following curation, the \u201ctest\u201d set of articles was processed with a version of REACH incorporating FamPlex. The partitioning of articles was performed to ensure that estimates of grounding accuracy would not be biased toward the specific set of articles used for curation. \n\n\n### Definition of protein families and complexes and their constituents \n  \nIdentifiers for protein families and complexes in FamPlex were created by drawing on two resources: 1) identifiers created   de novo   in FamPlex to correspond to named entities encountered in event extraction, and 2) identifiers drawn from the OpenBEL resource. In the first case, identifiers were prioritized by their frequency of occurrence among extracted events, with common entities such as \u201cNF-kappaB\u201d, \u201cRas\u201d, \u201cPI3-kinase\u201d, \u201cAkt\u201d, etc., accounting for a significant fraction of grounding errors. In the case of OpenBEL, identifiers for protein families and complexes were drawn from the resource files protein-families.xbel and named-complexes.xbel, accessible via the OpenBEL GitHub repository at  . The full list of all FamPlex identifiers is contained in the text file entities.csv. \n\nMembers of protein families and complexes are enumerated in the file relations.csv using two types of relations:   isa   and   partof  , denoting membership in a family or a complex, respectively (Fig.\u00a0 ). These relationships can be applied hierarchically to describe multi-level protein subfamily relationships or protein complexes that are hetero-oligomers of subunits belonging to distinct families (Fig.\u00a0 ). For example, 5\u2019 AMP-activated protein kinase, or AMPK, is a heterotrimeric protein consisting of alpha, beta, and gamma subunits: the alpha and beta subunits comprise families with two isoforms each, and the gamma subunit family has three isoforms. This hierarchical structure can be represented in FamPlex by using a combination of   isa   and   partof   relationships to link the identifiers for the subunit genes to FamPlex-specific identifiers for the subunit families and the full complex (Fig.\u00a0 ).\n   \nFamPlex links identifiers for families and complexes to members, other databases, and lexical synonyms.   a   FamPlex uses   isa   and   partof   predicates to represent the hierarchical relationships between specific genes, families and complexes. Lexical synonyms can be associated with entities at each level.   b   Mappings of FamPlex identifiers to outside databases.   c   Number of lexical synonyms curated for FamPlex identifiers in the grounding map \n  \n\nInformation on protein family and complex membership was drawn from OpenBEL resource files, HGNC, Reactome, InterPro, and Wikipedia, and manually curated for consistency. Where there were discrepancies among sources about family or complex membership we prioritized what we judged to be the most common usage. For example, the InterPro entry corresponding to the Ras protein family (IPR020849) lists 145 human proteins as members, whereas usage in literature and interaction databases recognizes only KRAS, NRAS, and HRAS as family members. \n\n\n### Mapping FamPlex identifiers to related resources \n  \nEntities defined in FamPlex are cross-referenced to corresponding identifiers in other databases and ontologies in the equivalences file (equivalences.csv; Fig.\u00a0 ). Figure\u00a0  shows the subsets of FamPlex identifiers containing mappings to different types of external databases: databases of interactions curated from literature (OpenBEL, Reactome), databases containing specific information about protein families and complexes (Pfam, InterPro, NextProt, and Gene Ontology), and general-purpose biomedical vocabularies (NCIT, MeSH). There are 32 unmapped entries for which no equivalent entry was found in external databases; these entries are implicitly defined in FamPlex by the specific genes that they contain as members. \n\nIdentifier mappings between FamPlex and Reactome and InterPro were obtained in a semi-automated fashion. The gene-level members of each FamPlex family and complex were used to query Reactome and InterPro for families and complexes containing these genes. Reactome and InterPro families with equivalent sets of members were incorporated into equivalences.csv. Python scripts for generating and updating these mappings are available in the FamPlex GitHub repository at import/reactome_mappings.py and import/interpro_mappings.py. Additional identifier mappings to Pfam, NCIT, NextProt, GO and MeSH were collected by entering FamPlex identifiers and lexicalizations into the TRIPS/DRUM web service available at   [ ]. The TRIPS/DRUM web service returned identifier mappings and their scores based on partial string matches to a variety of databases, which were then manually curated for inclusion in FamPlex. \n\n\n### Curation of lexical synonyms for entities \n  \nEntities defined in FamPlex are associated with lexical synonyms in the grounding map (grounding_map.csv; Fig.\u00a0 ). These synonyms allow natural language processing tools to match named entities extracted from text to the protein families and complexes contained in the FamPlex hierarchy. \n\nLexical synonyms were curated in two ways. First, named entities extracted from the \u201ctraining\u201d articles read by REACH were sorted by frequency, and named entities corresponding to FamPlex families and complexes were added to the grounding map. Entries were also added to the grounding map for frequently occurring but incorrectly grounded named entities of other types (e.g., proteins, chemicals, and biological processes). For less-frequently encountered families and complexes, synonyms were curated using a different approach: names and synonyms for the gene-level members of families and complexes were used to search the named entities extracted by REACH. Potential matches were identified by fuzzy string matching (Levenshtein distance [ ]) using the Python fuzzywuzzy package and subsequently manually curated. \n\nOf the 2413 entries in the FamPlex grounding map, 1458 map to FamPlex identifiers; the remaining 955 map to frequently occurring proteins, chemicals, and biological processes. The distribution of lexical synonyms across the set of FamPlex identifiers is shown in Fig.\u00a0 . The frequently-occurring entities NFkappaB and ERK have the most synonyms, with 13 and 9, respectively; many other less-frequently occurring entities have only a single synonym. Examples of synonyms for NFkappaB include \u201cNF-kB\u201d, \u201cNFkappaB\u201d, and \u201cNF-kappaB TFs\u201d; synonyms for ERK include \u201cERK 1/2\u201d, \u201cERKs\u201d, and \u201cExtracellular Signal Regulated Kinase\u201d. \n\n\n### Curation of gene/protein affixes \n  \nReferences to genes and proteins in the literature are often modified by affixes that describe modifications or other context. For example, \u201cmmu-AKT1\u201d and \u201cpAKT1\u201d refer to murine and phosphorylated AKT1, respectively. A list of 137 case-sensitive affixes was tabulated by alphabetically sorting a list of \u223c80,000 named entities resulting from event extraction and manually identifying common affix patterns. These affixes were subsequently grouped into six semantic categories (Table\u00a0 ). The largest category, \u201cexperimental context\u201d, contains affixes used to identify the precise variant of a gene used in an experiment; these often refer to protein tags or gene delivery methods. Two of the six categories affect event extraction as well as grounding: \u201cprotein state\u201d affixes contain information on modification, location and mutation states, while \u201cinhibition\u201d affixes invert the apparent polarity of an extracted event. For example, a positive regulation event mediated by \u201cBRAF siRNA\u201d actually represents a negative regulation by BRAF itself. The full list of affixes can be found in the CSV file gene_prefixes.csv (Fig.\u00a0 ).\n   \nGene/protein affix types \n  \n\n\n### Resource structure and scope \n  \nFamPlex comprises 441 families and complexes that together cover 2040 specific genes through   isa   and   partof   relations. Most FamPlex entries (315) are at the top level of the hierarchy, having no parent entities; 111 entries are at an intermediate level, having both parent and child entities; 15 entities function as placeholders with no parent or child relations currently specified. This latter category consists primarily of functional categories with many potential protein members, e.g., GTPase, Phosphatase, Protease, etc. \n\nThe top-level entries vary in terms of the depth of the hierarchy they subsume with the majority of entries (275 in total, two examples being RAS and RAF) directly being resolved to a set of specific constituent genes. 37 entries have two subsumed levels (for instance PLC which subsumes the subfamilies PLCD, PLCG, and PLCB, which in turn subsume a total of nine constituent genes), and 3 entries (G_protein, HSP90 and PI3K) subsume three levels. \n\nFamPlex entries vary in terms of the number of children they subsume with an average of 6.0 \u00b1\u20097.1 children, the large standard deviation indicating the long-tailed nature of the distribution. While the median FamPlex entry has 3 children, several entries have a much larger number, including RAB (68 children), Histone (60 children) and Cyclin (31 children). \n\nTo characterize the scope and relevance of the different identifiers we quantified the prevalence of each FamPlex entry in PubMed-indexed articles. We conducted PubMed searches for each lexicalization of a given FamPlex entry (using the relatively restrictive \u201ctext word\u201d search mode of PubMed to avoid partial matches and matches to meta-information) and counted the total number of unique articles found for each FamPlex entry itself and also for each entry and all its children. The total number of PubMed-indexed articles mentioning one or more FamPlex entries (or children) was 4,012,468, or roughly 14% of all PubMed-indexed literature. The mean number of citations per FamPlex entry was 13,091 \u00b1\u200926,733 with a median of 3034, reflecting a distribution skewed toward a small number of highly cited entries. When including the children of each entry, the number of citations per entry was higher, with a mean of 16,136 \u00b1\u200929,491 and a median of 4929. The most commonly appearing FamPlex entry was Interferon with 204,228 associated articles; only 11 FamPlex entries had fewer than 100 associated PubMed citations. Thus, FamPlex covers entities that are frequently mentioned in the biomedical literature. \n\n\n\n## Utility and discussion \n  \n### Protein families and complexes appear frequently in events extracted from literature and are often incorrectly grounded \n  \nTo evaluate baseline grounding performance without FamPlex we manually scored a random sample of 300 named entities generated by running REACH on the training corpus. Entities were categorized by type (protein/gene, family/complex, small molecule, biological process, microRNA, and other/unknown) and the database mappings identified by REACH were scored for correctness (Table\u00a0 ). Where the entity text alone was insufficient to evaluate grounding accuracy, the sentence in which the entity was embedded was examined in the context of the original paper.\n   \nEntity frequency and grounding accuracy for 300 entities, with and without FamPlex \n  \nStandard error was calculated using the formula   where   k   is the number of samples in the given category and   n   is the total number of samples \n\nRow 1: significant with   p  <0.01 \n\nRow 2: significant with   p  <10 \n  \n\nWe found that references to protein families and complexes were second only to genes and proteins in the frequency of their occurrence in events extracted from text, accounting for 17.7% of all extracted entities (Table\u00a0 ). Grounding accuracy was substantially lower for families and complexes relative to genes and proteins, with only 15.1% of families and complexes correctly grounded compared to 78.7% for individual proteins (Table\u00a0 ). The 15% rate of correct grounding for families and complexes reflected accurate matches to identifiers in InterPro or Pfam. Notably, seven of the top ten most frequently occurring   ungrounded   entity texts in the training corpus represented families or complexes (\u201cNF-kappaB\u201d, \u201cERK1/2\u201d, \u201cmTORC1\u201d, \u201cNFkappaB\u201d, \u201cPDGF\u201d, \u201cIKK\u201d, and \u201chistone H3\u201d; Table\u00a0 ). Overall, REACH identified a total of 163,428   unique   named entity strings involved in events, out of which 2873 were grounded (correctly or incorrectly) to a protein family or complex (1.8%).\n   \nTop 10 most frequently occurring ungrounded entity texts with and without FamPlex in the training and test corpora, respectively \n  \n\nClose inspection of errors made by REACH in grounding frequently-occurring families and complexes in the absence of FamPlex revealed the causes of both missing and incorrect groundings. Missing groundings occurred when named entities for families and complexes had no corresponding identifiers or synonyms in any of the indexed databases. This was true of the entity \u201cRas\u201d, as well as the most frequently occurring family-level entity, \u201cNF-kappaB\u201d. \n\nOn the other hand, incorrect grounding of family-level entities occurred due to exact (but spurious) matches to obscure synonyms for other genes listed in Uniprot or HGNC. In some cases these genes were unrelated to the family but had synonyms shadowing the family name: for example, \u201cERK\u201d and \u201cCyclin\u201d were grounded to the human genes   EPHB2   (Uniprot P29323) and   PCNA   (Uniprot P12004) due to the presence of these strings as synonyms. Another class of grounding errors involved the matching of a string representing the basename of a human protein family to the single ortholog of the family in a different organism. Representative examples include the misgrounding of \u201cAKT\u201d to the   Dictyostelium discoideum   gene   pkbA   and of \u201cJNK\u201d to the   Drosophila melanogaster   gene   bsk  , both of these listing the human gene family name as synonyms. \n\nThe most common ungrounded strings (those in the highest percentile by frequency of occurrence) accounted for a surprisingly large proportion of the   overall   number of ungrounded string occurrences, as shown by the orange curve in Fig.\u00a0 . The deviation of this curve from a uniform distribution (shown by the dotted gray line in Fig.\u00a0 ) arises because the empirical distribution of ungrounded entities is highly skewed, with a small number of very common entities accounting for a large percentage of occurrences. For example, half of all ungrounded string occurrences in the training corpus involved the top 2.4% most frequently occurring strings (2666 distinct strings). This explains why curation that is focused specifically on frequently occurring misgrounded entities has the potential to substantially improve overall grounding and reading performance.\n   \nFamPlex improves grounding accuracy.   a   Cumulative occurrences of ungrounded entities by frequency of the entity text. Deviation from the dotted gray line, representing a uniform frequency distribution, indicates the extent to which a small number of frequently occurring entities account for a disproportionate share of missed groundings.   b   Improvements in grounding accuracy for proteins/genes and families/complexes, with and without the use of FamPlex.   c   Reduction in the proportion of extracted events containing ungrounded entities, with and without FamPlex.   d   Number of groundings to FamPlex identifiers in the test corpus. The 15 most frequent identifiers account for 50% of all groundings and are shown in blue \n  \n\n\n### Use of FamPlex in text mining improves grounding and relationship resolution for protein families and complexes in two event extraction systems \n  \nFollowing the manual curation of FamPlex identifiers and associated synonyms and the integration of FamPlex into REACH and INDRA, we performed a second evaluation on a random sample of 300 named entities drawn from the results of processing the test corpus (Table\u00a0 ). The frequency of entity types was comparable between the training and test samples, with proteins/genes and families/complexes accounting for roughly three-quarters of all entities. Improvements in grounding were substantial for both classes, with grounding accuracy for families and complexes rising from 15 to 71% (Fig.\u00a0 ; Table\u00a0 ). Grounding accuracy for proteins and genes increased from 79 to 90%, an improvement attributable to the curation of synonyms for frequently occurring proteins. With the incorporation of FamPlex, the overall percentage of unique entity strings grounded to protein family or complex identifiers doubled relative to the training corpus, with REACH grounding 2080 of 57,088 unique entities to a FamPlex, InterPro or Pfam identifier (3.6%). \n\nAn analysis of the distribution of the remaining ungrounded entities showed that FamPlex addressed a substantial proportion of the most frequently occurring grounding failures (Fig.\u00a0 , green curve). As shown in Table\u00a0 , the top ten most frequently occurring ungrounded entities in the test set occur at a lower overall frequency and include a functional category (\u201creceptor\u201d) but no specific protein families or complexes. To examine the impact of grounding improvements at the level of extracted events, we calculated the proportion of events consisting either of any or all ungrounded entities, and found that both metrics improved with the use of FamPlex (Fig.\u00a0 ). These measures, which deal only with event entities that were   ungrounded  , represent an underestimate of the overall improvement in grounding because they do not account for cases in which entities were grounded to the   wrong   identifier in the absence of FamPlex. \n\nTo characterize whether improvements in grounding were driven by a small subset of frequently-occurring entities in FamPlex or were more broadly distributed across families and complexes, we counted the occurrences of mappings to each FamPlex identifier in events extracted from the test corpus. We found that the 15 most frequently-referenced FamPlex identifiers accounted for 50% of all FamPlex groundings (blue bars in Fig.\u00a0 ); the top five are shown in Table\u00a0 . At the same time, 363 of the 441 FamPlex identifiers were mapped to text at least once, suggesting that the great majority of identifiers and lexical synonyms in FamPlex are useful for improving grounding (Fig.\u00a0 ).\n   \nFamPlex entries most frequently grounded to in test corpus, with the absolute number of times grounded to in the test corpus and the percentage normalized to all FamPlex groundings \n  \n\nAs a second means to evaluate FamPlex we used the TRIPS/DRUM reading system [ ]. Unlike REACH, which uses strict string matching against a set of dictionaries, TRIPS uses soft matching to provide a ranked, scored list of groundings for each named entity. Relevant dictionaries used by TRIPS include Pfam and NextProt for protein families, GO for protein complexes and NCIT for both. \n\nWe compiled two versions of TRIPS, one in which FamPlex was included as a grounding resource, and one in which it was omitted. Since the throughput of TRIPS is substantially lower than that of REACH, we selected a random sample of 100 abstracts from the combined training and test set for reading with and without FamPlex. We then manually curated 500 randomly sampled entities appearing in TRIPS extractions, determining whether each entity represented a protein family or complex, and if so, whether: (i) the   top scoring   grounding match was correct, and (ii)   any   of the grounding matches were correct. In contrast to our evaluation of entity grounding in REACH, in which the curated entities were limited to arguments of events, here we considered   all   entities identified in text by TRIPS as candidate families or complexes for curation. This broader pool of candidate entities included names of cell lines, organisms, biological processes, etc., and therefore also a smaller proportion of molecular entities such as families and complexes. \n\nIn the case of TRIPS without FamPlex, 36 of 500 entities sampled from the TRIPS output corresponded to families or complexes. Of these, we found that the top scoring grounding was correct for 23 (64%); 29 entities (81%) had at least one correct grounding. The higher baseline accuracy of family/complex grounding in comparison with REACH likely reflects broader coverage of relevant identifiers due to the inclusion of NextProt and NCIT (used by TRIPS but not by REACH) and the more robust but computationally costly soft-matching and ranking procedure used for grounding. While no single resource accounted for the majority of all matches, top-scoring matches were roughly equally distributed between NCIT and NextProt. Moreover, of the 17 entities that were correctly grounded in NCIT, 7 (41%) had no identified child concepts, making it impossible to link these families and complexes to constituent genes. Thus, while TRIPS was more successful than REACH at finding relevant groundings for families and complexes in the absence of FamPlex, the multiplicity of alternative groundings and the unresolved nature of these terms in the ontologies used posed a distinct problem, that of relationship resolution. \n\nIncorporating FamPlex into TRIPS improved both the accuracy and consistency of grounding. In a sample of 500 entities extracted by TRIPS using FamPlex, 33 corresponded to families and complexes; the top-scoring grounding was correct for 26 (79%) of these and a further four (91% overall) had at least one correct grounding. While the small sample sizes limit quantitative conclusions about the degree of improvement, we noted that in 18 of 26 (69%) cases in which the top-scoring grounding was correct, it was grounded to a FamPlex identifier, and in 20 of 26 (77%) a FamPlex grounding was among the top two matches. This indicates that FamPlex identifiers and lexicalizations have a higher coverage for families and complexes encountered in text by TRIPS than other resources used, allowing for more consistent relationship resolution and integration of information. \n\n\n### FamPlex includes a large majority of families and complexes annotated by human curators in text \n  \nIn addition to the evaluations of grounding   precision   described above, we sought to establish a measure of the   recall   of FamPlex in terms of its coverage of relevant families and complexes in a manually curated dataset. Evaluations solely against machine reading output, as described above, do not provide a true recall measure because the readers extract only a subset of the events and entities from the underlying text. \n\nTo evaluate recall we used the dataset prepared for the bioentity normalization task from Biocreative VI Task 1.1 ( ). The dataset, drawn from the EMBO SourceData annotation project [ ], contains a corpus of entity text strings from figure legends in published papers, most of which have been annotated with database identifiers by human curators. Our aim was to evaluate the extent to which FamPlex incorporates identifiers and lexicalizations for the family and complex-level entities identified in text by human curators. \n\nInspection of the Biocreative dataset revealed that curators annotated family- and complex-level strings in multiple ways: to a single gene, multiple genes, or simply left ungrounded. We therefore partitioned the annotation data into multiple subsets for the purposes of evaluation (Table\u00a0 ). The first of these was the subset of 19,228 entities grounded to human Uniprot or NCBI gene identifiers, which we denote Annotation Subset 1 (AS1; 18.7% of the total). Of these, 2439 entities (2.4% overall) were grounded to   multiple   human gene or protein identifiers; these therefore correspond to gene families or protein complexes (denoted AS2). We also drew from \u201cungrounded\u201d entities, i.e., annotations labeled \u201cgene\u201d or \u201cprotein\u201d but lacking identifiers. A large majority of these represented experimental elements or protein tags, e.g. \u201cGFP\u201d, \u201cFLAG\u201d, \u201cGST\u201d, etc. To streamline curation, we filtered ungrounded entities against the affixes included in FamPlex; a high proportion of ungrounded entities (8250 of 14,227, or 58%) had matches in the FamPlex affixes list in gene_prefixes.csv, leaving 5977 entities for further curation, a subset denoted AS3 (Table\u00a0 ).\n   \nSubsets of the Biocreative VI entity normalization dataset relevant to the FamPlex evaluation \n  \nEntities evaluated against FamPlex were drawn from the categories highlighted in bold \n  \n\nAn initial round of scoring focused exclusively on identifying the proportion of the 2439 entities in AS2 (the subset containing multiple gene/protein groundings) covered by FamPlex; we found that 1908 (78%) had case-insensitive matches in the FamPlex grounding map. Of the remaining 531 unmatched entities (representing 109 unique strings), manual curation indicated that 51 corresponded to non-coding RNAs and were excluded, leaving 2388 entities (1908 + 480) with multiple gene/protein groundings. Of the remaining 480 entities representing proteins, manual curation indicated that 97 had corresponding identifiers in FamPlex. We therefore calculated that FamPlex contained both string matches   and   identifiers for 79.9% of the entity texts in AS2, and identifiers but not string matches for a slightly higher proportion (84%; Table\u00a0 ).\n   \nExtent of FamPlex family/complex coverage evaluated against subsets of the Biocreative VI entity normalization dataset \n  \n\nBecause families were not always grounded to multiple gene/protein identifiers by human curators, we performed a second evaluation in which we manually curated a random sample of entities drawn from AS1 + AS3. Of 764 curated entity strings, 109 were found to be synonyms for protein families or complexes (note that, unlike in the evaluation against AS2 above, this assessment was made independently of the annotations contained in the dataset). As in the previous evaluation, these were scored for the presence of string matches and/or corresponding IDs in FamPlex, yielding similar figures of 81.7 and 88.1%, respectively (Table\u00a0 ). Taken together, these results demonstrate that FamPlex incorporates identifiers and lexical synonyms for a large proportion of the families and complexes relevant to manual biocuration tasks from literature. \n\n\n### FamPlex resolves hierarchical relationships in extracted events \n  \nA key feature of FamPlex is that it allows for relationship resolution not only \u201chorizontally\u201d (between different databases) but also \u201cvertically\u201d (between genes, families, complexes, and any intermediate sets involving these elements). Lexical synonyms can be defined at all levels in the FamPlex hierarchy (Fig.\u00a0 ). The combination of a hierarchical representation with a mapping of entities to text at each level allows information about biological interactions to be correctly organized and cross-referenced. \n\nFor example, the FamPlex family PLC, representing the family of phospholipase C enzymes, contains both individual genes (e.g.,   PLCE1  ) and FamPlex subfamilies (e.g., PLCG, a sub-family consisting of the genes   PLCG1   and   PLCG2  ) as members (Fig.\u00a0 ). In results from the test corpus we found descriptions of meaningful biochemical mechanisms associated with all three levels of this hierarchy\u2014family, subfamily, and genes (Fig.\u00a0 ). Moreover, relevant events were extracted for 12 of the 15 entities in the phospholipase C entity hierarchy, demonstrating the diversity of available mechanistic information and the importance of relationship resolution.\n   \nFamPlex facilitates hierarchical resolution of extracted information.   a   Hierarchical organization of the phospholipase C protein family (FamPlex identifier PLC) along with the proportion of occurrences of each member in the test corpus and examples of sentences yielding information at the different levels. Pink nodes indicate FamPlex families; gray nodes indicate genes.   b   Proportion of groundings in the test corpus to gene-level, intermediate-level, or top-level entities for five multi-level families/complexes in FamPlex \n  \n\nTo characterize the relevance of multi-level relationship resolution more broadly, we counted the number of times a named entity identified by REACH in the test corpus was mapped to a FamPlex identifier at three or more hierarchical levels: the gene level (lowest), the top-level family or complex (highest), and any intermediate level. Distributions of groundings for five FamPlex entries with three or more entity levels are shown in Fig.\u00a0 . Overall, we found that 33 top-level FamPlex entries (i.e. ones that are not subsumed through an   isa   or   partof   relation by another FamPlex entry) were associated with groundings at three or more distinct levels, and 242 top-level FamPlex entries had groundings at two levels (i.e. grounding to the FamPlex entry itself and its constituent genes), showing that gene functions are commonly discussed across multiple levels of specificity. \n\nWe also found that the identifier level used most frequently for grounding differed among protein families and complexes, limiting generalizations about the relative priority of gene- vs. family-level grounding for event extraction. For example, for AMPK, the majority of references in the literature were to the top-level AMPK complex, with a relatively small fraction of references to constituent genes or intermediates. On the other hand, most mappings to the family representing Phospholipase C (PLC in FamPlex) were to constituent genes such as   PLCG1  ,   PLCD1  , etc. Finally, for the family of Activins (hetero- and homo-dimers of the transforming growth factor beta family, Activin in FamPlex), most references were to specific dimer subtypes\u2014Activin A, Activin AB and Activin B\u2014which are found at an intermediate level in the FamPlex hierarchy. \n\n\n### Comparison of FamPlex with other resources \n  \nFamPlex bears similarities to three types of existing resources. The first of these are large, systematic assemblies of protein families derived from sequence and domain analysis; this set includes Pfam, InterPro, and Homologene. As a curated resource, FamPlex is less comprehensive, since it includes only human genes and focuses primarily on gene families and lexicalizations that are described in existing literature. However, FamPlex includes complexes as well as families, based on the observation that these high-level groupings of proteins are often interwoven in discussions of gene function (e.g., \u201cAMPK\u201d and \u201cAMPK-alpha\u201d; Fig.\u00a0 ). FamPlex also provides lexical synonyms for families and complexes, a feature generally absent from large protein family databases. \n\nA second class of comparable resources are the taxonomies of protein families and complexes defined as part of biocuration projects or tools; examples include Reactome, SIGNOR, and OpenBEL. These taxonomies are designed to meet the need of biocurators to specify mechanistic interactions at the family or complex level. Of these resources, we found the families and complexes defined by OpenBEL to be the most systematic and reusable, and we therefore drew heavily on OpenBEL in the construction of FamPlex. FamPlex differs from the families and complexes defined in resources such as Reactome, SIGNOR and OpenBEL in three important ways: (i) it includes an extensive set of lexicalizations to assist in grounding, (ii) it enumerates equivalent family/complex identifiers   between   many of these resources, allowing for mechanistic information to be integrated at the family/complex level, and (iii) it allows for a   multi-level   entity hierarchy corresponding to the terms and concepts used in the literature. \n\nThe third category of related resources are biomedical ontologies such as GO and terminology resources such as NCIT and MeSH. While these resources are the most broadly extensive and often contain synonyms for concepts, they have uneven coverage of protein families and complexes specifically. In addition (as described in our evaluation of grounding to NCIT in the TRIPS reading system) many identifiers representing protein families and complexes do not incorporate child concepts at the gene level, limiting their value for relationship resolution. \n\nThus, while FamPlex draws on and provides cross-references to all three classes of resources described above, it differs from all of them in providing a consistent, multi-level taxonomy of human protein families and complexes that is suitable for grounding and relationship resolution in text mining and biocuration. \n\n\n### Limitations \n  \nThe relatively high recall achieved by FamPlex on the Biocreative entity normalization dataset suggests that it provides substantial coverage of relevant protein families, complexes and their lexical synonyms. However, it is not exhaustive. Further empirically-guided curation of the identifiers and grounding map is likely to improve grounding precision and recall still further, and with additional work mappings to other ontologies can be made more comprehensive. \n\nFamPlex does not directly address the problem of   ambiguity  , selecting among multiple alternative groundings for the same entity. For example, \u201cMEK\u201d can refer to the family of MAPK/ERK Kinases or to the solvent methyl ethyl ketone. Resolving such ambiguities requires an examination of the named entity in the broader context of the sentence or article [ ]. However, the use of FamPlex does increase the likelihood that relevant groundings to protein families will not be missed, and can therefore be considered alongside alternative groundings during an ambiguity resolution procedure. \n\n\n### Accessibility and Extensibility \n  \nWe chose CSV files as the primary format for FamPlex to maximize accessibility and extensibility. CSV files can be opened and edited in any spreadsheet program or text editor, allowing biologists with no background in literature mining to assist in the curation of the grounding map or create mappings to other resources. Because the files are hosted on GitHub, other users can easily fork and make use-case specific extensions or other contributions that can be merged back into the main repository. \n\nIn addition to the CSV files, FamPlex includes an Open Biomedical Ontologies (OBO) [ ] export feature to facilitate integration into OBO-based workflows. FamPlex relations and mappings have been integrated into the TRIPS/DRUM reading system [ ] via OBO-exported content. \n\n\n\n## Conclusions \n  \nIn this paper we describe the challenge posed by protein families and multi-protein complexes for machine reading of the biomedical literature. We introduce FamPlex, a new lexical and ontological resource that addresses these challenges and improves grounding and relationship resolution in two different reading systems [ ,  ]. FamPlex fills a gap in existing bioinformatics resources, linking information about families and complexes in protein and pathway databases to a set of lexical synonyms that occur with high frequency in the scientific literature. Empirical evaluation shows that the hierarchical organization of FamPlex enables the integration of mechanistic information about gene families, complexes, and their individual subunits. This is important because information about biochemical mechanisms is often reported in terms of classes of entities whereas large-scale profiling experiments yield data about individual genes and proteins. FamPlex therefore supports the broader goal of making text mining a key contributor to the process of obtaining biological insight from high throughput -omic data by drawing on relevant mechanistic knowledge. We speculate that similar resources for resolving hierarchical relationships among entities could be useful in other domains of machine reading and natural language processing. \n\n\n## Availability and requirements \n  \n Project name:   FamPlex \n\n Home Page:  \n\n Operating systems:   Platform-independent \n\n Programming language:   Not required (helper scripts available in Python 3) \n\n License:   Creative Commons CC0 \n\n \n", "metadata": {"pmcid": 6022344, "text_md5": "a0cc4175dde5559edfe5d890afc01d4f", "field_positions": {"authors": [0, 60], "journal": [61, 79], "publication_year": [81, 85], "title": [96, 232], "keywords": [246, 388], "abstract": [401, 2896], "body": [2905, 49488]}, "batch": 1, "pmid": 29954318, "doi": "10.1186/s12859-018-2211-5", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6022344", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6022344"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6022344\">6022344</a>", "list_title": "PMC6022344  FamPlex: a resource for entity recognition and relationship resolution of human protein families and complexes in biomedical text mining"}
{"text": "Raza, Shaina and Schwartz, Brian\nBMC Med Inform Decis Mak, 2023\n\n# Title\n\nEntity and relation extraction from clinical case reports of COVID-19: a natural language processing approach\n\n# Keywords\n\nNatural language processing\nData cohort\nCOVID-19\nNamed entity\nRelation extraction\nTransfer learning\nArtificial intelligence\n\n\n# Abstract\n \n## Background \n  \nExtracting relevant information about infectious diseases is an essential task. However, a significant obstacle in supporting public health research is the lack of methods for effectively mining large amounts of health data.\n \n\n\n## Objective \n  \nThis study aims to use natural language processing (NLP) to extract the key information (clinical factors, social determinants of health) from published cases in the literature.\n \n\n\n## Methods \n  \nThe proposed framework integrates a data layer for preparing a data cohort from clinical case reports; an NLP layer to find the clinical and demographic-named entities and relations in the texts; and an evaluation layer for benchmarking performance and analysis. The focus of this study is to extract valuable information from COVID-19 case reports. \n\n\n## Results \n  \nThe named entity recognition implementation in the NLP layer achieves a performance gain of about 1\u20133% compared to benchmark methods. Furthermore, even without extensive data labeling, the relation extraction method outperforms benchmark methods in terms of accuracy (by 1\u20138% better). A thorough examination reveals the disease\u2019s presence and symptoms prevalence in patients. \n\n\n## Conclusions \n  \nA similar approach can be generalized to other infectious diseases. It is worthwhile to use prior knowledge acquired through transfer learning when researching other infectious diseases. \n\n\n## Supplementary Information \n  \nThe online version contains supplementary material available at 10.1186/s12911-023-02117-3. \n\n \n\n# Body\n \n## Introduction \n  \nThe COVID-19 pandemic has generated massive amounts of clinical, behavioral, social, and epidemiological data. As of August 2, 2022, COVID-19 has infected more than 578 million people, with over 6.4 million deaths [ ]. There are serious concerns regarding the impact of the infectious disease on society, global health, and the economy [ \u2013 ]. It is necessary to develop an efficient surveillance system that can track the spread of infectious diseases by collecting, analyzing, and reporting data to those responsible for preventing and controlling the disease. \n\nDespite significant advances in informatics, some hurdles to studying infectious diseases remain unresolved. First, the conventional methods are typically trained on limited data [ ]. In real-world scenarios, a substantial amount of clinical data is available as free-text [ ] data, such as electronic health records (EHRs), or published literature [ ]. Second, the existing methods mainly rely on manually labeled structured datasets for predictive modeling. Although governments and organizations can always collect data on real-world pandemic events, it is highly costly and time-consuming. Also, when these datasets are made available to the research community, reporting lags behind the current number of COVID-19 cases. \n\nGiven the aforementioned challenges, we propose a NLP framework that uses deep neural networks and transfer learning to automatically extract valuable information from texts for analyzing COVID-19 and other infectious diseases. Transfer learning [ ] is a technique that allows models trained on one task to be applied to another related task, making it a powerful tool for NLP tasks. The main objective of this research is to bridge the gap that exists between NLP techniques and their applications in public health (PH). The specific contributions of this work are:   \nA data cohort is created by curating published COVID-19 case reports from the National Institutes of Health (NIH) source between March 1, 2022, and June 30, 2022. The data is parsed to create a COVID-19 database. A portion of the data is prepared for gold annotations, and the technique of active learning [ ] is applied for corpus re-annotation. \n  \nA deep learning-based named entity recognition (NER) algorithm is proposed to learn the clinical (disease, condition, symptom, drug) and non-clinical (social determinants of health) concepts from the case reports data. Additionally, a relation extraction (RE) model for predicting relationships (such as disease-brings-complications, treatment-improve-condition, and drug-and-adverse effect) between entities is proposed. The performance of these approaches is evaluated through an extensive comparison with several baseline methods, including ML, deep learning, and Transformer-based models on various datasets. The results are discussed in the context of public health surveillance and monitoring. An empirical analysis of the proposed approach in extracting key information from the texts, along with a discussion of the benefits and limitations of this approach is also presented. \n  \n\nThe current study improves upon previous efforts by extracting clinical and non-clinical entities from the case report data. The key contribution of this work is the integration of various NLP components into a pipeline structure, which enables the efficient extraction of valuable information from texts. The research question that guides this study is \u201cHow effective is transfer learning in enhancing the performance of NLP tasks for identifying and extracting information about infectious diseases in the clinical and public health domain?\u201d Through this study, we aim to provide solutions that can assist policymakers in their decision-making processes and accelerate research on the subject. \n\n\n## Related work \n  \nNamed entity recognition (NER) [ ] is a subtask of NLP that involves identifying and classifying named entities in text into predefined categories such as person names, organizations, locations, medical codes, etc. Biomedical NER [ ] is a specialized NER task that focuses on identifying and classifying biomedical entities, such as genes, proteins, and diseases, in unstructured text. State-of-the-art biomedical NER models include BiLSTM-based [ ] and Transformer-based [ \u2013 ] models, which can capture contextual dependencies and are robust to noise and variations in the input data. They can also be combined with other techniques such as attention mechanisms [ ] and convolutional neural networks (CNN) [ ] to further improve their performance. Recent developments in biomedical NER include the use of transfer learning [ ], BERT-like [ ], attention-based [ ], multi-task learning [ ], and hybrid models [ ] to improve the performance of these models. \n\nRelation extraction (RE) [ ,  ] is the process of identifying and classifying relationships between entities in text. Statistical and machine learning (ML) methods [ ], such as rule-based systems, SVMs, and decision trees, can be used for RE tasks, although they may struggle with more complex relationships. Deep learning models, such as CNN [ ] and recurrent neural networks (RNN) [ ], can be used for RE tasks and can handle complex relationships. These deep neural network-based models usually require a large amount of labeled training data and computational resources. \n\nZero-shot learning (ZSL) [ ] is an ML problem in which a learner observes samples from classes that were not observed during training and must predict which class they belong to. The topic of ZSL and RE in combination has received relatively little research to date. One such study [ ] manually creates templates of new-coming unseen relations, while the other study [ ] treats the zero-shot prediction as the text entailment task. Some other works [ ] consider ZSL by leveraging knowledge gained from BERT-like models [ ] to predict unseen relations without manual labeling, which is also a motivation in this research. \n\n### Comparison with related works \n  \nSeveral works focus on extracting clinical information, particularly related to COVID-19, from unstructured text data. For example, Lybarger et al. [ ] present a corpus of EHRs from COVID-19 patients with annotations for diagnoses, symptoms, and other clinical events. They propose a neural event extraction framework using a BiLSTM-CRF model for identifying and classifying these events. Luo et al. [ ] propose a Transformer architecture trained on a large annotated dataset of COVID-19 symptoms. CORD-19 [ ] is another large dataset of COVID-19 research papers compiled by Kaggle that can be used for tasks such as information extraction and text classification. Silverman et al. [ ] present a NER model based on a BiLSTM-CNN architecture for extracting symptoms from unstructured COVID-19 data. These works have the potential to be used for tasks such as public health surveillance and monitoring. In recent works, relations from texts are typically extracted using statistical methods such as decision trees [ ]. Recently, deep neural networks such as BiLSTM-based models CRF [ ,  ,  ] and BERT-like methods [ ,  ] have also been used to extract relations, which are both very robust and accurate but require a large amount of labeled data. \n\nIn contrast to earlier studies [ \u2013 ], our NER method also identifies non\u2013clinical factors like social determinants of health (SDOH) [ ] in addition to a variety of clinical factors. This is significant because SDOH factors have a significant impact on health outcomes, particularly during a pandemic like COVID-19. Additionally, our RE method extracts relationships from clinical texts without the need for labeled data, which sets it apart from existing works [ ,  ] that require labeled data. We have combined ZSL, transfer learning, and RE in the context of COVID-19 to offer a comprehensive approach to understand the impact of the pandemic on population health. We have thoroughly tested and optimized our method through ablation studies to ensure maximum effectiveness. \n\n\n\n## Materials and methods \n  \nIn this study, we propose an NLP architecture for extracting key information from case reports data. This architecture has three layers: a data layer, which is responsible for preprocessing, preparing, and annotating the text data; an NLP layer, which includes a NER module to extract named entities (e.g. diseases, symptoms, conditions, social determinants of health) from the data and a RE module to infer relationships between the entities (such as disease-symptoms relationships, etc.); and an evaluation layer, which is used to evaluate the performance of the NLP modules and to assess the effectiveness of the proposed methods through empirical analysis. \n\n### Data cohort preparation \n  \nIn this study, we create a data cohort from electronic case reports of COVID-19 patients. A case report [ ] is a published article describing a patient\u2019s disease, diagnosis, treatment, symptoms, or therapy. We curated the case reports using a search query (Additional file  : Table S1) using the NIH National Library of Medicine (NLM) [ ] API that allowed us to get case reports from various journals. These case reports comply with CARE (CAse REports) principles [ ], which specify that case reports should not contain any patient-identifiable information. \n\n#### Inclusion and exclusion criterion \n  \nWe consider the case reports to meet the eligibility criteria in Table  . The exclusion criteria for this study are as:   \nGrey literature, preprints, and clinical trials are excluded. \n  \nNon-English content is excluded. Evidence suggests that excluding non-English publications from evidence synthesis does not bias conclusions [ ]. \n    \nSelection criteria of the data \n  \n\nAfter applying these filtrations, we obtained 4338 case reports \n\n\n\n### Proposed framework \n  \nThe proposed framework is shown in Fig.\u00a0  and explained next.   \nProposed framework for pandemic surveillance \n  \n\n#### Data layer \n  \nWe began by gathering the biomedical data, which are COVID-19-related case reports from NIH sources (data cohort discussed above). The biomedical data goes into the scientific parser. We used the Spark OCR [ ] to parse the case reports\u2019 PDFs and extracted the content in a data frame format, with each row corresponding to a case report (document) and a column containing extracted text from the PDF texts. The parsed documents were indexed using Elasticsearch [ ], which helps with speedy document retrieval, to create a reference-standard dataset. A   reference-standard dataset   [ ], generally, refers to the collection and compilation of primary and secondary data sources that can be re-used and cited for various purposes, such as biomedical data retrieval. \n\n Gold-standard data   A random portion of the data (200 case reports) was issued to create a gold-standard dataset (manually annotated corpus) [ ]. We draw inspiration from a previous work [ ] on case reports annotations and consider only the case report texts rather than complete manuscript texts for this data preparation. Four experts from a biomedical domain annotated around 200 case reports with the clinical and non-clinical named entities. We use the Spark NLP annotation tool [ ] to annotate the chosen case reports with the named entities. These named entities are given in Additional file  : Table S2. \n\nWe have drawn inspiration from the guidelines for annotating named entities in literature sources [ ,  ]. For Inter-annotator agreement (IAA) [ ], we employ the simple agreement method, which calculates the percentage of annotations that match among all annotators without considering random chance (as in Cohen and Fleiss\u2019 kappa statistics). The guidelines for our annotation task are provided in Table S2. We save these annotations into the CONLL-2003 [ ] format, a prototypical data format for NER tasks. At the end of this step, we found approximately 500 sentences and 3048 gold labels. \n\n Active learning   Since our models are based on deep neural networks, it is best to train with more labeled data. We used the   active learning   [ ] technique (shown in Additional file  : Figure S1) for more data labeling. We began this active learning loop with 500 sentences (from the gold-standard dataset) and added batches of 100 samples (reports) until it reached around 1000 samples, with the best accuracy of approximately 92.80%. By the end of this task, we obtained 47,888 sentences with approximately 387,899 named entities. We used this data to train our NER model. \n\n Task-specific Transformer model   We fine-tuned the Bidirectional Encoder Representations from Transformers (BERT) for Biomedical Text Mining (BioBERT) [ ] using our annotated dataset to prepare a task-specific model, which is more lightweight than a typical task-agnostic model [ ]. We release the weights of our fine-tuned model here [ ] and show our task-specific transformer model in Additional file  : Figure S2. \n\n\n#### NLP layer \n  \nWe develop two NLP models in this layer, which are (1) a NER module to produce named entities; (2) a RE module to define relationships between the named entities. \n\n Named entity recognition model   This model is inspired by bi-directional (Bi) long short-term memory (LSTM) model with a conditional random field (CRF) layer [ ], but we add a Transformer layer to produce a variant of the model. We show our Transformer-BiLSTM-CRF model in Fig.\u00a0  and explain its working next. The notations in formulae are given in Additional file  : Table S3.   \nProposed model for named entity recognition \n  \n\n Transformer layer   The input text sequence (shown in the bottom layer of Fig.\u00a0 ) goes into the Transformer layer. In this layer, we adapt the BERT architecture for the embedding. The core of BERT is implemented by multi-layer Transformer encoders [ ] which is dependent on the self-attention mechanism. The self-attention information in this layer is obtained by formula ( ) [ ]: \n\nThe output of this layer is word embeddings in the form of word vectors, which are fed into the next layer, the BiLSTM layer, to learn context features. Our technical contribution here entails the use of our task-specific Transformer model for extracting these entities. \n\n BiLSTM layer   The Transformer output vector is input to the BiLSTM layer. The BiLSTM units receive a dynamic sequence of word vectors as input and learn to extract local features from the sentence. The forward LSTM model generates hidden state sequences, and the backward LSTM model combines them to produce the complete hidden state sequence for the sentence sequence. The related information is obtained by the formula (2) [ ]. \n\nThe input   x   to the BiLSTM layer is the output of the Transformer. The BiLSTM layer uses both a forward and backward LSTM to capture contextual information and obtain global features for each moment in the input sequence. This allows the BiLSTM to effectively process the input sequence and extract useful features from it. The output of the BiLSTM layer is a sequence of hidden states, one for each input word. \n\n CRF layer   The input to the CRF layer is the output sequence of the BiLSTM layer. The CRF layer captures the dependency relationship between the named tags and constrains them to the final predicted labels [ ]. The conditional probability distribution in CRF is represented by   and shown in formula ( ) [ ]. \n\nThe output of the CRF layer is the Inside\u2013Outside\u2013Before (IOB) format, a scheme for tagging tokens in NER chunking tasks [ ]. We convert the IOB representation of our model to a user-friendly format by associating chunks (recognized named entities) with their respective labels, as shown in Additional file  : Figure S3. We also filter out the NER chunks with no associated entity (tagged as \u2018O\u2019). The model\u2019s output is the named entities given in Additional file  : Table S2. \n\nWe note a case report on long-COVID, titled: \u201cCase report: overlap between long covid and functional neurological disorders\u201d [ ], and show the named entities extracted by our model in Additional file  : Table S4. We also show the visual representation of named entities from the snippet of the case report in Additional file  : Figure S4. This approach can also detect information from a non-COVID-19 case report [ ], as shown in an example in Additional file  : Table S5. \n\n Relation Extraction   A relation can be defined as a triple (shown in formula  ) with indices in   and   that delimit a named entity mentioned in   x   (sequence of tokens). \n\nThe RE task can identify a specific relation between two co-occurring entities [ ], such as symptom-disease, disease-disease, and drug-effects associations. Prior to the RE, there is a dependency parsing (DP) task (example shown in Additional file  : Figure S5), which refers to examining the dependencies between the words of a sentence to analyze its grammatical structure [ ]. For instance, to identify the subjects and objects of a verb and the terms that modify (describe) the subject. These dependencies go as input to the RE module. \n\nTaking inspiration from recent NLP works on RE [ ,  ], we employ zero-shot learning (ZSL) [ ] to infer relations from the texts. ZSL is an ML technique in which a model observes samples from classes which has not been explicitly observed before during training. We already our NERT model as a base for the RE model. The RE model can predict relations between named entities without any additional training through ZSL. \n\nFigure\u00a0  shows the working of our ZSL-based Transformer model for Relation Extraction (ZSL-TRE). Our ZSL-TRE consists of a BERT encoder (Transformer layer) and a classifier layer, as shown in Fig.\u00a0 . The BERT encoder takes an input sequence of text and produces a fixed-length encoding that captures the contextual information in the input. This encoding is then passed to the classifier layer, which uses it to predict the relation expressed by the input. We use the softmax function in the classifier layer.   \nZero-shot learning-based transformer model for relation extraction \n  \n\nThe BERT encoder is already trained on a large dataset of input\u2013output pairs where the inputs are text sequences, and the outputs are class labels for a set of predefined relations. The trained model can classify a new input as expressing one of these relations, even if it has not seen that specific relation before, as long as the input is similar enough to the examples the model was trained on. The classifier layer is then trained to predict the relation based on the output of the BERT encoder, which encodes contextual information about the input. When presented with a new input, the BERT encoder encodes it and passes the encoding to the classifier, which then predicts the most likely relation based on the information it has learned from the training data. We show an example of RE from our corpus in Additional file  : Figure S6. \n\n\n#### Evaluation layer \n  \nThe evaluation layer receives the NLP layer\u2019s output (named entities and relations) and evaluates the results of the proposed NER and RE methods. This research uses a two-fold evaluation approach: quantitative analysis and qualitative analysis. We compare the proposed tasks\u2019 accuracy to baseline methods across benchmark datasets (including our test set for NER) for the quantitative analysis. We also perform the ablation study to show the effectiveness of each part of our proposed model. To show how effective our proposed approach is for pandemic surveillance, we carried out a qualitative analysis using case report data. We demonstrate the use of different named entities and specify the unseen relations on the run. We show the summary of some data statistics of our reference dataset in Additional file  : Table S6. \n\n Evaluation protocol   We randomly split each dataset into 70% train, 15% validation, and 15% test set for evaluation. We used 30% of our annotated data for the NER task test set. For the RE task, we used benchmark datasets as we don't have a test set. The details of the benchmark datasets and the baseline methods used in the evaluation are given in Additional file  : Table S7. Similar to related works [ ], we also employ the micro-average F1-score metric to evaluate NER and RE tasks. All the baselines are optimized to their optimal settings, and we report the best results for each method. The BERT encoder layers are implemented using the PyTorch BERT implementation available from Huggingface [ ]. The general hyperparameters used during training are given in Additional file  : Table S8. \n\n Training setup   We set the experimental environment as: Intel(R) Core(TM) i7-8565U CPU @ 1.80\u00a0GHz, 1.99\u00a0GHz, 16.0\u00a0GB RAM, 64-bit operating system,\u2009\u00d7\u200964-based processor; GPU: Google Colab Pro with cloud-based GPUs (K80, P100, or T4), 32\u00a0GB RAM and training capabilities. We connect the Google Colab to Google Drive to get enough storage for transfer learning. \n\n\n\n\n## Results \n  \n### Overall performance comparison \n  \nThe NER model performance is compared against the baseline methods using different benchmark datasets, including our own test set. \n\nOur proposed method outperforms all the baseline methods on all of the datasets in Table  . We observe that the BERT-based methods generally perform better than the BiLSTM-based methods, but the performance difference between the two sets of methods is relatively small, indicating that BiLSTM models can achieve a high level of accuracy if trained properly. We adopt a hybrid approach where we use the BERT-based model with the BiLSTM. Our approach to NER achieves a higher F1 score of 90.58% on the NCBI-disease dataset with significantly less feature engineering. On the BC5CDR dataset, our method obtains a micro-averaged F1 score of 89.90% for both chemical and disease entities. On the BC4CHEMD dataset with chemical entities, BERT-based methods, our method and Att-BiLSTM-CRF achieve scores above 90%. When evaluating the performance on the BC2GM and JNLPBA datasets for protein and gene names, our approach and the BERT-based methods perform well, with the overall performance on the JNLPBA dataset being relatively lower. This trend (of the lower performance of JNLPBA) is also observed in most related works [ ] for these datasets. For the i2b2 datasets that are trained on clinical named entities, we find that clinical embeddings like those provided by BioBERT significantly improve the performance of clinical NER tasks, suggesting that a method performance is closely tied to the entity types it was trained on.   \nTest results in biomedical NER task using the micro-averaged F1-score \n  \nBold means best score, italic second-best score \n  \n\nIn the following experiments, we compare the performance of our proposed RE model with that of baseline methods using benchmark datasets. Since we do not have a specific labeled test set for the RE task, we evaluate the performance of our RE model on benchmark datasets and provide a detailed analysis of our approach on case reports in later sections. Our RE model utilizes a ZSL approach, in which we do not utilize the provided relation labels from the benchmark datasets. Instead, we attempt to infer these relations using the knowledge from our fine-tuned Transformer model. The baseline methods, on the other hand, are run using the relation labels provided by the benchmark dataset providers. \n\nAs shown in Table  , our approach outperforms the other methods on all seven datasets. These other models employ various techniques to extract relationships from the input sentences and make predictions, and they have achieved strong performance with full supervision. However, our model can predict these relationships, which it has not seen before, with a higher level of accuracy. The superior performance of our method is attributed to its use of a transfer learning mechanism, where the relationship attributes are generated using zero-shot learning predictions.   \nTest results of relation extraction task on benchmark datasets using micro-averaged F1-score \n  \nBold means best score, italic second-best score \n  \n\n\n### Ablation study \n  \nTo verify the validity of each part of our proposed NER and RE models, we conducted the ablation experiment. To keep it concise and avoid presenting repetitive test results, we have focused the ablation experiment on the i2b2-clinical and our own test set. The ablation experiment settings are as follows:   \n Full  : Proposed Transformer-BiLSTM-CRF model. \n  \n BiLSTM-CRF  : Remove the Transformer layer. Equivalent to BiLSTM-CRF [ ] with character-level embedding via the CNN layer. \n  \n BiLSTM:   Remove the Transformer and CRF layers, equivalent to the BiLSTM model with a softmax layer to predict NER labels. \n  \n Transformer-BiLSTM  : Remove the CRF layer only, a softMax layer to predict NER labels. \n  \n Transformer:   Remove the BiLSTM and CRF layers. \n  \n\nAccording to the results presented in Table  , the full proposed model has the highest micro-average F1 score among the model variants on both test sets. The BiLSTM-CRF model can capture global features, but its performance is decreased by 4\u20136% compared to the full model, probably because it lacks contextualized representations by missing Transformer input. The model variants with a Transformer layer outperform those without it. The CRF layer, which is added after the BiLSTM and is used to correct the named entity tag sequence, is not present in the Transformer variants. Despite this, the Transformer variants still perform well with a simple enhancement function applied to the IOB representation of the output. Overall, this result suggests that transfer learning has improved the performance of the model variants.   \nAblation experiment results of the proposed NER model on i2b2-clinical and our test set using micro-averaged F1-score \n  \nBold means best score, italic second-best score \n  \n\nWe again perform the ablation experiment on the proposed RE model. We present the results for the i2b2-clinical dataset in this report, as we do not have our test set for this task. The model variants are:   \n Full  : Proposed RE model without any labeled data, and task-specific model weights. \n  \n Without fine-tuning step  : BERT-based layer without a task-specific model. \n  \n Without full Transformer layer  : BILSTM-CRF model without the Transformer layer. \n  \n\nOverall, the results in Table   show that the full model with task-specific fine-tuning performed the best. This is because the fine-tuning process adjusts the model weights to be more suitable for the specific task, leading to better performance. When the fine-tuning step is skipped and only the Transformer layer is retained, the model is not as effective at the task, resulting in lower performance. When the full transformer layer is missing, the model performs even worse, likely because the model is unable to predict the relations in the test set effectively.   \nAblation experiment results of the proposed NER model using micro-averaged F1-score on the relations provided in the i2b2-clinical testset \n  \nBold means best score \n  \n\n\n### Evaluation outcomes \n  \n#### Effectiveness of named entity recognition approach \n  \nWe evaluated the effectiveness of our proposed approach in practical use cases. \n\nWe observe in Fig.\u00a0  that fever/chills, nasal congestion, pains, and running nose are the most frequent COVID-19 symptoms, which are reported by the CDC [ ]. Next, we show the most frequent medical conditions in COVID-19 patients in Fig.\u00a0  and find pneumonia and respiratory disorders as the most frequent among others.   \nFrequency of COVID-19 symptoms \n    \nDistribution of most frequent medical complications in the population \n  \n\nWe also show the prevalence of conditions in COVID-19 patients based on the most occurring disease disorder (occurring more than 70%). The results are shown in Fig.\u00a0 .   \nCondition prevalence related to different disease syndromes (Cerebrovascular, Cardiovascular, Pulmonary, Psychological). Bars represent the number of respondents who experienced each symptom at any point in their illness \n  \n\nAccording to Fig.\u00a0 , stroke appears to be the most common condition among patients with the cerebrovascular syndrome, chest pain is the most common condition among those with cardiovascular disease, and shortness of breath is the most prevalent condition among those with pulmonary disease. These findings highlight the serious nature of these conditions among patients with COVID-19. Additionally, we see that psychological conditions such as anxiety, depression, and post-traumatic stress disorder (PTSD) are present in COVID-19 patients, which may be the result of the impact of COVID-19 on their mental health. \n\nFigure\u00a0  depicts COVID-19 hospitalization by race, revealing that Hispanics are the most affected (37%), followed by blacks (35%), Asians (17%), and whites (11%). This finding is based on a sample of the population and is not representative of the whole population.   \nCOVID-19 hospitalization by race \n  \n\nIn Additional file  : Table S9 we show that named entities occur frequently in 1000 random case reports. We also show the hospitalization, ICU admission, and mortality in COVID-19 patients of various ages in Additional file  : Figure S7. \n\n\n#### Effectiveness of relation extraction approach \n  \nWe demonstrate the effectiveness of our ZSL-TRE approach by specifying relationships on the run. Table   displays the 'after' relationships\u2014condition/symptom followed by a disease disorder.   \n\u2018After\u2019 relation\u2014disease disorder\u2014condition/symptom \n  \nDisease disorders are chosen based on the frequency of prevalence (occurring\u2009>\u200970%) \n  \n\nTable   shows that certain symptoms are followed by a specific disease; for example, the symptoms of COVID-19 are visible once a patient has the disease. We also specify the temporal relation: before, after, and overlap relations [ ], as defined in the 2012 i2b2 challenge in Fig.\u00a0 . We observe that conditions (fever and cough) are seen after the vaccine (treatment).   \nTemporal relations in a text \n  \n\nNext, we specify the relationship between \u201cDRUG AND [EFFECT]\u201d and show the top 3 effects for a commonly mentioned drug (Oral amoxicillin) in Table  . The result shows that abdominal pain and diarrhea are some of the effects associated with amoxicillin and Pirfenidone.   \nRelations type: DRUG -EFFECT \n  \n\nWe also show the adverse drug effect (ADE) associated with the Paxlovid drug, which is most frequently for treating COVID-19, in Fig.\u00a0  and see the associated effects with this drug are hives, trouble breathing, skin and swelling.   \nAdverse drug effect with Paxlovid drug \n  \n\n\n\n\n## Discussion \n  \n### Principal findings \n  \nWe have observed that pneumonia, respiratory infections, and acute respiratory distress syndrome (ARDS) are common symptoms among COVID-19 patients, which is consistent with the reports from the WHO [ ]. We have also noted the prevalence of various conditions among patients with multiple comorbidities and how different symptoms and conditions become more prominent in these patients. Our findings on psychological conditions and their potential relationship with Long-COVID and mental health sequelae [ ] may be useful for practitioners to consider when treating patients. We have also identified relationships between certain drugs and side effects such as headache, nausea, and dizziness, which can help healthcare providers quickly identify potential adverse effects in patients without having to manually review EHRs. \n\n\n### Impact of transfer learning for predicting COVID-19 patients\u2019 outcomes \n  \nIn the context of detecting COVID-19 named entities and relationships, transfer learning can be used to leverage existing models and knowledge about NLP to improve the performance of a new model on a specific task. This can be particularly useful when there is a limited amount of annotated data available for the specific task, and the model can use the knowledge acquired from other tasks to better understand the data and make more accurate predictions. In our experiments, we found that our methods, BioBERT, and BLUE, which all use transfer learning, performed very well in detecting named entities and relationships, suggesting that this approach can be effective in this domain (addressing RQ stated above). \n\n\n### Generalizability of the proposed approach \n  \nOur proposed framework has the potential to be applied to other domains and tasks with some adjustments to the data and possibly minor code modifications. The extent to which this framework can be generalized to other domains and tasks depends on the specific characteristics of the domain and task at hand. Some adjustments will likely need to be made to the data and possibly to the code to apply the framework to other domains and tasks. \n\n\n### Active learning experience \n  \nIn our study, we discovered that active learning has the potential to reduce annotation costs for building NER models. Our current experience with active learning is based on a simple use case, but the overall goal with this method was to show that it is worth considering, particularly in applications where the data domain is limited (such as COVID-19 or a specific use case). We suggest further investigation into different active learning techniques for large-scale re-annotation. \n\n\n### Predicting unseen relations in the texts using NLP \n  \nIn this work, we attempted to infer relationships in text using NLP techniques. While this approach allows us to identify relationships between entities, it is not the same as the causal relationship extraction task that is commonly used in epidemiological studies. Most existing relationship extraction techniques in ML require labeled data for supervised learning tasks, and this can be a significant challenge. However, our approach does not require labeled data, making it a potential alternative for extracting relationships in the texts. \n\n\n### Limitations and ethical implications of associations and relations with NLP extraction tools \n  \nNLP techniques are often used to extract and analyze relationships and associations from text data [ ,  ], but like any analytical method, they have limitations and potential biases that should be carefully considered. Association analysis can be a useful tool for identifying patterns and relationships in data [ ], but it is important to recognize that the presence of an association does not necessarily indicate a causal relationship. There may be other factors at play that contribute to the observed association, and it is important to consider those factors when interpreting the results. For example, in the case of hospitalization by race discussed above, there may be other factors that contribute to the association between race and hospitalization [ ], such as socio-economic status, access to healthcare, or pre-existing health conditions. If these factors are not taken into account, the analysis of the data could be misleading and potentially perpetuate harmful stereotypes or biases. It is important for researchers and analysts to be mindful of these limitations in text-based analysis and to consider potential alternative explanations for observed associations. \n\nThe source data we used is focused on published case reports. As a result, the sample is likely biased toward sicker patients, those hospitalized, those who had Long-COVID, and those who were seen by academic physicians who would write them up for publication. It excludes minor cases, those who were not hospitalized, and those who were not cared for by these providers, who were likely poorer, lived-in remote areas, did not receive proper care and were less likely to see academic physicians so on. \n\nDeploying a language model on a large dataset, particularly in the clinical text domain, requires powerful computing resources to process and analyze the data efficiently. Insufficient hardware such as lack of memory and graphics processing units can impede the speed and accuracy of the analysis and decision-making process. Furthermore, it is vital to ensure patient privacy and comply with regulatory requirements, such as the Health Insurance Portability and Accountability Act (HIPAA) [ ], when working with clinical text. Therefore, it is essential to include measures such as implementing secure protocols and ensuring compliance with Protected Health Information (PHI) regulations throughout the NLP pipeline to ensure the success of the deployment. \n\n\n### Future directions \n  \nWe propose leveraging the Bradford Hill criteria [ ] in the RE task. The Bradford Hill criteria are a tool that can be used to assess the causal relationship between an exposure and an outcome, and by leveraging these criteria and coordinating PH initiatives with the RE task, it may be possible to identify and address potential health risks and issues within a population more effectively. \n\nThe integration of additional data sources, such as real-time EHRs and pathologic reports, is important in effectively utilizing AI in the fight against the COVID-19 pandemic. Not only will this provide a more comprehensive understanding of the disease, but it will also aid in the development of more accurate and effective treatment plans. Furthermore, it is essential that we address specific aspects of the pandemic, such as misinformation spread, as this can greatly impact the effectiveness of our efforts. \n\nIn terms of Inter-Annotator Agreement (IAA), it is imperative that we utilize additional statistical measures [ ], such as Cohen Kappa, Fleiss Kappa, and Krippendorff Alpha. These measures provide a more in-depth understanding of the reliability and consistency of annotations made by different annotators and can help identify any areas of disagreement or confusion that may require attention. By using various measures, we can ensure the highest level of accuracy and precision in our AI-assisted efforts to combat the pandemic.\n \n\n\n\n## Conclusion \n  \nThis study has shown that NLP-based methods can be used to detect the presence of diseases, symptoms, and risk characteristics. Transfer learning shows promise for developing predictive disease models with limited data, and our proposed methodology offers a useful way to identify named entities and relationships in clinical texts. In comparison to state-of-the-art methods, our proposed methods achieve a higher micro-averaged F1 score for both the NER and RE tasks. The analysis of the case report data shows that the proposed approach can be an effective tool for pandemic surveillance. Overall, this study demonstrates the potential of NLP-based methods for detecting and understanding diseases and other clinical concepts in text data.\n \n\n\n## Supplementary Information \n  \n\n\n\n\n \n", "metadata": {"pmcid": 9879259, "text_md5": "82f40e3d6731329d25213529d3f3186d", "field_positions": {"authors": [0, 32], "journal": [33, 57], "publication_year": [59, 63], "title": [74, 183], "keywords": [197, 321], "abstract": [334, 1881], "body": [1890, 40689]}, "batch": 1, "pmid": 36703154, "doi": "10.1186/s12911-023-02117-3", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9879259", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9879259"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9879259\">9879259</a>", "list_title": "PMC9879259  Entity and relation extraction from clinical case reports of COVID-19: a natural language processing approach"}
{"text": "Murugesan, Gurusamy and Abdulkadhar, Sabenabanu and Bhasuran, Balu and Natarajan, Jeyakumar\nEURASIP J Bioinform Syst Biol, 2017\n\n# Title\n\nBCC-NER: bidirectional, contextual clues named entity tagger for gene/protein mention recognition\n\n# Keywords\n\nBiomedical text mining\nNamed entity recognition\nConditional random fields\nHybrid NER approaches\nMargin-infused relaxed algorithm\nBidirectional parsing\n\n\n# Abstract\n \nTagging biomedical entities such as gene, protein, cell, and cell-line is the first step and an important pre-requisite in biomedical literature mining. In this paper, we describe our hybrid named entity tagging approach namely BCC-NER (bidirectional, contextual clues named entity tagger for gene/protein mention recognition). BCC-NER is deployed with three modules. The first module is for text processing which includes basic NLP pre-processing, feature extraction, and feature selection. The second module is for training and model building with bidirectional conditional random fields (CRF) to parse the text in both directions (forward and backward) and integrate the backward and forward trained models using margin-infused relaxed algorithm (MIRA). The third and final module is for post-processing to achieve a better performance, which includes surrounding text features, parenthesis mismatching, and two-tier abbreviation algorithm. The evaluation results on BioCreative II GM test corpus of BCC-NER achieve a precision of 89.95, recall of 84.15 and overall F-score of 86.95, which is higher than the other currently available open source taggers. \n \n\n# Body\n \n## Introduction \n  \nScientific literature is the major source of biomedical knowledge, and the interest in developing automated text mining solutions to extract useful information from biomedical text is increasing every year. Bio-named entity recognition (NER) is the key step for such information extraction from biomedical literature [ \u2013 ]. \n\nBiomedical-named entities include genes, proteins, RNA, cell, and cell-line. NER in the biomedical domain is generally considered to be more difficult than other domains such as newswire as there is no standard nomenclature naming biomedical entities like genes and protein names resulting in ambiguity, and further, there are millions of biomedical entity names in use and more entities are added regularly [ ,  ]. Moreover biomedical entities such as gene and protein names have similar morphology and context [ ]. \n\nThe commonly used techniques for NER task are rule-based approaches [ ], dictionary-based approaches [ ], machine learning approaches [ ], and recent hybrid systems which use a combination of two or more approaches. Presently, hybrid approaches give best results in NER task [ ,  ]. To understand the current state-of-the-art, we briefly introduce some of the recent hybrid approaches explored for biomedical NER task. Raja et al. [ ] used a hybrid named entity rule-based tagger with 14 hand-crafted rules and a set of post-processing methods and an abbreviation algorithm to tag the human gene/proteins from biomedical articles. \n\nLeaman et al. [ ] proposed a machine learning-based open source biomedical named entity system which was a combination of conditional random fields (CRF) and some post-processing methods to tag gene/proteins. Campos et al. [ ] designed a biomedical hybrid tagger with machine learning algorithm and lexicon-based approaches. Zhu et al. [ ] used both support vector machines (SVM) and CRF for better performance. SVM, a binary classifier, was used to separate the biological terms from non-biological terms, and CRF was used to determine the types of biological terms. Thus, the results of SVM as well as CRF were fused and a useful algorithm was developed after applying two rules. \n\nIn this paper, we describe our hybrid approach, namely BCC-NER (bidirectional, contextual clues named entity tagger for gene/protein mention recognition) which uses bidirectional parsing with the tagging results of forward and backward models combined and trained using CRF model. In order to achieve a better performance, we further applied surrounding text features, parenthesis mismatching, and two-tier abbreviation algorithms post-processing steps. Figure\u00a0  depicts the workflow of various modules in BCC-NER.   \nThe workflow of various modules in BCC-NER \n  \n\n\n## Materials and methods \n  \nBCC-NER is the hybrid named entity recognition system trained and tested on BioCreative II GM corpus [ ]. The system is composed of the following three modules.   \nText processing module which includes NLP preprocessing, feature extraction, and selection \n  \nCRF training module, which uses bidirectional CRF for learning and labeling in both directions and model integration using MIRA \n  \nPost-processing module, which includes contextual clues and abbreviation identification algorithm. \n  \n\nThe details of each module are described in the following sections. \n\n### Text processing \n  \n#### Text preprocessing \n  \nIn order to prepare the corpus for feature extraction and NER, the following preprocessing steps were applied initially: (i) sentence splitting for splitting the articles or abstracts to sentences, (ii) tokenization for splitting the sentences into individual tokens, (iii) lemmatization to convert the tokens to the basic form of the word, (iv) POS tagging, and (v) chunking. OpenNLP [ ] was used for sentence splitting, tokenization, POS tagging, and chunking. BioLemmatizer [ ] was employed for lemmatization. \n\n\n#### Feature extraction \n  \nAfter preprocessing, for each token, various kinds of features were extracted. These features were based on the local context of each token such as orthographic, morphological, prefix, and suffix features. Orthographic features include the rules of spelling, capitalization, digitization, and punctuation. Morphologic features include identification of the role of words in sentences and their prefixes/suffixes as features. In addition to those above, the features of the preceding two words and the next two words are also used as offset conjunction features. In total, we have used 32 features which are illustrated in Table\u00a0 .   \nExamples of orthographic, morphologic, and prefix-suffix features \n  \n\nIn addition to the above basic features, we have also used the general NER features such as N-gram [ ] and corpus frequency words [ ] to improve the performance of the tagger. For the N-gram feature, we used character N-grams using a sliding window of size 4. The sliding window starts from the beginning to four characters of each token [ ]. \n\nIn our initial tagging, we found some of the tokens tagged as a bio-entity in one sentence but not tagged in some other sentence due to the variation in sentence structure. For example, the gene   COR   was tagged as gene in one occurrence but not in the second in the same abstract (BC2GM000136143). In order to overcome this issue and uniformly tag such tokens, we used corpus frequency words feature only for noun tokens. \n\nFor corpus frequency words, we calculated the total number of times each word or sequences of words occur in the corpus. We took the words or sequences of words with a minimum threshold frequency in the range of (1\u201310) into consideration. \n\nNotation: We write   \u03b8  freq as the count for set of gene/protein names occurring in the corpus. \n\nwhere   cf  (  w  ) denotes the   corpus frequency   of the word   w  , i.e., the total number of times   w   occurs in the corpus. In our experiments, we set   \u03b8  freq\u2009=\u200910. \n\n\n#### Feature selection \n  \nDue to the inclusion of rich set of features including word N-grams, the number of features associated with each token is very large and many of them may not be related to the tokens. Further, redundancy may also occur during the training phase of the samples which can cause performance degradation of the tagger. In order to use only the most informative features for classification task and to discard unrelated features, we employed a principle component analysis (PCA)-based feature selection method [ ]. PCA is the most commonly used dimension reduction method. The vital scheme of PCA is to shrink the dimensionality of a feature set albeit trying to retain the variance present in the original predictor features to the greatest degree possible [ ]. \n\nPCA converts the data to a new dimensional space in such a way that the features with the highest eigenvalue component comes to the first coordinate, next, the eigenvalue component on the second coordinate, and so on. The dimensionality of the data is then shrunk by ignoring the lower eigenvalue components. Thus, PCA provides the most essential directions that can efficiently represent the data which is shortly explained below [ ]. \n\nThe full principal component decomposition of data matrix   X   can be given as T\u2009=\u2009XW where   W   is a 2D matrix whose columns are the eigenvectors of   X   X.   The transformation T\u2009=\u2009XW maps a data vector x(i) from an original space of   p   variables to a new space of   p   variables which are uncorrelated over the dataset. However, not all the principal components are kept during the transformation. Only the first L principal components produced by using the first L-loading vectors that are kept gives the truncated transformation T \u2009=\u2009XW  where the matrix T  now has n rows but only L columns. In other words, PCA learns a linear transformation where the columns of p\u2009\u00d7\u2009L matrix W form an orthogonal basis for the L features that are de-correlated [ ]. Among all the data matrices thus transformed to only L columns, this score matrix maximizes the variance in the original data that has been preserved, while minimizing the total squared reconstruction error. \n\n\n\n### Bidirectional CRF model and integration \n  \n#### CRF \n  \nConditional random fields or CRFs are probabilistic frameworks first introduced by Lafferty et al. for labeling and segmenting sequential data. CRFs are a class of statistical modeling methods often applied in pattern recognition and machine learning problems. Whereas an ordinary classifier predicts a label for a single sample with no regards to \u201cneighboring\u201d samples, a CRF can take the context into account [ ]. The CRF learning [ ] is given by \n\nHowever, in named entity recognition task, CRF computes the probability P (A|B) of given input sequence B\u2009=\u2009b \u2026.b  in the form of a tokenized text and predictable label sequence A\u2009=\u2009a \u2026..a  in the form of a labeled tokenized text. In the context of NER, a labeled text indicates I-inside [Gene/Protein], O-outside, and B-beginning [Gene/Protein]. In order to learn conditional distributions between A  and feature functions from the observable data in NER, it is necessary to calculate the probability of a given label (B, I, O) in sequence A given B. The model assigns a numerical weight to each feature, and then those weights are combined to determine the probability of A . The conditional probability is calculated as follows: \n\nwhere \u03bb  is a parameter to be estimated from training data and indicates the informativeness of the respective feature, Z(B) is a normalization factor and F (A, B)\u2009=\u2009n i\u2009=\u20091 f (A , A , B, i), where each f (A , A , B, i) is either a state function s(A , A , B, i) or a transition function t(A , A , B, i). \n\n\n#### Bidirectional CRF \n  \nIn CRF labeling, we used the special forward and backward parsing techniques to dissect the entities in both directions. In forward parsing, the input tokens are read and tagged in their original order (left to right), while in backward parsing, it is done in the reverse order [ ]. \n\nFurther, we have used the second-order CRF for both forward and backward learning. In a higher order model of \u201cn,\u201d each label depends on a specific number of \u201cn\u201d previous labels. Thus, the probability will consider not only the previous observation and its features but also n-previous observations and features. Though the higher order models provide improved results, the training complexity of higher order models increases exponentially. So we used the second-order CRF which is generally used in NER task [ ]. \n\n\n#### Model integration \n  \nThe common ways to combine the results of bidirectional parsing includes simple set operations such as intersection and union. Usually, intersection will improve the precision and reduce the recall, while using union will improve the recall and reduce the precision [ ]. In general, union and intersection methods failed to improve the performance because they lead to a trade-off between recall and precision. For better model integration, we used MIRA algorithm proposed by Crammer and Singer [ ]. MIRA solves the above\u00a0mentioned trade-off problem, since it combines the forward and backward models by adding the feature weights of both models [ ]. Further, MIRA successfully reduces the training time by exploiting the no update procedure if the instance is classified as correct and also reduces the memory space by following no fixed step size for the update procedure. Hence, we have used MIRA for model combination and compared its performance with union in terms of both results and processing time. \n\nMIRA is an online algorithm for multiclass classification problems. The objective of using MIRA is for building combined model with reduced training time and memory space. It is designed to learn a set of parameters (vector or matrix) by processing all the given training examples one-by-one and updating the parameters according to each training example, so that the current training example is classified correctly with a margin against incorrect classifications at least as large as their loss [ ]. The change of the parameters is kept as small as possible. \n\nThe score of the current correct training    y    must be greater than the score of any other possible   y  \u2032 by at least the loss (number of errors) of that   y  \u2032 in comparison to   y  . \n\n\n\n### Post-processing \n  \nTo further improve the performance of our system, we applied the following three post-processing techniques   \nSurrounding text features \n  \nParenthesis mismatching \n  \nAbbreviation resolution algorithm \n  \n\n#### Surrounding text features \n  \nWe used two different types of surrounding text features: (a) relation word features and (b) connective word features.   \n Relation words  : In biomedical text, existence of some relation keywords (binding, activate, etc.) implies that some protein names might occur [ ]. We compiled around 400 interaction keywords from biomedical texts. If any relation keyword was present in the sentence, then its previous and next words were checked for protein/gene names occurring three or more times in the training set. They were then tagged as gene names if occurring so. \n  \n Connective words  : Similar to the relation word features, here, we checked for the linguistic cue connective words such as \u201cand\u201d and \u201cor\u201d in the sentence. If these words were present in the sentence, then the previous and next words were checked for protein/gene names that occur three or more times in the training set and tagged as gene names. \n  \n\n\n#### Parenthesis mismatching \n  \nOne problem with CRF modeling is that it wrongly identifies the parenthesis, and it leads to parenthesis mismatching problem. For example, in the case of an opening curly brace being tagged and the closing curly brace not tagged, we need to remove the mismatched curly brace. We used left parenthesis and right parenthesis extension method to remove the mismatched parenthesis tagging [ ]. This is shown in the following example 1. \n\nExample 1: \n\n Sentence  : The hepatocyte nuclear factor-3 (HNF-3)/forkhead (fkh) proteins consist of an extensive family of tissue-specific and developmental gene regulators. \n\n Before post-processing  : (|B HNF|I -| I 3|I) |O \n\n After post-processing  : (|O HNF|B -| I 3|I) |O \n\n\n#### Two-tier abbreviation algorithm \n  \nIn abbreviation resolution, we used two popular techniques, namely (a) identification of the missed LF (long form) and SF (short form) [ ] and (b) abbreviation disambiguation [ ].   \nIdentifying the LF (long form) and SF (short form): CRF tagger most of the time tags only long form or short form and misses either one. To tackle this and identify the missed long-form and short-form abbreviations, we used a modified version of the simple abbreviation algorithm which is used in BioC [ ] and named as \u201cextract abbreviation method\u201d. This is shown in the following Example 2. \n\n Example 2:  \n\n Sentence:   Brown adipose tissue (BAT) and brown-like cells in white adipose tissue (WAT) can dissipate energy. \n\n Before Post-processing:   Brown |O adipose |O tissue |O (|B BAT|I) |I \n\n After Post-processing:   Brown |B adipose |I tissue |I (|B BAT|I) |I \n  \nAbbreviation disambiguation: The second technique in two-tier abbreviation algorithm is abbreviation disambiguation. Sometimes two proteins or two genes have the same abbreviation, for example, \u201c  angiotensin converting enzyme (ACE)  \u201dand \u201c  acetylcholinesterase (ACE)  .\u201d  .  In the above example, \u201cACE\u201d denotes both angiotensin converting enzyme (ACE) and acetylchlinesterase (ACE). To overcome this problem, we used the abbreviation disambiguation method word sense disambiguation (WSD) [ ]. In WSD, along with other features, we used domain specific features such as CUI (concept unique identifiers) [ ] and MeSH terms [ ]. WSD identifies all related words in the text which could be mapped to CUI or MeSH terms and disambiguates them to their correct sense of the long form or short form. \n  \n\n\n\n\n## Results and discussions \n  \nBCC-NER training and testing is based on BioCreative II GM corpus which contains 15,000 training sentences and 5000 testing sentences [ ]. While training and testing, we employed our feature set with bidirectional CRF models in both forward and backward directions. Finally, we applied MIRA algorithm to integrate both models to construct the combined model. \n\nThe measures of precision, recall and F-scores were used to evaluate the performance of learning models. The following four different learning were performed   \nCRF\u2009+\u2009Forward parsing\u2009+\u2009post-processing, \n  \nCRF\u2009+\u2009Backward parsing\u2009+\u2009post-processing, \n  \nCRF+ Union (Forward\u2009+\u2009Backward)\u2009+\u2009post-processing and \n  \nCRF\u2009+\u2009Combined model MIRA\u2009+\u2009post-processing. \n  \n\nWe carried out our experiment with the following performance measure criteria using the three equations given below. \n\nWhere, TP refers to the number of proportion of biological entities correctly identified by our hybrid approach, FN refers to the number of proportion of biological entities that the approach failed to identify and FP refers to the number of proportion of biological entities that were incorrectly identified by this approach \n\nTable\u00a0  represents the results of the evaluation in BioCreative II GM test corpus. The results illustrate that our hybrid approach accomplishes improved results. In forward parsing model, it gives an F-score of 86.21 and in bidirectional combined model using union method it improves the F-score to 86.51. The final combined model with MIRA algorithm resulted in a higher F-score of 86.95. The additional advantage of using MIRA is in terms of training time. The training time of both forward and backward model in a Linux sever with 48\u00a0GB RAM was approximately 4\u00a0h. The training time of the combined model using union model was about 7\u00a0h. However, the MIRA combined model completed the training in 5\u00a0h.   \nSystem performance on various models \n  \n\nComparison with other systems: \n\nTable\u00a0  shows the performance comparison of our system with other open source machine learning systems in BioCreative II GM corpus [ ]. While LingPipe, ABNER and BANNER systems uses single CRF model with different feature set for gene/protein mentions. Our system attains better results by using combined MIRA model with rich set of features and post-processing module. The unique features and methodology of our system which contributes performance improvement is discussed in the following section.   \nComparison of our system with other open source systems \n  \n\n\n## Discussions \n  \nIn this paper, we describe our hybrid named entity recognition system named BCC-NER for tagging biomedical entities. BCC-NER integrated all major happenings in current NER task and includes three modules. For example, we have used a rich set of features combining the major 32 basic ones, word N-grams, and corpus frequency words. The state-of-the-art feature selection and extraction algorithm PCA was applied to reduce the high number of features associated with each tokens. \n\nThe latest results on biomedical NER clearly indicate that better performance can be achieved by combining several systems. In these lines, BCC-NER employs bidirectional CRF model combined with MIRA. We are the first one to explore such a combination using MIRA in biomedical NER which gives improvised outcomes than the traditional union and intersection methods. Another important feature that contributes to our hybrid approach is the consideration of contextual clues. In contextual clues, our tagger finds the interaction words and checks if the previous and next words are present in the post-keyword list (pkl). If present, it is tagged as a gene. \n\nFor example, \n\n Before post-processing  : \n\n Early complement components  ,   <gene>C1q</gene> and C4  ,   and IgA secretory piece were absent.  \n\nIn the above example, our contextual clues find the connective word \u201cand,\u201d and then checks for the previous and next words for their presence in pkl. The previous word C1q has been already tagged as gene. So the next word \u201cc4\u201d is then checked for its presence in pkl and tagged as gene after this step. \n\n After post-processing  : \n\n Early complement components  ,   <gene>C1q</gene> and <gene>C4</gene>  ,   and IgA secretory piece were absent.  \n\nFollowing this step, we re-implemented the same step again but with relation words. If any relation keywords are found in the sentence, then both its previous and next words were checked for protein/gene names occurring three or more times in the training set. \n\nFinally, we applied the parenthesis post-processing and two-tier abbreviation algorithm as explained above in the Sections\u00a0  and  . Thus, the issues of parenthesis mismatching and abbreviation disambiguation were subdued. \n\n\n## Conclusions \n  \nWe propose a hybrid named entity tagging approach BCC-NER which on evaluation indicates that the inclusion of rich set of features and utilization of bidirectional CRF combined with MIRA gives best results. Additional performance improvement was achieved by post-processing steps including surrounding text features, parenthesis mismatching and two-tier abbreviation algorithm. \n\nAlthough in BCC-NER we tried to integrate various state-of-the-art methods on existing tools, some aspects can be further explored. We are currently investigating other approaches including domain knowledge information through the use of dictionaries or machine learning-based solutions. In addition, we plan to include methods like co-training, superior abbreviation algorithms and different features to generate improved results. \n\n \n", "metadata": {"pmcid": 5419958, "text_md5": "f7c9bfb12b3b842033ff4fac68f2ed03", "field_positions": {"authors": [0, 91], "journal": [92, 121], "publication_year": [123, 127], "title": [138, 235], "keywords": [249, 400], "abstract": [413, 1576], "body": [1585, 23155]}, "batch": 1, "pmid": 28477208, "doi": "10.1186/s13637-017-0060-6", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5419958", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5419958"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5419958\">5419958</a>", "list_title": "PMC5419958  BCC-NER: bidirectional, contextual clues named entity tagger for gene/protein mention recognition"}
{"text": "Lever, Jake and Altman, Russ and Kim, Jin-Dong\nGenomics Inform, 2020\n\n# Title\n\nExtending TextAE for annotation of non-contiguous entities\n\n# Keywords\n\neditor\ntext annotation\ntext mining\nvisualization\n\n\n# Abstract\n \nNamed entity recognition tools are used to identify mentions of biomedical entities in free text and are essential components of high-quality information retrieval and extraction systems. Without good entity recognition, methods will mislabel searched text and will miss important information or identify spurious text that will frustrate users. Most tools do not capture non-contiguous entities which are separate spans of text that together refer to an entity, e.g., the entity \u201ctype 1 diabetes\u201d in the phrase \u201ctype 1 and type 2 diabetes.\u201d This type is commonly found in biomedical texts, especially in lists, where multiple biomedical entities are named in shortened form to avoid repeating words. Most text annotation systems, that enable users to view and edit entity annotations, do not support non-contiguous entities. Therefore, experts cannot even visualize non-contiguous entities, let alone annotate them to build valuable datasets for machine learning methods. To combat this problem and as part of the BLAH6 hackathon, we extended the TextAE platform to allow visualization and annotation of non-contiguous entities. This enables users to add new subspans to existing entities by selecting additional text. We integrate this new functionality with TextAE\u2019s existing editing functionality to allow easy changes to entity annotation and editing of relation annotations involving non-contiguous entities, with importing and exporting to the PubAnnotation format. Finally, we roughly quantify the problem across the entire accessible biomedical literature to highlight that there are a substantial number of non-contiguous entities that appear in lists that would be missed by most text mining systems. \n \n\n# Body\n \n## Introduction \n  \nInformation extraction and retrieval methods are essential tools to enable scientists to find and read the appropriate papers to enable discoveries. Many of these methods require identifying mentions of specific biomedical entities in the text and make use of named entity recognition (NER) tools for this task. Most entities are represented by a single span of text, e.g., the name of a drug. However, some entities are represented by multiple spans of text that are separated by other words and together identify the entity, for example, the separate words \u201cskin\u201d and \u201ccancer\u201d in \u201cskin and lung cancer.\u201d These are known as non-contiguous, or discontiguous entities.   illustrates several more examples from public text mining resources. It should be noted that non-contiguous entities are different from anaphora or coreference resolution, in which multiple spans refer to the same entity separately and do not work together to identify the entity. \n\nRobust annotation tools that are capable of annotating non-contiguous entities are important so that valuable entity information is not missed. These tools are needed to create corpora with non-contiguous entities that can be used as training data for machine learning-based NER methods and also evaluate all NER methods fairly. The leading NER methods frequently use machine-learning methods such as conditional random fields (CRF) that are incapable of capturing non-contiguous entities without additional postprocessing. Popular tools such as BANNER [ ], tmChem [ ], and DNorm [ ] do not support non-contiguous entities. \n\nMany annotation tools have been developed for manual tagging of entities within a document for the biological domain and other domains. A detailed recent review of the strengths and weaknesses of different methods can be found in Neves and Seva\u2019s study [ ]. To gauge the support for non-contiguous entities, we manually tested the 15 tools selected in that review with an overview shown in  . We were able to run all but one, PDFAnno which displayed an error message that others have reported on Github. We found that only 2 support non-contiguous entities, BRAT [ ], and Catma. Furthermore the AlvisAE [ ] tool that was not included in the review also supports non-contiguous entity annotation. We suggest that more tools need to provide support for non-contiguous entities. \n\nTo that goal, we describe the addition of non-contiguous entity support to TextAE. TextAE is an annotation platform that forms part of the PubAnnotation system for storing and editing text annotations [ ]. It is a Node.js web component that accepts text annotations in PubAnnotation JSON format. The PubAnnotation format currently has support for non-contiguous entities but are converted to an alternative representation when edited using the current release of TextAE, known as the chaining representation. This representation converts an entity that contains multiple spans to multiple entities and links them with a relation with type \u201c_lexicallyChainedTo.\u201d This representation is time-consuming to edit and visualizes poorly.   illustrates the current representation of three non-contiguous entities within a sentence using the chaining method. With the current TextAE interface, it is time-consuming to annotate each entity. Assuming TextAE has been set up with appropriate entity and relation types, for each non-contiguous entity, it requires creating two entities (2 mouse clicks), designating one entity with the type \u201c_FRAGMENT\u201d (2 clicks), switching to the relation mode (1 click), creating a relation between the two entities (2 clicks), changing its type to \u201c_lexicallyChainedTo\u201d (2 clicks) and switching back to Term Mode to continue entity annotation (1 click). Even for a TextAE power user, ten clicks for each non-contiguous entity is time-consuming for a large-scale annotation and produces an unwieldy result which is not visually clear. \n\nIn this paper, we describe our solution of an extension to the existing TextAE annotation platform to provide seamless support for annotating non-contiguous entities. Finally, we provide evidence of the widespread nature of non-contiguous entities in the biomedical literature using a rule-based extraction system to roughly quantify the scale of non-contiguous entities across all PubMed abstracts and accessible PubMed Central full-text papers. \n\n\n## Methods \n  \nTo develop improved methods to capture non-contiguous entities, well-annotated data needs to be prepared and examined that contain non-contiguous entities. We extend the TextAE annotation platform that is part of the PubAnnotation system [ ]. This enables annotation of entities with multiple spans as shown in   with a new subspan mode. The user can select new spans of text that will be added to an existing entity and displayed clearly. \n\nThe first task for implementation was changing the underlying span model in TextAE so that all spans are represented as a list of subspans. We dynamically check the input annotation data (in PubAnnotation format) to check if an entity has a single span, or a list of spans, and convert all entities to contain lists of spans, even for single spans. Previously, spans were rendered using a single HTML span tag around the section with appropriate CSS styling to identify the span as an entity. To visualize the new subspans, we removed the styling from the span class, and create subspans for each part of the span and transferred the stylings to the subspans. TextAE implements an Undo/Redo system so changes were required across the codebase to enable the existing functionality to work with the new underlying data structure and allow entities to be manipulated as before. \n\nA new toggleable button (Add subspan) was added to the toolbar. When this button is toggled, any new spans that are selected by the user are added to the previously selected entity. This requires checking that new subspans were compatible with the structure that is enforced by the HTML page. This means that spans and subspans cannot intersect unless one is contained within the other entirely. This means that in the snippet: \u201cbreast cancer gene\u201d, it would not be possible for \u201cbreast cancer\u201d and \u201ccancer gene\u201d to be tagged as entities. However \u201cbreast cancer\u201d and \u201ccancer\u201d could be tagged as \u201ccancer\u201d is fully contained within \u201cbreast cancer.\u201d We have not come across use-cases where this functionality is currently needed but cannot discount the potential of this limitation.   illustrates the user interface with an example of non-contiguous entities. \n\nTextAE has several user interface shortcuts to enable fast annotation and correction of entities. Users can extend an entity annotation by highlighting text that begins within an entity annotation and goes beyond the entity. Inversely, users can also shorten entity annotations by highlighting text that begins outside an entity span and finishes within an entity, thereby removing the selected text from the entity annotation. We extended this functionality to work for the new subspan system so that it would extend the appropriate first or last subspan in an entity outwards, or would shorten or even remove subspans that are highlighted. We further added user interface tweaks so that when a user selected a subspan, it would select all the subspans for the entity. Finally, we implemented export functionality so that the new subspans would be correctly stored in the PubAnnotation format with a list of spans for those entities with multiple subspans. \n\nThe code for this paper is available at  . \n\n\n## Results \n  \nWe first tested to check that all existing functionality of TextAE remained operational. We confirmed that the new subspan model was able to load data containing non-contiguous entities and annotations with non-contiguous entities could be saved correctly to the PubAnnotation format. Furthermore, we tested that all existing functionality, including relation annotation, worked correctly with non-contiguous entity annotations. \n\nWe quantified the user interactions required to annotate non-contiguous entities. With this new interface, the user needs to annotate a single span (1 click), enable the Add subspan mode (1 click), add a new subspan (1 click), and disable the Add subspan mode (1 click). With only four clicks, we have drastically reduced the user effort, compared to the 10 clicks required previously, and no longer require the user to switch annotation modes within TextAE. Furthermore, the output is visually clearer. This performance is similar to the Catma tool, which requires four clicks to annotate a non-contiguous entity (1 to activate the discontinuous mode, 2 to select the two spans and 1 to select the entity type). And it is marginally easier than BRAT which takes five clicks (2 to annotate the first entity, 1 to edit the entity, 1 to select Add Frag and 1 to select the new span). \n\n\n## Discussion \n  \nWhile non-contiguous entities initially seem like a limited problem for text annotation, we note that two other BLAH 6 hackathon projects requested this functionality during the event: a project working on annotations from the recent BioNLP Shared Task [ ] and a project focused on Medical Device Indication annotation. To understand the scale of this problem, we quantified the number of non-contiguous entities that appear in lists, as shown in the CancerMine examples in  . We focussed on this format as these can be extracted using a modified dictionary matching method. \n\nWe used the PubTator Central resource [ ] as it provides text-level entity annotations of a very large set of biomedical publications and also a rough set of synonyms for different entity types. The annotations provide locations of biomedical entities that may be the final element in a list. For example, the phrase \u201cprostate, skin and lung cancer\u201d would only likely be tagged for \u201clung cancer\u201d in PubTator. We aimed to retrieve other entities from these lists using the set of synonyms from PubTator Central, so that \u201cprostate cancer\u201d and \u201cskin cancer\u201d would be extracted from the example phrase. We used a simple rule-based system that identified candidate lists by searching for tagged biomedical entities that follow the word \u201cand.\u201d We then searched the preceding words in the candidate list and attempted word substitutions with the final term to find terms that were in the lexicon. \n\nAcross the 30,044,935 abstracts and 2,485,641 full-text papers that were minable, we find 3,269,632 potential mentions of non-contiguous entities in the example list format. We manually reviewed 100 of them to understand the error profile and found that 42% were true positives. The main errors were caused by spurious mistakes in the lexicon and a more conservative lexicon would likely improve precision but may affect overall recall. Nevertheless, this initial result suggests that many biomedical entities are described in the list form that would be missed with most current methods. While there are considerable false-positive dues to the dictionary matching method, we would argue that this will only be a fraction of non-contiguous entities across the biomedical literature as we examine only one type of linguistic structure that could contain non-contiguous entities. \n\n shows an overview of the results from the literature analysis. Lists appear more in full-text papers than in abstracts even when taking account of the substantially larger number of abstracts than full-text articles in the corpus. They can even appear in the article title. Furthermore, disease has substantially more non-contiguous entities, likely due to the larger number of multiple word terms in that lexicon (837,390 compared to 103,427 for genes for example). \n\nThis analysis strongly suggests that non-contiguous are a substantial problem in biomedical text mining and that methods that ignore them will be missing large amounts of potential extracted information. We hope our contribution to an annotation tool that could help visualize and annotate these problematic entities may take a step towards new methods to identify them. \n\n \n", "metadata": {"pmcid": 7362949, "text_md5": "e3b99bb9a434d5594fa71cc678564ca0", "field_positions": {"authors": [0, 46], "journal": [47, 62], "publication_year": [64, 68], "title": [79, 137], "keywords": [151, 200], "abstract": [213, 1929], "body": [1938, 14068]}, "batch": 1, "pmid": 32634869, "doi": "10.5808/GI.2020.18.2.e15", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7362949", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7362949"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7362949\">7362949</a>", "list_title": "PMC7362949  Extending TextAE for annotation of non-contiguous entities"}
{"text": "Larmande, Pierre and Do, Huy and Wang, Yue\nGenomics Inform, 2019\n\n# Title\n\nOryzaGP: rice gene and protein dataset for named-entity recognition\n\n# Keywords\n\nnamed-entity recognition\nnatural language processing\nOryza sativa\nplant molecular biology\nrice\ntext mining\n\n\n# Abstract\n \nText mining has become an important research method in biology, with its original purpose to extract biological entities, such as genes, proteins and phenotypic traits, to extend knowledge from scientific papers. However, few thorough studies on text mining and application development, for plant molecular biology data, have been performed, especially for rice, resulting in a lack of datasets available to solve named-entity recognition tasks for this species. Since there are rare benchmarks available for rice, we faced various difficulties in exploiting advanced machine learning methods for accurate analysis of the rice literature. To evaluate several approaches to automatically extract information from gene/protein entities, we built a new dataset for rice as a benchmark. This dataset is composed of a set of titles and abstracts, extracted from scientific papers focusing on the rice species, and is downloaded from PubMed. During the 5th Biomedical Linked Annotation Hackathon, a portion of the dataset was uploaded to PubAnnotation for sharing. Our ultimate goal is to offer a shared task of rice gene/protein name recognition through the BioNLP Open Shared Tasks framework using the dataset, to facilitate an open comparison and evaluation of different approaches to the task.  \n \n\n# Body\n \n Availability:   A part of the OryzaGP dataset is publicly available through PubAnnotation ( ). The full dataset will become available soon after a portion to be a hidden test data set is determined. \n\n## Introduction \n  \nThe last few decades have witnessed a massive explosion of information in the life sciences. However, an important proportion of this information, relevant to this field, is not available from databases, but is instead present in unstructured scientific documents, such as journal articles, reviews, abstracts, and reports. Agronomy is an overarching field that is comprised of diverse domains such as genetics, plant molecular biology, ecology and soil science [ ]. Despite advancements in information technology, scientific advancements in agronomy are still commonly based on text. To effectively develop applications to improve crop production through sustainable methods, however, it is important to overlap research findings from these various subdomains, as they are highly interconnected. However, the collection of content is growing continuously, and the information currently available is unstructured text. Using these resources more efficiently, and taking advantage of associated cross-disciplinary research opportunities, poses a major challenge to both biologists and information technologists. One important subtask of information extraction is to identify biological entities, and their classifications, an endeavor known as named-entity recognition (NER). \n\nIdentifying biological entities, from text, is not trivial. Despite the existence of many available approaches to handle this problem in general, and in biomedical domains in particular, few comprehensive studies have been implemented for plants, especially rice. Moreover, we found that rare benchmarks are available for many plant species, but none for rice. Thus, taken together, we faced various difficulties to exploit advanced machine learning methods, for the accurate analysis of rice. \n\n\n## Objective \n  \nOn the large scale, we are currently building a Resource Description Framework (RDF) knowledge base termed Agronomics-Linked Data (AgroLD [ ],  ). This knowledge base is designed to integrate data from various public, plant-centric databases such as Gramene [ ], Oryzabase [ ], and TAIR [ ], to name a few. The aim of the AgroLD project is to provide an integrated portal for both bioinformatics and domain experts, to exploit a homogenized data model for filling knowledge gaps. Using this landscape, we aim to extract relevant information from the literature, to enrich the content of integrated datasets. \n\nDue to the scope of the project, we exploited information from the Oryzabase database to build a dataset aimed to recognize named text entities such as rice genes and proteins. Our main purpose was to solve NER of rice biological entities, to find the best approach. By sharing this dataset on the PubAnnotation platform and be available at the   BioNLP Open Shared Tasks   (BioNLP-OST,  ), we invited participants to implement their own methods to solve NER tasks for this dataset. Furthermore, to evaluate the performances, we compared their approaches, implemented during the task, with our method [ ], implemented before the hackathon. \n\n\n## Contribution \n  \nIn this project, we used data from Oryzabase ( ), a rice comprehensive database for Oryza sativa species published online since 2,000 by Japanese researchers. The latest version of Oryzabase contains 21,739 of rice genes, collected from 44,837 distinct scientific articles. Consequently, we used this information to create the basis of the OryzaGP dataset. Then we used PubMed as a resource to collect the raw data that was later preprocessed to compose the dataset, and developed a custom script implementing the BioPython library to query and retrieve the specific abstracts from PubMed. However, a number of scientific articles were not available in the PubMed database, due to some historical issues and lack of published resources. Due to the limited access of some resources, 10,400 articles were processed after filtering. The detailed raw data is shown in  . \n\nBy focusing on the entities of the rice genome, we used the Oryzabase gene list as the ground truth to build up our dataset by keyword matching terms. The first step to preprocessing the data was filtering to remove special characters from the raw data. In fact, due to the number of articles used, the time range of articles was also wide, in that several articles were published in previous decades. To handle the problems of OCR-errors (which appear in the scanned text), we manually removed all the null and nonsense characters in utf-16 in the raw text. All the work was processed by our scripts and then after first step preprocessing. Moreover, we added part-of-speech (POS) tags for each word, to define its type with the aim, to ensure the accuracy of the identification of entities. The POS tagging process is supported by the Natural Language ToolKit (NLTK). To tokenize data, each word was considered a token, given in the following lines; one token per line, and included three tabs: the word itself, the POS tag, and the entity type ( ). To minimize the errors of inaccurate tags assignation when running the script, preprocessed data were checked manually, based on the existing resources (Oryzabase gene list, etc.) \n\nDuring the 5th Biomedical Linked Annotation Hackathon (BLAH5,  ), a portion of the dataset (29,098 annotation instances made to 6,107 abstracts) was uploaded to the PubAnnotation repository ( ), which uses JSON (JavaScript Object Notation) as its default format, to store annotations. Resultantly, the dataset is accessible through or downloadable from PubAnnotation. Sharing it through PubAnnotation also means that the dataset can be compared to annotations from other projects if they share the same documents [ ]. \n\n\n## Future Work \n  \nOur ultimate goal of sharing the dataset is to offer a shared task of rice gene/protein NER, through the BioNLP Open Shared Tasks (BioNLP-OST) framework, to facilitate open comparison and evaluation of various approaches to the task. Toward the goal, we will further upload remaining annotation data, while keeping some portion of it hidden for test dataset. \n\n \n", "metadata": {"pmcid": 6808627, "text_md5": "9c976941b338fade824f7f5081ecf4ba", "field_positions": {"authors": [0, 42], "journal": [43, 58], "publication_year": [60, 64], "title": [75, 142], "keywords": [156, 263], "abstract": [276, 1573], "body": [1582, 7875]}, "batch": 1, "pmid": 31307132, "doi": "10.5808/GI.2019.17.2.e17", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6808627", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6808627"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6808627\">6808627</a>", "list_title": "PMC6808627  OryzaGP: rice gene and protein dataset for named-entity recognition"}
{"text": "Yoon, Wonjin and So, Chan Ho and Lee, Jinhyuk and Kang, Jaewoo\nBMC Bioinformatics, 2019\n\n# Title\n\nCollaboNet: collaboration of deep neural networks for biomedical named entity recognition\n\n# Keywords\n\nNER\nDeep learning\nNamed entity recognition\nText mining\n\n\n# Abstract\n \n## Background \n  \nFinding biomedical named entities is one of the most essential tasks in biomedical text mining. Recently, deep learning-based approaches have been applied to biomedical named entity recognition (BioNER) and showed promising results. However, as deep learning approaches need an abundant amount of training data, a lack of data can hinder performance. BioNER datasets are scarce resources and each dataset covers only a small subset of entity types. Furthermore, many bio entities are polysemous, which is one of the major obstacles in named entity recognition. \n\n\n## Results \n  \nTo address the lack of data and the entity type misclassification problem, we propose CollaboNet which utilizes a combination of multiple NER models. In CollaboNet, models trained on a different dataset are connected to each other so that a target model obtains information from other collaborator models to reduce false positives. Every model is an expert on their target entity type and takes turns serving as a target and a collaborator model during training time. The experimental results show that CollaboNet can be used to greatly reduce the number of false positives and misclassified entities including polysemous words. CollaboNet achieved state-of-the-art performance in terms of precision, recall and F1 score. \n\n\n## Conclusions \n  \nWe demonstrated the benefits of combining multiple models for BioNER. Our model has successfully reduced the number of misclassified entities and improved the performance by leveraging multiple datasets annotated for different entity types. Given the state-of-the-art performance of our model, we believe that CollaboNet can improve the accuracy of downstream biomedical text mining applications such as bio-entity relation extraction. \n\n \n\n# Body\n \n## Background \n  \nThe amount of biomedical text continues to increase rapidly. There were 4.7 million full-text online accessible articles in PubMed Central [ ] in 2017. One of the obstacles in utilizing biomedical text data is that it is too large for a human to read or even search for needed information. This has led to the demand for automated extraction of valuable information. Text mining can be used to turn the time-consuming task into a fully automated job [ \u2013 ]. \n\nNamed Entity Recognition (NER) is the computerized procedure of recognizing and labeling entities in given texts. In the biomedical domain, typical entity types include disease, chemical, gene and protein. \n\nBiomedical named entity recognition (BioNER) is an essential building block of many downstream text mining applications such as extracting drug-drug interactions [ ] and disease-treatment relations [ ]. BioNER is also used when building a sophisticated biomedical entity search tool [ ] that enables users to pose complex queries to search for bio-entities. \n\nNER in biomedical text mining is focused mainly on dictionary-, rule-, and machined learning-based approaches [ \u2013 ]. Dictionary based systems have a simple and intuitive structure but they cannot handle unseen entities or polysemous words, resulting in low recall [ ,  ]. Moreover, building and maintaining a comprehensive and up-to-date dictionary involves a considerable amount of manual work. The rule based approach is more scalable, but it needs hand crafted feature sets to fit a model to a dataset [ ,  ]. These rule and dictionary-based approaches can achieve high precision [ ] but can produce incorrect predictions when a new word, which is not in the training data, appears in a sentence (out-of-vocabulary problem). This out-of-vocabulary problem occurs frequently especially in the biomedical domain, as it is common for a new biomedical term, such as a new drug name, to be registered in this domain. \n\nRecently, studies have demonstrated the effectiveness of deep learning based methods. Sahu and Anand [ ] demonstrated the efficiency of Recurrent Neural Network (RNN) for NER in biomedical text. The model by Sahu and Anand is composed of a bidirectional Long Short-Term Memory Network (BiLSTM) and Conditional Random Field (CRF). Sahu and Anand [ ] also used character level word embeddings but could not demonstrate their benefits. Habibi et al. [ ] combined the BiLSTM-CRF model implementation of Lample et al. [ ] and the word embeddings of Pyysalo et al. [ ]. Habibi et al. [ ] utilized character level word embeddings to capture characteristics, such as orthographic features, of bio-medical entities and achieved state-of-the-art performance, demonstrating the effectiveness of character level word embeddings in BioNER. \n\nAlthough these models showed some promising results, NER is still a very challenging task in the biomedical domain for the following reasons. First, a limited amount of training data is available for BioNER tasks. Gold-standard datasets contain annotations of one or two entity types. For example, the NCBI corpus [ ] includes annotations of diseases but not of other types of entities such as genes and proteins. On the other hand, the JNLPBA corpus [ ] contains annotations of only genes and proteins. Therefore, the data for each entity type comprises only a small portion of the total amount of annotated data. \n\nMulti-task learning (MTL) is a method for training a single model for multiple tasks at the same time. MTL can leverage different datasets that are collected for different but related tasks [ ]. Although extracting genes is different from extracting chemicals, both tasks require learning some common features that can help understand the linguistic expressions of biomedical texts. Crichton et al. [ ] developed an MTL model that was trained on various source datasets containing annotations of different subsets of entity types. An MTL model by Wang et al. [ ] achieved performance comparable to that of the state-of-the-art single task NER models. Inspired by the previous studies, we propose CollaboNet which uses the collaboration of multiple models. Unlike the conventional MTL methods which use only a single static model, CollaboNet is composed of multiple models trained on different datasets for different tasks. Each model in CollaboNet is trained on dataset annotated on a specific type of entity and becomes an expert on their own entity type. \n\nDespite the high recall obtained by the MTL based models, the precision of these models is relatively low. Since MTL based models are trained on multiple types of entities and larger training data, they have a broader coverage of various biomedical entities, which naturally results in high recall. On the other hand, as the MTL models are trained on combinations of different entity types, they tend to have difficulty in differentiating among entity types, resulting in lower precision. \n\nAnother reason NER is difficult in the biomedical domain is that an entity could be labeled as different entity types depending on its textual context. In our experiments, we observed that many incorrect predictions were a result of the polysemy problem, in which a word, for example, can be used as both a gene and disease name. Models designed to predict disease entities misidentify some genes as diseases. This misidentification of entity types increases the false positive rate. For instance, BiLSTM-CRF based models for disease entities mistakenly label the gene name   \u201cBRCA1\u201d   as a disease entity because there are disease names such as   \u201cBRCA1 abnormalities\u201d   or   \u201cBrca1-deficient\u201d   in the training set. Besides, the training set that annotates   \u201cVHL\u201d   (Von Hippel-Lindau disease) as a disease entity confuses the models because VHL is also used as a gene name, since the mutation of this gene causes VHL disease. \n\nTo solve the false positive problems due to polysemous words, CollaboNet aggregates the results of collaborator models, and uses them as an additional input to the target model. Consider the case of predicting the disease entity VHL utilizing the outputs of gene and chemical models. Once a gene model predicts VHL as a gene, the gene model informs a disease model that VHL is a gene entity so that the disease model will not predict VHL as a disease. In CollaboNet, each model is individually trained on an entity type and then further trained on the outputs of other models that are trained on the other entity types. The models in CollaboNet take turns in being the target and collaborator models during training. Consequently, each model is an expert in its own domain and helps improve the accuracy by leveraging the multi-domain information from the other models. \n\n\n## Methods \n  \nIn the following section, we first discuss a BiLSTM-CRF model for biomedical named entity recognition. The overall structure of the BiLSTM-CRF model is illustrated in Fig.\u00a0 . Next, we introduce the structure of CollaboNet, which is comprised of a set of BiLSTM-CRF models as shown in Fig.\u00a0 .\n   \nCharacter level word embedding using CNN and an overview of Bidirectional LSTM with Conditional Random Field (BiLSTM-CRF). Single-task model structure \n    \nStructure of CollaboNet. Arrows show the flow of information when target model   M   is training. The models in CollaboNet take turns in being the target model \n  \n\n### Problem Definition \n  \nNamed entity recognition involves annotating words in a sentence as named entities. More formally, given an input sequence   S  =[  w  ,  w  ,...,  w  ], we predict corresponding labels   Y  =[  y  ,  y  ,...,  y  ]. We use the BIOES scheme [ ] for representing   y  , where B stands for Beginning, I for Inside, O for Out, E for End, and S for Single. \n\n\n### Embedding layer \n  \n#### Word Embedding (WE) \n  \nWord embedding is an effective way of representing words. As word embeddings capture semantic and syntactic meanings of words, they have been widely used in various natural language processing tasks including named entity recognition. The experiment of Habibi et al. [ ] showed that word embeddings trained on biomedical corpora notably improved the performance of BioNER models. Pyysalo et al. [ ] were the first to suggest training word embeddings on biomedical corpora from PubMed, PubMed Central (PMC), and Wikipedia. The results of Pyysalo et al. [ ] and Habibi et al. [ ] suggest that using word embeddings trained on biomedical corpora is essential for BioNER. We also use the trained word embeddings provided by Pyysalo et al. [ ]. For each word   w   in a sequence   S  , we denote a word represented by a word embedding as   where   d   is a dimension of the word embedding. \n\n\n#### Character Level Word Embedding (CLWE) \n  \nTo give our model character level morphological information (  e.g.,   \u2018  -ase  \u2019 is common in protein entities), we also leverage the character level information of each word. We build character level word embeddings (CLWEs) using a convolution neural network (CNN), similar to the work of Santos and Zadrozny [ ]. Given a word   w  , composed of   M   number of characters, we represent   where   is a randomly initialized character embedding for each unique character. Note that unlike the word embeddings trained on separate biomedical corpora, character embeddings are learned from only the BioNER task. For the CNN, padding of the proper size ((  k  \u22121)/2) according to window size   k   should be attached before and after each word. We obtain a window vector   by simply concatenating the character embeddings of   with the character embeddings of (  k  \u22121)/2 characters on both sides: \n \n\nFrom the window vector  , we perform a convolution operation as follows: \n \n\nwhere   and   denote a trainable filter and bias, respectively. We obtain the element-wise maximum values, and the output is a character level word embedding denoted as  . We concatenate the character level word embedding with the word embedding trained on biomedical corpora as   to utilize both representations in our model. \n\n\n\n### Long Short-Term Memory (LSTM) \n  \nA Recurrent Neural Network (RNN) is a neural network that effectively handles variable-length inputs. RNNs have proven to be useful in various natural language processing tasks including language modeling, speech recognition and machine translation [ \u2013 ]. Long Short-Term Memory (LSTM) [ ] is one of the most frequently used variants of recurrent neural networks. Our model uses the LSTM architecture from Graves et al. [ ]. Given the outputs of an embedding layer  , the hidden states of LSTM are calculated as follows: \n \n\n\n\n\n\n\n\n\n\nwhere   \u03c3   and tanh denote a logistic sigmoid function and a hyperbolic tangent function, respectively, and \u2299 is an element-wise product. We use a forward LSTM that extracts the representations of inputs in the forward direction, and we use a backward LSTM that represents the inputs in the backward direction. \n\nWe concatenate the two states coming from the forward LSTM and the backward LSTM to form the hidden states of the bi-directional LSTM (BiLSTM). BiLSTM, proposed by Schuster and Paliwal [ ], was extensively used in various sequence encoding tasks. We obtain a set of hidden states   where   and   are hidden states of forward and backward LSTMs, respectively, at a time step   t  . \n\n\n### Bidirectional LSTM with Conditional Random Field (BiLSTM-CRF) \n  \nWhile BiLSTM handles long term dependency problems as well as backward dependency issues, modeling dependencies among adjacent output tags helps improve the performance of the sequence labeling models [ ]. We applied a Conditional Random Field (CRF) to the output layer of the BiLSTM to capture these dependencies. \n\nFirst, we compute the probability of each label given the sequence   S  =[  w  ,...,  w  ] as follows: \n \n\n\n\nwhere   and   are parameters of the fully connected layer for BIOES tags, and the softmax(\u00b7) function computes the probability of each tag. Based on the probability   p   and the CRF layer, our training objective to minimize is defined as follows: \n \n\n\n\n\n\nwhere   L   is the cross entropy loss for the label   y  , and   L   is the negative sentence-level log likelihood. The score of a tag is the summation of the transition score   and the emission score from our LSTM   at time step   t  . \n\nAt test time, we use Viterbi decoding to find the most probable sequence given the outputs of the BiLSTM-CRF model. \n\n\n### CollaboNet \n  \nCollaboNet, our novel NER model, is composed of multiple BiLSTM-CRF models (Fig.\u00a0 ), and following the terminology of [ ], we call each BiLSTM-CRF model a single-task model (STM). In CollaboNet, each STM is trained on a specific dataset and each STM is regarded as an expert on a particular entity type. These experts help each other since the knowledge of each expert is transferred to all the other experts. Training CollaboNet consists of phases and in each phase, except for the first preparation phase, only the target STM is trained on a single dataset for one epoch while the other STMs are not trained but only used to generate input for the target STM which is trained. \n\nMore formally, let us denote a set of datasets as   D  , and a single-task model as  , which is trained on the   k  -th dataset in phase   P  . In the preparation phase (  P  ) of CollaboNet, each STM is trained independently on a corresponding dataset until the performance of each model converges. \n\nNote that an STM in the preparation phase   is the same as a single BiLSTM-CRF model. In the preparation phase, we assume that each model   has obtained the maximum amount of knowledge about the   k  -th dataset. \n\nIn the subsequent phases   P  , where   n  \u22651, we select an STM   which is an expert on the dataset   d  . We refer to the target STM   as the   target model  , and the remaining STMs as the   collaborator models  . To train the target model  , we use inputs from the target dataset   d   and BiLSTM outputs from collaborator models  . We train each STM on its dataset for one epoch, and change the target STM   as follows: \n \n\n\n\nwhere [\u00b7;\u00b7] denotes concatenation and \u2298 denotes an aggregation operation such as max pooling or concatenation. We used weighted max pooling for the aggregation operation.   S   is the input sequences of   d  -th dataset, and   is output   h  , defined by Eq.  . When aggregating the results of collaborator models, we multiply each of the results by a weight   \u03b1  , which is a trainable parameter. The results are used to train the model  . Using the outputs obtained by Eq.  , we train   for one epoch, and it becomes   in the next phase. The CRF layer is attached to the final output of  . Once we iterate all the target datasets   d  \u2208  D  , the next phase begins. \n\nDuring the training phase   P   for   d  , the target STM, which is composed of the BiLSTM layer and the CRF layer, and weights   \u03b1  {  k  |  k  \u2260  d  ,  k  \u2208  D  } are trained. Parameters of the other STMs are not trained but the STMs generate only inferences on dataset   d   in the training phase   P  . For example, when the disease dataset is the target dataset, the BiLSTM of the other STMs produces inferences about the other entity types for the disease dataset. More specifically, inferences about genes for the disease dataset   which has rich information on gene entities, will benefit the disease STM. \n\n\n\n## Experiments \n  \n### Datasets \n  \nWe used 5 datasets (BC2GM [ ], BC4CHEMD [ ], BC5CDR [ \u2013 ], JNLPBA [ ], NCBI [ ]), all of which were collected by Crichton et al. [ ] (Table\u00a0 ). Each of the 5 datasets were constructed from MEDLINE abstracts, and we used the BIOES notation format for named entity labels [ ]. Each dataset focuses on one of the three biomedical entity types: disease, chemical, and gene/protein. We did not use cell-type entity tags from JNLPBA for the entity types.\n   \nDescriptions of datasets \n  \n\nAll the datasets are comprised of pairs of input sentences and biomedical entity labels for the sentences. While the JNLPBA dataset has only training and test sets, the other four datasets contain training, development and test sets. For JNLPBA, we used part of its training set as its development set which is the same size as its test set. Also, we found that the JNLPBA dataset from Crichton et al. [ ] contained sentences that were incorrectly split. So we preprocessed the original dataset by Kim et al. [ ] with a more accurate sentence separation. \n\nThe BC5CDR dataset has the sub-datasets BC5CDR-chem, BC5CDR-disease and BC5CDR-both, and they contain chemical entity types, disease entity types, and both entity types, respectively. We reported the performance on BC5CDR-chem and BC5CDR-disease. We have a total of six datasets: BC2GM, BC4CHEMD, BC5CDR-chem, BC5CDR-disease, JNLPBA, and NCBI. \n\n\n### Metric \n  \nFor the evaluation of the named entity recognition task, true positives are counted from exact matches between predicted entity spans and ground truth spans based on the BIOES notation. \n\nWe also designed and applied a simple post-processing step that corrects invalid BIOES sequences. This simple step improved precision by about 0.1 to 0.5%, and thus boosted the F1 score by about 0.04 to 0.3%. \n\nPrecision, recall and F1 scores were used to evaluate the models. \n   \nM = total number of predicted entities in the sequence. \n  \nN = total number of ground truth entities in the sequence. \n  \nC = total number of correct entities. \n  \n\n\n\n\n### Settings and hyperparameters \n  \nWe used the 200 dimensional word embedding (WE) by Pyysalo et al. [ ] which was trained on PubMed, PubMed Central (PMC) and Wikipedia text, and it contains about 5 million words. Word2vec [ ] was used to train the word embedding. For character level word embedding (CLWE), we used window sizes of 3, 5, and 7. \n\nWe used AdaGrad optimizer [ ] with an initial learning rate of 0.01 which was exponentially decayed for each epoch by 0.95. The dimension of the character embedding (  d  ) was 30 and dimension of the character level word embedding (  d  ) was 200*3. We used 300 hidden units for both forward and backward LSTMs. Weights for aggregating the results of collaborator models were uniformly initialized with 1. We applied dropout [ ] to two parts of CollaboNet: output of CLWE (0.5) and output of BiLSTM (0.3). The mini-batch size for our experiment was 10. \n\nMost of our hyperparameter settings are similar to those of Wang et al. [ ]. Only a few settings such as the dropout rates were different from the hyperparameters of Wang. We tuned these hyperparameters using validation sets. \n\nThe preparation phase   P   for 6 datasets takes approximately 900 min, which is the same amount of time it takes to train 6 single-task models. The rest of the phases   P  ,  n  \u22651 require 3000 min for complete training. If we exclude BC4CHEMD, the largest dataset, then the training time for   P   is reduced to 1500 min, which is half the time required for the remainder phases. Experiments were conducted on a 10-core CPU (Intel Xeon E5-260 v4 CPU 2.2 GHz) with one graphics processing unit (NVIDIA Titan Xp). Our code is written in TensorFlow 1.7 (GPU enabled version) for Python 2.7. \n\n\n\n## Results \n  \nThe experimental results of the baseline models and CollaboNet are provided in Tables\u00a0  and  , respectively. Table\u00a0  shows the results of the single-task models (STMs) where Table\u00a0  shows the comparison between the existing state-of-the-art multi-task learning model (MTM) and our CollaboNet.\n   \nPerformances of single-task models \n  \nOur STM achieved the best performance on 3 datasets among 6. Scores in the asterisked (*) cells are obtained in the experiments that we conducted; these scores are not reported in the original papers. The best scores from these experiments are in bold \n    \nPerformance of CollaboNet and the Multi-Task Model by Wang et al. [ ] \n  \nScores in the asterisked (*) cells are obtained in the experiments that we conducted; these scores are not reported in the original papers. The best scores from these experiments are in bold \n  \n\nSince Wang et al. [ ] used BC5CDR-both for their experiments, we reran their models on BC5CDR-chem and BC5CDR-disease for a fair comparison with other models. The rerun scores are denoted with asterisks. We conducted 10 experiments with 10 different random initializations on our STM. We take arithmetic mean over the 6 datasets to compare the overall performance of each model. \n\n### Performance of single-task models \n  \nTable\u00a0  shows the results of the STMs of Habibi et al. [ ] and Wang et al. [ ] (baseline STMs), and our STM on the 6 datasets. While the baseline STMs applied BiLSTM for the Character Level Word Embedding (CLWE) layer [ ,  ], our STM used Convolution Neural Network (CNN) for the CLWE layer. \n\nOn average, our STM outperforms the baseline STMs in terms of precision, recall and F1 score. Although, Sahu and Anand [ ] tried to improve the performance of NER models with CNN based CLWE layer, they have failed to do so. In our experiments, however, our STM outperforms other baseline STMs, demonstrating the effectiveness of STM with CNN based CLWE layer. \n\n\n### Performance of CollaboNet \n  \nComparing Tables\u00a0  and  , CollaboNet achieves higher precision and F1 score than most STM models on all datasets. On average, CollaboNet has improved both precision and recall. CollaboNet also outperforms the multi-task model (MTM) from Wang et al. [ ] on 4 out of 6 datasets (Table\u00a0 ). While multi-task learning has improved performance in previous studies [ ], using CollaboNet, which consists of expert models trained for each entity type, could further improve biomedical named entity recognition performance. \n\n\n\n## Discussion \n  \nCompared to baseline models, CollaboNet achieves higher performance on macro average (Tables\u00a0  and  ). The increase in precision is supportive when considering the practical use of the bioNER systems. In a number of biomedical text mining systems, important information tends to be repeated in a large size text corpus. Therefore, missing a few entities may not hinder the performance of an entire system, as this can be compensated elsewhere. However, incorrect information and the propagation of errors can effect the entire system. \n\nIn Table\u00a0 , we report the error types of our STM and CollaboNet. We define   bio-entity   error   as recognizing different types of biomedical entities as target entity types. For instance, recognizing \u2018  VHL  \u2019 as a gene when it was used as a disease in a sentence is a bio-entity error. Note that a bio-entity error could occur when an entity is a polysemous word (e.g. VHL), or comprised of multiple words (e.g. BRCA1 deficient), and thus correcting bio-entity errors requires contextual information or supervision of other entity type models. The error analysis was conducted on 4334 errors of our STM and 3966 errors of CollaboNet on 5 datasets (BC2GM, BC5CDR-chem, BC5CDR-disease, JNLPBA, NCBI). Error analysis was conducted on models which showed best performance in our experiments.\n   \nThe number of bio-entity type errors, the total number of errors, and the ratio of bio-entity errors to the total numbers of errors for each model prediction \n  \nNegative values at the difference tab indicate that CollaboNet reduced the number of false positives, especially false biomedical entities \n  \n\nThe error analysis of our STM, which is a single BiLSTM-CRF model, shows that the majority of errors are classified as bio-entity errors which comprise up to 49.3% of the total errors in JNLPBA. According to the error analysis of our STM model, bio-entity errors constitute 1333 errors out of 4334 errors, comprising 30.8% of all the errors. Although bio-entity error was not the most common error type, the importance of bio-entity error is much greater that of other errors such as span error which was the most common error type, constituting 38% of incorrect errors. While most span errors can be easily fixed by non-experts, bio-entity errors are difficult to detect and fix, even for biomedical researchers. Also, for biomedical text mining tasks such as drug-drug interaction (DDI) extraction, span errors of an NER system have a minor effect on DDI results but bio-entity errors could lead to completely different results. \n\nThe performance improvement of CollaboNet over STM may not seem significant when considering the increased complexity of CollaboNet\u2019s structure. We found by error analysis that CollaboNet had an increased number of span errors. As our metric is based on the exact match evaluation, consistent annotation of the ground truth dataset is important for reducing span errors which are caused by modifiers. For instance, in the phrase \u201cacute adult renal failure,\u201d \u201cadult renal failure\u201d may be labeled as an entity in some datasets. In this case, predicting \u201cacute adult renal failure\u201d or \u201crenal failure\u201d as an entity will be counted as a false negative and a false positive. On the other hand, some other datasets may include the modifier \u201cacute\u201d in an entity, considering \u201cacute adult renal failure\u201d as the only true prediction. Therefore, unlike STM, CollaboNet uses various datasets that have been annotated differently. Even though CollaboNet outperforms STM, its results may be lower due to this inconsistency in annotation. \n\nIn CollaboNet, each expert model is trained on a single entity type dataset, and their training inputs are a concatenation of word embeddings and outputs of the other expert models. We expect that the other expert models will transfer knowledge on their respective entity to the target model, and thus improve the bio-entity type error problem by collaboration. As Table\u00a0  shows, CollaboNet performs better than our STM in detecting polysemy and other entity types. Among 3966 errors from CollaboNet, 736 errors are bio-entity errors, comprising 18.6% of all the errors. \n\n### Case study \n  \nWe sampled the predictions of CollaboNet and those of our STM (single-task model) to further understand the strengths of CollaboNet in Table\u00a0 .\n   \nCase study \n  \nThis table contains sentences that were incorrectly predicted by of our STM but were correctly predicted by CollaboNet. The predicted labels or the ground truth labels are underlined \n  \n\nThe first example from chemical dataset in Table\u00a0  shows our expected result from CollaboNet. Our STM annotates   antilymphocyte globulin   as a chemical entity. However, it is clear that the entity is not a chemical but a type of globulin which is a protein. The second example sentence from the chemical dataset is about an   ACE / ARB   entity. Again, our STM misidentifies the entity as a chemical entity. On the other hand, in CollaboNet, the target model (chemical model) obtains knowledge from one of the collaborator models (the gene/protein model) to avoid mistakenly recognizing the entity as a chemical entity. As   globulin   or   ACE   entities appear in the gene/protein dataset, the chemical model obtains information from the gene/protein model. \n\nIn the disease dataset, the first example shows a multi-word entity in parentheses. As a gene model can pass syntactic and semantic information about a word   e.g., mutated   and its surrounding words to a disease model, CollaboNet can abstain from predicting   A-T, mutated   as the disease entity, which our STM model failed to do. The second example in the disease dataset is on   cardiac troponin T  . Since   cardiac + noun   in biomedical text can be easily considered as a disease name, our STM misidentified this word as a disease entity. However, with the help of a gene model, CollaboNet did not mark it as a disease entity. \n\nThe gene/protein entity type further demonstrates the effectiveness of CollaboNet in reducing bio-entity type errors. Two example sentences contain abbreviations, which are one of the distinct characteristics of gene entities.   LMB   and   cHD   are incorrectly predicted as gene/protein entities by our STM, since lots of gene/protein entities are abbreviations. However, the target model (gene/protein model) in CollaboNet can obtain information on   leptomycin   and   disease   from the chemical and disease models, respectively. With the help of information from collaborator models, CollaboNet can effectively increase the precision of other entity type models. \n\nIn addition, we found some labels in the ground truth set, which we believe are incorrect. Tsai et al. [ ] also reported that the inconsistent annotations in the JNLPBA corpus limit the NER system. We report our findings in Table\u00a0 .\n   \nCase study \n  \nThis table shows the questionable answers from the ground truth datasets. Our model achieves better performance in detecting entities in these example sentences. The predicted labels or the ground truth labels are underlined \n  \n\nIn the first row of Table\u00a0 , the gene/protein entity   osteopontin   was not marked in the ground truth labels, whereas our network correctly predicted it as a gene entity. The second row also displays questionable results of the ground truth labels. Although   lg   and   bcl-6  , which are abbreviations of   Immunoglobulin   and   B-cell lymphoma 6  , where not labeled in the ground truth labels, our model detected them as a gene / protein entity. The example sentences of gene/protein annotations in Table\u00a0  were reviewed by several domain experts and medical doctors. As shown in the third row,   beta-muricholate   is a chemical entity but it was not annotated in the ground truth labels. However, the last row shows another type of annotation error.   Contrast media   is a general term for a medium used in medical imaging and since is not a proper noun, it is not a named entity. \n\nThese examples shows the presence of incorrect ground truth labels, which can harm the performance of bioNER models. However, we believe that these missed or misidentified ground truth labels can be corrected by our system. \n\n\n### Future works \n  \nFor future work, we plan to cover more target entity types and use more datasets. For example, CRAFT [ ], LINNAEUS [ ] and Variome [ ] are manually annotated datasets and are valuable resources that can be used for expanding our model. Second, we plan to apply CollaboNet to downstream biomedical text mining systems. For example, entity search engines such as BEST [ ] could be improved by using more accurate NER models. \n\n\n\n## Conclusion \n  \nIn this paper, we introduced CollaboNet, which consists of multiple BiLSTM-CRF models, for biomedical named entity recognition. While existing models were only able to handle datasets with a single entity type, CollaboNet leverages multiple datasets and achieves the highest F1 scores. Unlike recently proposed multi-task models, CollaboNet is built upon multiple single-task NER models (STMs) that send information to each other for more accurate predictions. In addition to the performance improvement over multi-task models, CollaboNet differentiates between biomedical entities that are polysemous or have similar orthographic features. As a result, our model achieved state-of-the-art performance on four bioNER datasets in terms of F1 score, precision and recall. Although our model requires a large amount of memory and time, which existing multi-task models require as well, the simple structure of CollaboNet allows researchers to build another expert model for different entity types in CollaboNet. As CollaboNet obtains higher precision than other models, we plan to apply CollaboNet in a biomedical text mining system. \n\n \n", "metadata": {"pmcid": 6538547, "text_md5": "2cbadb1aea1ba8b474bf8a39d20d0421", "field_positions": {"authors": [0, 62], "journal": [63, 81], "publication_year": [83, 87], "title": [98, 187], "keywords": [201, 256], "abstract": [269, 2051], "body": [2060, 33596]}, "batch": 1, "pmid": 31138109, "doi": "10.1186/s12859-019-2813-6", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6538547", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6538547"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6538547\">6538547</a>", "list_title": "PMC6538547  CollaboNet: collaboration of deep neural networks for biomedical named entity recognition"}
{"text": "Nomoto, Tadashi\nSN Comput Sci, 2022\n\n# Title\n\nKeyword Extraction: A Modern Perspective\n\n# Keywords\n\nHistorical survey\nMeta-analysis\nKeyword extraction\nAutomatic indexing\nNatural language processing\nInformation extraction\nText generation\n\n\n# Abstract\n \nThe goal of keyword extraction is to extract from a text, words, or phrases indicative of what it is talking about. In this work, we look at keyword extraction from a number of different perspectives: Statistics, Automatic Term Indexing, Information Retrieval (IR), Natural Language Processing (NLP), and the emerging Neural paradigm. The 1990s have seen some early attempts to tackle the issue primarily based on text statistics [ ,  ]. Meanwhile, in IR, efforts were largely led by DARPA\u2019s Topic Detection and Tracking (TDT) project [ ]. In this contribution, we discuss how past innovations paved a way for more recent developments, such as LDA, PageRank, and Neural Networks. We walk through the history of keyword extraction over the last 50 years, noting differences and similarities among methods that emerged during the time. We conduct a large meta-analysis of the past literature using datasets from news media, science, and medicine to business and bureaucracy, to draw a general picture of what a successful approach would look like. \n \n\n# Body\n \n## Introduction \n  \nThe notion of \u2018keyword\u2019 has long defied a precise definition. Boyce et al. [ ] called it   a surrogate that represents the topic or content of a document,   which in turn gives rise to another question: What is a topic or content? Which is equally elusive. History witnessed the rise of two major schools of thought, one in terminology science (TS) and the other in information retrieval (IR). The two have crisscrossed each other as they progressed in their scientific endeavor. Terminologists are generally concerned with finding terms that are specific to a particular technical domain, useful to organize knowledge relating to that domain, while\u00a0people in information retrieval are\u00a0focused more on identifying terms (which they call indexing terms) capable of distinguishing among documents to improve document retrieval.   \nKeywords, technical terms, and indexing terms \n  \n\nDespite some fundamental differences, there is one principle that cuts across\u00a0TS and IR: that keywords are terms that reside in the document. Hulth [ ] reported, however, that people, when asked to provide keywords for their own scientific writings, picked words not only from their work, but also drew upon their own personal knowledge, suggesting that keywords may not be confined to the text alone. In Sect.  , we show that this is indeed the case, drawing on evidence from data from online sources. We argue that there is more to keywords than indexing and technical terms (Fig.\u00a0 ). \n\nIn this work, we use term   keyword   as an overarching term to refer to linguistic expressions that take on one or more of the following roles.   \n Terminology:   words or phrases that are used in a specific domain to denote a particular technical idea; e.g.,   phosphogypsum  ,   progressive taxation  ,   return on equity  ,   Planck constant  ,   sarcoma  ,   carcinogen  . \n  \n Topics:   terms and labels that are part of a set of concepts systematically assembled under a particular classification policy; e.g., Wikipedia category names, Dewey Decimal Classification. \n  \n Index terms:   terms indicating major concepts, ideas, events, and people, referred to in a document or book; e.g.,   JFK  ,   Martin Luther King, Jr.  ,   Malcolm X  . \n  \n Summary terms:   words or phrases that are meant to serve as a quick description of the content; e.g.,   global warming  ,   deforestation  ,   extreme weather  . \n  This work is intended as an introduction to major ideas that have evolved and shaped the field for the last 50 years, which retain relevance to this day. Some of them came from TS, others from IR or from computational linguistics. One may ask why we need another survey of keyword extraction, given that there have been a number of efforts out there with an aim similar to ours, in particular Firoozeh et al. [ ].  The reason has to do with one problem they all share: the failure to recognize the limits of extractive methods. Keywords arise not only from inside the document but also from outside, i.e., an external source the author has access to. To be able to address keyword extraction requires delivering solutions to both types of keywords. We will see how some of the recent developments such as a generative paradigm based on deep learning, can address the challenge with their unique ability to \u2018invent\u2019 keywords as needed. \n\nAnother aspect of a keyword, often dismissed as less important in the past literature, including UNESCO [ ]\u2014which forms a moral foundation for Firoozeh et al. [ ]\u00a0 \u2014is the length: we will demonstrate empirically that it plays an important role in defining what it takes to be a keyword, and argue that this recognition of a role the length plays provides a key to solving what Hasan and Ng [ ] called \u2018conundrums in keyword extraction.\u2019 \n\nOur overall contribution lies in putting in a new light aspects of a keyword that have been left largely unexplored and untouched in previous like-minded surveys. In the final section, we will line up major methods that emerged over the past 50 years, and compare them against one another, giving some idea of where they stand in terms of performance and design choices. \n\n\n## History \n  \n### Automatic Term Indexing \n  \nTerm indexing, described by Boyce et al. [ ] as a field of study concerned with finding surrogates that represent the topic or content of documents, remains as relevant as ever in information retrieval today. TFIDF, a widely acclaimed method for finding important words, came into being in the early 1970s when Salton and Yang [ ] proposed to measure the importance of a word using the following formula: It marks\u00a0a huge break from approaches prevalent at the time that were mostly focused on the frequency of terms in and across documents. TFIDF takes the importance of a word in document as consisting of two components. The first component   is the frequency of a word   i   in the document   j  , also known as TF. The second component  , known as\u00a0the inverse document frequency or IDF, is to indicate how uncommon the word is.   = 1 if   appears in document   j  ; otherwise 0. Therefore,   equals the number of documents containing  .   n  (  D  ) is the total number of documents in a collection. \n\nThe discovery of TFIDF was followed a year later by another formulation [ ], which expanded and refurbished the idea to deal with two-term keywords: Note that this can be transformed into where we are able to see the TF and IDF components more clearly.   denotes the importance of a two-word term   in document   j  . \n\nMeanwhile, Robertson and Jones [ ] defined the term importance in terms of how well it served the document search. The importance of term   was given by where for a given query   q  The problem with the approach, from a standpoint of keyword extraction, is that to determine the term importance,\u00a0one needs to find\u00a0relevant documents for query   q  , which can be challenging, because this would require\u00a0asking humans to make a judgement on relevance for each of the documents collected. Another issue is that a term will no longer have a unique score as it is made relative to a query: the use of a different query may result in a different score even for the same term. This is troublesome, as it implies that the importance of a word cannot be determined without a reference to a query. These issues inherent to the idea make it unlikely that a relevance-based term indexing would meaningfully contribute to keyword extraction. \n\nSalton et al. [ ] takes somewhat a different route, exploring what they call the discrimination value analysis. The idea is based on the intuition that one can determine the importance of a term by looking at how well it is able to discriminate documents in a collection: a good indexing term is one that would separate documents from one another, making a collection sparse. \n\nAssume that we have a document represented as a vector which keeps track of the frequency of every term we find in the document. Averaging the in-document frequency of each term will give us a centroid vector,  , where each element   looks like  n   is a number representing how many unique terms we have in the collection, and   f  (  w  ) the in-document frequency of   w  . Define the density of a collection by where   denotes the cosine similarity,   C   a centroid, and   a document vector, where  M   indicates the number of documents the collection contains. Define a function   for   as For a given term  ,   is a   Q   score one gets by setting   for every  .   means that   w   has the ability to discriminate documents (because its removal from the collection causes an increase in density,\u00a0making documents more similar). Salton et al. [ ] define the discriminative value of term   in document   j   by The authors reported that their approach, when applied to three datasets, CRANFIELD [ ], MEDLARS [ ], and TIME [ ], led to an improvement by 10% over an approach which only makes use of the term frequency. \n\nNagao et al. [ ], inspired by   statistic, came up with an interesting alternative where  represents the importance of term   i  . The idea is that if its frequency   deviates from its expected frequency  , we take it as worthy. One caveat is that one must have a large collection of documents, to guarantee that an estimated   follows   the distribution. \n\nThe idea was further explored by Matsuo and Ishizuka [ ], who proposed to replace Eq.\u00a0  with  indicates how many sentences there are that contain   w   and   g   together.   f  (  w  ) is the count of sentences in a document which contains   w   and   the probability that any given term appears together with   g   (in a sentence), i.e.,  , with   V   indicating the total number of uniques words in a collection.   thus corresponds to the expected frequency of   w   co-occurring with   g  .   G   is a pre-defined set of frequent terms in a document (with stopwords and other minor words removed). The authors\u2019 goal was to find how far the observed co-occurrence frequency of   w   and   g   deviates from its expected frequency. The further it veers off, the greater its significance. Matsuo and Ishizuka [ ] went on to suggest using the following in place of Eq.\u00a0 : The formula penalizes a term if\u00a0its chi-squared value is backed by\u00a0only a small number of high frequency terms.\u00a0Thus, if we have two terms A and B ( ), and if A occurs only with B and not any other member of   G  ,   will get a high score, but   will get 0.   \nPerformance of   vs. baselines \n  \n\nTable\u00a0  gives some sense of how well it works. The test was done using\u00a020 scientific papers. We observe that   is doing almost as good as TFIDF, without relying on the document frequency, which the latter requires. Every material that   makes use of comes from inside the document. The table also shows the performance of keygraph, another method based on a co-occurrence metric, described below. TF is the simplest of all, relying only on the term frequency. \n\nOhsawa et al. [ ] are the earliest attempt (to our knowledge) to leverage the notion of   word graph   to extract keywords,\u00a0an approach they termed \u2018  keygraph  .\u2019\u00a0A word graph is an interconnected network of words built by linking words based on how closely a pair of words are associated, e.g., the number of times the pair co-occurs in a sentence. Ohsawa et al. [ ] defined the strength of association   between a pair of words,   and  , using the formula  D   is a set of sentences in the document,   represents the number of times   occurs in sentence   s  , and similarly for  .  .   was meant to ignore weakly connected pairs.   \n\u00a0A word graph \n    \nSubgraphs \n  \n\nFigure\u00a0  shows a word graph consisting of five nodes, each corresponding to a word, and a link between nodes, indicating that corresponding words occur together in some of the sentences in   D  . The width of a link indicates the strength of association as given by Eq.\u00a0 . \n\nThe approach further divides a word graph into a set of subgraphs which the authors claimed to correspond to distinct topics the writer may have had in mind when penning the document. Figure\u00a0  gives some idea of what they are like. Each node (or vertex) represents a word, with an edge (solid line) signaling the presence of a co-occurrence relation between words, indicating\u00a0that there are sentences in which they appear together. A subgraph is a set of nodes (words) where every member of the set is linked to every other. Ohsawa et al. [ ] assume that a keyword is a word that participates in multiple subgraphs,\u00a0such as\u00a0words\u00a0denoted\u00a0by\u00a0   and   in Fig.\u00a0 . The following is a formal definition of the importance of word   w   that encapsulates the idea: where and  g   stands for a subgraph in document   D  .  . Note that  B  (  w  ,\u00a0  g  ) indicates how many times   w   co-occurred with a member of a subgraph   g  .   N  (  g  ) is a normalizing factor. Intuitively,   I  (  w  ) says that the greater the number of subgraphs is that contain   w  , the more important it will be, a proposition which could be interpreted as saying that important words are those whose occurrence is widespread across the document. According to Ohsawa et al. [ ], the approach performed on par with TFIDF in document retrieval. \n\nWe conclude the section by pointing out that the past approaches to term indexing share a particular view about its nature [ ]: an indexing term is something that resides in a document, occurs frequently across documents, and exhibits a distinct distributional pattern. Term weighting schemes proposed in the Automatic Indexing literature all reflect this principle one way or another. \n\n\n### Computational Linguistics \n  \n  \nFrequencies of terminological terms (Table   in Justeson and Katz [ ]) \n  \n\nJusteson and Katz [ ] were the first attempt to look into linguistic properties of technical terms. They examined the terminology used in technical dictionaries from various domains, including fiber optics, physics and mathematics, medicine, and psychology. The study concluded that noun phrases accounted for 92.5\u201399% of the technical terms found, with about 70% of them having more than one word; there were a few cases where they accompanied adjectives and to a lesser degree, prepositions; but there was no instance which involved verbs. Table\u00a0  summaries their findings. Most of the terms are made up of two words with an exception of medical terms (for which the authors attempted a linguistic explanation). \n\nNot surprisingly, the authors were more\u00a0into\u00a0developing\u00a0linguistics of keywords than engineering a solution, as is manifest in questions they asked, such as \u2018Why do technical terms resist the use of conjunctions, prepositions, and adverbs?\u2019 Their answer to that was that technical terms take shape under two opposing linguistic forces, one that pushes them to become shorter and the other pulling them towards more transparency. Because none of the excluded types (terms which include verbs, conjuncts, and prepositions) are able to accommodate the demand of either of the two forces, they are disfavored. \n\nThe authors further suggested the following two tests to identify technical terms: (1) whether a term is two word long; and (2) whether it matches a regular expression of the form: \u2018  A  \u2019 denotes an adjective, \u2018  P  \u2019 a preposition, and \u2018  N  \u2019\u00a0a noun. For details on linguistics notions, refer to Manning and Sch\u00fctze [ ]. \n\nDaille et al. [ ] generally echoed what was found in Justeson and Katz [ ], though they argued that technical terms were something built out of basic multi-word units (MWU) via compositional operations, and went on to say that a complex multi-word term such as   geostationary communication satellite   was the result of combining two MWUs,   geostationary satellite   and   communication satellite.   Salton [ ] took a step further, suggesting that we should include discontiguous terms, i.e., those made up of elements separated by some intervening words, such as\u00a0\u2018  building\u00a0dictionary  \u2019 from \u2018  a\u00a0building\u00a0of a dictionary.  \u2019   \nPotential glossary items (Table   in Park et al. [ ]) \n  \n\nPark et al. [ ] took on the issue from a somewhat different angle. Their primary interest was in glossary extraction, where a main goal was to locate and extract terms related to a specific domain. What made their work different was a set of syntactic patterns they used to identify candidate terms, examples of which are shown in Table\u00a0 . Of a particular interest is the use of conjuncts (i.e.,   and, or  \u00a0in ACAN, AACAN and ACAAN)\u00a0\u00a0which Justeson and Katz [ ] explicitly argued against. The motivation for using a particular set of syntactic patterns primarily came from their need to work for a specific domain. To improve a sensitivity to the domain, the authors further proposed a scoring function that favored those of high relevance to a specific domain. \n\nAnother interesting idea came from Barker and Cornacchia [ ], who promoted a notion of \u2018head-driven keyword extraction.\u2019 The idea was to define keywords as NPs (noun phrases)\u00a0containing most frequent heads. The authors reported a modest improvement over baselines. The work deserves mention because of their unique effort to relate\u00a0a syntactic theory (then current) to\u00a0keyword extraction. \n\n\n\n## New Perspectives \n  \n### PageRank Inspired Approaches \n  \nMihalcea and Tarau [ ], on the heels of the success of PageRank, set off on a project they called TextRank. They were interested in finding a way to exploit PageRank in their effort to find keywords in the text. In their adaptation of PageRank, a text is broken into a set of nodes, and edges, with nodes representing words and edges connections among them. The importance of a word is given by the following formula:  i  ,   j  ,   k   are all words.   A  (  i  ) represents a set of words that appear in the proximity of   i  .   w  (  j  ,\u00a0  i  ) represents the strength of the bond between   j   and   i   based on their co-occurrence.   is what is known as a damping factor. \n\nImagine that you are at word   i  , thinking about whether to jump to somewhere else in the text. The equation describes the probability of moving to some other word, which is given as the sum of the probability of jumping to some random word and that of moving to some popular word. Intuitively, TextRank reflects an idea that a word you are looking at is important if you see important words around it. TextRank could also be viewed as a modern-day reincarnation of graph-based approaches discussed earlier [ ,  ]. Recall that they defined the importance of a word by how often it co-occurs with surrounding words. The only difference is that TextRank takes into account weights of contextual (surrounding) words, which the latter do not. \n\nThere is, however, one area\u00a0where\u00a0TextRank completely breaks ranks with the conventional widsom. Kageura and Umino [ ] argued that the frequency of a term is an important component of an index term. The past work in Automatic Term Indexing tends to agree that a word that occurs frequently often works as an index term. The fact that TextRank has no way of accessing the word frequency implies that words TextRank favors do not necessarily coincide with those that the traditional indexing would find important.   \nDetails of the corpora (Table   in Hasan and Ng [ ]) \n    \nPerformance of TextRank and its variants (Table   in Hasan and Ng [ ]) \n  \n\nHasan and Ng [ ] conducted a series of experiments in an effort to find whether TextRank, along with other like-minded approaches such as SingleRank and ExpandRank [ ],  has any advantage over TFIDF.  (See Table\u00a0  for some details on the datasets they used. ) \n\nTable\u00a0  shows results. What is striking is that graph-based approaches failed to perform at a level comparable to TFIDF, a finding which took Hasan and Ng [ ] by surprise. However, we view it as an inevitable consequence of not paying attention to the term frequency and in particular the length of a candidate phrase (we demonstrate that this is the case later in the paper). In this light, Matsuo and Ishizuka [ ] and Ohsawa et al. [ ], both graph-based, may have worked\u00a0better\u00a0if Hasan and Ng had tried them,\u00a0 as they have a means to access frequency information. \n\n\n### Using External Knowledge \n  \nUsing external information has been one of the popular topics in keyword extraction. MAUI [ ] is a keyword extractor which has an option to produce keywords from a custom vocabulary. It does this by replacing ngrams with matching descriptors in the vocabulary. One may view the process as a term normalization via external knowledge. Medelyan [ ] reported that MAUI, when tested on three datasets, each with a different vocabulary, was able to recover about 40 to 80% of human assigned keywords. The work went on to explore the use of Wikipedia as an external source, which eventually evolved into an approach that treats a Wikipedia title as a keyword. The author suggests the following formula to find a title that matches a given word:  w   represents a term we want to project into Wikipedia.   C   represents a   context   of   w  , a set of ngrams that co-occur with   w   in a given page.   T   denotes a Wikipedia title.   is the probability of seeing   T   given   w  .   R  (  x  ,\u00a0  y  ) measures how closely   x   and   y   are related. The greater the value, the closer the association between   x   and   y  . The formula looks like the following:  x   and   y   are ngrams,   X   (or   Y  ) a set of incoming links to a page to which   x   (or   y  ) is mapped, and   its size.   N   is the total number of articles in Wikipedia. Clearly,  . We may interpret Eq.\u00a0  as saying: if you have an ngram   w   which relates to multiple Wikipedia pages, pick one which is contextually relevant to   w   and moreover which occurs frequently with   w  . For example,   apple   could mean a number of things; an edible fruit, a place, an American computer company. Equation\u00a0  is intended to disambiguate a term based on a context in which it occurs and on how frequently each of the associated senses is used. Thus, if it is found with words like   orange  ,   banana  ,   juice  , and   mango  , it is more likely to be mapped into a page representing an apple as an edible fruit. \n\nIt is worth noting that MAUI draws upon a technique known as wikification [ ]. Wikification is yet another keyword extraction method, which leverages Wikipedia to identify potential keywords. To test if a given word is a keyword, it goes to Wikipedia to see if it is used as an anchor. If it is, then the word will be stored in a set of candidates before they are scored according to a metric it calls   keywordness,   a measure indicating how likely a particular word occurs as an anchor in Wikipedia. The more frequently a word appears as an anchor, the higher it is ranked as a keyword. One drawback is that a newly minted word or a word that entered the public conversation recently is likely to be undervalued, because it has little presence in Wikipedia. This means that to avoid a failure, MAUI\u00a0needs to keep \u2018knowledge-lean\u2019 methods like  , or TFIDF as a\u00a0backup. \n\n\n### Classificatory Keyword Extraction \n  \nClassificatory keyword extraction (CKE) represents a class of approaches that work by scanning contiguous spans of a text for keyword, where we visit each word, determining whether or not to include it in a pool of potential keywords. Much of the past and present work, supervised or unsupervised, falls under this category. \n\nIn their short paper published in 2018 for AAAI, Florescu and Jin [ ] introduced an approach based on a random walk. Like Ohsawa et al. [ ], Matsuo and Ishizuka [ ], and Mihalcea and Tarau [ ], it treated a text as a network of words, with the strength of an association represented by how often they occur together. A major difference between Florescu and Jin [ ] and what preceded them lies in their use of latent representations acquired from random walks [ ] to determine the strength of connections. The authors reported an improvement over past methods that relied on a word-based representation. \n\nZhang et al. [ ] conceived\u00a0an approach which combined CRF (Conditional Random Fields) [ ] with deep learning machinery (CRF/DL, hereafter). They optimized the model along dual targets. One, given in an IOB format (\u2018inside-outside-begin\u2019), specifies where a keyword begins and ends. The other indicates whether a particular token is part of a keyword, in a binary format. The idea resulted in a performance better than R-CRF [ ], a more faithful implementation of CRF in RNN (Recurrent Neural Networks).  It was unfortunate that Zhang et al. [ ], despite their focus\u00a0on the tweet domain, did not consider\u00a0problems particular\u00a0to tweets, e.g., (in-group) abbreviations, slang words, and misspellings, in contrast to Marujo et al. [ ], who\u00a0put these issues at the forefront.   \nClassificatory DL Extractor (CDL) maps each token in the input into a pre-defined label space, for instance one with  , where \u2018  S  \u2019 indicates a single-token keyword, \u2018  B  \u2019 a beginning of a multi-part keyword (MPK), \u2018  I  \u2019 an in-between element and \u2018  E  \u2019 an end of an MPK. CDL builds a model in a way that maximizes a quantity  , where   denotes  ,   a natural language text. CDL typically\u00a0makes use of an encoder/decoder architecture of the sort shown in Fig.\u00a0 \n    \nA schematic view of an encoder/decoder neural (sequence to sequence) model. An encoder rolls out a sequence of a recurrent neural network (LSTM), each feeding on a composite representation of a token, and sends the result to a decoder which converts it to probability distributions of labels/tokens, with the output built from labels/tokens with highest probabilities\u00a0 \n  \n\nThe idea of Wang et al. [ ] centered around how to transfer a keyword model over to a domain for which there is no ground truth available. The\u00a0authors pursued an extractive approach guided\u00a0by what is generally known as \u2018Generative Adversarial Networks\u2019 or Jensen\u2013Shannon GANs [ ]. The idea was to move latent representations of data in an unannotated domain as close to those acquired from a domain for which we know the ground truth, as possible. The model was set up in a way reminiscent of the unsupervised multilingual translation, where multiple independent networks work\u00a0together to achieve diverse objectives (reconstruction loss, IOB loss, discriminator loss, and the like). The work reported a substantial gain over strong baselines, which included a model similar to CRF/DL [ ,  ,  ]. (Figs.\u00a0  and   give a high-level picture of how it works.) \n\n\n### Generative Keyword Extraction \n  \nWhile the external knowledge allows us to move beyond the confine of document, another possibility emerged recently thanks to advances in deep learning (DL), where the focus is more on   generating   keywords. If successful, it may deliver a one-shot solution to acquiring out-of-document and in-document keywords, an issue that plagued the past research. This section introduces a line of work that embraced this particular strategy [ ,  ,  ], while giving a cursory look at other DL-based approaches that are essentially extractive in nature [ ,  ]. \n\nWe start with Meng et al. [ ]. Assume that we have two RNNs (recurrent neural networks): one encodes the source text (Encoder) and the other generates keywords (Decoder). The input is transformed into hidden representations through the Encoder, which the Decoder takes over to construct an\u00a0output. While decoding the output, a beam search is typically applied to select candidate keywords. An innovation that\u00a0Meng et al. [ ] bring to the table is\u00a0a particular objective (loss function) the authors proposed to train the network, namely  x   is an input text.   denotes the probability that a token   is generated using the general vocabulary and   the probability that   is generated using the vocabulary from the input text.   adds a functionality to the network to be able to reuse part of the input as it creates a keyword. This feature is critical for keyword extraction, because without it, it would be impossible to extract elements from the source. Equally important, it provides the model with the capability to build out-of-document keywords (via  ). It implicitly replicates what MAUI achieved through Wikipedia and a set of mapping rules. Yuan et al. [ ] extended Meng et al. [ ] by adding a capability to output multiple keywords simultaneously.  In addition, they introduced a learnable switch which allowed them to decide whether to use   or   during the generation. By contrast, Meng et al. [ ] had no control over which one to emit as they relied on the combined probability,  .   \nGenerative DL Extractor (GDL) takes as input an entire sequence of tokens and generates a keyword, using a training vocabulary, which may or may not appear in the source sequence. The learning proceeds in a way similar to CDL, with an aim to maximize  , where   denotes   ( ),   a natural language text.   represents a vocabulary (a set of words) derived from training data. Note  . GDL typically uses the same architecture as CDL (see Fig.\u00a0 ) \n  \n\nChen et al. [ ] share with Yuan et al. [ ] a goal of generating multiple keywords in one fell sweep, but depart\u00a0from the latter in\u00a0their emphasis on the diversity, which the former realized using what they called a coverage mechanism (CM), an idea originally from machine translation [ ]. CM works as a sort of a ledger to keep a\u00a0record of how much attention was given to tokens during the encoding. The authors reported that it had successfully prevented a repetitive generation of tokens. Yuan et al. [ ], pursuing a somewhat different line while aiming for the same objective, proposed a loss function called orthogonal regularization (OR) [ ] , where   is a hidden representation (a   d  -dimensional vector) used\u00a0to derive\u00a0the   i  -th keyword.\u00a0  is a squared Frobenius norm.   n   is the number of keywords. Minimizing   has the effect of increasing the diversity among  , resulting in keywords that vary in form and meaning. \n\nBeing able to generate keywords on the fly is a double-edged sword: while it allows you to \u2018concoct\u2019 a new word, it may get you inadvertently assigning keywords that are not remotely relevant to what a text is about (for instance, one might end up with a keyword like\u00a0\u2018bible concordance\u2019 from the input given\u00a0in Fig.\u00a0 ), a problem that rarely affects the classificatory regime.   \nA typical setup to use DL as a text classifier. We project the output of an embedding layer into a softmax layer via a convolutional neural network (CNN), and get a probability distribution of potential categories. The input will be labeled with one or more terms with the highest probability \n    \nAn alternative design (of a kind pursued by Lee et al. [ ]) \n  \n\n\n### Keyword Extraction as Text Classification \n  \nText classification (TC) provides another interesting angle from which to look at keyword extraction. One uses TC mostly\u00a0to associate a document with labels from some pre-defined vocabulary. TC has a long history of research going back many decades, with much of the current effort happening within the realm of DL [ ,  ,  ]. While TC is confined to a fixed set of topics, we can turn it into a keyword extractor by enlarging the vocabulary it covers. \n\nA most typical setup to use DL for TC is shown in Fig.\u00a0  [ ,  ]. We start with some word embeddings, possibly along with character-level embeddings. We work through convolutional layers (which could be many), and arrive at a condensed representation, which we use to label the text (via a softmax layer). Lee et al. [ ] pursued an alternative strategy which made use of LSTM, a recurrent neural network (RNN), in place of CNN, allowing them to incorporate temporal information (Fig.\u00a0 ). Yin et al. [ ], wondering about which approach works better, conducted experiments on various classification tasks ranging from sentiment to relation to entailment classification. They found no significant differences in performance between RNN and CNN. \n\nYet, some people expressed a concern over applying TC to keyword extraction, worried that the number of categories it had been tested on in the literature was very small (somewhere around 4\u201310 [ ]). There is no work so far, to our knowledge, that addressed the concern. Its ultimate success may hinge on whether it can be extended to work for a large number of categories. \n\n\n### Working with Textual Cues \n  \nAn observation that a keyword rarely contains a stop word led Rose et al. [ ] to a development of a widely used method known as RAKE.  It extracts keywords by dividing a text into a set of contagious word sequences by stop words, and choosing those that occur most often. Consider a sentence   \u2018a volcanic explosion killed at least 8 people.\u2019   Assume that one has a list of stop words   \u2018an\u2019  ,   \u2018killed\u2019  ,   \u2018at\u2019  ,   \u2018least\u2019  ,   \u20188\u2019  , and   \u2018.\u2019  .\u00a0Delimiting the sentence with them gives us and by eliminating the separators, we get arriving at terms   \u2018volcanic explosion\u2019   and   \u2018people.\u2019   While surprisingly simple, it was found to rival more sophisticated approaches like TextRank and Hulth [ ]. \n\nIn a similar vein, KPM or KP-miner [ ]  takes anything that occurs between stop words and punctuations as a keyword candidate. A decision on which one to choose is made based on where it occurs in the text and how often it appears. Anything that appears beyond the initial 850-word block of the document and/or those that occur less than three times are discarded. The importance of a candidate is measured by a version of TFIDF, which takes into account additional textual features such as location, and the\u00a0within-document frequency of candidates. \n\nYAKE [ ,  ]  is an outlier among unsupervised systems (which include RAKE and KPM) due to its treatment of the context. The approach is motivated by an intuition that any word that appears in company of many different words is essentially a function word, and thus should be discarded. The claim is an interesting one, because it attempts to identify keywords not by how important they are, but by how   insignificant   they are. They proposed the following to measure the insignificance of a word: where   x   is a word.   R  (  x  ) indicates how many unique tokens are found in company with   x  ,   P  (  x  ) the position of   x  \u2019s first occurrence.   C  (  x  ) records how many times   x   occurs with its initial letter capitalized or appears as an acronym.   F  (  x  ) represents the frequency of   x   and   D  (  x  ) the number of sentences in which   x   appeared. The lower the value of   S  (  x  ), the better. The insignificance of a keyword K (which may involve more than one word) is given as where  . \n\n\n### Going Bayesian: LDA \n  \nLDA or Latent Dirichlet Allocation [ ]  is another favorite method people turn to. LDA builds a language model, operating on the premise that there is an implicit set of topics that dictate the distribution of words we observe in a document. In LDA, a topic is not a single summary term that describes some aspect of the document, but rather something that represents a probability distribution that spans the\u00a0entire vocabulary. Words (normally, uni-grams) that occur frequently with a topic are given higher probabilities. In LDA, a topic takes a form like:   = {opera, lyrics, hip-hop, jazz, ambient, ...} or   = {market, fed, slump, recession, exchange, ...}, each spanning the entire vocabulary, with associated probabilities (suppressed here). You may interpret them as you please. How to make sense of   or   is entirely left to the user. \n\nIn LDA, every word in the document is assigned to some topic:  \nAfter a long  tiring  week , House  Democrats  decided  to move  forward  with a request  for the two articles  of the impeachment  against the President  . \n Here,   is a topic index. One can have as many or as few topics as he or she wants. There are basically two ways to turn LDA into a keyword extractor. (1) one is to simply take as keywords, words that are most likely to occur under LDA; (2) the other is to select those associated with the\u00a0most prominent topic. The worthiness of a word under the first approach can be given as  T   is the number of topics that we assume cover documents.   d   denotes a document.  ,   are parameters responsible for generating probability distributions that determine how likely   w   (word)\u00a0and   t   (topic\u00a0 index)\u00a0occur, or more precisely,   represents a matrix of shape   with   K   = the size of topic indices and   K   = the size of the vocabulary. \n\nThe second approach can be written as where   K   is a set of topic indices  . \n\nLiu et al. [ ] were the first in a line of research [ ,  ,  ] working on TopicalPageRank (TPR) to combine PageRank and LDA. TPR takes a form almost identical to Eq.\u00a0 where  g   indicates how strongly words   u   and   w   are associated,   A  (  w  ) a set of words that sit in the proximity of   w   (see Sect.\u00a0  for further details). The authors reported the composite system performed competitively against LDA and PageRank. \n\n\n\n## Where Do They All Stand?: A Meta-Analysis at Scale \n  \nIn this section, we examine the effectiveness of approaches we discussed above, comparing them side by side on\u00a0a large number of datasets. We also look at whether performance is affected by a degree to which keywords are indigenous to the text. Table\u00a0  provides a sense of what they look like.  The indigeneity varies from one dataset to another. We want to know if or how it impacts keyword extraction.   \n\u2018Native\u2019 versus \u2018foreign\u2019 keywords in a\u00a0PubMed article.\u00a0\u2018Native\u2019 keywords are ones found in the text (like those\u00a0marked with an underscore), whereas \u2018foreign\u2019 keywords are those that are not. In the\u00a0  keywords   \u00a0section,\u00a0we find keywords supplied by humans for the\u00a0abstract. \n  \n\nA success of an extractive approach depends on how many of the target keywords come from inside, because if most of them are from outside, there is no way for it\u00a0to be able to find them. We highlight the issue by introducing three measures: IDP (ratio of in-document keywords), ODP (ratio of out-of-document keywords), and RIO (ratio of IDP over ODP) RIO indicates the extent to which\u00a0a given corpus depends on keywords of an internal origin: the greater the value, the more likely a keyword is found within the text. We focus on how RIO interacts with systems that employ extraction as a primary means to acquire keywords. \n\n### Datasets \n  \nPart of the data came\u00a0from the Guardian, the New York Times, PubMed Central, Reuters, Amazon, and [ ]. The Guardian contained 40,000 online stories from January to late September 2014. The New York Times\u00a0(NYT), approximately the size of Guardian, contained stories from January to December 2011. PubMed Central was another corpus based on abstracts in various domains\u00a0found in\u00a0the PubMed Central Open Access repository. Reuters was a news corpus containing online articles that appeared on Reuters\u2019 website from 2011 to 2015. The Meng dataset came from Meng et al. [ ], which was made up of\u00a0papers in computer science. Amazon was part of what is generally known as \u2018Amazon-12K,\u2019 a large corpus of product descriptions, each of which comes with categories or tags. In contrast to\u00a0much of the previous work, which was\u00a0based on\u00a0documents numbered in the\u00a0hundreds to thousands, we work here\u00a0with considerably larger and more\u00a0diverse datasets.   \nDatasets.   (and  ) = the number of documents;   = the average length of documents in words;   = the average number of keywords per document;   = the average length of keywords in words.   indicates datasets that contain a training block.   indicates those that do not (used only for unsupervised systems). \n  \n\nIn addition, we made use of some 15 publicly available datasets, including 500N-KPCrowd [ ], citeulike180 [ ], Nguyen2007 [ ].  Table\u00a0  provides a statistical profile of each of the corpora we used\u00a0for this study. \n\n\n### Methods \n  \nIn addition to TextRank (TEXTR) (Sect.\u00a0 ), KP-miner (KPM) (Sect.\u00a0 ), YAKE (Sect.\u00a0 ), TopicalPageRank (TPR) (Sect.\u00a0 ), and RAKE (Sect.\u00a0 ), we also conducted tests for TFIDF, MAUI and ONMT-k. MAUI and ONMT-k were supervised systems. \n\nA particular version of TFIDF  we used here extracts from a document, n-grams with the length up to   n  , which do not contain punctuations, and scores them based on the TFIDF metric: TF multiplied by\u00a0IDF, where TF is the term frequency and IDF is defined as  , with   df   representing the document frequency, the number of documents in which a term appears, and   n  , the number of documents. TFIDF favors words that occur frequently in a small number of documents. \n\nMAUI [ ] goes through two phases to acquire keywords: candidate acquisition and ranking. In the acquisition phase, it\u00a0focuses on collecting and normalizing n-grams of up to a given length. It\u00a0has an option to use a controlled vocabulary. If enabled, it\u00a0will work with entries in a pre-defined vocabulary in place of words found in a document. In the ranking phase, it\u00a0activates features related to the text statistics, such as TFIDF, how much of the text a word covers, the location, and   keywordness,   to determine how good each candidate is. MAUI is trained with bagged decision trees [ ]. \n\nONMT-k [ ] is a deep learning algorithm equipped to create keywords not just from words within the document but also words from a general vocabulary found in the training data. It has the ability to generate a novel phrase which neither appeared in documents nor in gold standard labels.   \nCandidate acquisition and weighting. \u2018Contiguity\u2019 indicates whether or not a model requires candidates to be contiguous.   = the number of times   w   occurs with other words in document   d  .   = the frequency of   w   in   d.\u00a0  \n  \n\nTable\u00a0  summarizes major differences and similarities among the approaches discussed above. \n\n\n### Results \n  \nTable\u00a0  shows results in F1@  k   averaged over test documents. F1@  k   represents an F1 score determined on the basis of top   k   candidates the system returned [ ]. In this experiment, we set   k   to 5. Regardless of how many candidates were returned, we assumed that   k   candidates were always available: if we got less than 5, say, 3, we pretended that there were 5, with two of them being empty or zero-length keywords. We called a prediction correct only if it exactly matched one of the associated answers. Word stemming was not performed apart from MAUI. All the tokens in the corpora were uncased.   \nPerformance in F1@5 \n  \n\n  \nRIO vs. F1 in unsupervised systems \n  \n\nFound under UNSUP in Table\u00a0  are a set of extractive systems which do not rely on supervision, and under SUP are ones that require it. For UNSUPs, F1 figures are based on their performance on the test sets, while those for SUPs are based on their performance on the same test sets after being trained on the training data.   \nImpact of a shortening of keywords on TextRank and TopicalPageRank \n    \nF1 vs. RIO in supervised systems \n  \n\nFigure\u00a0  shows a relationship between RIO and performance. The   x  -axis represents RIO and the   y  -axis F1@5. A solid line in each panel denotes a regression line indicating how performance is affected by a change in RIO. One interesting pattern that we see in the figure is that systems on the left exhibit a behavior that consistently diverges from those on the right: the left group improves with RIO, but those on the right are not as responsive, with their performance showing no sign of improvement as RIO increases. \n\nTable\u00a0  shows how many words the\u00a0keywords returned by a given method contained on average. RAKE has as many as 9, followed by TEXTR and TPR whose outputs are on average 3.4 words long, all of which as we observed earlier, deviated from the left group in Fig.\u00a0 , whose keywords averaged around 1 to 2 in length. The divergence\u00a0in performance is most likely\u00a0caused by the difference in length of keywords\u00a0that they returned. \n\nOne simple way to see that\u00a0this is the case\u00a0is to look at what happens when TEXTR and TPR are forced to keep keywords less than 2 word long. \n\nThe results are shown in Fig.\u00a0 . TEXR_N2 and TPR_N2 are tweaked versions of respective methods (whose keywords averaged around 1.9) (Table\u00a0 ). This arrangement led to a visible improvement as seen in the figure, confirming that it is the average length of keywords that separates TEXTR and TPR from YAKE, TFIDF, and KPM.   \nAverage lengths of keywords \n  \n\nFinally, we move to a question of whether RIO impacts supervised systems (SUPs) as well. The result is shown in Fig.\u00a0 . The effect is more pronounced in MAUI than in ONMT-k. This is something we would have expected because of the way MAUI identifies its candidates: it looks for n-grams of up to 3 words in length, just like TFIDF and KPM. ONMT-k is largely unresponsive to RIO, which again comes as no surprise, because it \u2018generates\u2019 rather than extracts keywords from the source document. It does not care how many of the keywords originate in a source document. It is interesting that keywords ONMT-k generates are generally two word long (Table\u00a0 ), indicating that the neural model implicitly learned how long they should be. \n\nIn this section, we broadly reviewed ideas that emerged over the years, with a reference to RIO. One important takeaway is that setting the length at around 2 is a critical part of making an UNSUP predictor a success. We showed that cutting the length of keywords from 3.4 to 2 improved performance of TextRank (TEXTR) and TopicalTextRank (TPR) (Fig.\u00a0  and Table\u00a0 ).  Now, we know why RAKE will not and should not work as well as YAKE: keywords the former looks for average around 9, while those by the latter about 1.2\u20131.5. \n\n\n\n## Conclusion \n  \nIn this work, we surveyed major ideas in keyword extraction that emerged over the last 50 years, from the early 1970s, when the field was mainly led by information retrieval, to the present day which sees an\u00a0escalating\u00a0dominance by deep learning. The experiment has brought to light strengths and weaknesses of the methods. The fact that TFIDF and KPM ranked higher among UNSUPs suggests that a weighting scheme based on some form of TFIDF is effective, which in turn vindicates Justeson and Katz [ ], who argued that there were some specific conditions for terms to qualify as an indexing term. In addition, we saw that Justeson and Katz [ ]\u2019s prediction about the length of a term: that important terms are generally two-word long, holds true across a wide range of datasets from science to business to media to bureaucracy, as well as for the corpora in Table\u00a0  [ ].  The evidence is so strong that some may consider giving it a status of \u2018universal constant.\u2019 We found through RIO that some of the underperforming approaches can be fixed by forcing them to shorten keywords they generate. Setting keywords at the right length is as important as other design choices such as a weighting scheme, an observation whose significance has\u00a0been\u00a0underappreciated\u00a0in the past literature. \n\nTaken together, this should point to what an ideal approach in the unsupervised regime should look like: it would seek n-grams that are at most two word long, and determine their importance according to a weighting scheme more or less like TFIDF, possibly together with linguistically and statistically motivated schemes like those employed by KPM. If one wants to go beyond that, it would be wise to move to the generative regime, as it offers a capability to build keywords from within as well as from outside. \n\nIn their 2010 paper, Hasan and Ng [ ] puzzled over an unexpected failure of TextRank, the state of the art at the time, to perform on par with TFIDF.  The results we saw from the previous section are consistent with their findings, suggesting that their failure was most likely caused by   overly long   keywords that it\u00a0produced. \n\nAn interesting area of research that has yet to be explored is an exploration of conditions under which unsupervised methods work most effectively. Granted that they lag miles behind supervised systems in terms of accuracy, that would not diminish their value: they run faster, require less resources, and are easier to deploy and adapt to novel domains. We hope to see increased research activities in this important subfield in coming years. \n\n \n", "metadata": {"pmcid": 9753895, "text_md5": "c317f78ed05d0928ea4e411bf9b64ab2", "field_positions": {"authors": [0, 15], "journal": [16, 29], "publication_year": [31, 35], "title": [46, 86], "keywords": [100, 237], "abstract": [250, 1300], "body": [1309, 48616]}, "batch": 1, "pmid": 36536753, "doi": "10.1007/s42979-022-01481-7", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9753895", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=9753895"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9753895\">9753895</a>", "list_title": "PMC9753895  Keyword Extraction: A Modern Perspective"}
{"text": "Hahn, Udo and Oleynik, Michel\nYearb Med Inform, 2020\n\n# Title\n\nMedical Information Extraction in the Age of Deep Learning\n\n# Keywords\n\nNeural networks\ndeep learning\nnatural language processing\ninformation extraction\nnamed entity recognition\nrelation extraction\n\n\n# Abstract\n  Summary  \n Objectives  \n: We survey recent developments in medical Information Extraction (IE) as reported in the literature from the past three years. Our focus is on the fundamental methodological paradigm shift from standard Machine Learning (ML) techniques to Deep Neural Networks (DNNs). We describe applications of this new paradigm concentrating on two basic IE tasks, named entity recognition and relation extraction, for two selected semantic classes\u2014diseases and drugs (or medications)\u2014and relations between them.\n \n\n Methods  \n: For the time period from 2017 to early 2020, we searched for relevant publications from three major scientific communities: medicine and medical informatics, natural language processing, as well as neural networks and artificial intelligence.\n \n\n Results  \n: In the past decade, the field of Natural Language Processing (NLP) has undergone a profound methodological shift from symbolic to distributed representations based on the paradigm of Deep Learning (DL). Meanwhile, this trend is, although with some delay, also reflected in the medical NLP community. In the reporting period, overwhelming experimental evidence has been gathered, as illustrated in this survey for medical IE, that DL-based approaches outperform non-DL ones by often large margins. Still, small-sized and access-limited corpora create intrinsic problems for data-greedy DL as do special linguistic phenomena of medical sublanguages that have to be overcome by adaptive learning strategies.\n \n\n Conclusions  \n: The paradigm shift from (feature-engineered) ML to DNNs changes the fundamental methodological rules of the game for medical NLP. This change is by no means restricted to medical IE but should also deeply influence other areas of medical informatics, either NLP- or non-NLP-based.\n \n \n\n# Body\n \n## 1 Introduction \n  \n\nThe past decade has seen a truly revolutionary paradigm shift for Natural Language Processing (NLP) as a result of which Deep Learning (DL) (for a technical introduction, cf.\n \n; for comprehensive surveys, cf.\n \nand\n \n) became the dominating mind-set of researchers and developers in this field (for surveys, cf.\n \n). Yet, DL is by no means a new computational paradigm. Rather it can be seen as the most recent offspring of neural computation in the evolution of computer science (cf. the historical background provided by Schmidhuber\n \n). But unlike in previous attempts, it now turns out to be extremely robust and effective for adequately dealing with the contents of unstructured visual\n \n, audio/speech\n \n, and textual data\n \n.\n \n\n\nThe success of Deep Neural Networks (DNNs) has many roots. Perhaps the most important methodological reason is that, with DNNs, manual feature selection or (semi-)automated\n  feature engineering  \nis abandoned. This time-consuming tuning step was at the same time mandatory and highly influential on the performance of earlier generations of ML systems in NLP based on Markov Models (MMs), Conditional Random Fields (CRFs), Support Vector Machines (SVMs), etc. In a DL system, however, the relevant features (and their relative contribution to a classification decision) are automatically computed as a result of thousands of iterative training cycles.\n \n\n\nThe ultimate reason for the success behind DNNs is a pragmatic criterion though:\n  system performance  \n. Compared with results in biomedical Information Extraction (IE), obtained in previous years with standard ML methods, DL approaches changed profoundly the rules of the game. In a landslide manner, for the same task and domain, performance figures jumped up to unprecedented levels so far and DL systems consistently outperformed by large margins non-DL state-of-the-art (SOTA) systems for different tasks. Section 3 provides ample evidence for this claim and features the new SOTA results with a deeper look at IE, a major application class of medical NLP (for alternative surveys, cf.\n \n).\n \n\n\nDespite specialized hardware at disposal now, training DNNs still requires tremendous computational resources and processing time. Luckily, for general NLP, huge collections of language models (so-called\n  embeddings  \n) have already been trained on huge corpora (comprised of hundreds of millions of Web-scraped documents, including newspaper and Wikipedia articles) so that these pre-compiled model resources can be readily reused when dealing with\n  general-purpose  \nlanguage. But medical (and biological) language mirrors\n  special-purpose  \nlanguage characteristics and comprises a large variety of sublanguages of its own. This becomes obvious in Section 3 where we deal with scholarly scientific writing (with documents typically taken from PubMed). Here, differences to general language are mostly due to the use of a highly specialized technical vocabulary (covered by numerous terminologies, such as MeSH, SNOMED-CT, or ICD). Even more challenging are clinical notes and reports (with documents often taken from the MIMIC\n \n(Medical Information Mart for Intensive Care) clinical database) which typically exhibit syntactically ill-formed, telegraphic language with lots of acronyms and abbreviations as an additional layer of complexity (cf. the seminal descriptive work distinguishing both these sublanguage types by Friedman et al.\n \n). Newman-Griffis and Fosler-Lussier\n \ninvestigated different sublanguage patterns for the many varieties of clinical reports (pathology reports, discharge summaries, nurse and Intensive Care Unit notes, etc.), while Nunez and Carenini\n \ndiscussed the portability of embeddings across various fields of medicine reflecting characteristic sublanguage use patterns. These constraints have motivated the medical NLP community to adapt embeddings originally trained on general language to the medical language.\n \nlists those medically informed embeddings, many of which are the basis for the IE applications discussed in Section 3.\n \n   An Overview of Common Embeddings\u2014Biomedical Language Models    \n\nOur survey emphasizes the fundamental methodological paradigm shift of current NLP research from symbolic to distributed representations as the basis of DL. It thus complements earlier contributions to the International Medical Informatics Association (IMIA) Yearbook of Medical Informatics which focused exclusively on the role of social media documents\n \n, had a balanced view on the relevance of both Electronic Health Records (EHRs) and social media posts\n \n, or dealt with the importance of shared tasks for the progress in medical NLP\n \n. The last two Yearbook surveys of the NLP section most closely related to medical IE were published in 2015\n \nand 2008\n \n. The survey by Velupillai et al.\n \ndealt with opportunities and challenges of medical NLP for health outcomes research, with particular emphasis on evaluation criteria and protocols.\n \n\n\nWe also refer readers to alternative surveys of DL as applied to medical and clinical tasks. Wu et al.\n \nreviewed literature for works using DL for a broader view of clinical NLP, whereas Xiao et al.\n \nand Shickel et al.\n \nperformed systematic reviews on the applications of DL to several kinds of EHR data, not only text. Miotto et al.\n \nand Esteva et al.\n \nfurther extended that scope to include clinical imaging and genomic data beyond the scope of classical EHRs. From an even broader perspective of the huge amounts of biomedical data, Ching et al.\n \nexamined various applications of DL to a variety of biomedical problems\u2014patient classification, fundamental biological processes, and treatment of patients\u2014and discussed the unique challenges that biomedical data pose for DL methods. In the same vein, Rajkomar et al.\n \nused the entire EHR, including clinical free-text notes, for clinical predictive modeling based on DL (targeted,\n  e.g  \n., at the prediction of in-hospital mortality or patient\u2019s final discharge diagnoses). They also demonstrated that DL methods outperformed traditional statistical prediction models.\n \n\n\n## 2 Design and Goals of this Survey \n  \nIn this survey, we concentrated on publications within the time window from 2017 to early 2020 and screened the contributions from three major scientific communities involved in medical IE: \n  \nMedicine and medical informatics are covered by PubMed; \n  \nNatural language processing is covered by the ACL Anthology, the digital library of the Association for Computational Linguistics; \n  \nNeural networks are covered by the major conference series of the neural network community (Neural Information Processing Systems (NIPS/NeurIPS)) whereas the artificial intelligence community gets in via the Association for the Advancement of Artificial Intelligence (AAAI) Digital Library which keeps the records from the AAAI and IJCAI conferences. \n  \nWe also included health-related publications from the digital libraries of the Association for Computing Machinery (ACM) and the Institute of Electrical and Electronics Engineers (IEEE). When necessary, we also refered to e-preprint archives such as arXiv.org, since they have become a new, increasingly important distribution channel for the most recent research results in computer science (yet, in that state typically without peer review) and thus foreshadow future directions of research. \n\n\nWe searched these literature repositories with a free-text query that can be approximated as follows: (information extraction\n  OR  \ntext mining\n  OR  \nnamed entity recognition\n  OR  \nrelation extraction)\n  AND  \n(deep learning\n  OR  \nneural network)\n  AND  \n(medic*\n  OR  \nclinic*\n  OR  \nhealth)\n \n\nFor this setting, we found approximately 1,000 unique publications, screened them for relevance, and, finally, included roughly 100 into this survey. \n\n\n## 3 Deep Neural Networks for Medical Information Extraction \n  \n\nIn this section, we introduce applications of DNNs to medical NLP for two different tasks, Named Entity Recognition (NER) and Relation Extraction (REX). The focus of our discussion relies on studies dealing with English as reference language since the vast majority of benchmark and reference data sets are in English\n \n. After a brief description of each task, we summarize the current SOTA in tables which generalize often subtle distinctions in experimental design and workflows. Our main goal is to show the diversity of major benchmark datasets, DL approaches, and embeddings being used. For these tables, we extracted all symbolic (\n  e.g  \n., corpus or DL approach) and numerical information (\n  e.g  \n., about annotation metadata, performance scores) directly from the cited papers.\n \n\n\nThe assessment of different systems for the same task is centered around their performance on gold data in evaluation experiments. We refrain from highlighting minor differences in the reported scores because of different datasets being used for evaluation, changing volumes of metadata, and sometimes even the genres they contain. Hence, from a strict methodological perspective, the reported results have to be interpreted with utmost caution for two main reasons\n \n. First, the choice of pre-processing steps, such as tokenization, inclusion/exclusion of punctuation marks, stop word removal, morphological normalization/lemmatization/stemming, n-gram variability, entity blinding strategies, and, second, the calibration of training methods (split bias, pooling techniques, hyperparameter selection (dropout rate, window size, etc.)) have a strong impact on the way a chosen embedding type and DL model finally performs, even within the same experimental setting. However, the data we report give valuable comparative information of the SOTA, though with fuzzy edges. This situation might be remedied by a recently proposed common evaluation framework for biomedical NLP, the BLUE (Biomedical Language Understanding Evaluation) benchmark\n \n, which consists of five different biomedical NLP tasks (including NER and REX) with ten corpora (including BC5CDR, DDI, and i2b2 that also occur in the tables below), or the one proposed by Chauhan et al.\n \nenabling a more lucid comparison of various training methodologies, pre-processing, modeling techniques, and evaluation metrics.\n \n\n\nFor the tables provided in the next subsections, we used the F\n \nscore as the main ordering criterion for the cited studies (from highest to lowest)\n \n. We usually had to select among a large variety of experimental conditions (with different scores). The final choices we made were led by the criterion to favor comparability among all studies. This means that higher (and lower) outcomes may have been reported in the cited studies for varying experimental conditions. Still, the top-ranked system(s) in each of the following tables defines the current SOTA for a particular application.\n \n\n### 3.1 Named Entity Recognition \n  \n\nThe task of Named Entity Recognition (NER) is to identify crucial medical named entities (\n  i.e.,  \nspans of concrete mentions of semantic types such as diseases or drugs and their attributes) in running text. For a recent survey of DL-based approaches and architectures underlying NER as a generic NLP application, see\n \n.\n \n\n#### 3.1.1 Diseases \n  \n\nA primary target of NER in the medical field is the automatic identification of diseases in scientific articles and clinical reports. For instance, textual occurrences of disease mentions (\n  e.g., \u201cDiabetes II\u201d  \nor\n  \u201ccerebral inflammation\u201d  \n) are mapped to a common semantic type,\n  Disease  \n. The crucial role of recognizing diseases in medical discourse is also emphasized by a number of surveys dealing with the recognition of special diseases. For instance, Sheikhalishahi et al.\n \ndiscussed NLP methods targeted at chronic diseases and found that shallow ML and rule-based approaches (as opposed to more sophisticated DL-based ones) prevail. Koleck et al.\n \nsummarized the use of NLP to analyze symptom information documented in EHR free-text narratives as an indication of diseases and similar to the previous survey found little coverage of DL methods in this application area as well. Savova et al.\n \nreviewed the current state of clinical NLP with respect to oncology and cancer phenotyping from EHR. Datta et al.\n \nfocused on an even more specialized use case\u2014the lexical representation required for the extraction of cancer information from EHR notes in a frame-semantic format.\n \n\n\nThe research summarized in\n \nis strictly focused on\n  Disease  \nrecognition and, for reasons of comparability, based on the use of shared data sets and metadata (gold annotations). Two benchmarks are prominently featured, BC5CDR\n \nand NCBI\n \n. BC5CDR is a corpus made of 1,500 PubMed articles, with 4,409 annotated chemicals, 5,818 diseases, and 3,116 chemical-disease interactions, created for the\n  BioCreative V Chemical and Disease Mention Recognition Task  \n. As an alternative, the NCBI Disease Dataset\n \nconsists of a collection of 793 PubMed abstracts annotated with 6,892 disease mentions which are mapped to 790 unique disease concepts (thus, this corpus can also be used for grounding experiments).\n \n   \nMedical Named Entity Recognition: Diseases. Benchmark Datasets from BC5CDR\n \nand NCBI\n \n.\n    \n\nThe current top performance for\n  Disease  \nrecognition comes close to 90% F\n \n. Lee et al.\n \nuse a Transformer model with in-domain training (BioBERT), but also (attention-based) BiLSTMs which perform strongly in the range of 88\u201389% F\n \nscore. For the choice of embeddings being used, self-trained ones might be a better choice than pre-trained ones,\n  e.g.,  \nthose provided by bio.nlplab.org\n \n. The incorporation of (large) dictionaries does not provide a competitive advantage in the experiments reported here. Though multi-task learning and transfer learning seem reasonable choices (\n \nand\n \n, respectively) to combat the sparsity of datasets, they generally do not boost systems to the top ranks.\n \n\n\nInteresting though are differences for the\n  same  \napproach on\n  different  \nevaluation data sets. For the second-best system by Sachan et al.\n \n, F\n \nscores differ for BC5CDR and NCBI by 2.0 (for the third-best\n \nby 2.7) percentage points, whereas for the best non-DL approach by Lou et al.\n \n, this difference amounts to remarkable 4.1 percentage points. This hints at a strong dependence of the results of the same system set-up on the specific corpus these results have been worked out and, thus, limits generalizability. On the other hand, corpora obviously cannot be blamed for intrinsic analytical hardness since cross-rankings occurs: the system by Lee et al.\n \ngets the over-all highest F\n \nscore for NCBI but underperforms for BC5CDR, whereas for the tagger used by Sachan et al.\n \nthe ranking is reversed\u2014their system performs better on BC5CDR than on NCBI (differences are in the range of 2 percentage points). The most stable system in this respect is the one by Zhao et al.\n \n. Finally, the distance between the best- and second-best-performing DL systems (\n \nand\n \n, respectively) and their best non-DL counterpart\n \namounts to 7.6 percentage points (for NCBI) and 3.1 percentage points (for BC5CDR), respectively.\n \n\n\n#### 3.1.2 Medication \n  \n\nThe second major medical named entity type we here discuss is related to medication information. NER complexity is increased for this task since it is split into several subtasks, including the recognition of drug names (\n  Drug  \n), frequency (\n  Dr-Freq  \n) and (manner or) route of drug administration (\n  Dr-Route  \n), dosage (\n  Dr-Dose  \n), duration of administration (\n  Dr-Dur  \n), and adverse drug events (\n  Dr-ADE  \n). These subtypes are highly relevant in the context of medication information and are backed up by an international standard, the HL7 Fast Healthcare Interoperability Resources (FHIR)\n \n.\n \nand\n \nprovide an overview of the SOTA on this topic.\n \n   \nMedical Named Entity Recognition: Drugs. Benchmark Datasets: n2c2\n \n; i2b2 2009\n \n; MADE 1.0\n \n; DDI\n \n.\n       \nMedical Named Entity Recognition: Medication Attributes. Benchmark Datasets: n2c2\n \n; i2b2 2009\n \n; MADE 1.0\n \n; DDI\n \n.\n    \n\nFor medication information, four gold standards had a great impact on the field in the past years. The most recent one came out of the 2018 n2c2 Shared Task on Adverse Drug Events and Medication Extraction in Electronic Health Records\n \n, a successor of the 2009 i2b2 Medication Challenge\n \n, now with a focus on Adverse Drug Events (ADEs). It includes 505 discharge summaries (303 in the training set and 202 in the test set), which originate from the MIMIC-III clinical care database\n \n. The corpus contains nine types of clinical concepts (including drug name), eight attributes (reason, ADE, frequency, strength, duration, route, form, and dosage \u2013 from which we chose five for comparison), and 83,869 concept annotations. Relations between drugs and the eight attributes were also annotated and summed up to 59,810 relation annotations (see Section 3.2.1). The third corpus, MADE 1.0\n \n, formed the basis for the 2018 Challenge for Extracting Medication, Indication, and Adverse Drug Events (ADEs) from Electronic Health Record (EHR) Notes and consists of 1,092 de-identified EHR notes from 21 cancer patients. Each note was annotated with medication information (drug name, dosage, route, frequency, duration), ADEs, indication (symptom as reason for drug administration), other signs and symptoms, severity (of disease/symptom), and relations among those entities, resulting in 79,000 mention annotations. Finally, the DDI corpus\n \n, originally developed for the Drug-Drug Interaction (DDI) Extraction 2013 Challenge\n \n, is composed of 792 texts selected from the (semi-structured) DrugBank database\n \nand other 233 (unstructured) MEDLINE abstracts, summing up 1,025 documents. This fine-grained corpus has been annotated with a total of 18,502 pharmacological substances and 5,028 drug-drug interactions\n \n. Hence, the medication NER task not only comes with a higher entity type complexity but also with text genres different from the disease recognition task\u2014while the former puts emphasis on clinical reports, the latter focuses on scholarly writing.\n \n\n\nExcept for route and ADE, all top scores for NER were achieved on the n2c2 corpus. For drug names, the current SOTA exceeds 95% F\n \nscore established by Wei et al.\n \n. As to the subtypes, their system also compares favorably to alternative architectures by a large F\n \nmargin ranging from 8.6 percentage points (for duration) down to 1.0 (for drug name). For route, the distance to the best system is marginal (around 1 percentage point)\n \n, whereas for ADE it is huge (more than 10 percentage points, a strong outlier). Overall, frequency, route, and dosage recognition reach outstanding F\n \nscores in the range of 95 up to 97%, while for duration information top F\n \nscores drop remarkably by at least 10 to 20 percentage points. Still, the recognition of ADEs seems to be the hardest task, with the best system by Wunnava et al.\n \npeaking at around 64% F\n \non MADE 1.0 data (here the top performing system by Wei et al.\n \nplummets down to 53% F\n \n). Interestingly, ADEs are verbally the least constrained type of natural language utterance compared with all the other entity types considered here.\n \n\n\nIn terms of DL methodology, BiLSTM-CRFs are the dominating approach. Yet, the type of embeddings used by different DL systems varies a lot ranging from pre-trained Word2vec embeddings and those self-trained on MIMIC-III (for the top performers) to GloVe embeddings pre-trained on CommonCrawl, Wikipedia, EHR notes, and PubMed. There seems to be no generalizable winner for either choice of embeddings given the current state of evaluations, but self-training on medical raw data, such as MIMIC-III, challenge data sets, or, more advisable, using the now available BioSentVec\n \nand BlueBERT\n \nembeddings pre-trained on MIMIC-III, might be advantageous.\n \n\n\nStudies in which the same system configuration was tested on different corpora are still lacking so that corpus effects are unknown (unlike for diseases; see\n \n). Yet, there is one interesting though not so surprising observation: Unanue et al.\n \nexplored the two slices of the DDI corpus, with a span of F\n \nscores of more than 16 percentage points. This obviously witnesses the influence of\n  a priori  \n(lack of) structure\u2014DrugBank data is considerably more structured than MEDLINE free texts and, thus, the former gets much higher scores than the latter.\n \n\nComparing DL approaches vs. non-DL ones (a CRF architecture) on the same corpus (MADE 1.0), we found that for the core entity type (Drug), the recognition performance differs by almost 3 percentage points, for frequency, route and dose marginally by less than 1, yet for duration and ADE it amounts to roughly 5 and 12 percentage points, respectively\u2014consistently in favor of Deep Neural Networks (DNNs). \n\n\n\n### 3.2 Relation Extraction \n  \nOnce named entities have been identified, a follow-up question emerges: does some sort of semantic relation hold among these entities? We surveyed this Relation Extraction (REX) task with reference to results that have been achieved for information related to medication attributes and drug-drug interaction. \n\n#### 3.2.1 Medication-Attribute Relations \n  \n\nIn Section 3.1.2, we already dealt with single named entity types typically associated with medication information, namely drug names and administration frequency, duration, dosage, route, and ADE, yet in isolated form only. In this subsection, we are concerned with making the close associative ties between\n  Drugs  \nand typical conceptual attributes, such as\n  Frequency  \n,\n  Duration  \n,\n  Dosage  \n,\n  Route  \n,\n  ADE  \n, and\n  Reason  \n(for prescription), explicit. Hence, the recognition of the respective named entity types (Drugs,\n  Dr-Freq  \n,\n  Dr-Dur  \n,\n  Dr-Dose  \n,\n  Dr-Route  \n,\n  Dr-ADE  \n, and\n  Dr-Reason  \n) turns out to be a good starting point for solving this REX task. Not surprisingly, the benchmarks for this task are a subset of the ones in\n \nand\n \ndepicting the results for medication-related NER.\n \nprovides an overview of the experimental results for finding medication-attribute relations in medical, in effect, clinical, documents.\n \n   \nMedical Relation Extraction: Medication-Attribute Relations (including ADEs). Benchmark Datasets: n2c2\n \n; MADE 1.0\n \n.\n    \n\nThe overall results from medication-focused NER are mostly confirmed for the REX task. The n2c2 corpus is the reference dataset for top performance. The group who achieved top F\n \nscores for the medication NER problem also performed best for the medication-attribute REX task\n \n, with extraordinary figures for\n  Frequency  \n,\n  Route  \n, and\n  Dosage  \nrelations (in the upper 98% F\n \nrange), a superior one for the\n  Duration  \nrelation (93% F\n \n), and good ones on the (hard to deal with)\n  Adverse  \nand\n  Reason  \nrelations (85% F\n \n). Still, the distances to the second-best system for the same corpus (n2c2) are not so pronounced in most cases, ranging by 1 percentage point (for\n  Frequency  \n,\n  Route  \n,\n  Dosage,  \nand\n  Duration  \n), yet increased up to 3 (for\n  Adverse  \n) and 7 (for\n  Reason  \n) percentage points.\n \n\n\nFor the MADE 1.0 corpus, a similar picture emerges. From a lower offset (typically around 3 F\n \npercentage points compared with n2c2), differences between the best and second-best systems were on the order of (negligible) 1 percentage point for\n  Frequency  \n,\n  Route,  \nand\n  Dosage  \n, yet increased by roughly 3, 5, and 7 percentage points for\n  Reason  \n,\n  Duration  \n, and\n  Adverse events  \n, respectively. Yet, in 4 out of 6 cases (\n  Frequency  \n,\n  Dosage  \n,\n  Duration  \n, and\n  Adverse events  \n) non-DL systems (CRFs, SVMs) outperformed their DL counterparts with small margins (in the range of (again, negligible) 1 percentage point) for\n  Frequency  \nand\n  Dosage  \n, yet with higher ones for\n  Duration  \nand\n  Adverse events  \n(5 and 7 percentage points, respectively). In cases where the DL approach ranked higher than a non-DL one, differences ranged between 1 and 3 percentage points (for\n  Route  \nand\n  Reason  \n, respectively). Thus, the MADE 1.0 corpus constitutes a benchmark where well-engineered standard ML classifiers can still play a competitive role. However, we did not find this pattern of partial supremacy of non-DL approaches for the n2c2 benchmark.\n \n\n\nThe top performers for the medication attribute REX task\n \nemployed a joint learning approach based on CNN-RNN (thus diverging from the most successful architectures for medication NER; see\n \nand\n \n) and rule-based post-processing that outperformed a simple CNN-RNN. Summarizing, the CNN-RNN approach seems more favorable than an (attention-based) BiLSTM, with preferences for self-trained in-domain embeddings.\n \n\n\n#### 3.2.2 Drug-Drug Interaction \n  \n\nThe second type of medication-focused relation we consider here are drug-drug interactions as featured in the DDI challenge (for surveys on the impact of DL on recent research on drug-drug interactions, cf.\n \n, for a survey on drug-drug interaction combining progress in data and text mining from EHRs, scientific papers, and clinical reports but lacking in-depth coverage of DL methods, cf.\n \n, for the NLP-focused recognition of ADEs also lacking awareness of DL contributions to this topic, cf.\n \n). Four main types of relations between drugs are considered: pharmacokinetic\n  Mechanism  \n, drug\n  Effect  \n, recommendation or\n  Advice  \nregarding a drug interaction, and\n  Interaction  \nbetween drugs without providing any additional information. Overall, the DDI corpus on which these evaluations were run is divided into 730 documents taken from DrugBank and 175 abstracts from MEDLINE and contains 4,999 relation annotations (4,020 train, 979 test).\n \n\n\nRecognition rates for these relations (cf.\n \n) are considerably lower than for the medication-related attributes when linked to drugs (cf.\n \n). The best systems peak at 85% F\n \nscore for\n  Advice  \n(a distance of more than 13 percentage points to the top recognition results for medication-attributes), they slip to 78%\n \nand 77% for\n  Mechanism  \nand\n  Effect  \n, respectively, and plummet to 59% for\n  Interaction  \n. Differences between the first and second-ranked systems are typically small, yet become larger on subsequent ranks (roughly between 3 to 4 percentage points relative to the top-ranked system). As with medication attributes, drug-drug interactions can also be recognized in a competitive way by CNN-RNN architectures, but attention-based LSTMs perform also considerably well. Again, self-trained embeddings using in-domain corpora seem to be advantageous for this relation class. Reflecting the drop in performance, one may conclude that drug-drug interactions constitute a markedly harder task than the conceptually much closer medication-attribute relations.\n \n   \nMedical Relation Extraction: Drug-Drug Interaction. Benchmark Dataset: DDI\n \n.\n    \n\nFinally,\n \nmost drastically supports our claim that DL approaches outperform non-DL ones. The difference between both approaches amounts to 5 percentage points for\n  Mechanism  \n, 7 for\n  Effect  \nand\n  Interaction  \n, and 8 for\n  Advice  \n.\n \n\n\n\n\n## 4 Conclusions \n  \nWe have presented various forms of empirical evidence that (with one exception only) Deep Learning-based neural networks outperformed non-DL, feature engineered, approaches for several information extraction tasks. However, despite their success, Deep Neural Networks and their embedding models have their shortcomings as well. \n\n\nOne of the most problematic issues is their dependence on huge amounts of training data: SOTA embedding models are currently trained on hundreds of billions of tokens\n \n. This magnitude of data volume is out of reach for any training effort in the medical/clinical domain\n \n. Also, embeddings are very vulnerable to malicious attacks or adversarial examples\u2014small changes at the input level may result in severe misclassification\n \n. Another well-known problem relates to the instability of word embeddings. Word embeddings depend on their random initialization and the processing order of the underlying examples and therefore they do not necessarily converge on exactly the same embeddings even after several thousands of training iterations\n \n. Finally, although DL is celebrated for not requiring manual feature engineering, the effects of proper hyperparameter tuning on DNNs\n \nremain an issue for DL\n \n. Apart from these intrinsic problems, Kalyan and Sangeetha\n \nand Khattak et al.\n \nrefer to extrinsic drawbacks of neural networks, such as opaque encodings (resulting in lacking interpretability) or limited transferability of large models (hindering knowledge distillation for smaller models).\n \n\n\nStill, the sparsity of corpora and special linguistic phenomena of the medical (clinical) sublanguage(s) create intrinsic problems for data-greedy DL approaches that have to be overcome by special learning strategies for neural systems, such as transfer learning or domain adaptation. Research on adapting general language models to medical language constraints is just in its beginning. Yet, there is no simple solution to this problem. Wang et al.\n \nevaluated Word2vec embeddings trained on private clinical notes, PMC, Wikipedia, and the Google News corpus both qualitatively and quantitatively and showed that the ones trained on Electronic Health Record data performed better on most of the tested scenarios. However, they also found that word embeddings trained on biomedical domain corpora do not necessarily have better performance than those trained on general domain corpora for any downstream biomedical NLP task (other experimental evidence of the effects of in- and out-of-domain corpora and further parameters, such as corpus size, on word embedding performance is reported by Lai et al.,\n \n).\n \n\nWhile this survey focused on the application domain of medical IE to demonstrate the outstanding role of DL for medical Natural Language Processing, one might be tempted to generalize this trend to other applications as well. There is, indeed, plenty of evidence in the literature that other application fields, such as question answering (and the closely related area of machine reading), summarization, machine translation, and speech processing, reveal the same pattern. However, for text categorization (in the sense of mapping free text to some pre-defined medical category system, such as ICD, SNOMED, or MeSH) this preference is less obvious, since traditional Machine Learning or rule-based models still play an important role here and, more often than for the IE application scenario, show competitive performance against DL approaches. Whether this exception will persist or will be swept away by future research remains an open issue. \n\n \n", "metadata": {"pmcid": 7442512, "text_md5": "25090d804d41d85ffd91a7dc377b64d6", "field_positions": {"authors": [0, 29], "journal": [30, 46], "publication_year": [48, 52], "title": [63, 121], "keywords": [135, 261], "abstract": [274, 2084], "body": [2093, 33075]}, "batch": 1, "pmid": 32823318, "doi": "10.1055/s-0040-1702001", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7442512", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7442512"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7442512\">7442512</a>", "list_title": "PMC7442512  Medical Information Extraction in the Age of Deep Learning"}
{"text": "Biziukova, Nadezhda and Tarasova, Olga and Ivanov, Sergey and Poroikov, Vladimir\nFront Genet, 2020\n\n# Title\n\nAutomated Extraction of Information From Texts of Scientific Publications: Insights Into HIV Treatment Strategies\n\n# Keywords\n\ntext mining\ndata mining\nnamed entity recognition\nNER\nvirus-host interactions\nHIV\nviremic control\n\n\n# Abstract\n \nText analysis can help to identify named entities (NEs) of small molecules, proteins, and genes. Such data are very important for the analysis of molecular mechanisms of disease progression and development of new strategies for the treatment of various diseases and pathological conditions. The texts of publications represent a primary source of information, which is especially important to collect the data of the highest quality due to the immediate obtaining information, in comparison with databases. In our study, we aimed at the development and testing of an approach to the named entity recognition in the abstracts of publications. More specifically, we have developed and tested an algorithm based on the conditional random fields, which provides recognition of NEs of (i) genes and proteins and (ii) chemicals. Careful selection of abstracts strictly related to the subject of interest leads to the possibility of extracting the NEs strongly associated with the subject. To test the applicability of our approach, we have applied it for the extraction of (i) potential HIV inhibitors and (ii) a set of proteins and genes potentially responsible for viremic control in HIV-positive patients. The computational experiments performed provide the estimations of evaluating the accuracy of recognition of chemical NEs and proteins (genes). The precision of the chemical NEs recognition is over 0.91; recall is 0.86, and the F1-score (harmonic mean of precision and recall) is 0.89; the precision of recognition of proteins and genes names is over 0.86; recall is 0.83; while F1-score is above 0.85. Evaluation of the algorithm on two case studies related to HIV treatment confirms our suggestion about the possibility of extracting the NEs strongly relevant to (i) HIV inhibitors and (ii) a group of patients i.e., the group of HIV-positive individuals with an ability to maintain an undetectable HIV-1 viral load overtime in the absence of antiretroviral therapy. Analysis of the results obtained provides insights into the function of proteins that can be responsible for viremic control. Our study demonstrated the applicability of the developed approach for the extraction of useful data on HIV treatment. \n \n\n# Body\n \n## Introduction \n  \nScientific publications represent the main source of knowledge for researchers in different fields of biology and medicine. Besides, the more pressing the problem for humanity is, the more articles devoted to this problem can be found in the repositories of scientific publications. The extraction of records from scientific publications provides the opportunity to analyze the information derived from primary sources; therefore, such an approach helps to obtain the most contemporary information (Cash,  ; Tarasova et al.,  ,  ; Saik et al.,  ). Currently, text-mining technologies aimed at rapid automated extraction of specific information are under rigorous development. \n\nAnalysis of interactions between named entities (NEs) representing proteins, genes, and chemical compounds can help investigate the particular molecular mechanisms of disease progression, the effect of drugs, and reveal the drug-drug interactions important for efficacy of therapy (Chen et al.,  ; Tannenbaum and Sheehan,  ; Lim et al.,  ; Szklarczyk et al.,  ). \n\nIdentification of associations between NEs in the texts of scientific publications includes two steps: (i) extraction of named entities from the texts, and (ii) recognition of associations. This is the focus of the Named Entity Recognition (NER) methods. There are two main groups of approaches used for NER: (i) based on rules and dictionaries and (ii) based on machine learning methods. The main disadvantage of rule and dictionary-based algorithms is the inability to extract information about entities not included in dictionaries. Another drawback is the requirements for the allocation of memory for storing dictionaries. \n\nMachine learning methods require sets of texts, in which the names of proteins, genes, chemical compounds, and so on are labeled by an expert or a group of experts. Then, using such texts as a training set, it is possible to adjust the algorithm to recognize NEs in a large number of articles. Finally, it is possible to identify relationships between the NEs extracted. Machine learning methods have advantages over dictionary-based methods because they provide the recognition of new NEs not included in dictionaries and, therefore, are the best option for the careful extraction of information. At present, many novel text corpora are constantly developing for the purposes of the scientific community and provide the possibility to extract information about a variety of NEs: genes, proteins and chemicals names, symptoms and syndromes of the diseases, side effects and toxicity of drugs, revealed during the clinical trials or as a result of medical studies, cases studies, etc. \n\nThere are methods aimed at NER that have been developing during the last years (Kaewphan et al.,  ; Korvigo et al.,  ; Hemati and Mehler,  ; Hong and Lee,  ; Huang et al.,  ; Kilicoglu et al.,  ). Most of them are based on algorithms for NER related either to chemicals or biological objects. In this study, we aim to develop and test an algorithm for the extraction of named entities of genes/proteins and chemical compounds and identify associations between them. Our method of NER is based on the conditional random fields and uses a set of originally developed word features that allow for context consideration. Thus, we suggest selecting a set of publications strictly relevant to the subject and extracting a set of chemical NEs, proteins, and genes to derive the NEs associated with one another. After that, their functions in a particular molecular mechanism of the disease can be analyzed. \n\nHuman immunodeficiency virus (HIV) still remains one of the challenges for humanity (Rojas-Celis et al.,  ; Tarasova et al.,  ). The number of new HIV cases per year reached 1.7 million (WHO, HIV incidence)  while is, Number of People (All Ages) Living With HIV  is around 38 million (WHO). Antiretroviral therapy (ART) helps to reduce viral load and disease progression, but antiretroviral medicine should be taken by a patient for term of life. The risk of HIV drug resistance and side effects of antiretroviral medicines decrease the effectiveness of ART (Iyidogan and Anderson,  ; Tarasova and Poroikov,  ). At the same time, an effective HIV vaccine does not exist (Ventura,  ). \n\nTaking into account the importance of the problem, we consider two main approaches for the HIV/AIDS treatment as case studies: (1) the usage of antiretroviral drugs and (2) studies of the ways of HIV/AIDS development in different groups of patients and attempts to affect the key proteins of the pathways providing a long period of disease progression. The mechanisms identified can be used for the development of novel strategies of HIV treatment and vaccine development. We have validated and tested our algorithm on the tasks of identification of (i) chemicals that can be considered as HIV inhibitors and (ii) groups of proteins that may be important for the different velocity of HIV/AIDS. To reach these purposes, we used the abstracts of publications relevant to HIV/AIDS treatment for case studies of HIV inhibition and HIV viremic control. More specifically, the algorithm developed is aimed at extracting (a) the names of HIV reverse transcriptase (RT) inhibitors and (b) the protein (gene) names described in articles relevant to the studies of HIV elite controllers, i.e., the group of HIV-positive individuals with an ability to maintain an undetectable HIV-1 viral load overtime in the absence of antiretroviral therapy. \n\n\n## Materials and Methods \n  \nThe extraction of the NE names includes several stages. First, we collected text corpora and made their preprocessing. Second, we developed an algorithm of NER and its parameter optimization. Third, we carried out validation and testing of an algorithm and analysis of the information obtained. \n\nTo extract NEs, we used annotated text corpora and applied an algorithm based on conditional random fields (CRF). Text annotation in the corpus implies an indication of NE position inside the text. \n\n### Text Corpora \n  \nWe used the CHEMDNER  (Krallinger et al.,  ) and ChemProt  as annotated corpora. Both corpora consist of freely available abstracts of articles. \n\nCHEMDNER consists of three sets (training, evaluation, and development sets) and includes 10,000 abstracts. It has been developed for the purpose of chemical NER (Krallinger et al.,  ). Annotations include the position of NE in the text and also the NE type: ABBREVIATION (as ATP), FAMILY (as steroid hormones), FORMULA [as S or C (sulfur or carbon)], IDENTIFIER (as GRN-529), MULTIPLE (as nucleoside tri- and di-phosphates), SYSTEMATIC (as sphingosine-1-phosphate), TRIVIAL (as progesterone). \n\nChemProt also consists of three sets\u2014training, development, and test\u2014and includes 2,482 abstracts in total. It also includes annotations of genes, proteins, and chemical compounds. ChemProt includes the following types of NEs: (i) GENE-Y is for proteins/genes that can be normalized or associated with a biological database identifier, (ii) GENE-N is for proteins/genes that cannot be normalized or associated with a biological database identifier and (iii) CHEMICAL is for chemical compounds . The proteins and genes are not considered in ChemProt separately, but search in the databases containing data on genes and proteins can help identify protein names and names of genes. \n\n\n### Text Preprocessing \n  \nAlgorithms for entity recognition in texts require tokenization. It is the process of splitting the whole text into elementary text units. As a result of tokenization, the text is presented as a set of tokens. Words, symbols, numbers, can be used as tokens. Thus, we used this method for all symbols and spaces implemented in the \u201cwordpunct_tokenize\u201d function of the NLTK Python library. \n\nTokens can be divided into groups: those that belong and do not belong to NEs. If an NE originally consists of two words (for example, reverse transcriptase) or includes any symbols (for example, sphingosine-1-phosphate), then after tokenization it will be presented by a list or array of tokens rather than a simple string. We used the labeling system SOBIE (Rockt\u00e4schel et al.,  ; Batista-Navarro et al.,  ; Dai et al.,  ; Leaman et al.,  ) to indicate the position of the term associated with a particular token. \n\nSOBIE is an abbreviation for tags: \u201cS\u201d (Single)\u2014if a token belongs to NE and NE consists of one token, \u201cO\u201d (Out)\u2014if a word doesn't belong to NE, \u201cB\u201d (Begin) is a label of the first word of composite NE if NE consists of two or more words, \u201cE\u201d (End)\u2014is a label of the last word of composite NE, if NE consists of two or more words and I (Inside)\u2014is a label of words belong to NE that are between \u201cB\u201d and \u201cE\u201d if NE consists of three or more words. The example of labeling text by SOBIE is presented in  . \n\nTokenized corpora texts with a set of SOBIE labels were placed to the database managed by PostgreSQL DBMS. The schema of the database is provided in  . \n\nWe compiled a features set for each word to train a model. For NER, it is essential to take into account the context, which means the words should follow one another in order without mixing. In our model, each word W is characterized by its features along with the features of one word before and after W. Each token was described with a triple set of features ( ) - the features of a particular token and the features of two tokens: one before and one after the considered one. \n  \nThe set of features used for CRF. \n  \n Features which do not belong to the standard set are highlighted by gray color  . \n  \nWe have compiled a list of non-specific terms and used the belonging of a token to one of them as a feature. Non-specific terms are general words that can indicate the presence of the term in the proximity to NEs of a protein (gene) or a chemical compound in the text. For example, such terms may include the words \u201cinhibit,\u201d \u201cchemical\u201d for the names of chemical compounds and \u201ctarget,\u201d \u201cgenes\u201d for the names of proteins and genes. The list of all these non-specific terms for NEs of proteins/genes and chemical compounds is presented in  . We used marks \u201cC\u201d for non-specific terms used for chemical compounds and mark \u201cG\u201d \u2013 for proteins and genes. If a non-specific term was used in both lists, it received both labels: \u201cC\u201d and \u201cG.\u201d \n\nAll the features were obtained using scripts prepared using Python 3.7. The features were represented as dictionaries to be able to access each feature individually by its keyword. Finally, the data were represented in the format that is shown in  . \n  \nScheme for the organization of data that are input for the models. \n  \n\n### Algorithm Realization \n  \nAs the next step, we built a model. For our approach, we used an algorithm named CRF in realization using Python 3.7 and SciKitLearn library. This library provides a lot of algorithms using machine learning, different metrics, etc. The CRF algorithm allows us to take the context of a phrase into account. We suggest that the set of features developed can help to improve the recognition of context near the NEs. We used the hyperparameter optimization function, which is included in the SciKitLearn to achieve the highest accuracy of the model. \n\nWe built several models. The first model (i) was built to provide the recognition labels of chemical entity mentions. The second model (ii) aimed at recognizing names of proteins and genes. The third model (iii) allowed recognition of types (ABBREVIATION, FORMULA, IDENTIFIER etc.) of chemical entity mentions. Based on the models (i\u2013iii), we built the fourth model (iv) to extract chemical compounds and proteins/genes. This model combines algorithm for recognizing the names of chemical compounds and proteins and identifying the types of chemical compounds. Thus, the models (i) and (iii) were built based on CHEMDNER as a training corpus. The model (ii) was built using ChemProt. And the model (iv) was built using both CHEMDNER and ChemProt. Initially, models were tested using 5-fold cross-validation (Stone,  ). \n\nWe used precision, recall, and F1-score to assess the quality of model recognition. \n\nPrecision is the proportion of positive identifications that was actually correct. It is calculated as the ratio of the number of true positive identification to the sum of true positive and false-positive identifications (1). \n\nRecall is the proportion of actual positive NE mentions that were identified correctly. It is calculated as the ratio of the number of true positive identifications to the sum of true positive and false negative decisions. \n\nF1-score is the harmonic mean of precision and recall. \n\nThe test set was formed based on the idea that if an article is strictly relevant to reverse transcriptase inhibition and our algorithm is able to recognize the NE of a protein and a name of a chemical compound or drug then there is a high probability that the chemical named entity identified is the name of HIV reverse transcriptase inhibitor. \n\nWe used the set of 148 publications abstracts collected from NCBI PubMed. We used the workflow developed earlier (Tarasova et al.,  ). In this workflow we were focused on the publications that included the description of HIV inhibitors and included the details of biological experiments used for their testing. Using the Python script, we automatically extracted over 15,000 abstracts of articles from PubMed using the query \u201cHIV AND reverse transcriptase AND inhibitors.\u201d Then, using the Lingpipe 4.1.2 tool (Carpenter,  ), we selected a set of over 1,000 abstracts strictly relevant to the development, synthesis, and testing of HIV RT inhibitors. We used Lingpipe 4.1.2 since it provides the possibility of building the models of text classification into the classes according to the content, and the selection of the texts strictly relevant to the particular subject. Earlier we used Lingpipe 4.1.2 to perform selection of the abstracts of publications, which included the description of HIV-1 reverse transcriptase inhibitors including the details of their biological testing (Tarasova et al.,  ) with mean accuracy over 0.83. We assume that the data on the biological experiment details in the publication text can help to confirm its relevance to the inhibition of HIV-1 RT. \n\nAfter that, we manually selected 148 of them, which consisted the description of HIV-1 reverse transcriptase inhibitors and the details of their biological testing. This set of abstracts was used to evaluate the accuracy of extracting the names of reverse transcriptase inhibitors. \n\nThe texts of abstracts can include the NEs of such chemical compounds as ATP, various ions, DNA, etc. If we want to extract the small drug-like compounds mainly, we would like to filter out ions and biological molecules remaining only drug-like compounds with molecular weight ranged from 300 to 700 (Da). So, to filter them out, we have additionally introduced filters for the classes of chemical compounds obtained based on the results of recognition using CRF. The classes FORMULA and FAMILY were excluded because the class FORMULA is mainly represented by ions, and the class FAMILY contains groups of chemical compounds that do not include the specific names of small molecule inhibitors of HIV RT. \n\nOnce we have developed an algorithm providing recognition of proteins (genes) names, we also tested the applicability of our algorithm to the extraction of proteins and genes responsible for the slow disease progressions of HIV-positive patients. To perform this analysis, we have collected a set of abstracts of publications from NCBI PubMed and NCBI PubMed Central (PMC) databases. We collected the abstracts strictly relevant to (1) the HIV elite controllers (ECs), a group of patients who do not progress into HIV/AIDS for years in the absence of antiretroviral therapy, and (2) the whole cohort of HIV-positive patients. The first set related to HIV elite controllers was obtained based on the query \u201c  (HIV[Title/Abstract] OR \u201chuman immunodeficiency virus\u201d[Title/Abstract] OR \u201cHIV\u201d[Mesh] OR AIDS[Title/Abstract] OR \u201cacquired immunodeficiency syndrome\u201d[Title/Abstract] OR \u201cAcquired Immunodeficiency Syndrome\u201d[Mesh]) AND (\u201celite control  \u201d  [Title/Abstract] OR \u201cElite suppress  \u201d  [Title/Abstract])  .\u201d We obtained over 840 abstracts strictly relevant to the HIV elite controllers. The second set of abstracts was collected using the query \u201c  HIV positive AND HIV/AIDS\u201d   and included over 30 thousand of abstracts. We excluded from the group (1) the abstracts belonging to the group (2) because the group (2) may include HIV-positives ECs and we are interested in the differences between the protein profiles of groups (1, ECs) and (2, HIV-positives excluding ECs) mainly. The abstracts of groups (1) and (2) were processed using the NER algorithm developed, and names of proteins (genes) were extracted from them. Then, we excluded the proteins found in the abstracts of the group (2) from a list of proteins obtained for the group (1). We suggest that it provides the opportunity to compile a list of proteins responsible for the slow HIV/AIDS disease progression. \n\nThe results of our computational experiments are described and discussed below. \n\n\n\n## Results \n  \n### Extraction of Chemical Named Entities \n  \nIn our study, we performed several computational experiments using CHEMDNER and ChemProt corpora and the texts related to (i) the inhibition of HIV-1 reverse transcriptase and (ii) HIV ECs. Earlier (Tarasova et al.,  ), we collected a set of papers consisted mainly of those relevant to the development and testing of HIV reverse transcriptase (RT) inhibitors, the \u201cHIV-RT-inhibitors corpus.\u201d In the current study, we curated this corpus carefully and enlarged it with publications strictly relevant to the inhibition of HIV RT. The chemical compounds extracted were supposed to be inhibitors of reverse transcriptase based on a specific selection of texts for the test set described below. \n\nWe built the models for chemical and protein/gene NER based on CHEMDNER and ChemProt corpora, respectively, and calculated their accuracy using five-fold cross-validation. We performed several computational experiments for predicting SOBIE labels of belonging a token to a chemical and protein/gene named entity (NE) as well as prediction of certain types of chemical entity mention (ABBREVIATION, FORMULA, IDENTIFIER etc.). We evaluated the best way of NER using the features of text developed. The detailed description of text corpora and our computational experiments are provided in the section Materials and Methods. \n\nFirst, we built a model to predict SOBIE\u2014the labels for the parts of chemical NE: S - Single\u2014for NE that contains one token; B, E\u2014Begin, End\u2014as the labels for the first and the last token of NE, respectively, if NE contains at least two tokens; I\u2014Inside\u2014as a label for the tokens that located between B and E, if NE includes three or more words; O\u2014Out\u2014as a label for the words that does not belong to NE. The results of five-fold cross-validation for annotated corpora CHEMDNER and ChemProt are represented in  . \n  \nPrecision, recall, and F1-score for model that predicts SOBIE for chemical entity mentions. \n  \n The average values of precision, recall and F1-score are given in bold  . \n  \n displays that the recognition of chemical entity mentions occurs with reasonable accuracy (F1-score is 0.89). We carried out an experiment combining the CHEMDNER and ChemProt to recognize the names of chemical compounds. The volume of training set increased by more than two thousand articles (more than 25%). However, this did not lead to a significant increase in accuracy. \n\nWe assumed that the next task might require filtering the names of chemical compounds by their type. Thus, for example, the texts may contain the names of ions (Ca , Mn ), which can be filtered out in case if we are focused on the small drug-like compounds only with molecular weight range from 300 to 700 (Da). Based on this conclusion, we built a model identifying the types of recognized names of chemical compounds. Accuracy was assessed with five-fold cross-validation and is presented in  . \n  \nPrecision, recall, and F1-score for predicting types of chemical entity mentions. \n  \n The types of chemical compounds are explained in Supplementary Material ( ). The average values of precision, recall and F1-score are given in bold  . \n  \nFrom the values of precision, recall, and F1-score displayed in  , one can conclude that filtering by the types of NEs of chemical compound can be used for the selection of the particular type of chemical NE of interest. \n\nThe algorithms obtained for recognition of the names of chemical compounds and their types and names of proteins (genes) were then combined and applied on a test set of texts devoted to inhibition of HIV reverse transcriptase. The prediction of genes and proteins names was obtained using the SOBIE labels only. The principle of the combined algorithm was the sequential recognition of the names of chemical compounds, their types, and then the names of proteins. \n\nAs a result, the names of proteins and chemical compounds were obtained. The rule for extraction of NEs of HIV RT inhibitors was the presence in the abstract of at least one recognized protein name and one named entity of chemical compound. So, in addition to the names of potential reverse transcriptase inhibitors, it is possible to extract NEs of chemical compounds that can interact with two HIV proteins, for instance, reverse transcriptase and protease. \n\nEvaluation of the results was carried out so that if the recognized chemical compound is not an actual inhibitor of reverse transcriptase, then the number of false positives (FP) increased by one. If an inhibitor was encountered in the text and was not recognized, then the number of false negatives (FN) also increased by one. \n\nAlso, we evaluated the accuracy of the recognition of all chemical compounds in the text. The results are shown in  . \n  \nPrecision, recall and F1-score obtained when testing the algorithm on a sample of texts devoted to the inhibition of HIV reverse transcriptase. \n  \nBased on the value of the recall metric, we can conclude that, in general, we managed to extract almost all names of reverse transcriptase inhibitors and chemical compounds from the test set. However, there are wrong recognized inhibitors and chemical compounds too. This is in agreement with the overall value of accuracy of NEs recognition in the text according to the results of 5-fold cross-validation. As we have mentioned earlier, we applied filters on classes of chemical compounds to reduce the number of false-positive results. This step allowed us to increase the precision of value to 0.85. We built automatic queries to PubChem database . For all recognized chemical named entities we obtained PubChem identifiers, if PubChem identifier was found. \n\nWe also tried to extract the names of chemical compounds from a set of texts dedicated to elite HIV/AIDS controllers. We assumed that if the texts contain proteins (genes) responsible for the non-progression of HIV/AIDS, then they can also indicate chemical compounds that slow down the progression of HIV/AIDS by influencing the protein (gene). Thus, using model (i), we recognized the names of chemical compounds in texts related to the non-progression of HIV/AIDS and then manually checked the presence of chemical compounds that slow down the progression of HIV/AIDS. Unfortunately, there are few texts of the set in which such chemical compounds are mentioned. We provide some examples of extracted chemical compounds in the Discussion section. \n\n\n### Extraction of the Protein and Gene Names \n  \nAs the next step, we aimed at testing the algorithm for the extraction of protein and gene names from the texts. We built CRF using SOBIE labels similar to the algorithm for the extraction of chemical named entities. The results of protein (gene) recognition based on ChemProt are provided in  . \n  \nPrecision, recall, and F1-score for model that predicts SOBIE for names of proteins/genes. \n  \n The average values of precision, recall and F1-score are given in bold  . \n  \nDespite the fact that the recognition of the named entities of genes and proteins is carried out with a slightly lower accuracy than the recognition of the NEs of chemical compounds, the prediction accuracy still remains reasonable. \n\nWe tested the algorithm developed for evaluation a performance of extraction of proteins responsible for HIV/AIDS control and non-progression. To obtain the results, we applied the developed algorithm and the model (ii) to the set of papers, relevant to (1) ECs and (2) the whole cohort of HIV-positive patients retrieved from NCBI PubMed and NCBI PMC databases. The number of proteins and genes extracted from the texts of group (1) and group (2) abstracts is given in  . The full list of proteins extracted for each of the groups represented in   is given in the  . \n  \nNumbers of proteins (genes) names associated with different velocity of HIV/AIDS progression. \n  \nAs   displays, there are different protein profiles of the names of genes and proteins extracted from the set of abstracts relevant to the ECs (group 1) and the overall group of HIV-positive patients. For further analysis, we automatically obtained the synonyms of protein (gene) names and UniProt  identifiers for each name of gene or protein extracted from group 1 of abstracts (i.e., articles relevant to ECs). We also identified the main functions of the proteins of this group. The interpretation of our results is given below in the Discussion section. \n\n\n\n## Discussion \n  \n### Comparison of the NER Algorithm With Earlier Developed Approaches \n  \nWe have compared the models obtained for NER of chemical compounds and proteins/genes with those developed earlier by other authors. Earlier NER approaches reached on average F1-score between 77.70 and 88.06% before post-processing (Campos et al.,  ; Khabsa and Giles,  ; Xu et al.,  ; Korvigo et al.,  ), and for recognizing proteins/genes. Taking into account the results provided, one can conclude that the accuracy of NER for our method is comparable with that of some methods developed earlier. We suggested modifying the text features that lead to an increase in the recognition accuracy: in particular, we expanded the list of non-specific terms used for the recognition of genes and proteins. \n\nWe tried to merge two corpora, CHEMDNER and ChemProt, to improve the accuracy of chemical compounds recognition. Despite the assumption that if we increase the number of examples to train the model, the accuracy may increase, this did not happen in our case. A detailed comparison of the precision, recall, and F1-score values for the recognition of chemical NE (i) based on CHEMDNER and (ii) merged CHEMDNER and ChemProt corpora and (ii) recognition of proteins and genes NE based on ChemProt corpus is provided in  . \n\nThe lower recognition accuracy of chemical compounds based on the combined corpus may be due to ChemProt was primarily aimed at finding relationships between proteins and genes. Therefore, chemical NEs in ChemProt are less common in the texts compared to CHEMDNER. \n\nThe algorithms obtained for recognizing the names of chemical compounds and proteins (genes) were examined on the test sets of abstracts related to (i) the inhibition of HIV reverse transcriptase (ii) the identification of proteins associated with HIV control and non-progression. \n\n\n### Extraction of Chemicals Names That Can Be Considered as Potential Medicines for HIV Treatment: A Case Study for HIV Reverse Transcriptase Inhibitors \n  \nFrom the texts relevant to the inhibition of HIV reverse transcriptase, we were able to extract inhibitors that actually exist and are currently used for the therapy. This allows us to conclude that using our method we can extract the names of chemical compounds considered as new inhibitors and can be used to treat HIV infection. The examples of the names of HIV reverse transcriptase inhibitors extracted from the texts of abstracts with their PubChem identifiers and their chemical structures are shown in  . \n\n\n### Identification of the Chemical Compounds Responsible for the Velocity of HIV/AIDS Progression: A Case Study for HIV Elite Controllers \n  \nMany of extracted compounds from elite controllers test set were parts of HAART or names of amino acids. But among all extracted compounds we were able to detect some that influenced HIV/AIDS progression. \n\nFor example, the article by Bermejo et al. ( ) describes the effect of the tyrosine kinase inhibitor dasatinib. During T-cell activation phosphorylation of SAMHD1 allows HIV infection. Dasatinib stopped SAMHD1 phosphorylation, which led to disruption of HIV reverse transcription. \n\nJoshi et al. ( ) reported the relationship between heat shock protein 90 (Hsp90) inhibitors and HIV transcription. It has been shown that administration of Hsp90 inhibitors tanespimycin [17- (allylamino)\u221217-demethoxygeldanamycin] and AUY922 durably prevented viral rebound in mice. \n\nAs was mentioned above (in Results section) there were not plenty recognized chemical compounds that may lead to HIV/AIDS non-progression. But some of them were extracted, it demonstrates the usefulness of the approach developed and the possibility of working out this direction in the future. \n\n\n### Identification of the Proteins and Genes Responsible for the Velocity of HIV/AIDS Progression: Case Study for HIV Elite Controllers \n  \nOnce we were able to extract the set of protein and gene names from the texts relevant to HIV/AIDS ECs, we aimed to automatically identify the main biological processes and functions associated with them based on Gene Ontology (Gene Ontology Consortium,  ) terms available from UniProt. \n\nAutomated queries to the UniProt database allow us to identify the belonging of a protein or a gene to either organism \u201chomo sapiens\u201d or a virus. There were some NEs associated with HIV, such as \u201cgag-pol protein\u201d or \u201cpol peptide.\u201d Also, we found that 11 names were not associated with any proteins, they represent false-positive results of our algorithm. Therefore, the automated verification using a database or a dictionary of proteins can help filter out the named entities that represent false positive results and therefore improve the recognition accuracy obtained using CRF (Song et al.,  ; Perera et al.,  ). It also helps select the names of protein belonging to the species of interest. We selected only proteins that were found in UniProt database and included the names of a protein extracted as one of the synonyms of names presented in UniProt. Therefore, some of proteins were filtered out because they were not associated with one unique record in UniProt. \n\nAs a result of automated processing of the files with gene/protein identifiers and their GO terms, we collected the most important biological processes associated with proteins extracted. They can be can be associated with HIV infection and can have an impact on the velocity of HIV/AIDS disease progression. We divided them into several groups according to the function most important for HIV-progression (see  ). \n  \nThe main functions of some proteins and genes found in the Uniprot database by the search using named entities extracted from abstracts of publications selected by their relevance to the description of HIV elite controllers. \n  \nFor some proteins, the names of which had been extracted from the texts of publications relevant to the studies of HIV ECs, we found direct associations of these proteins with the HIV progression (Taylor et al.,  ; Oleksyk et al.,  ; Marras et al.,  ; Slavov et al.,  ; Roy et al.,  ; Parodi et al.,  ; Hersberger et al.,  ; Wendel et al.,  ). We provide a few examples of the association between biological processes known for the proteins identified and their possible role in HIV disease progression. For instance, H. Fausther-Bovendo and co-authors reported that the increased expression of NKp44L was observed in CD4+ T cells of HIV-positive patients (Fausther-Bovendo et al.,  ); that leads in an increased sensitivity of NKp44LCD4 T cells to the NK lysis activity. The cd85j receptor (LIR-1) was extracted from abstracts and is found to be associated with negative regulation of CD8-positive T cell activation according to UniProt data ( ). Also, there are data on its role in the control of HIV-1 replication in autologous dendritic cells (Scott-Algara et al.,  ). Also, we found the studies aimed at the identification of the interactions between S100A9 protein (a calcium-binding protein of the S100 family) and cd85j receptor. In particular, it was shown that HIV-1 infection modulates S100A9 expression on the surface of the monocyte-derived dendritic cells. Interaction between S100A9 protein and cd85j receptor, in turn, can have an impact on the anti-HIV activity of human NK (natural killer) cells (Arnold et al.,  ). Vincent Arnold, and co-authors suppose that an exogenous peptide S100A9 can be considered as the potential ligand for the control HIV-1 replication by NK cells (Arnold et al.,  ). Therefore, we can identify the existing novel approaches for HIV infection control that can be useful for the development of novel strategies to cure HIV. It also can help to create new hypotheses about potential mechanisms of HIV control leading to the development of new approaches to HIV treatment. \n\nFor some other proteins, there was no direct evidence of their role in HIV infection control and progression. But the analysis of their functions led to understanding that the differences in the expression of these proteins in the CD4+ or CD8+ T cells and some other immune cells may be associated with the velocity of HIV/AIDS progression. For instance, since HIV-1 glycoprotein 41 (gp) 41 prefers to interact with the cell-surfaced human leukocyte elastase (Bristow et al.,  ), one can suggest that the low levels of HLE expression can slow down the dissemination of HIV particles and therefore have an important role in HIV/AIDS progression. \n\nThere are experiments that provide the insights into CD8+ T cell response associated with the function of carcinoembryonic antigen-related cell adhesion molecule 1 (Khairnar et al.,  ). Base on the experiments carried out with lymphocytic choriomeningitis virus it was shown that carcinoembryonic antigen-related cell adhesion molecule 1 is essential for activation of CD8+ T cells. However, such results should be considered with awareness and more experiments are needed to adopt these hypotheses to HIV-1 viremic control. \n\nBased on the text and data mining, we have earlier identified a set of proteins and discussed some molecular mechanisms shared by a novel coronavirus SARS-CoV-2 and HIV-1 (Tarasova et al.,  ). There were a few molecular pathways, including those related to immunology, autophagy, cell cycle regulation, shared by these two viruses if they infect humans. The present study is focused on the possibilities of text mining to extract data from the strictly relevant publications and investigate whether such an approach can address questions related to the aspect of biological studies. Our approach is based on a particular set of proteins that can be associated with the slow HIV/AIDS disease progression. \n\nThe two case studies aimed at the extraction of chemical names from the texts relevant to HIV reverse transcriptase inhibition, proteins and genes from the texts relevant to HIV control allow us to determine the advantages and disadvantages of text mining approaches to new information. The main advantage of text mining approaches is the possibility of covering the huge amount of textual data (Ruusmann and Maran,  ; Capuzzi et al.,  ,  ; Kandhro et al.,  ; Azam et al.,  ; Gambardella and di Bernardo,  ; Guin et al.,  ; Ivanisenko et al.,  ; Alves et al.,  ). Text mining approaches allow retrieving the most recent and important information about chemicals, proteins, and genes associated with HIV treatment including their tissue-specific expression level (Ivanisenko et al.,  ). The main disadvantage is that our approach does not consider the expression level of the proteins extracted; this is due to the incomplete description of expression data in abstracts. On the other hand, we have evaluated the fully automated workflow for the purposes of extraction and analysis of the proteins or genes names that can be associated with the investigation of HIV ECs. Our study demonstrates that automated analysis of protein functions associated with HIV elite controllers allows us to hypothesize about the role of this protein in the HIV/AIDS progression. These observations and hypotheses may help to plan new experiments and develop new methods for HIV/AIDS treatment including the search for novel chemical compounds that can modulate the level of expression of target proteins, and vaccine development. We suppose that the suggested approach can be applied to an analysis of other viral infections, including those that have been affecting humanity for the last years (Basak et al.,  ; Tarasova et al.,  ; Tworowski et al.,  ). In addition it may help provide the possibility to analyze other various pathological processes non-related to viral infections that can involve the changes in gene expression (Kovalenko et al.,  ; Kandhro et al.,  ; Bizzarri et al.,  ) and find possible strategies to combat pathological conditions. \n\n\n\n## Conclusions \n  \nIn our study, we have developed and tested a new approach for automated extraction of named entities representing proteins and chemical compounds from the texts of scientific publications. Our method is based on the conditional random fields algorithm. We have developed a set of text features providing the reasonable accuracy of named entity recognition. We proposed the retrieval of named entities from the set of papers strictly relevant to (i) HIV reverse transcriptase inhibition and (ii) to the control of HIV/AIDS progression to test the ability of the algorithm developed to extract both the names of chemicals and proteins (genes). Our algorithm was tested on the retrieval of data on inhibitors of HIV reverse transcriptase. We were able to identify the HIV RT inhibitors with the precision 0.80 and recall 0.94. Then, we tested the applicability of our algorithm to identify a set of proteins potentially responsible for slow HIV/AIDS disease progression and HIV control. For this purpose, we collected a set of abstracts strictly relevant to the HIV elite controllers, a group of HIV positive patients who did not progress into HIV/AIDS for years in the absence of antiretroviral therapy. The extraction of proteins unique for the studies of HIV elite controllers allows us to identify the set of proteins responsible for the velocity of HIV/AIDS disease progression. Investigation of these proteins and their functions can provide insights into novel approaches for HIV/AIDS treatment. \n\n\n## Data Availability Statement \n  \nThe original contributions presented in the study are included in the article/ , further inquiries can be directed to the corresponding author/s. \n\n\n## Author Contributions \n  \nNB: provided realization, optimization and testing of the CRF algorithm, extraction of the names of chemical compounds and proteins/genes, computational experiments on the chemical data extraction, and manuscript preparation. OT: study design, preparation of test sets, collection of the abstracts of publication relevant to HIV reverse transcriptase inhibitors, HIV elite controllers, HIV-positive patients from NCBI PubMed and NCBI PMC databases, automated search of the extracted proteins/genes by their name in UniProt database, analysis of the results, manuscript preparation and review, and general supervision. SI: selection of the publications relevant to HIV elite controllers, manuscript review, and useful comments. VP: manuscript review, comments to the study design and to the manuscript content. All authors contributed to the article and approved the submitted version. \n\n\n## Conflict of Interest \n  \nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. \n\n \n", "metadata": {"pmcid": 7783389, "text_md5": "7ee6dee8522e294f1bf7f9d956b19245", "field_positions": {"authors": [0, 80], "journal": [81, 92], "publication_year": [94, 98], "title": [109, 222], "keywords": [236, 333], "abstract": [346, 2567], "body": [2576, 42914]}, "batch": 1, "pmid": 33414815, "doi": "10.3389/fgene.2020.618862", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7783389", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=7783389"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7783389\">7783389</a>", "list_title": "PMC7783389  Automated Extraction of Information From Texts of Scientific Publications: Insights Into HIV Treatment Strategies"}
{"text": "Korvigo, Ilia and Holmatov, Maxim and Zaikovskii, Anatolii and Skoblov, Mikhail\nJ Cheminform, 2018\n\n# Title\n\nPutting hands to rest: efficient deep CNN-RNN architecture for chemical named entity recognition with no hand-crafted rules\n\n# Keywords\n\nNamed entities recognition\nTokenisation\nChemical\nText mining\nDeep learning\nRecurrent neural network\nConvolutional neural network\nBiocreative\nChemdner\nConditional random fields\nNeural attention\n\n\n# Abstract\n \nChemical named entity recognition (NER) is an active field of research in biomedical natural language processing. To facilitate the development of new and superior chemical NER systems, BioCreative released the CHEMDNER corpus, an extensive dataset of diverse manually annotated chemical entities. Most of the systems trained on the corpus rely on complicated hand-crafted rules or curated databases for data preprocessing, feature extraction and output post-processing, though modern machine learning algorithms, such as deep neural networks, can automatically design the rules with little to none human intervention. Here we explored this approach by experimenting with various deep learning architectures for targeted tokenisation and named entity recognition. Our final model, based on a combination of convolutional and stateful recurrent neural networks with attention-like loops and hybrid word- and character-level embeddings, reaches near human-level performance on the testing dataset with no manually asserted rules. To make our model easily accessible for standalone use and integration in third-party software, we\u2019ve developed a Python package with a minimalistic user interface. \n \n\n# Body\n \n## Background \n  \nModern data-generation capabilities have clearly surpassed our capacity to manually analyse published data, which is ever-more evident in the era of high-throughput methods. Naturally, this fuels the development of automatic natural language processing (NLP) systems capable of extracting and transforming specific information from a body of literature with human-level precision. Among all the subtasks NLP introduces, named entity recognition (NER)\u2014aiming to identify objects of particular semantic value (e.g. chemical compounds)\u2014is one of the most fundamental for higher level event-focused analyses. Traditionally, chemical NER systems have relied on curated dictionaries and hand-crafted rules (e.g. regular expressions for systematic IUPAC names or databases of trivial names and identifiers), which are hard to develop and maintain due to diverse morphology and rich vocabulary of biomedical literature. On the other hand, various machine learning (ML) models can automatically infer efficient rules (input transformations) from annotated corpora reducing development and maintenance costs. In ML terms named entity recognition is a supervised labelling problem. \n\nTo facilitate the development of new and superior NER systems, BioCreative announced the CHEMDNER challenge, which ended in 2015 [ ]. As part of this task, a team of experts has produced an extensive manually annotated corpus covering various chemical entity types, including systematic and trivial names, abbreviations and identifiers, formulae and phrases. Due to many difficulties inherent to chemical entity detection and normalisation [ ], even manual annotation yields the inter-annotator agreement score of 91%, which can be regarded as the theoretical limit for any automatic system trained on this corpus. Twenty six teams have submitted their NER systems for the challenge, best of which have reached the F1 score of  72\u201388% [ \u2013 ] on two subtasks: chemical entity mention (CEM) and chemical document indexing (CDI). \n\nThe systems were quite diverse in terms of text preprocessing, which is a separate NLP problem in its own right. Obviously, it\u2019s possible to represent any text as a raw sequence of characters (e.g. byte-like sequences or Unicode character codes), yet it is more common to break the characters into word-like structures known as tokens, which can be further normalised and/or encoded. Although tokenisation typically reduces the number of time-steps in the sequence, thus reducing the input complexity, it can introduce severe artefacts, e.g. merged/overlapping entities [ ,  ]. It makes it essential to use an adequate tokeniser with rules finely adjusted for the task at hand. \n\nWhile there are many token encoding strategies, they all can be divided into two major groups: morphology aware (character-level) and unaware (word-level). In the latter case, one usually builds a vocabulary of all tokens occurring in a corpus and applies a minimal frequency cutoff to remove noisy entries (e.g. misspelled words and typos). Consequently, all tokens in the vocabulary get a unique identifier  , while all out-of-vocabulary (OOV) tokens get a special shared identifier. The vocabulary itself can be represented as a matrix   of orthogonal unit vectors (also known as one-hot encodings), both sparse and purely categorical: their pair-wise distances carry no underlying information about semantical similarity. In their chemical NER system, Lu et al. [ ] successfully used the skip-gram embedding model to overcome these limitations. The model uses context information and a shallow neural network to embed high-dimensional one-hot encoded vectors in a lower-dimensional vector space, wherein pair-wise distances represent semantical similarity [ ,  ]. Despite this strategy\u2019s increasing popularity, few CHEMDNER task participants have employed it for morphology unaware encoding, relying instead on manually selected features to expand token identifiers into feature vectors. While word-level encodings are efficient for morphologically rigid corpora (e.g. standard English texts), morphologically rich biomedical and chemical literature introduces many infrequent words and word-forms, resulting in high out-of-vocabulary (OOV) rates [ ,  ]. Consequently, most CHEMNDER participants have additionally (or exclusively) used morphology aware-encodings, targeting various manually designed character-level features. Machine-learning models were far less diverse: since textual data are sequential, that is a value   at time-step   i   can be conditioned on the values occurring before and after the time-step, it is only natural to use sequential models for NER problems. Although many such models exist, most of the top-scoring ML-based tools submitted for the CHEMDNER task utilised conditional random fields (CRF), which are traditionally used for sequence labelling. CRFs are graphical models related to hidden Markov models (HMMs). They take a sequence of feature vectors as inputs and generate a sequence of labels, which can be further modified during post-processing. The participants used hand-crafted post-processing rules as diverse as the preprocessing procedures. \n\nFrom this brief overview of the NER systems submitted for the CHEMDNER task, it becomes quite evident that, despite the introduction of machine learning methods, in many ways these systems remain conceptually close to manually curated sets of rules (feature extractors). This might explain why LeadMine [ ] (another contender), a purely rule-based system, outperforms most of the submitted ML-based counterparts. At the same time, it is possible to reduce manual interventions to the bare minimum by treating tokenisation, word encoding and feature extraction as subtasks in a global machine learning task, and this is exactly the kind of problems that deep artificial neural networks (ANNs) excel at. As we have already mentioned, neural networks can automatically learn morphology unaware word representations, and the same is true about morphology aware encodings. Furthermore, deep convolutional neural networks can automatically optimise feature extraction during training [ ]. Most importantly, the labelling itself can be done by recurrent neural networks. Recurrent networks are naturally sequential and Turing-complete, extremely powerful in sequence-to-sequence (also known as seq2seq or many-to-many) modelling (including labelling) [ ]. In an unreviewed paper by Rei et al. [ ] the authors have experimented with deep-learning applications in NER on several datasets, including CHEMDNER. Some of their models used a bidirectional RNN for character-level word embedding combined with a variation of the attention technique used to choose between word-level and character-level embeddings, though the labelling itself was done by a CRF. Convolutional networks have also been used for biomedical NER. Zhu et al. [ ] have applied a deep CNN to automatically infer local context-sensitive features fed into a CRF classifier. In an unreviewed article Chiu and colleagues have showcased a complete ANN-only design, based on a combination of convolutional and recurrent layers [ ]. The model uses word-level and character-level token embeddings. While the former were pretrained, the latter were optimised during training by transforming a word\u2019s matrix of per-character linear embeddings into a single vector using a bidirectional RNN. Concatenated word-level and character-level embeddings were then fed into a CNN to extract local features. In contrast with the former examples, this model opted for a deep RNN instead of a CRF for sequence labelling. The authors claim state-of-the-art performance on the datasets they\u2019ve used, though quite unfortunately they have not tried to apply their model to a chemical dataset. All these examples make it self-evident that a pure ANN specifically targeting the CHEMDNER CEM subtask can perform as well (if not better) that conventional models, whilst relying on no imposed rules or databases whatsoever. Having set this as the main purpose of this study, we have developed a highly modular deep-learning model incorporating multiple novel features, including trainable targeted tokenisation. \n\n\n## Materials and methods \n  \n### Problem formulation \n  \nWe consider named entity recognition as a combination of two problems: segmentation and sequence labelling. Given:   \nan ordered set of   N   character sequences  , where   is a character sequence; \n  \nan ordered set of   N   annotations  , where   is a sequence   and   is a tuple of two boolean labels   showing whether the corresponding character is the beginning of a chemical entity and/or part of one, respectively; \n  our task is to create a   predictor  , where   is a set of inferred annotations similar to   Y  . We also introduce a   tokeniser  , where   is an ordered sequence of character subsequences (tokens), thus slightly redefining the objective function to target per-token annotations. Provided that the   tokeniser   is fine enough to avoid tokens with overlapping annotations, this redefined problem is equivalent to the original one. \n\n\n### Datasets \n  \nWe used the CHEMDNER corpus [ ] to train and validate our models. The corpus contains ten thousand abstracts from eleven chemistry-related fields of science with over 84k manually annotated chemical entities (20k unique) of eight types:   \nABBREVIATION (15.55%) \n  \nFAMILY (14.15%) \n  \nFORMULA (14.26%) \n  \nIDENTIFIER (2.16%) \n  \nMULTIPLE (0.70%) \n  \nSYSTEMATIC (22.69%) \n  \nTRIVIAL (30.36%) \n  \nNO CLASS (0.13%) \n  The MULTIPLE class represents phrases containing several entities of other classes separated by non-chemical words. The CHEMDNER corpus comprises three parts: training (3.5k abstracts), development (3.5k) and testing (3k) datasets. We joined the first two datasets, randomly shuffled the result and separated 10% for a validation dataset to monitor overfitting. The other part of the split was used for training. We only used the official test dataset to estimate performance upon training completion. \n\n\n### Design choices \n  \n Deep-learning models   We have utilised three types of neural networks: one-dimensional (1D) convolutional neural networks (CNN), recurrent neural networks (RNN) and time-distributed dense (fully-connected) networks (TDD). In their essence, one-dimensional convolutional neural networks are trainable feature extractors applied along a sequence evolving in time. A deep CNN [ ] trains to extract time-invariant hierarchies of features at each time-step while optimising an objective function. Since texts are sequential, that is a value   at time-step   i   can be conditioned on the previous and/or the following time-steps, a time-invariant model alone is not sufficient. We used recurrent neural networks\u2014highly powerful trainable state machines theoretically capable of modelling relationships of arbitrary depth\u2014to process CNN-extracted features. These networks train by back-propagating the error through time, which in deep sequences may lead to vanishing or exploding gradients. Several types of RNNs have been developed to better handle long-term dependencies, most notably the long short time memory network (LSTM) and gated recurrent unit network (GRU) [ ]. Both architectures use trainable gates controlling the data flow and memory updates. The GRU architecture is a newer and lighter alternative to the widely adopted LSTM, with two trainable gates instead of the latter\u2019s three resulting in less parameters to optimise, a desirable trait when training data are scarce. Comparative studies haven\u2019t found any consistent performance advantages of either GRUs or LSTMs, though the former tend to converge faster [ ]. To further improve performance, it is common to use bidirectional RNNs (biRNNs) \u201creading\u201d sequences in both directions. Finally, we used a time-distributed fully connected network [ ] (also known as dense networks or multilayer perceptrons) with the sigmoid activation function to generate label probabilities. In contrast with traditional bulky dense networks that process the entire input at once, TDDs apply a lightweight multi-layer perceptron (MLP) to each time-step in a sequence, drastically reducing the number of parameters and making it possible to analyse variable-length inputs. \n\n Stateful learning   Texts come in all sizes, which is quite problematic for most machine-learning methods. Although one of the RNNs\u2019 key selling points is their ability to naturally handle variable-size inputs, it\u2019s hard to implement an RNN in a way that takes full advantage of this feature whilst staying computationally efficient. Two mainstream solutions exist. The most natural (and arguably the least computationally efficient) solution implies grouping and encoding (i.e. representing as numeric tensors) equally sized texts together. This method introduces a lot of extremely small sample batches greatly increasing gradient variance and, by extension, hindering model convergence. Alternatively, one can use zero-padding (artificially increasing length by appending zeros to numerically encoded sequences). This procedure greatly increases sparsity and the memory overhead, because full-sized texts can vary greatly in length. It is thus more efficient (and common) to break texts into individual sentences. Despite being more computationally efficient, this method is less flexible, because it introduces a sentence length limit and requires a sentence segmentation model. It also strips aways text-wide context. Quite fortunately, there is another relatively novel technique known as stateful learning. Although it has not yet gained any noticeable adoption in the community (partly due to complicated data handling described below), it combines the best of both aforementioned methods: no text-size restrictions, no sentence segmentation model dependencies and negligible memory overhead. Normally RNNs only keep their state within a single batch of samples and reset it between batches, because there is no guarantee that the next batch is somehow related to the previous one. In contrast with conventional setups, an RNN configured for stateful learning treats a sample (row)   j   in batch   as the direct continuation of sample   j   in batch   i  , making it possible to break long sequences into fixed-size windows without resetting the context when moving from one batch to another. Simply put, stateful learning allows RNNs to transcend the batch barrier and, in theory, keep track of the context as long as required. To make it technically possible, the batch-size must be fixed at construction time and the data must be preprocessed to satisfy the aforementioned property. We ve developed a bin packing-based data preprocessing algorithm to achieve this goal. Given a batch size of   n  , we distribute input texts into   n   bins while trying to keep the accumulated lengths equal between all bins. We then concatenate texts inside each bin into super-sequences, stack them and break into chunks of   l   time-steps. This procedure is easily reversible, making it possible to recover annotations for individual texts. Additionally, since in bidirectional RNNs it only makes sense to keep track of the forward-evolving state, we have developed a \u201chalf-stateful\u201d bidirectional RNN wrapper layer (HS-biRNN) that takes care of forward inter-batch state transfers and can be used with any RNN architecture (e.g. LSTM or GRU). \n\n\n### Text preprocessing \n  \nWe have done no text-preprocessing except for tokenisation. Accurate tokenisation is highly important in token-level NLP tasks [ ]. On the one hand, this process can isolate semantically and morphologically stable character sequences, making it easier for the model to focus on the data. On the other, tokenisation may lead to overlapping annotations if the rules fail to separate several adjoint entities or non-entity characters from entities. Most popular tokenisers rely on a hierarchy of rules optimised for standard English, though there are some specifically designed for biomedical and chemical texts. For example, the tokeniser implemented in the ChemDataExtractor package [ ] overrides some rules in the Penn Treebank policy to better handle chemical entities:  \nTokens are split on all whitespace and most punctuation characters, with exceptions for brackets, colons, and other symbols in certain situations to preserve entities such as chemical names as a single token. \n In other words, as with any rule-based technique, it\u2019s notoriously hard to create an optimal tokeniser equally adequate for recovering standard vocabulary and diverse chemical entities, because they have different underlying morphology -\u00a0a tokeniser has to be context-aware. We believe that instead of trying to manually create a general-purpose tokeniser one can use an alternative specifically trained to accurately recover target entities. Such a tokeniser will only be used to preprocess text for a subsequent NER model alleviating the need to recover irrelevant words. Since we have found little to no research on trainable tokenisers, we have developed our own model based on a \u201cbreak and stitch\u201d strategy: a primary extra-fine segmentation followed by a refinement step trying to recover target entities (Fig.  ). We have used the following Perl-style regular expression to carry out the first step:  . The expression groups together Unicode word characters (i.e. most characters that can be seen in a word in any language, including numbers) and separates all other characters. For example, it breaks   2-amino-1-methyl-6-phenylimidazo[4,5-b]pyridine   into nineteen fragments:   2  ,   -  ,   amino  ,   -  ,   1  ,   -  ,   methyl  ,   -  ,   6  ,   -  ,   phenylimidazo  ,   [  ,   4  ,   ,  ,   5  ,   -  ,   b  ,   ]  ,   pyridine  . As expected, the result is heavily over-fragmented. On the bright side, our analyses of the tokenised CHEMDNER corpus showed a near-complete absence of tokens overlapping several entities, making it possible to accurately reconstruct large entity tokens by stitching several fragments together. To detect stitch points we have designed a lightweight stateful sequence-to-sequence CNN-RNN model processing raw untokenised text. The model consists of a linear character encoding layer, two consecutive 1D CNN layers, each with 256 (3-characters wide) filters, followed by two half-stateful bidirectional GRU layers (32 cells each) and a time-distributed sigmoid classifier that outputs a binary tag for each character in the sequence. Positive tags mark stitch points. We have used the same training and validation splits to train this tokeniser alongside the NER model.   \nText tokenisation. The break and stich targeted tokenisation strategy employed by our trainable tokeniser \n  \n\n\n### The NER model \n  \nAll the models we trained had two input nodes: one for pretrained word-level embeddings and another one for encoded token strings. The strings were encoded as integer vectors containing character identifiers. We trained 300-dimensional Glove embeddings with default configurations [ ] on a corpus of   random PubMed abstracts from the same categories as the CHEMDNER abstracts: BIOCHEMISTRY & MOLECULAR BIOLOGY, APPLIED CHEMISTRY, MEDICINAL CHEMISTRY, MULTIDISCIPLINARY CHEMISTRY, ORGANIC CHEMISTRY, PHYSICAL CHEMISTRY, ENDOCRINOLOGY & METABOLISM, CHEMICAL ENGINEERING, POLYMER SCIENCE, PHARMACOLOGY & PHARMACY and TOXICOLOGY [ ]. Character-level embeddings were optimised during training using the same approach described in [ ]. This block consisted of a trainable linear character-embedding layer transforming vectors of character codes into matrices of 32-dimensional character embeddings. These word matrices are then processed by a standard biGRU (16 cells) layer producing a 32-dimensional vector per token [ ]. \n\nInstead of concatenating word- and character-level embeddings before feeding them into a single CNN or RNN block, we used separate two-layers deep 1D CNNs for each embedding type to increase the number of degrees of freedom without using too many convolutional filters. Features extracted by these independent blocks were subsequently concatenated and fed into a two-layers deep HS-biGRU. The network then bifurcates again:   \nThe first branch continues with an additional HS-biRNN followed by a time-distributed sigmoid layer. The layer outputs the probability that a given time-step is a part of a named entity. \n  \nThe second one starts with an arithmetic node multiplying the probabilities produced by the other branch and the output from the preceding HS-biGRU block at each time-step. The result is then fed into a single HS-biRNN layer followed by a time-distributed sigmoid layer, yielding the probability that a given time-step is the beginning of an entity. \n  First of all, it\u2019s important to show that this labelling method remotely resembles the widely used IOB scheme with three mutually exclusive labels: Inside/Outside/Beginning (of an entity) [ ]. At the same time, in contrast to this scheme (or any other scheme with mutually-exclusive tags), our labels are not mutually exclusive and are codependent at the same time due to the multiplication node. Since we try to minimise both predictors their error is back-propagated through the graph, creating a reinforcing loop with two effects: (1) it theoretically encourages the part-detected to better pay more attention to single-token entities and (2) it helps the beginning-detector attend to entity parts. \n\nHere we have described all design elements of the final (fully-featured) NER model. We also examined the impact of several large-scale changes on its performance. In particular, we have compared GRU and LSTM architectures and tried replacing the CNNs with additional recurrent layers. More importantly, we have compared stateful and conventional biRNNs trained unsegmented full-sized texts and stacked sentences respectively. We used the GeniaSS [ ] sentence segmentation model to carry out this comparison. Having limited computational resources and time constraints we have not tried to fine-tune any hyper-parameters: all convolutional layers comprised 256 (3 time-steps wide) filers, and all HS-biRNN layers contained 32 recurrent cells. \n\n\n### Training and testing \n  \nThe project was implemented in Python 3.5 using deep-learning frameworks Keras 2 [ ] and TensorFlow 1.3 [ ]. All computations have been carried out on an Ubuntu Linux server with two Intel Xeon E5 CPUs (10 cores and 20 threads each), 512GB of RAM and four Nvidia Titan X GPUs. We used the Adam optimiser [ ] with default parameters recommended by the authors. The networks were trained for 40 epochs with a callback saving weights upon improvements in performance on the validation dataset. \n\nDuring testing, we specifically targeted the CHEMDNER chemical entity mention (CEM) subtask. Since deep-learning models are inherently non-deterministic due to random weight initialisation and stochastic optimisation, we have evaluated each design variation by averaging estimated probabilities from 10 independently trained networks (as in [ ]). To add some perspective, we report all models that have achieved a CEM F-score of 80% and above in the CHEMDNER challenge  , though their current accessibility is worth mentioning. The models introduced by teams 184, 185, 192 either have not been published at all or the links have become inactive. LeadMine (179) [ ] is exclusively commercial. While there is a GitHub repository for the model devised by Lu et al. (team 231) [ ,  ], it literally contains nothing but a link to an archive (which supposedly contains the model), uploaded to a file-hosting service that requires a proprietary application to download the archive. Since both the file-hosting and the application are only available in Chinese, we have been unable to download the archive and thus consider it inaccessible. Becas (team 197) [ ] and tmChem (team 173) [ ] both provide different web-based APIs, making it possible to submit texts to annotation servers or, in case of tmChem, even download precomputed annotations for PubMed abstracts. With tmChem there is also an option to build the tool from sources, though the source archive does not seem to come with a trained model, because our stand-alone installation has produced random annotations. Both Chemspot (team 198) [ ] and BANNER-CHEMDNER (team 233) [ ] are available for stand-alone installation from sources.   \nPerformance scores for the CHEMDNER chemical entity mention (CEM) subtask \n  \nCHEMDNER challenge team IDs are given in parenthesis in the Model column (where available; performance scores for these models have been taken from Table 4 in [ ]). We provide ChemDataExtractor performance scores reported by the authors \n  \n\nApart from the models submitted for the CHEMDNER challenge we have also considered ChemDataExtractor [ ], a recently introduced general purpose package for chemical text analyses, because its NER model is very much akin to [ ], which is unavailable. Both utilise unsupervised word-clustering and CRFs, though ChemDataExtractor uses a hierarchical detection system with a built-in database updated in an online fashion to help it extract abbreviations and identifiers. ChemDataExtractor comes with a highly user-friendly Python API making it extremely easy to install and utilise. More importantly its overall combined CEM F-score of 87.8% puts it on top of all models submitted for the CHEMDNER challenge. \n\n\n\n## Results and discussion \n  \n### Tokenisation, overlapping annotations and sequence lengths \n  \nWe have processed the entire CHEMNDER testing dataset and searched for entities with overlapping annotations. Out of 25347 annotated entities in the testing dataset less than 0.19% spanned the same token, which is truly negligible. At the same time the tokeniser had a recall of 91.75% and precision of 93.32%. Therefore, it is able to accurately recover most of the annotated entities. \n\n\n### Performance \n  \nFirst of all, in terms of time required to complete one training epoch the reference network (Fig.  ) incorporating stateful biRNNs trained over two times faster than its sentence-based sibling with conventional biRNNs and had a lighter memory footprint. We observed no significant impact on the F-score, though. There was no observable advantage in using LSTM cells over GRUs, either. On the contrary, GRUs trained and converged faster and showed slightly better performance on the testing dataset. Convolutional layers were crucial for good performance. On average, replacing the CNN-layers with one or two hs-biGRU layers reduced the F-score by   and hampered the training process.   \nModel architecture. The figure illustrates the topology and of the best-performing full-featured model.   convolutional neural network;   half-stateful bidirectional gated recurrent unit;   fully-connected network \n  \n\nOn the CHEMDNER CEM subtask our fully-featured network has gained the F-score of 88.7%. Therefore, it outperforms all models submitted for the CHEMDNER task by a significant margin, though the edge over ChemDataExtractor is less impressive (see Table   for more details). Considering the inter-annotator agreement score of 91%, the model demonstrates near-human performance. Since the model does not discriminate between entity types, there is no way to calculate per-class precision values and, by extension, F-scores. Nevertheless, following Krallinger et al.\u00a0[ ] we report per-class precision in Table  . It\u2019s important to note that following the CHEMDNER CEM evaluation rules we have only considered perfect matches. It is immediately clear that the model struggles greatly with rare entity types, i.e. NO CLASS and MULTIPLE, and excels at systematic and trivial names. Considering how rare the MULTIPLE type entities are (195 entities) and that they span several standard English words intertwined with different chemical entity types, this subpar performance is not surprising and is actually consistent with that of other tools reported in [ ] (Additional file 3).   \nRecall values estimated for individual entity types \n  \nOnly perfect matches were considered correct \n  \n\n\n### Accessibility and the user interface \n  \nWhile analysing the NER systems submitted for the CHEMDNER task, we have found that neither the source code, nor the trained models are available for some of the best-performing tools, limiting the ability to use and validate them. We thus made it our priority to publish all the source code needed to train and use our models. All materials are openly available on GitHub [ ] in a separate frozen branch (chemdner-pub) of a natural language processing package SciLK. While the package itself, being in the early stages of development, is bound to change, the separate branch will retain the version required for these models to work. In addition to the core library, the branch contains Jupyter notebooks with code and commentaries sufficient to reproduce our work, i.e. train a tokeniser and a named entity detector, a notebook with usage demonstration and an archive with trained models. A standard dual-core laptop with 8 GB of RAM should be sufficient to use the models for inference. While the software should theoretically work under Microsoft Windows, we have only tested it on machines running Mac OS X and GNU-Linux. Should a user want to train a similar model, we recommend doing it on a machine with at least 32 GB of RAM and a graphics processing unit (GPU) with at least 8 GB of VRAM. Although a GPU is not strictly required for training, it takes roughly twice as much time to train a fully-featured model on a 20-core CPU-only system. \n\n\n\n## Conclusions \n  \nHere we have presented our deep-learning model for chemical named entities recognition in biomedical texts, trained and evaluated on the CHEMDNER corpus. Given its high performance, the model proves that chemical named entity recognition can be done efficiently with no manually created rules or curated databases whatsoever. We also showcase several novel or rarely used approaches and design choices that, to the best of our knowledge, have never been used in biomedical or chemical NER. Most notably, we advocate the use of specialised trainable tokenisers and stateful recurrent neural networks. Nevertheless, we clearly see several directions for further improvement. For one, due to time constraints we have not investigated many hyper-parameter and topology options. Secondly, while avoiding complicated preprocessing has been one of the top-priorities, we still believe that additional information that cannot be extracted from the CHEMDNER corpus itself can further increase performance. In particular, we think that part of speech tags or other external annotations can greatly benefit the system. We also think that much more research should be done on targeted tokenisers, considering that our tokeniser had a rather primitive design. \n\n \n", "metadata": {"pmcid": 5966369, "text_md5": "f80591cfb4dfae2e11d2132e46389b55", "field_positions": {"authors": [0, 79], "journal": [80, 92], "publication_year": [94, 98], "title": [109, 232], "keywords": [246, 439], "abstract": [452, 1649], "body": [1658, 32657]}, "batch": 1, "pmid": 29796778, "doi": "10.1186/s13321-018-0280-0", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5966369", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5966369"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5966369\">5966369</a>", "list_title": "PMC5966369  Putting hands to rest: efficient deep CNN-RNN architecture for chemical named entity recognition with no hand-crafted rules"}
{"text": "Lithgow-Serrano, Oscar and Cornelius, Joseph and Kanjirangat, Vani and M\u00e9ndez-Cruz, Carlos-Francisco and Rinaldi, Fabio\nGenomics Inform, 2021\n\n# Title\n\nImproving classification of low-resource COVID-19 literature by using Named Entity Recognition\n\n# Keywords\n\nclassification\nCOVID-19\nNamed Entity Recognition\nNLP\n\n\n# Abstract\n \nAutomatic document classification for highly interrelated classes is a demanding task that becomes more challenging when there is little labeled data for training. Such is the case of the coronavirus disease 2019 (COVID-19) clinical repository\u2014a repository of classified and translated academic articles related to COVID-19 and relevant to the clinical practice\u2014where a 3-way classification scheme is being applied to COVID-19 literature. During the 7th Biomedical Linked Annotation Hackathon (BLAH7) hackathon, we performed experiments to explore the use of named-entity-recognition (NER) to improve the classification. We processed the literature with OntoGene\u2019s Biomedical Entity Recogniser (OGER) and used the resulting identified Named Entities (NE) and their links to major biological databases as extra input features for the classifier. We compared the results with a baseline model without the OGER extracted features. In these proof-of-concept experiments, we observed a clear gain on COVID-19 literature classification. In particular, NE\u2019s origin was useful to classify document types and NE\u2019s type for clinical specialties. Due to the limitations of the small dataset, we can only conclude that our results suggests that NER would benefit this classification task. In order to accurately estimate this benefit, further experiments with a larger dataset would be needed. \n \n\n# Body\n \n## Introduction \n  \nThe current pandemic took the world by surprise in all aspects, among which is the efficient broadcasting of relevant findings to reach interested researchers, clinicians and other stakeholders. The medical literature relevant to coronavirus disease 2019 (COVID-19) is growing exponentially and, doctors, clinicians, and health workers in general need tools for monitoring and prioritizing the literature to make the most of their time by allowing them to quickly identify the most relevant information. \n\nWe have witnessed an enormous effort and solidarity that experts around the world have put in contributing and sharing a broad scope of experiences and findings. In this paper we consider in particular the case of an interinstitutional COVID-19 cooperation group ( ) which has been working to provide tools and resources for COVID-19 literature exploration. \n\nThe main contribution of the group is a clinical repository consisting of continuously updated academic literature related to COVID-19 ( ). Considering the PRECEPT [ ] scheme, their clinical experience and, other common classifications, the group has adopted a 3-way classification strategy to organize the repository to make it easier for potential users to find relevant literature. Despite a big effort to read and manually classify these documents, the pace of production of this literature hamper the utility of this work. In response the group has been working on automatic document classification machine learning models ( ), however the number of classes, the interconnected nature of the clinical fields and the very little labeled examples imposes considerable challenges [ , ]. \n\nOn the other hand, another important contribution has been the inclusion of a specialized COVID-19 named-entity annotation service, OntoGene\u2019s Biomedical Entity Recogniser (OGER) [ ]. OGER is an annotation tool that performs Named Entity Recognition and Disambiguation (NERD) using shallow and deep dependency parsing combined with state-of-the-art machine learning techniques. It interacts with   The Bio Term Hub   ( ) which enables OGER to process and link named entities from major life science databases: cellosaurus, cell ontology, ChEBI, CTD, EntrezGene, Gene Ontology, MeSH, Molecular Process Ontology, NCBI Taxonomy, Protein Ontology, RxNorm, Sequence Ontology, Swiss-Prot, Uberon. \n\nAiming to leverage the convergence of both contributions we hypothesized that the named entities recognized by OGER could be used as a free-lunch feature augmentation strategy to boost the classification performance [ , ]. \n\nDuring the Biomedical Linked Annotation Hackathon 7 ( ) within the task \u201cAnalyzing COVID-19 literature with OGER\u201d we proposed a subtask oriented to investigate this hypothesis ( ). \n\nThe task\u2019s aim was to classify COVID-19 literature according to three independent dimensions: clinical specialties, types, topics-and-subtopics, with special emphasis in exploring the use of Named Entities to better leverage the title, abstract and text during classification. \n\nHere, we present a proof-of-concept experiment performed during the hackathon. We processed the literature with OGER and used the resulting identified named-entity-recognition (NER) and their links to major biological databases (NED) as extra input features for a basic classifier model. We then compared the results with the same classification model but without the OGER extracted features. Although very preliminary, the results are promising and show a clear gain on COVID-19 literature classification by using named-entity-recognition as an auxiliary feature augmentation step, suggesting the benefit of NER for this task. \n\n\n## Methods \n  \nIn the preprocessing phase the original PDF documents were converted to text using the Linux utility   pdftotext   and no other cleaning or normalization steps were performed. \n\nNext, the documents\u2019 title and full text were converted to features applying Term Frequency-Inverse Document Frequency (tf-idf), Latent Semantic Analisis (LSA), and using the CLS embedding (first embedding) of the pretrained BERT-base as a sentence-embedding [ ]. \n\nBesides, the full-text was processed with the OGER\u2019s API specialized for COVID-19 applications ( ). This process resulted in an average of 1,622 annotations per article (The same token can be annotated as multiple entities because OGER uses multiple sources) including, among others, the entity type (e.g., organism, disease) (The full list of entity types annotated by OGER in this dataset were: organism, sequence, cellular_component, clinical_drug, cell_line, organ/tissue, gene/protein, chemical, cell, disease, molecular_process, biological_process, molecular_function), the matched term (e.g., coronavirus), the preferred-form (e.g., coronavirus), the entity ID and, the origin database (e.g., MeSH diseases, CTD MeSH). OGER has the capacity to return results in different formats, for our purpose TSV was the most convenient (  for an output example). \n\nThe classification task was approached as a multi-class problem for each of the three classification axis; this is, each document can be labeled with at most one topic, one type, and one specialty. \n\nThe general approach to use the NERD results consisted in treating each field of OGER output as an alternative representations of the documents and, then, apply different feature extraction over these representations ( ). To create each representation, all the values of one field of the OGER results were joined as words, with a space as separator. For example, to obtain the   preferred-form   representation of a document, all the value of the preferred-form field of all the identified entities in the document were concatenated. The resulting string is then treated as if it were an additional sentence in the input text and thus, converted using two basic feature extraction strategies: tf-idf and LSA. \n\nThe result was 11 features: six corresponding to the tf-idf, LSA and bert-embedding (Due to the BERT model limitation, we use only the first 512 tokens of the article body to build the text embedding) for the document\u2019s title and the text representations, and 5 from the NERD extraction: tf-idf of the type, tf-idf and LSA [ , ] of the   preferred-form  , tf-idf of the   entity-id   and, tf-idf of the   origin  . These features were later reduced (independently for each classification axis) through a supervised exhaustive feature selection with a Support Vector Machine (SVM) [ ] on a 3-fold cross validation, i.e., through a greedy search evaluating with the SVM classification model all possible combinations of features. \n\nThe final validation consisted in training the selected model (the SVM with the reduced features) on a stratified split corresponding to the 80% of the labeled dataset and, test it in the remaining 20%. This was repeated in 30 runs with random train-test split. \n\nOne possible limitation of this study is that we performed the search for the best features combination by 3-fold cross-validation in the full dataset. This was done because the dataset was very small, highly unbalanced and some classes had only three examples. Although in the final validation phase training and testing were done in clearly separated subsets of the original data, further experiments with more data are needed to validate the observed benefits. \n\n### Dataset \n  \nThe set of manually labeled documents from the clinical repository was used for these experiments. \n\nIt is worth noticing that the labeled dataset consists of 646 articles with a rounded average of 12 words for the title and 3,540 for the body. However not all documents are classified in each of the 3-classification axis: the   Document type   classification (e.g., observational studies, systematic reviews) has 269 examples for the 6 classes; the   Clinical specialties   classification (e.g., immunology, Cardiology) has also 269 examples but for 36 classes; and the   Topic & Subtopic   classification (e.g., epidemiology, diagnosis) counts 383 examples for the 16 topics and 161 examples for the 27 subtopics. Moreover within each classification axis the examples are not uniformly distributed through the classes. \n\n\n\n## Results \n  \nThe baseline was selected as the model with the best combination of features extracted from the document title and text, i.e., without the NERD results. In this case, the best baseline model was obtained by using the tf-idf of the title and LSA of the text. \n\nFor the Document type classification, the selected features were: tf-idf of the title, LSA of the text and tf-idf of the NE origin. The F1-weighted mean of the 30 validation runs was 0.739 (  s   = 0.041 [sample standard deviation]) and, by including the NE origin as feature the classifier had a significant 10% gain (t-test with p < 0.05) compared to the 0.669 (  s   = 0.044) F1-weighted score of the baseline ( ). \n\nIn the specialty classification the best model consisted of the features tf-idf of the title, LSA of the text and tf-idf of the NE type. This combination resulted in a statistically significant 8% gain in the F1-weighted mean compared to the baseline, 0.646 (  s   = 0.044) versus 0.597 (  s   = 0.048). \n\nFinally, for the topic classification the best features were tf-idf of the title, LSA of the text and tf-idf of the NER origin. The F1-weighted mean of the 30 runs for this model was 0.599 (  s   = 0.051) whereas the baseline scored 0.595 (  s   = 0.049). Although this represented an improvement of 0.8%, it was not statistically significant.   shows the comparison of the results mentioned above. \n\n\n## Discussion and Conclusion \n  \nIt is worth noticing that the pdf to text conversion was done using a basic approach which produces quite noisy results, i.e., the extracted text includes lines that are not complete sentences due to how they are displayed in the PDF; it also includes statements interrupted by spurious chunks of text like footnotes, page numbers, etc. and other kinds of not cleanly extracted text. This might explain why BERT features played no role, and was detrimental to the OGER processing and the quality of its results. \n\nInterestingly, we found that the named entities\u2019 type and origin, i.e., more general features, were more informative for the classification than the more specific ones, like the matched term, the preferred-form, and the entity ID. One possible explanation is that due to the granularity of the general features, there are more examples in the training data and, thus, the estimator can better learn the relation between those features and the target classes. The fact that NE\u2019s type (e.g., organism, disease) helped classify specialty may unveil an interesting pattern where some biomedical entities are more prevalent in some specialties. On the other hand, the fact that using the NE origin (MeSH diseases, CTD MeSH) improved document type classification is an interesting finding that should be further investigated. These experiments also opened the question of whether using BioBert Embeddings, trained for Biomedical Domain, instead of general Bert Embeddings may help classification. \n\nIt is important to stress that these were proof-of-concept experiments, and bearing in mind the methodological limitations due to the small dataset the conclusions here presented are only suggestive, and further experiments with more data are needed to accurately estimate the observed benefits of NER in the classification task. \n\nFinally, it is important to highlight that due to the limited time in the hackathon, the experiments presented here were applied and compared to a baseline and not to the classification strategy that the COVID-19 cooperation group is developing. The next step would be to investigate if similar gains could be obtained when integrating NERD features in that strategy and applied to a larger dataset. \n\n \n", "metadata": {"pmcid": 8510872, "text_md5": "3acb1eaeeba15e947cd70ecca6792452", "field_positions": {"authors": [0, 119], "journal": [120, 135], "publication_year": [137, 141], "title": [152, 246], "keywords": [260, 313], "abstract": [326, 1712], "body": [1721, 13622]}, "batch": 1, "pmid": 34638169, "doi": "10.5808/gi.21018", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8510872", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8510872"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8510872\">8510872</a>", "list_title": "PMC8510872  Improving classification of low-resource COVID-19 literature by using Named Entity Recognition"}
{"text": "Eftimov, Tome and Korou\u0161i\u0107 Seljak, Barbara and Koro\u0161ec, Peter\nPLoS One, 2017\n\n# Title\n\nA rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations\n\n# Keywords\n\n\n\n# Abstract\n \nEvidence-based dietary information represented as unstructured text is a crucial information that needs to be accessed in order to help dietitians follow the new knowledge arrives daily with newly published scientific reports. Different named-entity recognition (NER) methods have been introduced previously to extract useful information from the biomedical literature. They are focused on, for example extracting gene mentions, proteins mentions, relationships between genes and proteins, chemical concepts and relationships between drugs and diseases. In this paper, we present a novel NER method, called drNER, for knowledge extraction of evidence-based dietary information. To the best of our knowledge this is the first attempt at extracting dietary concepts. DrNER is a rule-based NER that consists of two phases. The first one involves the detection and determination of the entities mention, and the second one involves the selection and extraction of the entities. We evaluate the method by using text corpora from heterogeneous sources, including text from several scientifically validated web sites and text from scientific publications. Evaluation of the method showed that drNER gives good results and can be used for knowledge extraction of evidence-based dietary recommendations. \n \n\n# Body\n \n## Introduction \n  \nNutritional sciences, such as clinical nutrition, food and nutrition management, public health nutrition, etc., combine a strong foundation in the biological, chemical and medical sciences, with a focus on nutrient/non-nutrient function and metabolism. The main objective of nutritional sciences is to establish food-based dietary guidelines (FBDGs) to achieve optimum health and the treatment or prevention of disease conditions as well as food production and safety [ ]. Even though FBDGs are simple messages, we must be aware that they are based on complex scientific facts, which include dietary reference values (DRVs). DRVs are nutrient recommendations and quantitative reference values for nutritional intakes, such as population reference intake, the average requirement, adequate intake level, and the lower threshold intake. Authorities (e.g. European Food Safety Agency (EFSA) [ ]) continuously identify and review the latest scientific studies, including reports of national and international authorities, for possible health effects of specific nutrients. For example, if the focus is on dietary fiber, health effects of dietary fiber are identified by reviewing scientific studies. Then, evidence of relationships between the intake of a nutrient and health outcome is evaluated. Finally, when nutrient-health relationships are established, the authority provides scientific advice that can be used by policy makers. In practice, this means that a daily intake of 25 g of a dietary fiber is set as a DRV because it is adequate for adults, while consuming greater than 25 g of dietary fiber per day may reduce the risk of coronary heart disease and type 2 diabetes and may improve weight maintenance [ ]. Most countries have established their own national DRVs that consider, beside international recommendations and guidelines, also local conditions and national/ethnical eating culture and habits, and are reviewed and updated from time to time. A comprehensive review of micronutrient recommendations in Europe, collected within the EU-funded project EURRECA (EURopean micronutrient RECommendations Aligned) [ ], was published several years ago in [ ]. In 2015, the non-profit association EuroFIR [ ] updated EURRECA micronutrient recommendations, enriched them with reference values for other nutrients, and developed a web service for accessing DRVs through the Quisper server platform [ ], aimed at collecting scientifically-validated food-related data and knowledge services for dietary advising. Beside DRVs for the public, there also exist disease-specific DRVs aimed at increasing the awareness of clinicians and persons with chronic disease about beneficial nutritional therapies. Recently, personalized DRVs have become relevant as they consider genetic predisposition to chronic disease and phenotype information on anthropometry, physical activity, clinical parameters, and biochemical markers of nutritional status, and give strategies to dramatically reduce the risk of chronic-disease. The EU-funded Food4Me project [ ] performed a pan-European study of over 1,500 participants, which showed that personalized advice is more effective at improving dietary behavior compared to conventional, population-based FBDGs [ ]. \n\nIn public health as well as in clinical practice, dietary recommendations should be based on evidence-based principles, considering scientific knowledge, expert consensus, and clinical experience. \n\nBoth DRVs and FBDGs are relevant and need to be combined in order to develop advanced health applications such as calculating nutritional values of dishes [ ,  ], health recommendation systems [ \u2013 ], etc. The problem with these applications is that they require complete and the latest knowledge about DRVs and FBDGs. Another problem is that existing resources consist of a vast amount of both structured and unstructured data and information. Recent developments in ICT and Computer Science enables collection of the latest knowledge by exploiting the recently published biomedical literature and scientifically validated public health web sites. These resources lack coded data (e.g., unique identifiers from ontologies), but do have a lot of unstructured text that needs to be analyzed to correctly interpret dietary information. The amount of information presented as unstructured text is huge and is growing rapidly, computer-based tools for systematic knowledge identification, extraction, and exploration are welcome to support human experts when making decisions about appropriate nutritional care for specific disease states or conditions in typical settings. \n\nThere are several questions that need to be considered in order to extract relevant knowledge. Having the dietary information represented as unstructured text, the knowledge that needs to be extracted is related to DRVs with corresponding life stage groups, gender groups, and reference values for heights and weights for life stage and gender groups, and food composition data that usually contains information for a huge number of components, such as energy, macronutrients (e.g. protein, carbohydrate, fat), and their components (e.g. sugars, fatty acids), minerals (e.g. calcium, iron, sodium), and vitamins. So the first question is how to select parts of the text (phrases) that will be candidates for the entities in which we are interested. For this purpose, a good tokenization needs to be applied, such that each phrase can be a candidate for an entity and a phrase should not contain information about more than one entity. For example, let us have the recommendation \u201c  The recommended intake for total fiber for adults 50 years and younger is set at 38 g for men and 25 g for women, while for men and women over 50 it is 30 g and 21 g per day, respectively, due to decreased food consumption.  \u201d [ ]. In this example, it is preferred to obtain \u201c  the recommended intake for total fiber  \u201d instead of \u201c  fiber  \u201d because it contains information about DRVs. Also, another question is how to extract all useful information from this recommendation and then to relate it together. The information that for \u201c  adults 50 and younger  \u201d the recommended intake is \u201c  38 g for men  \u201d and \u201c  25 g for women  \u201d, while the recommended intake for \u201c  men and women over 50  \u201d is \u201c  30 g  \u201d and \u201c  21 g per day  \u201d. Then, let us have another dietary recommendation, \u201c  Some breakfast cereals contain 150 to 300 mg of sodium before milk is added.  \u201d. In this example, the phrase \u201c  150 to 300 mg of sodium  \u201d is not preferred because it contains information about more than one entity. It is preferred to have two different phrases \u201c  150 to 300 mg  \u201d and \u201c  sodium  \u201d. Then, a good and representative knowledge base for each entity we are interested in needs to be selected, in order to link each phrase to find a set of candidates for each entity. An additional information is also reported in the action of the recommendation, such as \u201c  contain  \u201d, \u201c  consist  \u201d, \u201c  should further increase  \u201d, etc. \n\nAutomatic identification and classification of words or phrases that describe important concepts (entities) can be done by a process known as Named Entity Recognition (NER) [ ]. NER is a process in which a label (class) or semantic category from a predefined set is assigned to the words or phrases known as entity mentions in order to describe the concept. There exist terminological-driven NER methods that aim to map mentions of concepts within texts to terminological resources, rule-based NER methods that use regular expressions of dictionary information with some characteristics of the entities of interest, and corpus-based NER methods that use evidence from text corpora and usually use machine learning (ML) approaches. Each of these different versions of NER method has its own advantages and limitations. \n\nDifferent ML approaches have been applied in the extraction of concepts from the biomedical literature and the relations that exist between them. For example, there exist NER methods used for extracting relationships between genes and proteins, disease-phenotype relationships, chemical entities, and relationships between drugs and diseases, etc., but to the best of our knowledge there is no research that is focused on the extraction of dietary information concepts and on the relations that exist between them. Even more, for most machine learning approaches an annotated corpora in the domain is required. The annotated corpora in each domain is done by experts from the domain and it requires time and effort to produce it. In the domain of dietary recommendations an annotated corpora provided by the experts is still missing. \n\nIn this paper, we present a method that is an extension of our previous work [ ], which was focused only on knowledge extraction of dietary information from a single sentence. We extended the method with several modifications. First, the extended version of the method works with text that could be a paragraph that contains more sentences instead of working only with one sentence. The sentence segmentation is introduced and each sentence is additionally split into more segments according to a set of rules in order to extract more useful information. Splitting the sentences into more segments improves the results that we obtained in our first attempt. New representative dictionaries, that are used for the entities we are interested in, were also added to improve the obtained results. For the evaluation of the method, instead of using a single sentence as one document as in our previous attempt, a test corpora was created, which contains documents form heterogeneous sources. We continue by explaining the related work, which includes the basic concepts for natural language processing (NLP) and ML, the definition of NER, and the overview of information extraction from the biomedical literature. Then, the newly proposed NER method for knowledge extraction of evidence-based dietary recommendations is explained in detail. At the end, the results and a discussion evaluating the proposed NER method are presented. \n\n\n## Related work \n  \nIn this section, we start by explaining the basic concepts of NLP and ML. Then, we give an explanation of the named entity recognition process that is used for automatic extraction of the useful information from the text. Finally, we give an overview of existing problems and solutions for information extraction from the biomedical literature. \n\n### Natural language processing and machine learning \n  \nNLP is a research area of computer science, artificial intelligence, and computational linguistics, concerned with the interactions between computers and human natural languages. More information about NLP can be found in [ ,  ]. NLP works with data represented as unstructured text, which depends on how people express themselves. Text is processed by sentence segmentation and further the segments are analyzed by applying tokenization that is a process of breaking the segments into words that are called tokens. Each of the tokens consists of a string of characters without white space. The tokens can be analyzed by applying lemmatization [ ] or stemming [ ,  ]. From linguistics, the lemmatization is the process of grouping together different inflected forms of a word so they can be analyzed as a single item. The uninflected form is called lemma. In computational linguistics, the lemmatization is a process of determining the lemma of a given token (word). It usually works by using vocabulary and morphological analysis of the token in order to return the lemma or dictionary form of the token. Stemming is another approach similar to lemmatization. It usually works by removing the suffixes of the token in order to give a good approximation to the lemma. Further, the tokens can be analyzed by applying part-of-speech (POS) tagging that is a process of assigning morphological tags or categories (classes) to each token (e.g, NN (noun, singular or mass), VB (verb, base form), and VBD (verb, past tense)) [ \u2013 ]. Sometimes it can happen that we are not interested into tokens, but we want to determinate text phrases that are concepts (entities). Chunking [ ] is one approach that uses POS tags and identifies short phrases such as noun phrase (NP), verb phrase (VP), preposition phrase (PP), etc. Chunking is usually combined with B-I-O tagging scheme, which gives a tag to each token at the beginning of the phrase (B), inside the phrase (I), and outside of any phrase that is tagged (O). For example, the noun phrase (B-NP, I-NP) consists of two tokens, in which the first token is the beginning of the noun phrase and the second token is inside the noun phrase. Despite the morphological analysis, the sentences can be analyzed according to their syntactic structure. The process of working with the syntactic analysis of the sentences is called parsing [ ]. \n\nAlternatively, we have ML, which is a subfield of computer science related to studies of pattern recognition and computational learning theory in artificial intelligence [ ]. It focuses on developing algorithms that can learn and make predictions based on data. The data is presented as a training set, which is a collection of instances described by attributes called features. If the training set consists of output labels (classes), given by an expert from the data domain, that are the desired output of the algorithm, then we have supervised learning. If the desired output labels are not present in the training set, we have unsupervised learning, the goal of which is to find some hidden patterns in the data. Also, semi-supervised learning exists, which is a combination of supervised and unsupervised learning. The idea of the ML algorithms is to give further analyses of new unseen instances that are not present in the training set. For example, in supervised learning the output label (class) of these new unseen instances needs to be predicted using the algorithm. Because the ML supervised algorithms perform well for the instances from the training set, their evaluation needs to be done by using a test set that consists of instances that are not found in the training set. For this purpose, the training set is often randomly split into two portions, the training set and the test set. Another approach for evaluating the performance of ML supervised algorithms is to use cross-validation [ ]. \n\nML supervised algorithms are the most used algorithms for information extraction from text. They are based on annotated corpora, which include text in which the labels of the entities of interest are assigned by domain experts. Using it, different ML models such as decision trees [ ], support vector machines (SVMs) [ ], hidden Markov models [ ], conditional random fields (CRFs) [ ], maximum entropy [ ], etc., can be applied in order to achieve better performance. Moreover, the idea of ensemble learning [ ] can be used to combine multiple learning algorithms to obtain better performance that could be obtained from any of the constituent learning algorithms alone. \n\n\n### Named entity recognition \n  \nNamed entity recognition (NER) [ ] is a part of information extraction that aims to determine and identify words or phrases in text into predefined labels (classes) that describe concepts of interest in a given domain. There exist various NER methods. \n\nTerminology-driven NER methods, also called dictionary-based NER methods [ \u2013 ] work by matching the text phrases with concept synonyms that exist in the terminological resources (dictionaries). In order to improve the performance of these methods, instead of strict matching they are combined with some heuristics such as the generating of words that occur in entity mentions, generating permutations of words in concept synonyms, solving disambiguation problem, etc. The main disadvantage of these methods is that only the entity mentions that exist in the resources will be recognized, but the benefit of using them is related to the frequent updates of the terminological resources with new concepts and synonyms. \n\nAnother NER methods are rule-based NER methods [ ,  ], which use regular expressions that combine information from terminological resources and characteristics of the entities of interest. The main disadvantage of these methods is the manual construction of the rules, which is a time-consuming task and depends on the domain. \n\nCorpus-based NER methods [ \u2013 ] are based on the evidence that exists in an annotated corpora provided by human experts from the domain and use of ML algorithms to predict the entities labels. These methods are less affected by terminological resources and manual created rules, but the limitation is the existence of an annotated corpora for the domain of interest. The construction of the annotated corpora for a new domain is a time consuming task and requires effort by the human experts to produce it. \n\n\n### Overview of information extraction from biomedical literature \n  \nNowadays, the information extraction from the biomedical literature is a very important task in order to improve public health. Because the NER methods with best performances are usually corpus-based NER methods, there is a need for annotated corpora from biomedical literature that will include the entities of interest. For this purpose, different annotated corpora are produced by shared tasks, where the main aim is to challenge and encourage research teams on NLP problems. \n\nBioNLP Shared Task 2013 [ ] aims to provide a common framework for information extraction in the biomedical domain. The biological questions addressed by this task were related to the molecular biology domain and its related fields. The BioNLP Shared task 2013 consists of six tasks: gene event extraction, cancer genetics, pathway curation, corpus annotation with gene regulation ontology, gene regulation networks in bacteria, and bacteria biotopes. BioNLP Shared Task 2016 comprises three tasks that address different aspects of knowledge acquisition from text and also encompasses a wide range of biological diversity [ \u2013 ]. The SeeDev task [ ] aimed at extracting the regulation of the seed development in plants using a rich model. The Bacteria Biotopes 3 (BB3) task [ ,  ] was used for the construction of a bacteria habitat database using external ontologies. The Genia 4 (GE4) task [ ] aimed at delivering a new shared task framework to construct a knowledge base of NFkB synthesis and regulation through IE. \n\nBioCreative II gene mention recognition [ ] was a task, where different systems were designed to identify substrings in sentences corresponding to gene name mentions. The annotated corpora was provided to the participants, on which different methods were used and results in the performances varied. The best system was a semi-supervised learning method known as alternating structure optimization (ASO) [ ]. Other systems were developed by using supervised ML algorithms. The second best performing system used CRFs [ ], the third best performing system used a combination of two SVMs and one CRF [ ], and the fourth best performing system used a multimodal approach with two CRFs [ ]. For example, one system that is evaluated on the BioCreative 2 GM task training corpus is BANNER [ ]. This is an open-source biomedical named-entity recognition system implemented using CRF. It represents an innovative combination go known advances beyond the existing open-source systems, in a consistent, scalable package that can easily be configured and extended with additional techniques. The work on gene mention recognition continued in BioCreative III [ ], where the focus was on three tasks: cross-species gene normalization using full text, extraction of protein-protein interactions from full text, including document selection, identification of interacting proteins and identification of interacting protein pairs, and an interactive demonstration task for gene indexing and retrieval task using full text. In BioCreative IV [ ], the gene ontology annotation task was reintroduced along with the following new tasks: interoperability of text mining systems, web service-based named entity recognition, and chemical/drug entity name recognition. In the chemical/drug named entity recognition two main aspects were covered, the chemical document indexing and the chemical entity mention recognition. The extraction of chemical entities from unstructured text is a very important task for different research areas, because they are related with metabolism, enzymatic reactions, potential adverse effects, etc. The systems presented are based on three general strategies: supervised ML approaches, rule/knowledge-based approaches, and chemical dictionary look-up approaches [ ]. The evaluation of the systems performances was made using the CHEMDNER annotated corpus that was provided by the workshop [ ]. Most systems that use supervised ML methods are based on the CRFs, some of them used SVMs, and some a combination of SVMs and CRFs. There were presented systems that use mainly rule-based methods, but these require a deep understanding of both the existing chemical nomenclature standards as well as of the CHEMDNER annotation guidelines. The use of dictionary-lookup based systems required efficient dictionary pruning and post-processing of the results. For example, becas [ ] provides annotations for isolated, nested, and intersected entities. It uses dictionary-matching techniques to recognise species, anatomical concepts, microRNAs, enzymes, chemicals, drugs, diseases, metabolic pathways, cellular components, biological processes, and molecular functions. It gives an opportunity to choose the types of entities in which you are interested. It was validated against CRAFT [ ], AnEM [ ], and NCBI Diseases corpora [ ], achieving an f-measure of 76% for genes and proteins, 95% for species, 65% for chemicals, 83% for cellular component, 92% for cells, 63% for molecular functions and biological processes, 83% for anatomical entities, and 85% for diseases. Becas[chemicals] [ ] is a web application and API for recognizing and annotating chemical compounds and drugs. It is a special branch of becas API focusing on identifying of a large array of chemical substances. It uses machine-learning techniques, with an optimized feature set including orthographic, morphological, natural language processing, domain knowledge, and local context features. It was validated against the BioCreative IV CHEMDNER task corpora, achieving an f-measure of 87.48% for chemical named entities. The work related to chemical named entity recognition continued also in BioCreative V [ \u2013 ], where the focus was on disease and symptoms related entities and relations that exist between chemical/drug entities and disease entities. BioCreative 2016 was focused on four main tasks: applications of text mining methods in areas such as crowdsourcing, database curation, the publication process, and metagenomics; methods for annotations such as disease, phenotype, and adverse reactions in different text sources literature, clinical records and social media; methods to achieve interoperability, generalisability, and scalability in text mining: BioC [ ], RDF and semantic web, among others; and the application of ontologies in text mining and text mining as an ontology builder. \n\nFrom an overview of the existing information extraction methods from the biomedical literature, we can see that a lot of NER methods exist in the domain of biomedical literature and they are focused on different biomedical domains. The commonly used NER methods are the corpus-based NER methods that rely on annotated corpora for the domain of interest, which is produced by the domain experts. We did not find any research that focuses on extracting dietary information from text. Also, we could not find an annotated corpora that can be used to developed corpus-based NER methods for dietary information. Therefore, we have developed a new NER method for knowledge extraction of evidence-based dietary recommendations from unstructured text data that is not annotated. The method uses a combination of NLP techniques with Boolean algebra rules and matrix theory in order to extract the entities from the domain. \n\n\n\n## Methods \n  \n### A rule-based NER method for information extraction of evidence-based dietary recommendations \n  \nThe dietary information presented in the evidence-based dietary recommendations is provided through the information about DRVs and FBDGs. Because an annotated corpora in this domain is still missing, after several discussions with the human experts from the domain, we realized that the basic dietary information that needs to be extracted is represented with food entities,   Food  , nutrient entities (or chemical components),   Nutrient  , and associated quantity/unit entities,   Quantity  /  Unit  . Also, there is a need to track an additional information that is reported as an action in the recommendation (e.g, increase, decrease, contains, etc.) in order to properly interpret the information provided. Despite the main entities that are important, the recommendation can also consist of other useful information that needs to be tracked such as life stage group of the population, national characteristics of the population, cultural habits, etc. \n\nIn order to extract the dietary information provided by evidence-based dietary recommendations, we proposed a novel rule-based NER method, that we call drNER. It is a combination of the terminological-driven NER and rule-based NER. The difference with the purely terminological-driven NERs is that we do not use only dictionaries with concepts and synonyms, but we allow the reuse of some corpus-based NERs that exist for some entities. So if corpus-based NERs exist for some entities we are interested in, we can use these systems to annotate the text data and then to see if some tokens have labels that correspond to entities of interest. We also combine corpus-based NERs that exist for some entities in which we are interested, following the idea of ensemble learning in order to achieve better performance that could be obtained from any corpus-based NER alone. The difference with the rule-based NERs is that we do not use rules associated with the characteristics of the entities. This is because having rules for each of the entities we are interested in, requires too much time and effort to produce them. We only used a small number of Boolean algebra rules that are not related to the characteristics of the entities, but help us define the phrases that are entities mentions. \n\nDrNER works with text that is composed of sentences and paragraphs. First, we split the text using sentence segmentation. Then, each sentence is split with an additional segmentation and each sentence segment is pre-processed and analyzed in two phases. The first one is detection and determination of the entities mentions. It is based on a combination of NLP methods, matrix theory, and boolean algebra. The second one is the selection and extraction of the entities. It uses the information from the first phase by representing it using graph theory and a small set of rules that define how to extract the useful entities. The workflow diagram of the NER method is presented in  . \n   DrNER workflow diagram.    \nFor a better understanding of the drNER, the pseudocode of the method is presented in Algorithm 1. \n\n Algorithm 1   drNER \n\n1: \u2009\u2009\u2009Apply sentence segmentation on the text document \n\n2: \u2009\u2009\u2009  for   each sentence, \u03a6 , in the set of sentences, \u03a6   do  \n\n3: \u2003\u2009\u2009\u2009Obtain   Chunks   vector by introducing the default chunking on \u03a6 \n\n4: \u2003\u2009\u2009\u2009Split \u03a6  on the position of each   ADV P  ,   CONJP  ,   SBAR   chunk, or two successive chunks (  O  ,  O  ), into set \u03a8 \n\n5: \u2003\u2009\u2009\u2009  for   each sentence segment, \u03a8 , in \u03a8   do  \n\n6: \u2003\u2003\u2009\u2009\u2009Obtain the   Tokens   vector by using the word-level tokenization on \u03a8 \n\n7: \u2003\u2003\u2009\u2009\u2009Obtain the   Chunks   vector by introducing the default chunking on \u03a8 \n\n8: \u2003\u2003\u2009\u2009\u2009Obtain the   X   matrix using the  \n\n9: \u2003\u2003\u2009\u2009\u2009Obtain the   X   matrix using the  \n\n10: \u2003\u2003Obtain the   X   matrix using the  \n\n11: \u2003\u2003Perform the first post-hoc chunking \n\n12: \u2003\u2003Perform the second post-hoc chunking \n\n13: \u2003\u2003Perform the third post-hoc chunking \n\n14: \u2003\u2003Recalculate the   X   matrix using the  \n\n15: \u2003\u2003Recalculate the   X   matrix using the  \n\n16: \u2003\u2003Select the   Action   entity by searching the predicate in the VP subtrees \n\n17: \u2003\u2003Extract all entities \n\n18: \u2003\u2003Add the labels for (Subject, Predicate, Object) using   S  ,   P  ,   O  \n\n19: \u2003  end for  \n\n20:   end for  \n\n21: Return list of entities together with (Subject, Predicate, Object) labels \n\n\n### Dictionaries \n  \nWe start by explaining the dictionaries or the terminological resources we used for the drNER. \n\nFor the   Quantity  /  Unit   entity, an ontology, called Units of Measurements Ontology (UO) [ \u2013 ], is used. The UO is currently being used in many scientific resources for the standardized description of measurement units. From it, the names of the units together with their symbols are extracted. In addition, a list of measurement units that are used for recipes, such as tablespoon, teaspoon, etc. are added. \n\nFor the   Nutrient   and the   Food   entity, dictionaries are constructed using the outputs of different NERs appropriate for the entity. \n\nFor the   Nutrient   entity, a combination of becas API [ ], becas[chemicals] API [ ] and a semantic tagger, known as USAS online English semantic tagger [ \u2013 ], is used. Both, becas and becas[chemicals], are web-services-based and corpus-based NER developed as a prat of BioCreative IV CHEMDNER task. Becas provides annotations for isolated, nested, and intersected entities. It uses dictionary-matching techniques to recognise species, anatomical concepts, microRNAs, enzymes, chemicals, drugs, diseases, metabolic pathways, cellular components, biological processes, and molecular functions. It also gives an opportunity to choose the types of entities. Becas[chemicals] is a web application and API for recognizing and annotating of chemical compounds and drugs. It is a special branch of becas API focused on the identification of a large array of chemical substances. It uses machine-learning techniques, with an optimized feature set including orthographic, morphological, natural language processing, domain knowledge, and local context features. The USAS online English semantic tagger is a part of the UCREL semantic analysis system, which is a framework of automatic semantic analysis of text that has been designed across a number of research projects since 1990. The USAS version contains 21 major entity labels, with the possibility of subdivision for some of them. For our purpose, the focus is on one category with the semantic label   O  , which is for terms related to substances, materials, objects, and equipment. From this entity label, we use only one subcategory   O   that is for terms relating to substances and materials generally. The idea of using a combination of NER systems comes from the idea of ensemble learning in order to achieve a better performance of identifying the chemical entities that could be obtained from any of them alone. So if a token is annotated by at least one of these systems, as a chemical entity using becas and becas[chemicals] or   O   using the USAS tagger, then we can assume that the token belongs to this dictionary. \n\nFor the   Food   entity the same semantic tagger as the   Nutrient   entity, known as USAS online English semantic tagger, is used. The focus is on two categories. The first one is the category for terms related to Food and Farming,   F  . From it, four subcategories are used. The first subcategory is for terms related to food and food preparation,   F  , the second is for terms related to drinks and drinking,   F  , the third is for terms related to cigarettes and drugs,   F  , and the forth for terms related to agriculture and horticulture,   F  . The second category is for terms related to Life and Living things,   L  . From it, two subcategories are used. The first one is for terms related to living creatures (e.g. non-human),   L  , and the second subcategory is for terms related to plants and plant-life,   L  . In  , the summary of the USAS English tagger categories and subcategories is presented. So if a token is annotated by this system as   F  ,   F  ,   F  ,   F  ,   L  , or   L  , then we can assume that the token belongs to this dictionary. \n   USAS categories.      \nIt is not possible to provide information about the size of various dictionaries being used because they are not classical dictionaries that consist of concepts with synonyms. For the   Quantity  /  Unit   entity, an ontology is used together with kitchen-related units. For the   Nutrient   entity, three corpus-based NER systems are used, so the results provided as annotations from these three systems are used and combined. Also, for the   Food   entity, a corpus-based NER system is used, so the results provided as annotations are used. \n\n\n### Pre-processing \n  \nBefore we start with knowledge extraction, the first step is to pre-process the text data. First, sentence segmentation is used for each text document. Then, default chunking [ ] is introduced on each sentence. In our implementation, the Apache OpenNLP Maxent sentence detector is used for sentence segmentation and the Apache OpenNLP Maxent chunker is used for default chunking. They are part of the   openNLP   R package [ ]. After default chunking, if the sentence consist of   ADV P  ,   CONJP  ,   SBAR   chunks, or two successive chunk tokens that are (  O  ,  O  ), we need to split the sentence on that place or places. The   ADV P   chunk is for an adverbial phrase, the   CONJP   is for a conjunctive phrase, the   SBAR   is for a subordinated clause, and (  O  ,  O  ) means that two successive tokens are outside of any chunk. Splitting the sentence into more segments is useful to extract more information that can stay hidden if the sentence is not split. Then for each sentence segment the double quotation marks and brackets are removed. \n\nTo explain the difference between sentence and sentence segments, we continue with an example. Let one sentence, obtained from sentence segmentation of a text document, be \u201c  The recommended intake for total fiber for adults 50 years and younger is set at 38 g for men and 25 g for women, while for men and women over 50 it is 30 g and 21 g per day, respectively, due to decreased food consumptions.  \u201d [ ]. The result of the default chunking on this sentence is presented in  . The column   Tokens   corresponds to the tokens obtained by the word-level tokenization and the column   Chunk tokens   corresponds to the chunk token obtained by the default chunking. Further, because this sentence consists of two   ADV P   chunks, it needs to be split on that places. After splitting, the obtained sentence segments are: \u201c  The recommended intake for total fiber for adults 50 years and younger is set at 38 g for men and 25 g for women.  \u201d, \u201c  For men and women over 50 it is 30 g and 21 g per day.  \u201d, and \u201c  Due to decreased food consumptions.  \u201d. These sentence segments are further used by the proposed method. \n   Default chunking result for one sentence.      \n\n### First phase: Detection and determination of the entities mentions \n  \nLet \u03a6 be a sentence or sentence segment that contains dietary information. We start by introducing the word-level tokenization on \u03a6. The result is a   n   \u00d7 1 vector,   Tokens  , whose elements are the tokens from \u03a6, and   n   is the number of tokens obtained after the tokenization. Then we continue with POS (part-of-speech) tagging and the result is an   n   \u00d7 1 vector,   POS  , which is a collection of POS tags for \u03a6. \n\nAfter processing the sentence at the word-level, we continue with default chunking, which segments and labels multitoken sequences called chunks. The result is an   n   \u00d7 1 vector,   Chunks  , whose elements are chunk tokens tagged in the B-I-O tagging format. \n\nThe next step is to define an   n   \u00d7   m   matrix,   X  , where   m   is the number of chunk tokens from the   Chunks   vector that begin with the prefix B- or O. The elements of the matrix   X   are defined by the  , so if the token belongs to the chunk we have 1, and 0 otherwise.\n \nwhere   i   = 1, \u2025,   n   and   j   = 1, \u2025,   m  . \n\nLet   k   be the number of entities we are interested in, which in our case is 3,   Food  ,   Nutrient  , and   Quantity  /  Unit  . In order to detect and determinate the entities mentions, we try to link each chunk with the information from additional terminological resources (dictionaries) related to the domains of the entities,   Dictionary  ,   l   = 1, \u2025,   k  . Once more we would like to point out that it is not necessary that these dictionaries are standard resources, which consist of concepts with synonyms, but also they can be NER systems that exist for some entities from the domain, or even more a combination of NERs in order to achieve better performance. Then an   n   \u00d7   k   matrix,   X  , is defined as\n \n\nAfter obtaining the matrices   X   and   X  , an   m   \u00d7   k   matrix,   X  , is defined as\n \nThe rows of the matrix   X   correspond to the chunks and the columns are the dictionaries we included for the entities. For example if the element   X  [  i  ,   l  ] \u2265 1, this means that the   i  -th chunk is an entity mention solution for the   l  -th entity. The additional step is to check if a chunk is an entity mention of more entities. If this is the case, then the chunk obtains the entity tag from the last token. \n\nIn most cases, the potential entity mentions are noun phrases that are the linguistically meaningful units, but sometimes it can happen that the entity mention can consist of more noun phrases or even combinations of noun phrases with some other morphological phrases. To improve the quality of text phrases that can be entities mentions, three additional post-hoc chunkings are introduced. The first post-hoc chunking combines the information from the default chunking and the entities labels for each chunk by defining Boolean algebra rules. The second post-hoc chunking uses the information from the first post-hoc chunking and the POS tags of the tokens. The last one combines the information from the second post-hoc chunking and the entities labels for each chunk by defining rules. \n\n#### First post-hoc chunking \n  \nIn the first post-hoc chunking, trigrams of successive chunks (  Chunk  ,   Chunk  ,   Chunk  ) are analyzed and merged into one new noun chunk if the trigram is composed as (  B   \u2212   NP  ,   B   \u2212   PP  ,   B   \u2212   NP  ). This is done except in cases when the two noun chunks correspond to entity mentions of different entities because merging them can lose information about one entity described by one of the noun chunks. In order not to lose this information, we define a boolean function when this chunking needs to be performed. In   the boolean function, together with the boolean variables   A   and   B  , that in our case can be different entities, is presented. Further, a Karnaugh map  , also known as a K-map [ ], is used to simplify the boolean algebra expression when this chunking needs to be performed. The boolean algebra expression or the boolean function, is obtained in a simplified form, as a sum of minterms, as\n \nBecause the number of the entities we are interested in can be greater than 2,   k   > 2, the boolean algebra expression obtained using   needs to be defined for each variation of pairs of entities. The number of functions is determined using the formula of the variations without repetition  , where   r   is the number of different elements, in our case the number of different entities,   k  , and   w   is the size of the variation or how many elements need to be selected from the set of   r   elements. In our case,   w   is 2 because we are working with a pair of entities. Then all the obtained functions are merged together with boolean   AND   conditions into one expression. This expression defines whether the first chunking needs to be performed. \n   The Boolean function for the first post-hoc chunking.         Karnaugh map of the Boolean function for the first post-hoc chunking.    \nTo see how the first post-hoc chunking works, let us focus on one example using the sentence \u201c  People of any age who are African Americans should further reduce sodium intake to 300 mg per day.  \u201d [ ]. This sentence does not consist of   ADV P  ,   CONJP  ,   SBAR   chunks, or two successive chunks that are (  O  ,  O  ), so it is not split into sentence segments. The results of the default chunking and linking each token to a dictionary is presented in  . The   Tokens   column corresponds to the result of the word-level tokenization. The   POS tags   column corresponds to the result of the POS tagging. The   Chunk tokens   column corresponds to the result of the default chunking, where each chunk token is presented in B-I-O tagging format and the beginning of each new chunk is marked with the symbol *. The   Food  ,   Nutrient  , and   Quantity/Unit   columns correspond to the linking of each token to the dictionaries used for each entity. In the column   Chunk  tokens   the result of the first post-hoc chunking is presented, here only the new chunks formed are presented in bold font. From this table, we can see how each chunk consists of one or more chunk tokens. For example, \u201c  sodium intake  \u201d is one noun chunk that consists of two chunk tokens (  B   \u2212   NP  ,   I   \u2212   NP  ). After using the information given in this table and calculating the matrices   X   and   X  , the matrix   X   is presented in  . \n   Example of first post-hoc chunking.           X .        \nFurther, the information from the matrix   X   is used for the first post-hoc chunking. In order not to lose information by applying it, the boolean function, must be defined. In our case, we are interested in three entities,   Food  ,   Nutrient  , and   Quantity/Unit  . By using the  , the function is defined for all variations of pairs of entities. In our case, the number of variations is   V   = 6. Let the trigram of successive chunks be (  Chunk  ,   Chunk  ,   Chunk  ). Then, the boolean function is defined as\n \nwhich is true only if there is no pair for which both values are one. In the example, there are three trigrams of successive chunks that satisfy the condition (  B   \u2212   NP  ,   B   \u2212   PP  ,   B   \u2212   NP  ). They are (\u201c  People  \u201d, \u201c  of  \u201d, \u201c  any age  \u201d), (\u201c  sodium intake  \u201d, \u201c  to  \u201d, \u201c  300 mg  \u201d), and (\u201c  300 mg  \u201d, \u201c  per  \u201d, \u201c  day  \u201d). For the first one, (\u201c  People  \u201d, \u201c  of  \u201d, \u201c  any age  \u201d), the boolean function defined by   is true because all entity labels are 0. So this post-hoc chunking needs to be performed and the chunks are merged into one new noun chunk, \u201c  People of any age  \u201d. For the trigram (\u201c  sodium intake  \u201d, \u201c  to  \u201d, \u201c  300 mg  \u201d), the boolean function is false because the label   Nutrient   = 1 and the label   Quantity  /  Unit   = 1, so there is a pair where both values are one. According to this, \u00ac(  Nutrient   \u2227   Quantity  /  Unit  ) = 0 and the value of the boolean function is false, so in this case the first post-hoc chunking should not be performed. In this example, if we merged the chunks we will lose information about one entity. For the last trigram (\u201c  300 mg  \u201d, \u201c  per  \u201d, \u201c  day  \u201d), the label   Quantity  /  Unit   = 1, while other labels are   Nutrient   = 0,   Food   = 0, so the boolean function is true. In this case, the first post-hoc chunking should be performed and the new chunk is \u201c  300 mg per day  \u201d. \n\n\n#### Second post-hoc chunking \n  \nIn the second post-hoc chunking, trigrams of successive chunks obtained by the first post-hoc chunking (  Chunk  ,   Chunk  ,   Chunk  ) are analyzed and merged into one new noun chunk if the trigram is composed as (  B   \u2212   NP  ,   B   \u2212   V P  ,   B   \u2212   NP  ) and the first noun chunk has a POS tag that is a Wh-pronoun [ ], such as who, what, which, etc. \n\nTo see how the second post-hoc chunking works, the obtained result from the example of the first post-hoc chunking is used. In this post-hoc chunking we are interested in the trigrams of successive chunks that satisfy the condition (  B   \u2212   NP  ,   B   \u2212   V P  ,   B   \u2212   NP  ). By using the obtained result from the first post-hoc chunking, which is presented in  , column   Chunk  tokens  , there are two trigrams that satisfied this condition, (\u201c  who  \u201d, \u201c  are  \u201d, \u201c  African Americans  \u201d) and (\u201c  African Americans  \u201d, \u201c  should further reduce  \u201d, \u201c  sodium intake  \u201d). From them, only the first one has the first noun chunk that is a Wh-pronoun, so they are merged together, \u201c  who are African Americans  \u201d. \n\n\n#### Third post-hoc chunking \n  \nIn the third post-hoc chunking, bigrams of successive chunks obtained by the second post-hoc chunking (  Chunk  ,   Chunk  ) are merged into one new noun chunk if the bigram is composed as (  B   \u2212   NP  ,   B   \u2212   NP  ) and only one of the noun chunks is labeled as an entity of interest, or both of them have the same label. \n\nBy using the obtained result from the example of the second post-hoc chunking, there are no bigrams of chunks that satisfied the condition (  B   \u2212   NP  ,   B   \u2212   NP  ), and only one of the noun chunks is labeled as an entity of interest, or both of them have the same label. \n\nAfter performing the three post-hoc chunkings, the matrix   X   needs to be recalculated because the number of chunks is different from the number obtained by the default chunking. At the end of the first phase, the matrix   X   is recalculated and their columns correspond to the sets of entity mentions for each entity. \n\n\n\n### Second phase: Selection and extraction of the entities \n  \nThe result from the first phase of the NER method are sets of entities mentions for each entity. The next step is the second phase in which the entities mentions form the sets that contribute to the dietary information need to be selected. \n\nFor this purpose, the sentence or sentence segment is represented as a graph, in which each chunk is connected only with its neighbors (predecessor and successor chunk). Then the start or initial node of the graph from where the search for all entities begins, is selected using syntactic bracketing or tree parsing. Each sentence or sentence segment, \u03a6, is represented by the parser as a tree having three children: a noun phrase (NP), a verbal phrase (VP) and a full stop (.). In addition, each sentence is formed as a combination of three parts,   Subject  ,   Predicate  , and   Object   [ ]. The   Subject   is the person or a thing who or which carries out the action of the verb. The   Predicate   in a sentence is what us tells about what a person or a thing does or did, or what happened to a person or to a thing. The   Object   is the person or a thing upon whom or upon which the action of the verb is carried out. \n\nThe initial node of the graph is the predicate of the sentence. The search for the predicate is performed in   V P  . The initial node can be found in the following subtrees   V B   (verb, base form),   V BD   (verb, past tense),   V BG   (verb, present participle or gerund),   V BN   (verb, past participle),   V BP   (verb, present tense, not 3rd person singular),   V BZ   (verb, present tense, 3rd person singular), and   MD   (verb, modal). Further, from all solutions returned by searching for the predicate, the initial node is the verb chunk that is closest to the root of the sentence (number of edges from the verb node to the root node) and it is located in the verbal phrase that is closest to the root. The extracted predicate is stored in an entity called   Action  . We need to note here that because of the segmentation defined in the pre-processing step, which is done in order to extract all relevant information, it can happen that some sentence segments do not have a   V P  , so the   Action   entity is not returned. \n\nIf the   Action   entity is selected, all other entities of interest need to be selected. Because it can happen that no entity mention is the subject in the sentence, one additional entity called   Group   is added, into which the noun chunks that perform the action are stored. The   Group   entity is searched in the predecessor chunks from the   Action   entity that is selected. The search starts from the   Action   entity and then goes back to the beginning of the sentence. The results are the successive noun chunks that can also be separated by punctuation. \n\nIn order to know on which side of the   Action   entity the extracted entities are located, one of the labels   S  ,   P  , or   O   that indicate (  Subject  ,   Predicate  ,   Object  ) is added to each extracted entity. The   Action   entity has the label   P   because it is the predicate of the sentence. All entities that are predecessor chunks of the   Action   entity have the label   S   as they are subjects in the sentence. The entities that are successor chunks of the   Action   entity have the label   O   because they correspond to the objects in the sentence. \n\nTwo scenarios of entities selection exist. In the first one if the   Action   entity is not selected, then all the entities mentions from the   X   matrix are extracted. In the second scenario, only one   Action   entity is returned. Then for each entity using the set of its entities mentions, the entity mention or the chunk that is closest to the   Action   entity is selected, according to the number of edges between the candidate and the   Action   entity in the graph. If the set of entities mentions consists of more candidates for the same entity, they are extracted if they are on the same side from the   Action   entity as the one extracted or they are on the other side of the   Action   entity, but there is no additional verbal phrase in this part of the sentence. \n\n\n\n## Results and discussion \n  \nIn this section, we present the result of evaluation of the proposed NER method in the domain of evidence-based dietary recommendations. The main basic entities in the domain are the   Food   entity,   Nutrient   entity, and   Quantity  /  Unit   entity. For a better understanding of how the method works, we provide two examples. The first example focuses only on one sentence and the second one provides the results obtained from several sentences in order to present the concepts in which we are interested. Further, the construction of the heterogeneous test corpora that consists of text paragraphs is explained. Then, the obtained result using the explained test corpora is presented. Finally, we compare the methodology of the drNER with the methodologies of some other approaches that exist for biomedical domains. \n\n### Examples \n  \n#### Example 1 \n  \nTo demonstrate how drNER works, we give an example where the focus is only on one sentence that provides dietary information. \n\nLet \u03a6  be the dietary recommendation \u201c  People of any age who are African Americans should further reduce sodium intake to 300 mg per day.  \u201d [ ]. \n\n gives results from the first phase of the NER method for \u03a6 . The   Tokens   column corresponds to the result of the word-level tokenization. The   POS   tags   column corresponds to the result of the POS tagging. The   Chunk   tokens   column corresponds to the result of the default chunking, where each chunk token is presented in the B-I-O tagging format and the beginning of each new chunk is marked with an *. The   Food  ,   Nutrient  , and   Quantity  /  Unit   columns correspond to the linking of each token to the dictionaries used for each entity. In the column   Chunk   tokens   the result of the first post-hoc chunking is presented, where new chunks formed by this chunking are presented in bold font. The new chunks formed by the second post-hoc chunking are presented in bold font in the   Chunk   tokens   column. In the   Chunk   tokens   column the result of the third post-hoc chunking is presented and in this example nothing is changed by applying this chunking. \n   The first phase of the drNER method for \u03a6 .        \nThen, by using the  , the   X   matrix is calculated, where rows correspond to the different chunks and columns correspond to the entities,   Food  ,   Nutrient  , and   Quantity  /  Unit  . The   X   matrix has 6 rows (different chunks) and 3 columns. The   Food   column gives the set of the entities mentions for the   Food   entity, which in our example is an empty set because the dietary recommendation does not consist of food entities. The   Nutrient   column gives the set of entities mentions for the   Nutrient   entity, and is a set with one element that is \u201csodium intake\u201d identified by the row and the chunk that has a nonzero element in the   Nutrient   column. The   Quantity  /  Unit   column gives the set of the entities mentions for the   Quantity  /  Unit   entity, and is a set with one element that is \u201c300 mg per day\u201d. \n\nAfter the first phase, the recommendation \u03a6  is represented as undirected graph, where each chunk is connected only with its neighbors. In   a graphic representation of the recommendation \u03a6  is presented. \n   Graphic representation of the recommendation \u03a6 .    \nThe first step of the second phase is to select the initial node of the graph or the   Action   entity from where the search for all entities will start. To select the   Action   entity the parse tree of the recommendation \u03a6  is used. In   we present the parse tree of the recommendation \u03a6 , from which we search for the predicate in the verbal phrases. The result is the verb \u201cshould\u201d from the   MD   subtree, since it is closest to the root of the sentence. So, the chunk that consists of the returned verb, \u201cshould further reduce\u201d, is selected as the   Action   entity. \n   Parse tree of \u03a6 .    \nThe last step of the second phase is to select all other important entities. By using the second scenario (since the   Action   entity is returned), we found one   Nutrient   entity \u201csodium intake\u201d, one   Quantity  /  Unit   entity \u201c300 mg per day\u201d, and for the   Group   entity we obtained \u201cPeople of any age\u201d and \u201cwho are African Americans\u201d, while we did not find   Food   entity because there are no food related terms in the recommendation. At the end, the labels for the   Subject  ,   Predicate   and   Object   are added (\u201cPeople of any age\u201d,   S  ), (\u201cwho are African Americans\u201d,   S  ), (\u201cshould further reduce\u201d,   P  ), (\u201csodium intake\u201d,   O  ), and (\u201c300 mg per day\u201d,   O  ), which is also the result from our method. The index of the labels indicates from which part of the sentence the entity is extracted. In this example it is 1, because the recommendation does not contain any   ADV P  ,   CONJP  ,   SBAR  , or two successive chunks that are (  O  ,  O  ), so it is not split at the beginning. \n\n\n#### Example 2 \n  \nIn the second example we present the results obtained for 15 sentences that are extracted from the Food and Nutrition Information Centre of United States Department of Agriculture [ ]. This example helps the readers to get more familiar with the concepts from the dietary domain that need to be extracted. The results are presented in  . \n   Knowledge extraction of 15 dietary recommendations.      \nIf we look at sentence 5, \u201c  1 teaspoon of table salt contains 2300 mg of sodium.  \u201d, the recommendation is not split by the splitting proposed in the pre-processing part because it does not consist of   ADV P  ,   CONJP  ,   SBAR  , or two successive chunks that are (  O  ,  O  ). By using the drNER method (\u201ctable salt\u201d,   S  ) is the   Food   entity extracted. There is one   Action   entity, (\u201ccontains\u201d,   P  ), one   Nutrient   entity, (\u201csodium\u201d,   O  ), and two   Quantity  /  Unit   entities, (\u201c1 teaspoon\u201d,   S  ) and (\u201c2300 mg\u201d,   O  ). The labels that are given to each of the extracted entities are the labels for the   Subject  ,   Predicate  , and   Object   that help us better to interpret the extracted information. For example, in this dietary recommendation we have two   Quantity  /  Unit   entities, one of them is related to an entity extracted in the subject of the sentence, and the other one is related to an entity that is extracted in the object of the sentence. For example, from the label of the   Quantity  /  Unit   entity, (\u201c1 teaspoon\u201d,   S  ), we can see that this entity is related to some other entity that is extracted from the same part of the sentence, or the   Food   entity, (\u201ctable salt\u201d,   S  ). The other   Quantity  /  Unit   entity, (\u201c2300 mg\u201d,   O  ), is related to an entity that is found in the object of the sentence, or in our case is related to the   Nutrient   entity, (\u201csodium\u201d,   O  ). Finally, the extracted knowledge can be interpreted as ((\u201c1 teaspoon\u201d,   S  ), (\u201ctable salt\u201d,   S  )) and ((\u201c2300 mg\u201d,   O  ), (\u201csodium\u201d,   O  )), and (((\u201c1 teaspoon\u201d,   S  ), (\u201ctable salt\u201d,   S  )); (\u201ccontains\u201d,   P  ); ((\u201c2300 mg\u201d,   O  ), (\u201csodium\u201d,   O  ))). \n\nAlternatively, the sentence 13, \u201c  The recommended intake for total fiber for adults 50 years and younger is set at 38 g for men and 25 g for women, while for men and women over 50 it is 30 g and 21 g per day, respectively, due to decreased food consumption.  \u201d, consists of two adverb chunks, so it needs to be split. If the recommendation is not split, than the entities \u201cThe recommended intake for adults\u201d and \u201c50 years\u201d are extracted as   Group   entities, \u201cis set\u201d and \u201cis\u201d are extracted as   Action   entities, \u201cdecreased food consumption\u201d is extracted as   Food   entity, and \u201c38 g for men\u201d and \u201c25 g for women\u201d are extracted as   Quantity  /  Unit   entities, and the information for men and women over 50 remains hidden, since it is not extracted. For this reason, the recommendation is split in the location of each adverb chunk. In this recommendation, there are two adverb chunks, \u201cwhile\u201d and \u201crespectively\u201d, so we split it in three parts, \u201c  The recommended intake for total fiber for adults 50 years and younger is set at 38 g for men and 25 g for women.  \u201d, \u201c  For men and women over 50 it is 30 g and 21 g per day.  \u201d, and \u201c  Due to decreased food consumption.  \u201d. The proposed method is then used on each part of the recommendation obtained after splitting. For the first part, the extracted entities are: (\u201cThe recommended intake for total fiber for adults\u201d,   S  ) as   Nutrient   entity, (\u201c50 years\u201d,   S  ) and (\u201cyounger\u201d,   S  ) as   Group   entities, (\u201cis set\u201d,   P  ) as   Action   entity, and (\u201c38 g for men\u201d,   O  ) and (\u201c25 g for women\u201d,   O  ) as   Quantity  /  Unit   entities. By applying the method on the second part of the recommendation, the extracted terms are (\u201cFor men and women over 50\u201d,   S  ) and (\u201cit\u201d,   S  ) as   Group   entities, (\u201cis\u201d,   P  ) as   Action   entity, and (\u201c30 g\u201d,   O  ) and (\u201c21 g per day\u201d,   O  ) as   Quantity  /  Unit   entities. For the third part of the recommendation, only one extracted term exists (\u201cdecreased food consumption\u201d,   S  ). \n\n\n\n### Evaluation \n  \n#### Test corpora \n  \nDue to the lack of annotated corpora in the domain of dietary information and in order to evaluate the newly proposed NER method for evidence-based dietary recommendations we created a test corpora. The main question was how to select the documents for the test corpora. We fixed the number at 100 because after extraction we need to manually check the extracted information. In order to promote diversity in the test corpora, we selected the documents from heterogeneous sources. We did this because different heterogeneous sources have different ways of reporting dietary recommendations. Fifty documents are dietary recommendation summaries, which are extracted from the scientifically validated web site, of the Food and Nutrition Information Center of United States Department of Agriculture [ ]. These documents are extracted from 12 different institutions and their distribution per institution is presented in  . \n   Distribution of documents per institution.    \nThe other 50 documents are abstracts of scientific publications. They were selected by using the PubMed API [ ] in combination with two keywords, \u201cfood composition\u201d and \u201cdietary intake\u201d. Further, 25 abstracts are selected randomly from the documents that are returned for each key word. The average number of sentences per document is 3.88 for documents from scientifically validated web sites and 6.54 for abstracts of scientific publications. This result is reasonable because the paragraphs from the scientifically validated web sites are summaries of dietary recommendations, while the abstracts from the scientific publications may contain dietary recommendations, but also contain other information about the study. The distribution of the number of sentences per document for the two subsets of the test corpora are presented in Figs   and  . \n   Distribution of number of sentences per documents from scientifically validated web sites.       Distribution of number of sentences per documents that are abstracts from scientific publications.    \nThis test corpora is not annotated. The results that are obtained by the drNER are further manually checked by 2 human experts, who are clinical dietitians, in order to see if the extracted entities have the correct label and if there are missing entities by drNER. The test corpora together with the obtained results for each document, separately, are available at the following link dx.doi.org/10.17504/protocols.io.hqbb5sn. \n\n\n#### Result \n  \nWe evaluated the drNER on the above test corpora. For each document in the test corpora, we tried to extract all useful information related to dietary recommendations, with focus on the   Food   entities,   Nutrient   entities, and   Quantity  /  Unit   entities. In  , the result of the evaluation for each entity is presented. In this table, the results for the documents that are extracted from scientifically validated web sites and scientific publications are provided, separately. At the bottom of the table the summary of results of our test corpora is presented. The results are presented by reporting the number of true positives, false positives, and false negatives. The true positives are the extracted entities for which the obtained entity label from drNER and the human expert is the same. The false positives are the extracted entities for which the drNER label (known as expectation) is positive, but this is false according to the external judgment of the human expert. For example, for the   Food   label, if an extracted entity is labeled as   Food   by drNER, but the human expert provided that the true label is   Nutrient  , it means that this is false positive for the   Food   label. The false negatives are the entities for which the drNER label (known as expectation) is negative, but this is false according to the external judgment of the human expert. For example, for the   Food   label, if an extracted entity is not labeled as   Food   by drNER, but the human expert provided that the true label is   Food  , it means that this is false negative for the   Food   label. Also, in false negatives are some entities that are not recognized as a given entity of interest by drNER, but the human experts assumed that this information from the document should be extracted. \n   Evaluation results.        \nThe number of true positive   Food   entities is 539. Out of them 326 are from the documents that are extracted from scientifically validated web sites and 213 are from scientific publications. Also, there are 5 false positives, from the scientifically validated web sites, related to phrases that consist of verbs related to food such as \u201cneed to eat\u201d, \u201ceating\u201d, etc., when they are not selected as an   Action   entity. In the future, this can be omitted by checking the tags of the chunks, so if they are verb chunks they can be removed from the candidate solutions. The number of false negatives for the   Food   entity is 25, out of which 22 are from the documents from the scientifically validated web sites and 3 of them are found in the scientific publications. They occur because \u201cgrains\u201d is not recognized as a   Food   item by the USAS English semantic tagger we use as our dictionary. For some false negatives when the information about the   Food   entity is not extracted because it is not found in the dictionary, the information about it can be extracted in the   Group   entity that is proposed by the method to catch some additional useful information related to dietary information. \n\nIn the case of the   Nutrient   entity, the number of true positives in our test corpora is 557. Out of them, 243 are found from the documents extracted from the scientifically validated web sites and 314 are extracted from scientific publications. The number of false positives is 2, and the number of false negatives is 17. Out of them, 13 are extracted from the documents from scientifically validated web sites and 4 are found in the scientific publications. Most of them are related to the fact that none of the corpus-based NER, that we used as dictionaries, recognize the concepts as a chemical entity. Some of them are related to \u201comega-3s\u201d. It is interesting that \u201comega-3\u201d is recognized, but the plural form is problematic for all dictionaries we used. In the future, these results can be improved by adapting a heuristic approach for linking the tokens to the dictionaries. \n\nFor the   Quantity  /  Unit   entity, the number of true positives is 86. Out of them, 47 are from the documents extracted from the scientifically validated web sites and 39 of them are found in the scientific publications. We did not find any false positives and the number of false negatives is 11. The false negatives are related to some units such as \u201cngg(-1)\u201d, \u201cmgkg\u201d, etc. This happens because these units do not exist in the dictionaries we used. Also, we use lemma of each token when we link it to the dictionaries related to the   Quantity  /  Unit   entity in order to distinguish between singular and plural forms of the units. We did this only for the   Quantity  /  Unit   entity because for others we used corpus-based NER systems that already include this information. \n\n\n\n### Discussion \n  \nTo the best of our knowledge, drNER is the first NER method that is focused on knowledge extraction of evidence-based dietary recommendations. The dietary domain brings a new application domain, with similar goals as previous IE shared tasks on biological event extraction. However, an annotated corpora does not exist, so there are no methods that focus on knowledge extraction. Because of that, a comparison of the drNER is made with some NER methods that can be used for each entity, separately, or they are NER methods that can in our case be used as dictionaries for some entities. For example, let us focus on one sentence \u201c  People of any age who are African Americans should further reduce sodium intake to 300 mg per day.  \u201d [ ]. By using the USAS English online tagger and becas[chemicals] API, the   Nutrient   entity which will be extracted is \u201csodium\u201d, while by using the becas API it is not recognized. By using the dictionaries applied for the   Quantity  /  Unit   entity, the only entity extracted here will be \u201cmg\u201d. The result by applying the drNER on this sentence is (\u201cPeople of any age\u201d,   S  ) and (\u201cwho are African Americans\u201d,   S  ) as   Group   enitites, (\u201cshould further reduce\u201d,   P  ) as   Action   entity, (\u201csodium intake\u201d,   O  ) as   Nutrient   entity, and (\u201c300 mg per day\u201d,   O  ) as   Quantity  /  Unit   entity. If the recommendation is \u201c  The RDAs for Mg are 300 mg for young women and 350 mg for young men.  \u201d, by using the USAS English online tagger the result for the   Nutrient   entity is \u201c  Mg  \u201d, while by using the drNER it is \u201c  The RDAs for Mg  \u201d. So instead of extracting only the nutrient component, the drNER also could extract the type of the DRVs reported. In our proposed method, by applying the three proposed post-hoc chunkings, we can obtain also the phrases that differ from the phrases that can be obtained by the corpus-based NERs used as dictionaries and give us more information for the entities. Also, adding the   Action   entity and the labels for the   Subject  ,   Predicate  , and   Object   provides additional information, and the   Group   entity helps to catch information that could be important to better interpret the extracted information. \n\nTo compare the methodology used in the drNER, we compare it with methodologies used by other NER methods used for other biomedical domains. For example, the SeeDev task that was a part of the 4th BioNLP Shared task consists two subtasks, SeeDev-binary on binary relation extraction and SeeDev-full on full event extraction. Because there is an annotated corpora, all 7 teams used supervised ML approaches. Five systems used SVMs and two systems were based on different algorithms, maximum entropy (MaxEnt) and a convolutional neural network. The methodology of the drNER method is completely different from the methodologies used by these approaches. The drNER is pure NLP method that is not based on annotated corpora, while all the SeeDev methods are based on ML approaches. Also, the entities involved in drNER are related to the dietary domain, and the SeeDev approaches involved in the extraction are related to genetic and molecular mechanisms involved in plant seed development. Another method is BANNER, evaluated on the BioCreative 2 GM training corpora, which is designed to maximize domain independence by not employing semantic features or rule-based processing steps. The domain-specific performance is not the purpose of the system, but researchers could adopt BANNER for a specific domain, by applying two types of post-processing. BANNER is based on annotated corpora and it uses CRFs. So the difference between the methodology used by BANNER and drNER is the same as the difference in the approaches presented on SeeDev task. Because an annotated corpora in the dietary domain does not exist, it is better to compare the methodology of the drNER with the methodologies of rule-based NERs that exist in the biomedical domains. Many rule-based NER methods use rules that combine terminological resources and the characteristics of the entities, but to write rules that depend from the characteristics for each entity is a time-consuming task. Further more, it requires a good understanding of the domain. For example, Lowe et al. [ ] give a grammar and dictionary driven approach to entity recognition that uses a mixture of expertly curated grammars and dictionaries, as well as dictionaries automatically derived from public resources. They have created 486 rules. The benefit of this approach is that it works well when you do not have annotated data but requires dictionaries and grammars related to each entity. In our case, we do not have expertly created grammars for the entities we are interested in, but we only used a small number of Boolean algebra rules that are unrelated to the characteristics of the entities, but help us define the phrases that are entities mentions. \n\n\n\n## Conclusion \n  \nIn this paper we present a NER method for knowledge extraction of evidence-based dietary recommendations, called drNER. The goal of this method is to promote progress in information extraction in the field of dietary domain, especially focused on three main entities:   Food  ,   Nutrient  , and   Quantity  /  Unit  . The dietary domain brings a new application domain, which has similar tasks on biomedical extraction, and is crucial for promoting health and well-being. \n\nThe proposed NER method for knowledge extraction of evidence-based dietary recommendations is a combination of terminological-driven NER and rule-based NER. The difference with the purely terminological-driven NER methods is that we allow for the use of corpus-based NERs as dictionaries for some entities of interest, instead of using dictionaries that consist of concepts and synonyms. The difference with the rule-based NERs is that we do not use rules based on the characteristics of the entities. We only have a small number of Boolean algebra rules that are not related to the characteristics, but help us to define the phrases that are entity mentions. The method consists of two-phases. The first phase involves the detection and determination of the entity mentions. It works by using some NLP methods and linking each token to a dictionary for each entity in which we are interested. After that, it uses three post-hoc chunkings in order to better determinate the entities mentions. The second phase is the selection and extraction of the entities. It is based on text syntactic analysis. Finally, by applying the rules defined in this phase, we can extract useful information related to dietary recommendations. \n\nTo the best of our knowledge, drNER is the first NER method where the focus is in the domain of evidence-based dietary recommendations, which is an untapped domain. The evaluation of drNER is done on test corpora that includes 100 documents. We fixed this number at 100 because an annotated corpora in this domain does not exist and after extraction, the extracted entities was manually checked by human experts to see if they have the correct labels. The test corpora included 50 summary paragraphs of dietary recommendations extracted from 12 different scientifically validated web sites and 50 abstracts of scientific publications that are related to \u201cfood-composition\u201d and \u201cdietary intake\u201d. The best results achieved rely on the fact that for some entities such as   Food   and   Nutrient  , the terminological resources are not classic dictionaries that consist of concepts with synonyms, but they could be some corpus-based NERs that exist. For example, the   Nutrient   entity is related to chemical-named entity recognition. By using chemical NERs we can obtain the chemical information, but the type of DRVs or some additional information associated with it, is not extracted. For this purpose, three post-hoc chunkings are presented and help in modelling the dietary domain. \n\nFor future work, we plan to normalize the extracted entities. Then, we will try to find a good way to represent the extracted knowledge to human experts. By using the extracted knowledge from the dietary domain and the knowledge for drugs, diseases, and genes, that can be obtained from methods presented as a part of shared workshops, we will try to build an annotated corpora and to increase adoption of linked data techniques as an effective solution to knowledge representation and management in Life and Health Sciences [ ]. Having an annotated corpora and knowledge representation, the next step will be to extract the relations that exists between these entities. \n\n \n", "metadata": {"pmcid": 5482438, "text_md5": "ff81e63f649b1383156d77634e17d118", "field_positions": {"authors": [0, 61], "journal": [62, 70], "publication_year": [72, 76], "title": [87, 198], "keywords": [212, 212], "abstract": [225, 1524], "body": [1533, 76546]}, "batch": 1, "pmid": 28644863, "doi": "10.1371/journal.pone.0179488", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482438", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=5482438"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482438\">5482438</a>", "list_title": "PMC5482438  A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations"}
{"text": "Galea, Dieter and Laponogov, Ivan and Veselkov, Kirill\nBioinformatics, 2018\n\n# Title\n\nExploiting and assessing multi-source data for supervised biomedical named entity recognition\n\n# Keywords\n\n\n\n# Abstract\n \n## Motivation \n  \nRecognition of biomedical entities from scientific text is a critical component of natural language processing and automated information extraction platforms. Modern named entity recognition approaches rely heavily on supervised machine learning techniques, which are critically dependent on annotated training corpora. These approaches have been shown to perform well when trained and tested on the same source. However, in such scenario, the performance and evaluation of these models may be optimistic, as such models may not necessarily generalize to independent corpora, resulting in potential non-optimal entity recognition for large-scale tagging of widely diverse articles in databases such as PubMed. \n\n\n## Results \n  \nHere we aggregated published corpora for the recognition of biomolecular entities (such as genes, RNA, proteins, variants, drugs and metabolites), identified entity class overlap and performed leave-corpus-out cross validation strategy to test the efficiency of existing models. We demonstrate that accuracies of models trained on individual corpora decrease substantially for recognition of the same biomolecular entity classes in independent corpora. This behavior is possibly due to limited generalizability of entity-class-related features captured by individual corpora (model \u2018overtraining\u2019) which we investigated further at the orthographic level, as well as potential annotation standard differences. We show that the combined use of multi-source training corpora results in overall more generalizable models for named entity recognition, while achieving comparable individual performance. By performing learning-curve-based power analysis we further identified that performance is often not limited by the quantity of the annotated data. \n\n\n## Availability and implementation \n  \nCompiled primary and secondary sources of the aggregated corpora are available on:   and  . \n\n\n## Supplementary information \n  \n are available at   Bioinformatics   online. \n\n \n\n# Body\n \n## 1 Introduction \n  \nPublications in the biomedical field have increased considerably over the years, with over 27 million total publications contained on the PubMed repository alone. With increasing information resources, searching and extracting valuable information has become more challenging using traditional methods. This has led to an increased interest and need of text mining systems that automate information extraction. Named entity recognition (NER) is a critical step in such workflow, classifying sequences of words to specific classes. In biomedical named entity recognition, this involves identification of biological/chemical entities such as genes, proteins, chemicals, cells and organs from unstructured text. \n\nSeveral approaches have been developed and employed throughout the years to perform this task. Dictionary lookup is the simplest approach and is used by literature mining tools such as PolySearch ( ). While the advantage of this is that annotated data is not required for training (although required for evaluation), this approach often suffers from low accuracy due to its inability to disambiguate words based on context or semantics. This requires further pre- and post-processing steps, which are often hand-crafted rules. Furthermore, such approaches are limited by the availability of a \u2018complete\u2019 dictionary, therefore are unable to adapt and identify new or unseen entities. \n\nThe GENIA project ( ) was amongst the first major efforts for the development and optimization of machine learning-based named entity recognition systems for bioentities, creating the GENIA corpus and initiating the \u2019Joint Workshop on Natural Language Processing in Biomedicine and its Applications\u2019 (JNLPBA-2004) ( ). DNA, RNA, cell line and cell types were recognized in this project with a maximum F-score of 72.55% utilizing hidden Markov models (HMM) and support vector machines (SVM) ( ). Other participating systems utilized maximum entropy Markov models (MEMM) and conditional random fields (CRF). \n\nThe BioCreAtIvE challenges have also been playing a role in this development, with the first challenge focusing on gene recognition utilizing the GENETAG corpus, reporting a maximum F-score of 83.2% ( ). Similar to the GENIA project, Markov models, SVMs, manually generated rules or a combination therefore were utilized. Since then, several publications have reported equivalent or improved performance, and several NER tools are currently available ( ;  ;  ;  ). \n\nThe highest accuracies for open-source NER tools were reported by Gimli ( ) at 72.23% overall F-score for JNLPBA corpus and 87.17% for the GENETAG corpus, using CRF-based models. This is comparable to the highest reported accuracies for these corpora with closed source software, where NERBio ( ) report 72.9% for the JNLPBA corpus while ( ) report 88.30% for the GENETAG corpus. These results for JNLPBA are also similar to others reported in literature ( ;  ). Genes and diseases were also reported to be identified with over 90% F-score by the NER module of DTMiner ( ). However, training and evaluation of the latter was performed on a custom corpus. \n\nWith the increase in popularity of neural networks, these have also been increasingly applied for biomedical NER ( ;  ;  ), improving on the state-of-the-art of traditional machine learning methods. \n\nDespite these highly promising score values, there are a number of outstanding investigations and potential limitations which need to be considered and addressed: i) are the trained models generalizable and robust?; and therefore ii) is the high performance reported translatable?; iii) is performance limited by the size of the training data available?; and consequently, iv) would more annotated data improve the results? \n\nIrrespective of the model utilized, machine learning NER approaches have often been trained and tested on a single corpus, frequently GENETAG or GENIA. This results in corpus-specific model optimizations, consequently introducing potential over-fitting which reduces model generalizability and reliability when applied to unseen text. This may be indicated from  ), where training for genes and proteins on the GENETAG corpus and testing on the CRAFT corpus achieved 45\u201355% F-score\u2014lower performance when compared to the GENETAG test F-score of 87.17% by Gimli ( ). The quality and different annotation standards of the different corpora may contribute to such a discrepancy in performance, however the variability in the style of writing of unseen text is also likely to increase when compared to the much smaller corpora, and thus the accuracy quoted for the models may not be representative. The difference between gold-standard performance and translational performance has indeed been previously shown for mutations ( ). \n\nSeveral available corpora share the same/related entity classes: OSIRIS ( ), SNPcorpus ( ), BioInfer ( ), various BioNLP 2011 subsets ( ), CellFinder ( ), GETM ( ), IEPA ( ), HPRD50 ( ), GREC ( ) and GENIA ( ) all contain gene/protein-related entities; GENIA ( ), CellFinder ( ) and AnEM ( ) contain cell line/type and tissue information; BioNLP2011 ( ), DDI corpus ( ) and GENIA ( ) share chemical/drug entities; and GENIA ( ), GREC ( ), CellFinder ( ) and BioNLP2011 ID ( ) contain annotated species terms. Despite the common entities, availability of such data is very dispersed and formats and not standardized, varying from CONLL, to BioC, leXML and several others. Thus, here we collate a number of biomedically-related corpora currently available, convert relevant corpora to a common standard BioC format ( ), and utilize multiple sources for training and testing of NER models to determine the effect of data size on evaluation and performance. \n\nPerforming this allows to generate corpus-independent models and determine if the current quantity of data available is enough to reach the maximum performance\u2014a task commonly referred to as power analysis. Power analysis has been performed limitedly on NER systems, particularly biomedical NER, yet is a crucial part of evaluation to determine whether a system is bottlenecked by the data size, irrespective of algorithmic developments. \n\n\n## 2 Materials and methods \n  \n### 2.1 Compiling and filtering corpora \n  \nSeventy-five biomedically-related corpora were compiled from primary or secondary sources. Annotation formats varied from standoff (.ann), IOB, BioC ( ) or otherwise. Where multiple formats were available for the same corpus, all were compiled for cross-reference. The list of corpora compiled, the formats available and additional information such as year of publication and number of documents in corpus are listed in  . A similar table with the download links from the original or secondary host(s) is also provided on:  . \n\nCorpora which provide annotations of biomedical entities were considered for further processing. Corpora labeling entity relationships such as drug\u2013drug interactions or protein\u2013protein interactions were also considered relevant as long as the entities were explicitly annotated individually. Corpora with no annotation term indices provided, or with multiple nested entities were excluded, along with corpora annotating abbreviations. Subset corpora were also excluded when the superset corpus was available. For example: MLEE ( ) and AnEM ( ) are subsets of the bigger AnatEM corpus ( ), thus were excluded. \n\nDetails on subsequent format \u2019standardization\u2019, processing and correction for annotation indexing mismatches are provided in  . \n\n\n### 2.2 Defining and remapping entity classes/ontologies \n  \nDifferent corpora annotate entities into different classes/labels. In order to merge different corpora with related entity classes, we devised and assessed eight initial super-classes based on ontologies: (i) ChemicalDrug; (ii) GeneProteinVariant; (iii) Cell; (iv) Anatomy; (v) Organism; (vi) Tissue; (vii) RNA; and (viii) Disease. Anatomy, tissue, cells, organisms and diseases are well-described, and their nomenclature is relatively static and consistent. In the case of organisms/species, their discovery requires mandatory registration, while for diseases these are documented in registers. Therefore, these entities are predicted to be well-recognized using dictionary matching approaches. Due to this, as well as the limited availability of unique training data other than the AnaTEM corpus, these classes will not be considered further here for machine learning training. \n\nOn the other hand, chemicals and drugs (particularly when mentioned using the IUPAC nomenclature), genes, proteins, RNA and especially mutations, are highly variable entities, with a greater possibility of mentioning non-previously-documented variants. These have thus been considered here for machine learning recognition. Based on preliminary results, some classes were further stratified. The original entity classes, the remapped classes and the number of entities for each corpus entity class are provided in  . \n\nDifferent copies of the remapped corpora were devised, with one entity class per corpus copy. This was performed in order to allow for training and prediction of one entity class at a time. A single entity class classification was chosen over a multi-class classification for several reasons:\n   \nScalability: with the availability of new corpora annotating new entity classes, recognizing the new entity class would require retraining the whole model in the case of an existing multi-class model. However with multiple single class models for each class, recognizing a new entity class would only require to train a new model for the new class and integrating with the existing models. \n  \nMultiple acyclic inheritance: An entity is not exclusive to one class and may thus belong to multiple classes. Classification and prediction in a multi-class model would not be straightforward with the current implementation. For example, on a single level, proteins are (a subset of) chemicals but not all chemicals are proteins, thus a protein entity belongs to both the class \u2018proteins\u2019 as well as \u2018chemicals\u2019 if these are considered separate. \n  \nCorpora available: training corpora available are highly varied; from specific corpora such as DDI (drug\u2013drug interaction) corpus to broader chemical classes such as CHEMDNER which annotate chemicals including drugs and proteins into a single class. \n  \nFrontend: with the ultimate aim of providing a realistic evaluation and training of machine learning-based NER models for deployment in a scalable end-to-end tool, applications were considered. With single class classification models, an entity may have multiple annotations. This is favorable over a single annotation as if an entity such as \u2018interleukin\u2019 is listed and classified as only a chemical, a user will not recall it if querying proteins. Having it labeled as both a chemical and protein will allow for such entity to be recalled in both instances. \n  \n\n\n### 2.3 Model training and prediction \n  \nSeveral existing and stable NER packages utilize CRF-based models. Tools such as GIMLI ( ), MALLET ( ) and Stanford NER ( ) have been used widely and are commonly employed in end-to-end information extraction workflows such as the recent DTMiner ( ). Here we train and predict using the Stanford NER CRF algorithm based in Java ( ). \n\nTo allow for an as fair as possible of a comparison with other tools, feature extraction methods were based on previous reports assessing the effect of features on performance by backward elimination. GIMLI ( ) report that features such as: capitalization and symbols have a positive effect on performance (for the majority of entity classes), while Stanford report the increase in performance by disjunction and word tag features. The use and importance of character-level features, especially in the biomedical domain, has also been reported in neural network architectures ( ). These features were thus included as part of the feature extraction step. Additional sequence-related features were tested however these were determined to have no overall performance improvements. Details and commands used to perform model training and prediction are provided in the \u2018model training and prediction\u2019 section of the  . \n\n\n### 2.4 Power analyses \n  \nTo determine the effect of training size on prediction performance, we performed power analyses to generate learning curves. Each corpus was split into 80% training and 20% test sets. Where applicable and possible, to avoid model bias, documents from the same manuscript were considered as either training or test. Prediction performance was measured by the F-score ( ). The F-score was computed for each corpus rather than calculating an overall average. This provides an indication of which corpus is predicted the best and the worst and indicates any variation. To provide a single overall metric, two statistics were computed: (i) a document-weighted average was also calculated, where the F-score from each corpus is weighted by the number of documents it contains to compute an overall average; and (ii) an equally-weighted mean where corpora contributed equally to the overall average.\n \nThe learning curve was represented by an inverse power law function, previously reported ( ). Briefly, the prediction F-score ( ) is defined as a function of the product of training sample size and minimum achievable error (a), learning rate (b) and decay rate (c) ( ). An initial decay rate of -0.1 and learning rate of 0.2 were used. Error was defined as the root mean squared error (RMSE).\n \nLearning curves were generated using three approaches:\n   \nCorpus-specific training: To determine the model generalizability, we trained a model for each corpus and used this to predict the test data of other corpora; \n  \nMerged corpora training: To determine the added value of increasing training size by integrating training data from multiple corpora, we merged/stacked the training data of all corpora and incrementally added the training data while predicting the same fixed test set; \n  \nLeave-corpus-out training: To determine the generalizability of the merged-corpora-trained model, we trained a model using all training data of all corpora except one corpus, and tested the model by predicting the left-out corpus test data. \n  \n\nThe addition of corpora was done using two approaches. In the first approach, when performing leave-corpus out, all corpora except one corpus were used for model training while the left-out corpus was used as \u2018fixed\u2019 test data. The \u2018fixed\u2019 test data was a random subset of all documents from each left-out corpus. For unbiased learning curve analysis, the data from the same corpus was excluded for model building. As for the training set, documents were randomly re-shuffled between corpora and added incrementally to the training data to obtain the learning curve. The leave-one-out corpora were predicted one at a time. In the second approach, when all corpora were used in the training, corpora were added sequentially, one document at a time. This determined how much added predictive value each corpus provides that is not captured by previously introduced training examples. \n\n\n### 2.5 Orthographic feature analysis \n  \nTo assess the distinguishing orthographic features between entity classes, we performed univariate tests on local morphological features. Labeled entities for each class of interest were extracted and labeled accordingly, and the rest of the tokens were considered \u2019nulls\u2019. These two classes were balanced by random sampling from the bigger class. GIMLI ( ) and regular expressions were used to extract a total of 31 morphological features for these tokens, including: different forms of punctuation, case sensitivity (initial caps, end caps, all caps), digits (number of digits) and number of characters. \n\nEach feature was represented by a binary representation for each token and therefore summation provided total occurrence of a feature in the \u2018null\u2019 class and \u2018entity\u2019 class. This allowed for the calculation of the percentage difference of a feature occurrence between the classes. Statistically significant feature differences were determined by performing Fisher exact tests followed by Benjamini-Hochberg FDR-correction for multiple testing. This was repeated n-times [where   n  \u2009=\u2009size(largest class)/size(smaller class)] and a mean   q  -value\u2009\u00b1\u2009standard deviation was computed for each feature. When a feature was determined to be significantly different between the classes (  q   < 0.05; including the upper deviation boundary), and the percentage difference was positive between the entities class and the null, such feature was considered \u2018characteristic\u2019 of that class (compared to null tokens;  ). \n\n\n\n## 3 Results and discussion \n  \n### 3.1 Identifying genes and proteins \n  \nOntologically, proteins, genes and variants are related. These were initially merged in a single superclass to test the overlap between the annotated entities across different corpora. SNPcorpus and tmVar achieved almost no predictive performance prior to the introduction of OSIRIS/SETH training data ( ). SNPcorpus and tmVar annotate mutations, while none of the training corpora prior to the introduction of OSIRIS/SETH data have mutations annotated. While expected, this confirms that mutation entities are significantly different from the gene/protein classes and were thus considered as a separate class in subsequent tasks. Contrastingly, while VariomeCorpus also annotates variants, this corpus also annotates genes, with 1690 mutant entities and 4613 genes. This explains why VariomeCorpus test data was better predicted in comparison to mutation-specific corpora SNPcorpus and tmVar. The difference between the GeneProtein class and variants is more evident when considering the orthographic features ( ), where genes and proteins from different corpora share several univariate features, but less so with entities in the variant class. \n\nTo determine the generalizability of the \u2018GeneProtein\u2019 class models, when excluding variants, we tested the cross-performance of models trained on corpora individually and applied it to other corpora (not seen in the training\u2014leave corpus out cross validation) ( ). While increasing training data increased performance in all cases, the best predictions of the test data were achieved when the test data originated from the same corpus, with varying predictive capacity for other corpora ( ). Furthermore, IEPA data was the hardest to predict (  E\u2013H), and inversely, IEPA-trained model was unable to predict any of the other test data for the other corpora ( ), suggesting data incompatibility or corpus bias. Considering the orthographic features on their own, IEPA was indeed the most inconsistent compared to other GeneProtein corpora ( ), with the IEPA corpus having very different \u2018fingerprint\u2019 of significant features compared to other corpora within the same class.\n \n  \nCorpus-specific learning curves for the \u2018GeneProtein\u2019 class. Learning curves for corpus-specific training and prediction of all corpora test data. (  A  ) AIMED, (  B  ) OSIRIS, (  C  ) CellFinder, (  D  ) IEPA, (  E  ) MIRTEX, (  F  ) SETH, (  G  ) VariomeCorpus and (  H  ) mTOR \n  \nMerging the different corpora for both training and testing increased the consistency and overall performance for the GeneProtein class ( ). With respect to sample size dependence, increasing the training data generally incrementally improved the performance. Some corpora left out from training were predicted by other corpora with similar performance to their own; for example, SETH was predicted with an F-score of 64% when all corpora were merged for training, while leaving SETH out of training obtained a 63% F-score. In case of miRTex, both merged training data and all corpora other than miRTex, converged at an F-score of 76% at 450 documents, although additional documents improved the performance of the former up to 84% F-score. \n\nGenerally, the maximum F-score for the test data of a specific corpus was only achieved when introducing training data from the same corpus. Nonetheless, in most cases, a relative performance plateau is reached after 1000 training documents, with a maximum weighted average of 78.32% F-score ( ). \n\n\n### 3.2 Identifying variants \n  \nBased on the predictive power and orthographic differences, variants were considered as a class on their own, despite the similarity in ontology and semantics. When predicting the test data of a corpus by other corpora (leave-corpus-out cross-validation), VariomeCorpus was poorly predicted by any corpus. Taking a closer look at the raw corpus, it appears that most entities are genes followed by the token \u2018mutant\u2019 rather than mutation entities following the standard nomenclature. With the entity structure being \u2018gene X mutant\u2019, this possibly explains why entities in the VariomeCorpus test data were identified and predicted as genes in the \u2018GeneProteinVariant\u2019 superclass ( ). Once again, this difference is highlighted by the orthographic feature map ( ) where VariomeCorpus has 4 significant features (OneDigit, OneCap, ThreeCap and Length3-5) which are not shared with any of the other \u2018variant\u2019 corpora. Inversely, whereas all other variant corpora were identified to have the \u2018+\u2019 character as significant (a symbol commonly used to denote mutations), VariomeCorpus was the only not to share such characteristic. Indeed, VariomeCorpus was recently reported to annotate many vague mentions such as \u2018de novo mutation\u2019 and \u2018large deletion\u2019, with only a subset mentioning position-specific variants ( ). Due to such differences, this was excluded from subsequent power analyses. However, if more training data is required, this subset can be identified and extracted, as described earlier ( ). \n\nWhen merging data from SETH, SNPcorpus, tmVar and OSIRIS, SETH and tmVar test data was predicted with  2% in both cases, plateauing around 500 documents. tmVar test data was predicted with equal performance when performing leave-corpus-out cross-validation. Contrastingly, the predictive capacity of other corpora on unseen corpora test data as well as when using merged training data was very low ( ). This suggests a subset of the entities are \u2019unique\u2019 in these corpora. OSIRIS obtained the lowest plateaued performance, and by looking at OSIRIS annotations, indeed they appear to contain non-standard nomenclature, with annotations such as: \u2018codon 72 (CCC/proline to CGC/arginine\u2019, \u2018(TCT TCC) in codon 10\u2019, \u2018-22 and -348 relative to the BAT1 transcription start site\u2019, \u2018A at positive -838\u2019, \u2018C in -838\u2019. However, these are still valid mutation-related entities, thus to recall such entities, more training data similar to OSIRIS is required, although standardization of nomenclature in more recent publications may render this unnecessary. Nonetheless, when omitting OSIRIS and re-plotting SETH, SNPcorpus and tmVar learning curve, SETH and tmVar obtained lower performance, backing up the positive contribution of OSIRIS training data to the predictive performance of tmVar and SETH. \n\nLooking at the corpus-specific learning curves ( ), tmVar training data only ( C) predicts tmVar test data with \u223c50%. The trend of the learning curve indicates that with more training data the performance is expected to increase. Indeed, the performance increased to >80% when training data from other corpora was added ( ). Leaving the tmVar training data out completely and using the other corpora to predict tmVar test data achieved the same performance, indicating high model generalizability ( ).\n \n  \nLearning curves for merged training data from multiple sources and prediction of the test data for each corpus individually, and leave-corpus-out cross-validation where each corpus is left out from training and its test data is predicted by all other corpora (where multi-source data is available). Training and testing of the classes: (  A  ) genes and proteins (dashed lines represents leave-one-out prediction learning curves), (  B  ) variants, (  C  ) chemicals (CHEMDNER corpus), (  D  ) metabolites (Metabolites corpus), (  E  ) RNA (miRTex corpus) and (  F  ) drugs (DDIcorpus) \n  \nSETH achieved the same performance (86.02% F-score) when merged training data ( ) and SETH only training data were used ( ). Leaving out SETH training data achieves lower F-score (67.09%), therefore the SETH corpus alone is contributing to 18.93% additional performance. With respect to generalizability, SETH predicted 66.67% of tmVar test data ( ), and considering the maximum tmVar performance was achieved even when tmVar train data was omitted, the remaining performance was achieved by the training data of other corpora (SNPcorpus and OSIRIS). \n\nSNPcorpus and OSIRIS achieved a similar trend in performance when merging corpora ( ) and when using corpus-specific training data ( D and A). The absolute performance is slightly lower in the former case, suggesting introduction of noise to the model. \n\nWith respect to orthographic features, the \u2018variants\u2019 class is quite variant across different corpora ( ), with very limited consistently significant features across corpora. Commonly, three or more digits are present in \u2018variant\u2019 entities, however overall there is no distinctly evident \u2018fingerprint\u2019 of univariate significant features across the different corpora. This variation has been explored in detail by  , where mutation mentions have been classified as \u2018standard\u2019, \u2018semi-standard\u2019 and \u2018natural language\u2019, with SETH and tmVar sharing a subset of standard mutations while only SETH captured natural language mentions ( ).\n \n  \nOrthographic feature analysis for entity classes determined per corpus. Features highlighted were identified to be univariately significant for an entity class in a given corpus. Each layer/row represents an orthographic feature while each column represents a corpus, grouped by entity classes to represent six main classes: GeneProtein, RNA, variants, chemicals, drugs and metabolites \n  \n\n### 3.3 Identifying chemicals, drugs and metabolites \n  \nBased on ontology, corpora annotating chemicals, drugs and metabolites were remapped into a single \u2018ChemicalDrug\u2019 superclass. However, corpora such as CHEMDNER annotate genes as chemicals while more specific corpora such as DDI and metabolites corpus only annotate a particular entity class: drugs and metabolites respectively, thus would not be able to predict genes in the test set. Given these annotation mismatches between corpora, we devised new classes. CHEMDNER is the biggest corpus with over 58\u2009000 chemical entities annotating formulae and multiple alternative names such as: systematic names and chemical families. This comprehensive, large and highly diverse naming system is not found in any other corpora and hence this corpus was considered on its own. Since genes/proteins such as \u2018interleukin-2\u2019 are also considered as an entity in this corpus, such entity would be annotated multiple times when the GeneProtein model and CHEMDNER model are used to predict its class. This is reasonable considering that such entity indeed can be considered as a child of the GeneProtein parent as well as the Chemical superclass. This backs up the reason why single entity class models (binary class classification problems) were considered in this study (compared to multi-class models). With the new classes, chemicals, metabolites and drugs could only be trained on single corpora. A stable performance was achieved after 1200 training documents for chemicals, 160 documents for metabolites and 400 documents for drugs. \n\nCHEMDNER achieved 84.8% F-score when all training documents are utilized during model training ( ), with performance stabilizing around 1500 documents. This is similar to the performance published by the authors ( ). \n\nWhen devising a model for drug NER, while DDIcorpus and mTor both contain drug annotations, mTor only annotates three unique drug entities ( ) and hence was excluded. The drug learning curve for DDIcorpus indicates a stable performance at and beyond 380 documents, with an average of 78.48% ( ). \n\nSimilarly, metabolites corpus is the only resource that specifically annotates metabolites. Other corpora such as CHEMDNER annotate metabolite entities, however these are labeled under a broad chemical class hence cannot be distinguished from other non-metabolite entities such as drugs and proteins. The learning curve for the metabolites corpus was generated and is shown in  . Prediction performance stabilizes around 160 documents with an average F-score of 71.98%. \n\n\n### 3.4 Identifying RNA \n  \nAs listed in  , RNA is annotated in miRTex and mTor corpora. While miRTex contains over 2700 entities, mTor only annotates 7 unique entities and thus the latter was excluded as this is insufficient for representative power calculations. Furthermore, with such small number of entities, any performance metrics would not provide meaningful insight following train/test split, especially with regards to model generalizability. \n\nMiRTex corpus achieved a plateaued performance of 91% with 21 documents ( ), which increases marginally to a stable F-score of 96.17%. This high performance and stability may be accounted for by the high consistency in RNA nomenclature. \n\n\n\n## 4 Conclusion \n  \nA generalizable model is crucial for applied machine learning-based named entity recognition. Here we show that merging training data from multiple sources may generate a more generalizable model. However, the absolute performance may be lower when comparing it to individual source-trained models due to annotation standard differences, as well as corpus over-fitting of the latter. Overfitting to one corpus may be the case when the corpus is a selection of publications from a medical subfield (e.g. cardiac diseases) and therefore the corpus is not representative of the class. The average performance achieved for genes and proteins for a merged data model is comparable to existing models, while variants showed high variability. Training data for chemicals, drugs, metabolites and RNA is limited due to lack of overlap of entity types across corpora, however individual models show no increased performance with increasing training data. Generally, collecting more training data is unlikely to increase performance of bioentity named entity recognition. However, with improved annotation standards and implementation of transfer learning approaches may not only improve performance but also generalizability\u2014providing a more realistic performance measure of translational named entity recognition. \n\n\n## Funding \n  \nThe authors acknowledge the financial support for bioinformatics developments as part of BBSRC (BB/L020858/1) and EU-METASPACE (34402) projects; DG acknowledges Imperial College Stratified Medicine Graduate Training Programme in Systems Medicine and Spectroscopic Profiling (STRATiGRAD); KV and DG acknowledge Waters corporation for funding and support throughout this study. \n\n Conflict of Interest  : none declared. \n\n\n## Supplementary Material \n  \n \n", "metadata": {"pmcid": 6041968, "text_md5": "e10f5c6c9d039c98003c74159663d247", "field_positions": {"authors": [0, 54], "journal": [55, 69], "publication_year": [71, 75], "title": [86, 179], "keywords": [193, 193], "abstract": [206, 2219], "body": [2228, 33356]}, "batch": 1, "pmid": 29538614, "doi": "10.1093/bioinformatics/bty152", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6041968", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6041968"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6041968\">6041968</a>", "list_title": "PMC6041968  Exploiting and assessing multi-source data for supervised biomedical named entity recognition"}
{"text": "Xu, Jun and Li, Zhiheng and Wei, Qiang and Wu, Yonghui and Xiang, Yang and Lee, Hee-Jin and Zhang, Yaoyun and Wu, Stephen and Xu, Hua\nBMC Med Inform Decis Mak, 2019\n\n# Title\n\nApplying a deep learning-based sequence labeling approach to detect attributes of medical concepts in clinical text\n\n# Keywords\n\nInformation extraction\nNatural language processing\nClinical notes\n\n\n# Abstract\n \n## Background \n  \nTo detect attributes of medical concepts in clinical text, a traditional method often consists of two steps: named entity recognition of attributes and then relation classification between medical concepts and attributes. Here we present a novel solution, in which attribute detection of given concepts is converted into a sequence labeling problem, thus attribute entity recognition and relation classification are done simultaneously within one step. \n\n\n## Methods \n  \nA neural architecture combining bidirectional Long Short-Term Memory networks and Conditional Random fields (Bi-LSTMs-CRF) was adopted to detect various medical concept-attribute pairs in an efficient way. We then compared our deep learning-based sequence labeling approach with traditional two-step systems for three different attribute detection tasks: disease-modifier, medication-signature, and lab test-value. \n\n\n## Results \n  \nOur results show that the proposed method achieved higher accuracy than the traditional methods for all three medical concept-attribute detection tasks. \n\n\n## Conclusions \n  \nThis study demonstrates the efficacy of our sequence labeling approach using Bi-LSTM-CRFs on the attribute detection task, indicating its potential to speed up practical clinical NLP applications. \n\n \n\n# Body\n \n## Background \n  \nClinical narratives are rich with patients\u2019 clinical information such as disorders, medications, procedures and lab tests, which are critical for clinical and translational research using Electronic Health Records (EHRs). Clinical Natural Language Processing (NLP) has been a feasible way to extract and encode clinical information in notes. Various clinical NLP approaches and systems [ \u2013 ] have been developed to extract important medical entities from text and encode them into standard concepts in ontologies such as the UMLS (Unified Medical Language System). However, downstream clinical applications, such as clinical decision support systems, often require additional attribute information of medical concepts. For example, to provide accurate information about what drugs a patient has been on, a clinical NLP system needs to further extract the attribute information such as dosages, modes of administration, frequency of administration etc. in addition to the drug names. Many current clinical NLP systems/applications extract individual medical concepts without modeling their attributes or with limited types of attributes, partially due to the lack of general approaches to extract diverse types of attributes for different medical concepts. \n\nA medical concept can be defined more precisely as an object and its allowable attributes. The object may be a disorder, drug, or lab test entity and attributes can be any of the sub-expressions describing the target concept. Attributes are prominent in clinical procedures and found in clinical notes frequently, and have surface forms that can be textual or numerical. Table\u00a0  shows some important attributes of different medical concepts in clinical text. Disorder concepts always have attributes that indicate whether a disorder is absent, hypothetical, associated with someone else, conditional etc. Detailed medication data are often expressed with medication names and signature information about drug administration, such as dose, route, frequency, and duration. Laboratory analysis always originates numerical values for different lab tests.\n   \nMedical concepts and their attributes \n  \n\nRecently, the Clinical NLP research community has increased its focus on the task of identifying attributes for medical concepts. For the past few years, a series of open challenges have been organized, which focused on not only identifying medical concepts but also their associated attributes from clinical narratives. The Third i2b2 Workshop focused on medication information extraction, which extracts the text corresponding to a medication along with other attributes that were experienced by the patients [ ]. Attribute information to be targeted included dosages, modes of administration, frequency of administration, and the reason for administration. The ShARe/CLEF 2014 [ ] and SemEval 2015 [ ] organized open challenges on detecting disorder mentions (subtask 1) and identifying various attributes (subtask 2) for a given disorder, including negation, severity, body location etc. These challenges have greatly promoted clinical NLP research on attribute detection by building benchmark datasets and innovative methods. \n\nThe detection of medical concept attributes is typically mapped to the NLP tasks of named entity recognition (NER) and relation extraction. Many rule-based approaches have been proposed to extract the medical concept-associated attributes, relying on existing domain dictionaries and hand curated rules. MedLEE, perhaps the oldest and most well-known system, encodes contextual attributes such as negation, uncertainty and severity for indexed clinical conditions from clinical reports [ ]. NegEx [ ] and ConText [ ] are other two widely used algorithms for determining contextual attributes for clinical concepts. ConText is an extension of the NegEx negation algorithm, which relies on trigger terms, pseudo-trigger terms, and termination terms to recognize negation, temporality, and experiencer attributes for clinical conditions. For medication information extraction, the earliest NLP system CLAPIT [ ] extracted drug and its dosage information using rules. The system achieved an 86.7% exact match F-score. In the work of Gold et al. [ ], a rule-based approach was proposed to extract drug attributes: dose, route, frequency and necessity. Another system, MedEx [ ], is a rule-based sequence tagger that combined dictionary lookup, regular expression, and rule-based disambiguation components to label drug names and signatures in clinical text. \n\nIn addition, many high-performing systems in the above challenges used machine learning methods. The USyd system [ ] achieved the best performances in the i2b2 2009 medication challenge, which incorporated both machine learning algorithms and rules engines. The system used a conditional random field (CRF) to identify medication and attribute entities, and a Support Vector Machine (SVM) determined whether a medication and an attribute were related or not. In the ShARe/CLEF 2014 and SemEval 2015 challenges, most participating systems also used machine learning-based approaches, coupled with related dictionaries, to extract disorder assertion attributes. For example, Team ezDI [ ] detected disorder attributes in two steps: 1) used CRF to recognize attribute mentions 2) trained SVMs classifiers to relate the detected mentions with disorders. \n\nThese previous machine learning systems performed well on different attribute detection tasks, but this success was undercut by an important disadvantage. Most of them used a traditional two-step cascade approach: 1) Named Entity Recognition (NER), to recognize attribute entities from text; and 2) Relation extraction, to classify the relations between any pair of attribute and target concept entities. The two-step approach is built on different machine learning algorithms with massive human curated features, which is complicated. Moreover, to get better performance, in some systems, different models need to be built for each attribute separately. For example, Apache cTAKES treats the task of locating body sites and severity modifier as two different extraction problems and builds two different extraction modules [ ]. In addition, the cascade approach may suffer from error propagation, so that any errors generated in the NER step may propagate to the step of relation classification. \n\nIn a previous shared task of \u201cAdverse Drug Reaction (ADR) Extraction from Drug Labels\u201d (2017 TAC-ADR), we proposed a sequence-labeling based approach to ADR attribute detection of drug mentions and it achieved superior performance (ranked No. 1 in the challenge) [ ]. The proposed approach recognizes attribute ADRs and classifies their relations with the target drug in one step, after we transform the ADR attribute detection into a sequence-labeling problem. In this study, we extend this approach by modeling target concepts in a neural architecture that combines bidirectional LSTMs and conditional random fields (Bi-LSTM-CRF) [ ] and apply it to clinical text to assess its generalizability to attribute extraction across different clinical entities including disorders, drugs, and lab tests. We conducted several experiments to compare our sequence labeling-based approach with traditional two-step extraction methods using three different corpora for disorders, medications and lab tests and our results show that the sequence labeling-based method achieved much better performance than traditional methods in all three tasks, indicating its utility to concept-attribute detection from clinical text. \n\n\n## Materials and methods \n  \n### Tasks and datasets \n  \nIn this study, we developed and evaluated our methods using three different attribute detection tasks: \n\n#### ShARe-disorder \n  \nThis task is to detect attributes of disorders in clinical documents. We used the ShARe corpus developed for the SemEval 2015 challenge task 14 [ ], which is to recognize disorders and a set of attributes including: Negation indicator (NEG), Subject Class (SUB), Uncertainty indicator (UNC), Course class (COU), Severity class (SEV), Conditional indicator (CON), Generic indicator (GEN), and Body location (BDL). For simplicity, we removed all dis-joint disorder and attributes mentions and ignored the GEN detection task since more than 99% of disorders have no GEN attribute [ ]. As the test dataset from this challenge was not released to public, we merged the training and development datasets (resulting in 431 de-identified clinical notes in total) and used them for this study. \n\n\n#### i2b2-medication \n  \nThis task is to detect signature attributes of drugs in clinical documents. We followed the 2009 i2b2 medication extraction challenge [ ], which is to extract medications and their dosages (DOS), modes (MOD), frequencies (FRE), durations (DUR) and reasons (REA). We used the test corpus in the challenge, which consists of 251 discharge summaries with \u201csilver\u201d standard annotations collectively annotated by the challenge participants. \n\n\n#### i2b2-LabTest \n  \nThis task is to detect values (VAL) associated with lab tests mentioned in clinical documents. We leveraged the corpus used in the 2010 i2b2/VA shared task [ ] to develop a newly annotated dataset for this task: we first extracted sentences containing lab test entities according to the original annotations in the challenge (2291 sentences in total) and then manually annotated values associated with each lab test mention (if any). \n\nTable\u00a0  shows the types of attributes for each of the three tasks, as well as statistics of the corpora used in this study.\n   \nConcepts and attributes types included in this study, as well as their distribution in the corpora \n  \n\n\n\n### Traditional two-step approach (baseline system) \n  \nWe developed a baseline system that uses the traditional two-step approach. It consists of two steps to identify attributes for a given medical concept. 1) Attribute entity recognition: NER task where named entities are attributes; we used a Bi-LSTM-CRF [ ] as our sequence labeling algorithm, which has obtained state-of-the-art performance in different NER Tasks [ ,  ]. 2) Attribute-concept relation extraction: We treated this task as that of relation classification between two entities. It was further divided into two tasks: candidate attribute-concept pair generation and classification. We generated all attribute-concept pairs within one sentence as candidates and then labeled them as positive or negative, based on the gold standard. We trained a binary classifier for each attribute to check if any relationship existed between an attribute mention and a concept. The first baseline system use the SVMs algorithm to classify candidate attribute-concept pairs, trained on both contextual and semantic features such as: words before, between, and after the attribute-concept pair; words inside attributes and concepts, and the relative position of attributes. The second baseline system combine a Bi-LSTM layer and a Softmax layer to classify candidate pairs [ ]. To train this classifier, we use word embedding and position embedding as input features. Both of the embeddings are randomly initialized. \n\n\n### Attribute detection by sequence labeling \n  \nBesides the issues of complexity and error propagation, the traditional two-step approach also faces a major problem, namely, omitted annotations of attribute entities. Attributes such as NEG and BDL may not be annotated in a gold standard corpus if they are not associated with a medical concept. For example, in the Fig.\u00a0 , \u2018Abdominal\u2019 is not annotated as a BDL entity in the ShARe-Disorder corpus. This makes it challenging to train an effective NER model for those attributes, and misses negative attribute-concept candidate pairs that are required to train an effective relation classifier. To address the above issues, we propose a novel sequence labeling approach for attribute detection, which identifies attribute entities and classifies relations in one-step. To address this issue, we proposed a new transformation method in the TAC ADR detection challenge and converted it into a sequence labeling problem [ ]. Here we extend this approach to make it generalizable for any types of clinical concepts of interests.\n   \nAn illustration of the concept-focused sequence (CFS) transformation, where each separate sequence encodes all attributes for each target concept (Disorder) \n  \n\nTaking an example of disorder-modifier extraction task (as shown in Fig.  ), one sentence may have multiple target concepts (i.e., disorders) mentioned. In this case, we will produce multiple training samples (named \u201cconcept-focused sequences\u201d - CFS) from the same sentence - one for each target concept. For each CFS, attributes that are associated with the target concept are labeled using the BIO scheme (the Beginning, Inside, or Outside of a named entity). For the example in Fig.  , there are two disorder concepts: \u201cenlarged R kidney\u201d and \u201cair fluid level\u201d, each of which will generate a CFS for training. In the CFS for \u201cenlarged R kidney\u201d, only attributes that are associated with it (i.e., \u201cmarkedly\u201d and \u201cR kidney\u201d) are labeled with B or I tags. Attributes associated with \u201cair fluid level\u201d (i.e., \u201cno\u201d and \u201csmall bowel\u201d) are labeled with the O tag in the CFS of \u201cenlarged R kidney\u201d. With such a transformation, the task is to label a CFS to identify attributes associated with a known target concept. \n\nTo model the target concept information alongside a CFS, we slightly modified the Bi-LSTM-CRF architecture, by concatenating the vector representations of the target concept with the vector representations of individual words. We used \u201cTarget\u201d and \u201cNotTarget\u201d tags to distinguish the target concept from other non-target concepts and embeddings of each tag was randomly initialized and learned directly from the data during the training of the model. \n\n\n### Experiments and evaluation \n  \nInitial experiments showed that pre-trained word embeddings did not improve overall performance much. Therefore, we initialized our word embeddings lookup table randomly in all our experiments. In the sequence labeling approach, the dimension of the semantic tag embeddings for target concept was set to 10. Tuning this dimension did not significantly affect model performance. For both methods, their Bi-LSTM-CRF models used the same parameters: a word embedding size of 50; a character embedding size of 25; a word-level hidden LSTM layer size of 100 and a character-level hidden LSTM layer size of 25; stochastic gradient descent with a learning rate of 0.005; dropout with a probability of 0.5. \n\nOur evaluation is based on correctness in assigning attribute mentions to the given medical concepts. Here, we use the standard precision (P), recall (R) and F-measure under strict criteria as our evaluation metrics. We align the gold standard and the system output using the given concepts (name and offset). Note that in these results, an attribute mention associated with multiple concepts will be calculated multiple times - this differs slightly from traditional NER tasks, in which entities can only be calculated once. We also adopt accuracy (Acc) to evaluate the ability of detecting specific attribute (including null) on concept level, defined as:\n \n\nWhere, N is the total number of gold standard concepts, N  is the number of concepts, and attributes are strictly matched. For each task, we conducted 10-fold cross validation and reported micro-averages for each attribute type. \n\nWe evaluated our system without the use of external data or knowledge bases. The attributes we have explored are not interchangeable in their meanings or linguistic patterns (e.g., compare concept negation to medication reason). So external data sources would have inconsistent effects on the task, and the generalizability of our methods would be less clear. Thus, we use only features that are learned directly from the data in our experiments. \n\n\n\n## Results \n  \nTables\u00a0 ,   and   show our results on attribute detection for disorders, medications, and lab tests, respectively. On the three datasets, the proposed sequence labeling approach using Bi-LSTM-CRF model greatly outperformed the traditional two-step approaches. On the detection of disorders attributes, as shown in Table\u00a0 , the F1 scores for COU and UNC detection were much lower than other attributes. On medication attribute detection, compared to the baseline systems, the sequence labeling approach achieved lower F-scores but higher accuracy on FRE, DUR and REA detection. The VAL attribute detection for lab tests was the easiest task, and the sequence labeling approach achieved an F1 of 0.9554. We show the state-of-the-art Usyd system [ ] for reference, though it is unfair to compare our system with USyd directly, since our system takes gold medications as inputs while USyd was an end-to-end system and trained with extra annotated corpora.\n   \nThe overall performance of different approaches on the share-disorder dataset in detecting 7 attributes of given disorders: negation (neg), subject (sub), conditional (con), severity (sev), course (cou), uncertainty (unc), body location (bdl). best results are shown in boldface \n    \nThe overall performance of different approaches on the i2b2-medication dataset in detecting 5 attributes of given medications: dosage (dos), mode (mod), frequency (fre), duration (dur), reason (rea). best results are shown in boldface \n    \nThe overall performance of different approaches on the i2b2-labtest dataset in detecting values (val) of given lab tests. Best results are shown in boldface \n  \n\n\n## Discussion \n  \nIn this paper, we investigated a sequence-labeling based approach for detecting various attributes of different medical concepts. The proposed approach transforms the attribute detection of given concepts into a sequence-labeling problem and adopts a neural architecture that combined bidirectional LSTMs and CRF as sequence labeling algorithm. It recognizes attribute entities and classifies their relations with the target concept in one-step. The experiments on three attribute detection tasks show good performance of our proposed method. \n\nA few specific types of attributes appear to be particularly difficult to detect; for example, the F1 of disorder uncertainties (UNC), medication durations (DUR), and medication reasons (REA) were all lower than 0.6. This could be due to diversity of the surface forms and low frequency of these attributes in our datasets. For example, in the i2b2-Medication dataset, there are 259 DUR entities in total, which is relatively small for training a machine learning model to recognize named entities without extra knowledge. In addition, we found that the data for the REA and DUR attribute relation classifiers were heavily biased towards positive samples. This bias may make the binary classifiers tend to relate the given medication with the detected DUR or REA attribute entities. \n\nFor each of the 13 attributes in Tables 3, 4 and 5, we randomly selected ten errors by our system for analysis. After manually checking these 130 errors, we classified the errors into the following five types: 1) Matching partially (26/130): the boundaries of the attribute entity do not perfectly match. 2) Relating with wrong target concept (21/130): the error where the system recognized an attribute entity and related it with wrong target concept. 3) Missing one of attribute cues (5/130): the attribute of the target concept has more than one cue. However, the system only finds one of them. 4) Annotation errors (13/130). 5) Other diverse, but unclear reasons, including unseen samples (65/130). For example, \u201cprecath\u201d is not extracted as a MOD from the sentence \u201c[Mucomyst] medication precath with good effect\u201d. A potential reason may be that the use of \u201cprecath\u201d is unusual. Table\u00a0  lists examples for each type of errors.\n   \nExamples of attribute detection errors \n  \n\nThis study has several limitations. First, our Bi-LSTM-CRF system was not fully optimized for the problem setting. For example, we did not use pretrained embeddings or external knowledge bases and we did not consider alternative deep learning architectures. In the future we will investigate existing domain knowledges and integrate them as features into our models to further reduce recognition errors discussed in the error analysis. Moreover, as contextual language representation has achieved many successes in NLP tasks [ ,  ], we will explore the usage of novel contextual word embeddings to replace randomly initialized word embeddings and pre-train them with external clinical corpora. Second, while we did achieve state-of-the-art performance on all three tasks, the generalizability of our approaches need further validation, as data sources used here were limited to a single corpus for each type of concept-attribute. Furthermore, we also suffered from the lack of sufficient annotated data for specific types of attributes, thus optimal performance was not achieved. \n\n\n## Conclusions \n  \nIn this study, we proposed a sequence-labeling based approach for detecting attributes of different medical concepts, which recognizes attribute entities and classifies their relations with the target concept in one step. Our experimental results show that the proposed technique is highly effective. This study demonstrates the efficacy of our sequence labeling approach using Bi-LSTM-CRFs on the attribute detection task. The proposed deep learning-based architecture provides a simple unified solution for detecting attributes for given concepts without using any external data or knowledge bases, thus streamlining applications in practical clinical NLP systems. \n\n \n", "metadata": {"pmcid": 6894107, "text_md5": "79295722d4935604cf06aebff309f688", "field_positions": {"authors": [0, 133], "journal": [134, 158], "publication_year": [160, 164], "title": [175, 290], "keywords": [304, 370], "abstract": [383, 1682], "body": [1691, 23469]}, "batch": 1, "pmid": 31801529, "doi": "10.1186/s12911-019-0937-2", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6894107", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6894107"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6894107\">6894107</a>", "list_title": "PMC6894107  Applying a deep learning-based sequence labeling approach to detect attributes of medical concepts in clinical text"}
{"text": "Bauer, Chris and Herwig, Ralf and Lienhard, Matthias and Prasse, Paul and Scheffer, Tobias and Schuchhardt, Johannes\nJ Transl Med, 2021\n\n# Title\n\nLarge-scale literature mining to assess the relation between anti-cancer drugs and cancer types\n\n# Keywords\n\nLiterature mining\nAnti-cancer drugs\nTumor types\nWord embeddings\nDatabase\n\n\n# Abstract\n \n## Background \n  \nThere is a huge body of scientific literature describing the relation between tumor types and anti-cancer drugs. The vast amount of scientific literature makes it impossible for researchers and physicians to extract all relevant information manually. \n\n\n## Methods \n  \nIn order to cope with the large amount of literature we applied an automated text mining approach to assess the relations between 30 most frequent cancer types and 270 anti-cancer drugs. We applied two different approaches, a classical text mining based on named entity recognition and an AI-based approach employing word embeddings. The consistency of literature mining results was validated with 3 independent methods: first, using data from FDA approvals, second, using experimentally measured IC-50 cell line data and third, using clinical patient survival data. \n\n\n## Results \n  \nWe demonstrated that the automated text mining was able to successfully assess the relation between cancer types and anti-cancer drugs. All validation methods showed a good correspondence between the results from literature mining and independent confirmatory approaches. The relation between most frequent cancer types and drugs employed for their treatment were visualized in a large heatmap. All results are accessible in an interactive web-based knowledge base using the following link:  . \n\n\n## Conclusions \n  \nOur approach is able to assess the relations between compounds and cancer types in an automated manner. Both, cancer types and compounds could be grouped into different clusters. Researchers can use the interactive knowledge base to inspect the presented results and follow their own research questions, for example the identification of novel indication areas for known drugs. \n\n\n## Supplementary Information \n  \nThe online version contains supplementary material available at 10.1186/s12967-021-02941-z. \n\n \n\n# Body\n \n## Background \n  \nCancer is one of the leading causes of mortality with an estimated number of 18.1M cases and 9.6M deaths in 2018 [ ]. For the chemotherapeutic or targeted treatment of cancer, there is a large number of anti-cancer drugs available. 156 anti-cancer drugs were approved by the FDA (from 1989 to 2017) [ ] but there are many more potential anti-cancer drugs. The NIH website  lists currently more than 600 drugs (generic names and brand names) approved for anti-cancer theraphy. There is a huge number of scientific publications describing the relation between tumor types and anti-cancer drugs (e.g. the effectiveness of a drug for a tumor type). The vast amount of literature makes it impossible for a human to extract all relevant information, even for a specific topic (e.g. search term \u2018breast cancer\u2019 results in 258K publications-on August 14, 2019). A second problem arises from inequality of attention a publication receives. While cited articles are receiving more attention and in turn more citations, approximately 15% will most likely never be cited [ ]. \n\nMost of the scientific knowledge is published as unstructured text. Although these texts are human readable they are not per se machine-interpretable. There are two different kinds of widely used approaches to transform text into machine-interpretable data. \n\nFirst: classical text mining tools aim to recognize certain types of entities (e.g. genes/proteins [ ,  ] or compounds [ ,  ]). In addition to specific entities, some tools are also trying to extract relations between those entities e.g. protein\u2013protein interactions [ ,  ] or relations between genes and miRNAs [ ]. These tools are typically using prior knowledge about entities such as synonyms, trivial names, or ontologies as well as knowledge about semantics (e.g. keywords that imply a certain relation between entities [ ]). Other approaches are extracting less clearly defined events such as CHAT, a text mining tool to visualize cancer hallmarks [ ]. There are also approaches to integrate several text mining tools in a standardized way, such as iTextMine which offers a web interface to search for relations between genes, miRNAs, diseases, or drugs [ ]. \n\nSecond: Word embeddings are designed to transform the text into machine interpretable numeric vectors. They allow to identify similar words by comparing the word vectors. The two most common implementations are Global Vectors for Word Representation (Glove) [ ] and Word2Vec [ ]. To learn the word embeddings a large amount of text is required. This large amount of text makes it difficult to analyze rare words. For better learning rare words, approaches with a sub-word embedding model have been proposed [ ]. Word embeddings have been applied in material science to demonstrate that an unsupervised method can recommend materials for functional applications several years before their discovery [ ]. There are also biomedical application, e.g. for combining sub-word information and biomedical controlled vocabulary (MeSH) [ ]. However, there is no consistent global ranking of word embeddings for all downstream biomedical natural language processing applications [ ]. Also a comparison between the two approaches (classical text mining vs. word embeddings) is difficult as they are quite different in nature. \n\nIn this manuscript, we aim to automatically assess the relations between the 30 most frequent cancer types and 270 anti-cancer drugs using biomedical publications. We apply two different text mining strategies, first a supervised approach with classical text mining strategies and second an unsupervised approach based on the calculation of word embeddings. To assess the relation between cancer types and anti-cancer drugs we downloaded and analyzed almost 4 million publications (abstracts from PubMed). We demonstrate that an automated text mining is able to assess the relation between cancer types and anti-cancer drugs. The relevance of the extracted relations was confirmed using three independent methods: First, by the comparison with FDA approval information, second, by using IC-50 values from a huge experimental dataset of compounds and cancer cell lines [ ] and third, by the comparison with clinical survival data. To our knowledge this is the first large scale application of literature mining to characterize the relation between compounds and cancer types. \n\nIn addition, we make the results of the analyses accessible in an interactive web-based knowledge base. The knowledge base enables researchers and clinicians to investigate relations between a certain combination of cancer type and compound very efficiently. We are not aware of other available tools with a similar scope. \n\n\n## Methods \n  \n### Selection of publications \n  \nFor retrieving cancer type related publications we selected the 30 most frequent cancer types (world wide for both sexes) according to the World Cancer Research Fund International . For each cancer type we manually defined a list of synonyms (e.g. for stomach cancer: \u2018stomach cancer\u2019, \u2018cancer of the stomach\u2019 or \u2019gastric cancer\u2019). The full list is provided as Additional file  . These synonyms were then used to build a PubMed query by concatenating the synonyms with a logical \u2018OR\u2019. The abstracts of the corresponding publication where downloaded as XML-files using the R package easyPubMed (version 2.11). For the 30 tumor types we downloaded abstracts from a total of   2.4M publications. \n\nFor retrieving compound related publications we selected a set of 266 compounds from cancerrxgene archive (Genomics of Drug Sensitivity in Cancer) [ ] (RRID:SCR_011956). Furthermore we included the 24 compounds used in the Cancer Cell Line Encyclopedia (CCEL) [ ] (RRID:SCR_013836). This resulted in a total number of 270 compounds. For each compound we retrieved all known synonyms by using the PubChem web service [ ] (RRID:SCR_004284). We tested each synonym for any results from the PubMed database (RRID:SCR_004846) and removed synonyms if they do not show any result. All synonyms for which we found any publications were then used to build a query by concatenating the synonyms with a logical \u2019OR\u2019. The abstracts of the corresponding publications where downloaded as XML-files using R package easyPubMed. For the 270 compounds we downloaded a total of   1.3M publication abstracts. \n\nFurthermore we downloaded all abstracts as XML files from 1996 to 2019 using the PubMed web interface. This reference set is used to calculate the significance of a set of publications based on the Fisher test (detailed examples of the Fisher test are given later for the extraction of genes and the relation between compounds and tumor types). The reference set includes more than 30M publications. \n\nPlease note: The keyword search was performed using a PubMed query (with standard settings). As an effect the keywords must not explicitly occur in the title or abstract since PubMed used Automatic Term Mapping (ATM). E.g. the search for \u2019cervical cancer\u2019 results in the following detailed query: \u201cuterine cervical neoplasms\u201d[MeSH Terms] OR (\u201cuterine\u201d[All Fields] AND \u201ccervical\u201d[All Fields] AND \u201cneoplasms\u201d[All Fields]) OR \u201cuterine cervical neoplasms\u201d[All Fields] OR (\u201ccervical\u201d[All Fields] AND \u201ccancer\u201d[All Fields]) OR \u201ccervical cancer\u201d[All Fields]. \n\n\n### Extraction of genes/proteins from literature \n  \nTo extract genes/proteins from literature is to apply an algorithm for named entity recognition (NER). For gene/protein NER and normalization we used the GNAT library [ ] (version 1.22) with all available filters (see also [ ] for a comparions of GNAT and GNormPlus). In literature, there are omni-present genes/proteins occuring in many publications without a specific relation (e.g. Albumin). Since we are not interested in omni-present proteins, we perform an over-representation analysis in order to assess if a gene is specific for a given context (e.g. if a gene is significantly over-represented in a set of publications for a compound or cancer type). For assessing the significance of an association we calculate a Fisher test p-value as well as an odds ratio. With this procedure we extract a ranked list of specific genes/proteins associated with a set of publications. \n\nThe calculation of the Fisher test p-value is demonstrated on the example of breast cancer and the two genes/proteins: \u2019Albumin\u2019 (ALB) and \u2019Breast cancer type 1 susceptibility protein\u2019 (BRCA1). For breast cancer we extracted   387K publications,   95K of which containing any genes/proteins. Albumin was found in 817 publications, BRCA1 was found in 5912 publications. As reference set we used   30M publications, 2.8M of which containing any genes/proteins. Albumin was found in 55K publications, BRCA1 was found in 9357 publications. This corresponds to a p-value of 1 for the Albumin (not specific for breast cancer) and   (highly specific for breast cancer) for BRCA1 (see contingency tables below).     \n\n\n### Assessing the relation between compounds and tumor types \n  \nTo calculate the significance of the association between a compound and a cancer type we use the Fisher test and perform an over-representation analysis. E.g. for breast cancer or doxorubicin we found 378K publications, 10K of which were found for both. The significance is calculated by comparing to all   30M publications and is highly significant  . \n\nThe significance for the overlap of genes is calculated in the same way. For breast cancer or doxorubicin we found 759 genes in all publications, 95 genes were found for both. The significance is calculated by comparing to all   15K genes found in all   30M publications and is highly significant  .   \n  \n\n### Identification of co-occuring compounds \n  \nIn order to identify co-occuring compounds for a tumor type, we are using all publications for the corresponding tumor type. For all pairs of compounds we are than using the Fisher test to assess if the co-occurrence of the compounds is statistically significant. The calculation of the statistics is analogously to the extraction of genes and the relation between compounds and tumor types. \n\nThe list of co-occuring compounds for a tumor type is than used to build a co-occuring network graph. To this end we are using all pairs of compounds with a   p-value   and generate the co-occurrence graph. \n\n\n### Word embeddings \n  \nFor the calculation of word embeddings we used the publications downloaded for all tumor types and all compounds. The most common way to calculate numerical vectors for text representation is to use neural networks as implemented in Word2Vec [ ]. For the calculation of word embeddings we used the Deeplearning4J (DL4J) platform (version 1.0.0-beta4) [ ]. We used the following set of parameters: minimal word frequency = 1 (consider all words); layer size = 200 (length of the word vector); window size = 500 (approximately the number of words per abstract). To calculate the similarity between compounds and cancers we use the cosine distance. \n\n#### Text preprocessing \n  \nTypically word embeddings are working on single words. They are used to identify similar words or synonyms. We want to apply word embeddings to assess similarity between compounds and cancer types (including the known synonyms for both). To this end we replaced all relevant synonyms with a specific compound or cancer ID during the extraction of sentences. E.g.: the phrase   \u2019liver cancer were treated with the same procedure employing 5-FU, mitomycin C, adriamycin\u2019   is transformed to   \u2019cancerlivercancer were treated with the same procedure employing 5-fluorouracil mitomycin-c doxorubicin\u2019  . Note: We used \u2019cancerlivercancer\u2019 as an internal ID for all synonyms of liver cancer for the calculation of word embeddings. \n\n\n\n### Validation \n  \n#### FDA approval information \n  \nIn order to get information about FDA (U.S. Food and Drug Administration, RRID:SCR_012945) approval of a drug for a certain cancer type we used the list published by Sun et al. [ ]. The drug annotations from this list where mapped to the drug identifiers used in our analyses based on the information from PubChem web service [ ] (RRID:SCR_004284). \n\n\n#### IC-50 values \n  \nIC-50 values for a compound and a tumor type were extracted from the cancerrxgene dataset [ ] (Genomics of Drug Sensitivity in Cancer, RRID:SCR_011956). To this end, cell lines where mapped manually to tumor types using the GDSC labels. The final IC-50 value is calculated as the 10% quantile of all corresponding IC-50 values. The 10% quantile is chosen in order to select a low IC-50 value of all cell lines representing the same tumor type (instead of using a minimal value which could lead to a certain instability of the results). \n\n\n#### Survival data \n  \nPatient survival data for different tumor types were downloaded from Broad Institute (RRID:SCR_007073)  . We downloaded merged clinical datasets, containing information about survival times and medication for a larger number of patients for each tumor type. The drugs annotations from the clinical datasets was mapped to the drug identifiers used in our analyses via PubChem web service [ ] (RRID:SCR_004284). \n\n\n\n### Statistics \n  \nAll statistics were calculated in R (version 3.5.1) (R Project for Statistical Computing, RRID:SCR_001905). Significance of overlaps for sets of publications or sets of genes was calculated with Fisher\u2019s exact test (see above for an example). P-values from Fisher\u2019s test were transformed to   p-values. Kaplan-Meier curves were calculated with the R packages: \u2019survival\u2019 (version 2.42-6) and \u2019rms\u2019 (version 5.1-2). Survival curves were compared with Cox proportional hazard regression. P-values of Cox regression are calculated using a log-likelihood test. \n\n\n\n## Results \n  \n### Relation between tumor types and compounds \n  \n#### Common publications \n  \nWe investigate the relation between 270 compounds and 30 cancer types by assessing the significance of the overlap of common publications compared to the reference. For each combination of compound and cancer-type the significance is calculated with Fisher\u2019s exact test as described in the Methods section. Based on this data we calculate a comprehensive heatmap visualizing the relation between compounds, between cancer types as well as the mutual correspondence as reflected by the literature (see Fig.  ).   \nHeatmap overlap of publications. Heatmap visualizing the significance of the overlap between compounds (y-axis) and cancer types (x-axis). The overlap is assessed based on the number of common publications in comparison to the total number of publications (  p-value of Fisher\u2019s test truncated at 100). Compounds showing no significant association to any of the tumor types are not shown \n  \n\nThis map facilitates to assess the structure in the space of cancer types and compounds. In general, the anti-cancer drugs can be divided in two different groups: On one side there are compounds which are very significantly related to many different (but not all) tumor types such as doxorubicin or 5-fluorouracil. On the other side some compounds are very specifically related to a single cancer type. E.g. nilotinib is exclusively related to leukemia or rucaparib is almost exclusively related to ovary cancer. These two groups typically reflect the different target pathways of the compounds. 13 compounds show very high association ( p-value   100) to more than 5 tumor types. These compounds mainly target very general pathways such as \u2019DNA replication\u2019 (8 compounds) or \u2019Mitosis\u2019 (3 compounds). On the other side 47 compounds are highly associated to one or two tumor types. These compounds target much more specific (signaling) pathways such as \u2019RTK signaling\u2019 (9 compounds), \u2019ERK MAPK signaling\u2019 (7 compounds) or PI3K/MTOR signaling (4 compounds). \n\nA similar grouping also applies for the tumor types. Cancer of pancreas, colon and stomach seem to be similar with respect to the compounds. Although lung and breast cancer have a large list of common compounds, they also show some distinct associations to drugs (e.g. alectinib for lung cancer-approved for treatment of non small cell lung cancer by the FDA on 2017-11-06 or palbociclib in breast cancer [ ]). Especially leukemia has a large number of specific compounds. \n\n\n#### Common molecular descriptor (genes/proteins) \n  \nAs a variation of the previous approach, we are now adding further knowledge about molecular background such as involved genes/proteins. We perform a similar analysis but focusing on the genes and proteins mentioned in the corresponding publications. For each compound and for each cancer type, we calculate a list of specific genes/proteins (see Methods section). For targeted compounds the top hit is typically the target itself, e.g. for erlotinib (tyrosine kinase inhibitor, acting on the epidermal growth factor receptor) we found a total of 2570 publications containing any gene names, 2164 (84%) of which containing the gene/protein EGFR. As a second example, for ruxolitinib (janus kinase inhibitor) we found a total of 499 publications containing any gene names, 345 (69%) of which containing the gene/protein JAK1 and 191 (38$) containing the gene/protein JAK2. The association between compound and cancer-type is assessed by calculating the significance of the overlap of specific genes (again using Fisher\u2019s test). The association is significant if the overlap of genes extracted from compound related publications and genes extracted from the cancer-type related publications is high compared to the reference of all publications. (see Additional file  : Figure S1 for the corresponding heatmap). \n\nThe number of significant associations is much smaller compared to the previous analysis. The main reason is that less than 20% of the publications contain genes or proteins. However, the   p-values of both approaches are nicely correlated with a Pearson correlation coefficient of  . Especially the clustering of cancer types is very similar with an adjusted Rand index of  . The adjusted Rand index of the compounds is  . Again the compounds doxorubicin and 5-fluorouracil are generic and related to many tumor types while nilotinib is only related to leukemia. \n\n\n#### Word embeddings \n  \nIn contrast to the two previous approaches word embeddings represent an unsupervised approach. Word vectors are trained using all publications of any cancer and any compound (2.4M + 1.3M) together. Distance matrix of compounds and cancer types is calculated using the cosine similarity of the corresponding word vectors. A high cosine similarity reflects a similar orientation of the corresponding word vectors and does not necessarily mean a direct relation. \n\nThe results from unsupervised word embeddings are slightly different from the previous results. The Pearson correlation coefficient between cosine similarity and the   p-values from the overlap of publications is only   (see Additional file  : Figure S2 for the corresponding heatmap). The clustering of cancer types is quite different with an adjusted Rand index of  . Also the clustering of compounds is different with an adjusted Rand index of  . \n\n\n\n### Validation \n  \nThe primary goal of this validation is to demonstrate that relations between tumor types and compounds extracted by the automatic literature mining are confirmed by different independent sources. In addition we will use the three validation strategies to compare the different literature mining approaches. \n\n#### Comparison with drug approval information \n  \nAs a first independent validation, we compare the literature results with data from FDA approval. We assume combinations of tumor types and drugs that are approved by the FDA to be often reported in the literature and thus, to result in a significant association. And indeed, the vast majority of the combinations that are approved by the FDA shows a very significant support from the literature. 85% of the approved combinations show very high support from the literature (p-value   for the overlap of publications). The other way around, almost 50% of the combinations of drugs and compounds with very high literature support are approved by the FDA. \n\nIn order to compare the three methods we calculated a single value ROC curve for a potential prediction of the FDA approval. Considering the overlap of publications, we receive an AUC of   for the prediction of FDA approval by literature results (see Fig.  ). For the other two methods (overlap of genes and word embeddings) AUCs are  .   \nROC curves FDA approval. Single value ROC curves comparing the scores ( p-value or cosine similarity) from the three approaches and the FDA approval data (prediction of approval) \n  \n\nOnly a very few FDA-approved combinations of drug and cancer-types show a low support from literature, e.g. imatinib and stomach cancer (We found only 47 publication for this combination while there are more than 8000 for imatinib and leukemia). When looking at the corresponding IC-50 values for stomach cancer cell lines we also see rather low support for this combination (lowest log IC-50 is 2 for imatinib and cell line SNU-1). \n\nSee Additional file  : Figure S3 for a heatmap limited to the entities where we have information about FDA approval with highlighted entries. \n\n\n#### IC-50 values \n  \nAs a second validation approach, we compare the significances of the relations between tumor types and compounds from literature analysis with the experimentally determined IC-50 values. We assume that a significant relation between a tumor type and a compound obtained by the literature data reflects a higher sensitivity of the tumor type to the compound and thus shows a lower IC-50 value. Especially for the first two methods (overlap of publications or genes), there is a high difference between the IC-50 values: IC-50 values for pairs of compounds and tumor types with a low association (p-value  ) are   4-6 fold higher compared to the group with high association (p-value between   and  ) (see Fig.   for a boxplot of the IC-50 values). When considering the overlap of publications there is no further difference of the IC-50 value between a high (p-value between   and  ) and a very high association (p-value  ). Considering significances of the relation based on overlap of genes, the difference of the IC-50 values is even higher comparing a high versus a very high association.   \nValication with IC-50 values. Left hand side: boxplot of the IC-50 values for cancer/compound combinations with low, high and very high association. For \u2019Overlap Publications\u2019 and \u2019Overlap Genes\u2019 grouping is based on   Fisher test p-values (low:  , high: between 20 and 80 and very high  ). For \u2018Word Embedding\u2019 grouping is based on cosine similarity (low:  , high: between 0.2 and 0.6 and very high  ) Right hand side: boxplot of the IC-50 values for compound Nilotinib comparing leukemia derived cell lines with the remaining cell lines \n  \n\nFor word embeddings there is no difference in IC-50 values for cancer/compound combinations between low (cosine similarity  ) and high cosine similarities (cosine similarity between 0.3 and 0.6). However, pairs of compounds and tumor types with very high cosine similarities ( ) show slightly lower IC-50 values. \n\nThe observation that nilotinib is only related to leukemia can also be confirmed using the IC-50 values (see right hand side of Fig.  ). IC-50 values of leukemia cell lines treated with nilotinib are much lower (  factor 6.5) compared to other cell lines. \n\n\n#### Comparison with patient survival data \n  \nAs a third independent validation strategies the compare the significance of the relations from the literature mining with patient survival data. To this end, clinical patient data was parsed for the annotation of survival and treatment data (see Methods). The treatment data was mapped to the used compounds and three survival curves were generated: first, patients without drug annotation, second, patients that received drugs with high literature support and third, patients that received drugs with low literature support (see Fig.   for survival curves for kidney cancer, ovary cancer, breast cancer, bladder cancer, uterine cancer and brain cancer). Please note that for many cancer types we could not generate these survival curves since we found either only a very limited number of patients with clinical or exclusively drugs with high literature support. Due to the limited number of data we restrict the analysis to the approach with overlap of publications.   \nKaplan\u2013Meier survival curve. Kaplan\u2013Meier survival curve for 6 selected tumor types (Top left: Kidney cancer, Top right: Ovary cancer, Middle left: Breast cancer, Middle right: Bladder cancer, Bottom left: Uterine cancer and Buttom right: Brain cancer). Three different survival curves are shown: noDrug: no drug was annotated for the patient, high: drugs with high literature support were annotated for the patient, low: drugs with low literature support were annotated for the patient. The p-value in the title compares the high vs. low group \n  \n\nFor the majority of cancer types where we found patients for all groups we observed lower survival rates for patients which received drugs with low literature support compared to patients receiving drugs with high literature support. For kidney cancer (0.016), lung cancer (0.008) and ovary cancer (0.04) we found significant differences between the corresponding survival curves. Patients with brain cancer show a slightly higher survival rate for drugs with low literature support at least for the first 900 days. For long term survival (> 900 days) we again observed an advantage for patients treated with drugs with high literature support. \n\n\n\n### Accessibility of results in an interactive knowledge base \n  \nThe interactive knowledge base is accessible freely using the following link: \n\n. \n\nThis resource can be used to perform in depth mining analyses using the data from this analysis. The user can select a compound and a cancer type either from a heatmap similar to Figure   or using an auto-completion search field. The results contain the following information:    \nA list of publications for the selected tumor and the selected cancer type. Occurrences of cancers, compounds and genes are highlighted for intuitive extraction of important information. Publications are linked to PubMed website. \n  \nA list of genes extracted from the set of corresponding publications. The genes are ordered according to absolute frequencies. The underlying publications can be accessed for further inspection. The gene symbols are visualized in a word cloud. \n  \nA list of protein protein interactions extracted from the set of corresponding publications. The interactions are visualized in an interactions graph using \u2018Cytoscape.js\u2019 [ ] (Cytoscape, RRID:SCR_003032). \n  \nA list of the corresponding IC-50 scores from the cell lines of the selected tumor type and the selected compound. \n  \nA list of words with the highest similarities to the selected tumor type and the selected compound retrieved from word embeddings. \n  \nA list of co-occuring compounds and the co-occurrence graph for the corresponding tumor type. \n  All lists can be downloaded for further analyses in text based format. \n\nFor example, clicking on the leftmost upper cell (5\u2019-Flourouracil and Leukaemia) retrieves 1320 publications having this co-occurrence. The tab \u2018Genes\u2019 refers to 288 cancer genes that have been found in these publications. The tab \u2018Interactions\u2019 gives us one interaction of proteins that has been reported. Further tabs show to compounds sensitivity values, word lists and related compounds. \n\n\n\n## Discussion \n  \nBy applying a large scale literature mining, we characterized the relation between 270 anti-cancer drugs and the 30 most frequent cancer types. The results are visualized in a large map depicting the relation between compounds, between cancer types as well as the mutual correspondence as reflected by the literature. \n\n### Good agreement between literature and experimental data \n  \nThe literature results were validated using three different approaches: first, by comparing with FDA approval data, second, based on experimentally measured IC-50 values and third, using clinical survival data. We observed a high agreement between the literature and the FDA drug approval. 85% of the approved combinations of drugs and cancer types showed very high support from the literature. The other way around, almost 50% of the combinations of drugs and compounds with very high literature support were approved by the FDA. The AUC of the ROC curve for a potential prediction of the FDA approval was high, up to  . Although this is a more or less expected result, it nicely shows the consistency of the results from literature mining with the clinical procedures and can be interpreted as a proof-of-principle. \n\nIn addition we were able to show that highly significant relations between a cancer type and a compound found in our literature mining analysis had lower experimentally measured IC-50 values compared to non-significant pairs of cancer types and compounds. Our analysis also showed that compounds specific for one cancer type, e.g. Nilotinib for leukemia, had low IC-50 values in these cell lines compared to cell lines for other cancer types. In other words, we have seen a good support of the literature results by experimental data. \n\nThe correspondence between the literature and experimental data might even be higher when including knowledge on molecular data such as genes or proteins. Comparing the association between cancer types and compounds based on the overlap of genes or proteins leads to a lower number of significant hits, but there is a strong support from experimental data. The lower number of significant results is mostly due to the lower number of publications, since less than 20% of the publications included genes or proteins. \n\nThe comparison of the results from literature mining with clinical survival data showed that drugs with higher literature support led to an improved survival compared to drugs with lower literature support for most tumor types. This comparison is limited by several factors: first, for some tumor types we did not find a sufficient number of patients with clinical data. Second, most of the applied drugs showed a high support from literature (see above: 85% of the approved drug/cancer type combinations showed very high support from the literature). Third, the application of non-approved drugs maybe the \u2018last hope\u2019 for the patient and thus these patients may have a very poor prognosis. Nevertheless, we see clear evidence that the literature support between drugs and cancer types correlates with the patient survival. \n\n\n### General vs. specific compounds \n  \nWe have seen that compounds can be divided in two groups: first, general compounds that are associated to many tumor types and second, specific compounds that are related to only one or two cancer types. This grouping corresponds to the mechanism of drug action. While general compounds typically target pathways that are not related to a specific tumor type but rather to growth and proliferation (\u2018DNA replication\u2019 or \u2018Mitosis\u2019), specific compounds typically target signaling pathways that are important for one type of tumor. The grouping identified with the automated literature mining very nicely corresponds to the actual targets of the compound and thus, substantiates the consistency of the retrieved relations between tumor types and compounds. \n\n\n### Identification of unexpected results \n  \nThe heatmap showing the significances of the relations between cancer types and compounds (Fig.  ) can be used to identify unexpected results. As an example there is no significant relation between brain tumors and docetaxel, while all other tumor types in neighborhood as well as other compounds in the neighborhood show very significant results. So one would assume that docetaxel might be very useful to treat brain tumors. This assumption is underlined by experiments showing very low IC-50 values for different cell lines (data not shown here - but easily accessible in the knowledge base). This combination is even approved by the FDA (see also Additional file  : Figure S3). The reason why there is almost no overlap between brain tumor and docetaxel are the physicochemical and pharmacological characteristics of the drug making the in vivo passage through blood-brain barrier extremely difficult [ ]. \n\n\n### Drug repositioning \n  \nFor each compound we searched for co-occuring compounds. This gives us for example for colon cancer 204 highly significant co-occuring pairs of drugs (with a co-occurrence   p-value  ). This information can now be used for a proposition for drug repositioning. For example, mining for the EGFR targeting drug erlotinib reveals 224 related publications suggesting a high relevance of this drug for colon cancer (  p-value  ). In contrast, afatinib, another EGFR-targeting drug, which has been used predominantly in lung cancer, has little literature relevance for colon cancer with only 37 publications (  p-value  ). Nonetheless, afatinib appears as the top related drug with erlotinib from the combined information on colon cancer. This would suggest afatinib as a potential candidate for repositioning to colon cancer and there are indeed several phase 2 and 3 studies that propose afatinib for colon cancer therapy. This suggestion is in line with the IC-50 data. Many colon cancer cell lines are very sensitive to afatinib treatment (e.g. cell line DiFi with with log IC-50 values up to \u2212 5.2). \n\n\n### Drug combinations \n  \nThe information of co-occuring compounds can also be used to search for promising drug combinations. E.g. the top co-occuring compound for 5-fluorouracil in colon cancer is the topoisomerase I inhibitor irinotecan (see also Additional file  : Figure S4; all co-occurrrence graphs are provided as Additional file  ). 5-fluorouracil together with irinotecan and folinic acid (called FOLFIRI) were introduced as the standard of care for colorectal cancer (see [ ] for a description of the drug-drug interactions). \n\nAs a second example, dabrafenib co-occurs with trametinib in different cancer types such as colon cancer or melanoma (see also Additional file  : Figure S4). Combination therapy with dabrafenib and trametinib in melanoma improves response rate, progression-free survival and overall survival when compared to dabrafenib alone [ ]. Both are targeting the ERK/MAPK signaling pathway, in particular the aberrant activation through BRAF mutations. The combination of dabrafenib and trametinib, targeting mitogen-activated extracellular signal-related kinase (MEK) which is downstream of BRAF in the MAPK pathway, was also approved by US FDA in 2014 based on increased survival over single dabrafenib monotherapy [ ]. \n\nAs a third example: the combination of bortezomib together with the immunomodulatory agents (IMiDs) such as lenalidomide has led to substantial improvement of survival rates in myeloma patients [ ]. Both both compounds show a very significant co-occurrence. \n\n\n### Comparison of the different approaches \n  \nWe used two different text-mining approaches (classical text mining vs. word embeddings), both with a similar goal: the detection of similarity of cancer types and anti-cancer drugs based on scientific literature. The classical text mining is supervised since we directly included information about cancer types and compounds. The similarities were calculated only for cancer types and compounds incorporating the prior knowledge. Word embeddings are unsupervised. The abstracts were transformed into word vectors without considering knowledge about cancer types and compounds. The similarity is calculated based on the word vectors considering all possible other words. The supervised approach leads to a better correspondance with experimental data and seems to be better suited for the analysis task. Considering all possible words as in the word vector approach probably leads to a higher noise level and thus less significant results. \n\n\n### Knowledge base for further data mining \n  \nWe provide all results as an interactive web-based knowledge base. This knowledge base is freely accessible and can be used to further inspect the presented results. Researchers can use the knowledge base to follow their own research questions. Furthermore all data can be downloaded for performing additional analyses. \n\n\n### Limitations \n  \nThe presented analysis used techniques from literature mining to investigate the relation between compounds and tumor types. It is based on already published knowledge. Due to this reason it is per se not possible to discover completely new results. However, a combination of different aspects of published knowledge might be very helpful to discover new relations and to formulate novel hypotheses. We feel that especially the feature of co-occuring compounds could indeed be used to estimate efficacy of a drug in particular in the field of drug repositioning where on the one hand literature is available for the drug (a pre-requisite for every mining challenge) and on the other hand novel indication areas are to be investigated. \n\nThe restriction to abstracts instead of full-text publications has lower effects to the first part of the analysis (comparison of compounds and tumor types based on the overlap of publications) since the actual search is performed by PubMed search engine. However, the extraction of genes and proteins may be more precise when using full text publications. Also the word embeddings may benefit from using full text publications. Nevertheless, we decided to perform this analysis on abstracts especially since the many full text publications are not openly accessible and hence, the number of usable publications is significantly higher. \n\nThe description of the tumor types we used in our analysis is very general. E.g. breast cancer is not subdivided according to the status of estrogen-receptor, progesterone-receptor, HER2 or BRCA1. Although this sub-typing is very useful for the choice of treatment the vast majority of publication abstracts does not contain information on the tumor subtypes. Therefore we decided to use the high level of description of tumor types. \n\nThe body of 270 chemicals was motivated by the GDSC database [ ] (RRID:SCR_011956). for which a rich body of data is available such as drug sensitivity information on a huge amount of human cell lines which is not generally available for every drug in drugbank. However, we plan to include more compounds in the next update of the knowledgebase tool. \n\n\n\n## Conclusions \n  \nWe demonstrated that an automated text mining is able to automatically assess the relations between the 30 most frequent cancer types and 270 anti-cancer drugs using biomedical publications (almost 4 million publications). To this end, we applied two different text mining strategies, first a supervised approach with classical text mining strategies and second an unsupervised approach based on the calculation of word embeddings. The supervised approach based on classical text mining seems to be better suited for the assessment of the relations cancer types and anti-cancer drugs. The relevance of the extracted relations was confirmed using three independent methods: First, by the comparison with FDA approval information, second, by using IC-50 values from a huge experimental dataset of compounds and cancer cell lines and third, by the comparison with clinical survival data. All three methods showed a good agreement between the results from automatic literature mining and independent validation. \n\nIn addition, we make the results of the analyses accessible in an interactive web-based knowledge base. The knowledge base enables researchers and clinicians to investigate relations between a certain combination of cancer type and compound very efficiently. All results can be exported for further in-depth analysis. \n\n\n## Supplementary Information \n  \n\n\n\n\n \n", "metadata": {"pmcid": 8236166, "text_md5": "269595246f62b54ab0f88ae98946d4f4", "field_positions": {"authors": [0, 116], "journal": [117, 129], "publication_year": [131, 135], "title": [146, 241], "keywords": [255, 328], "abstract": [341, 2240], "body": [2249, 42390]}, "batch": 1, "pmid": 34174885, "doi": "10.1186/s12967-021-02941-z", "pmc_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8236166", "efetch_url": "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=8236166"}, "display_title": "pmcid: <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8236166\">8236166</a>", "list_title": "PMC8236166  Large-scale literature mining to assess the relation between anti-cancer drugs and cancer types"}
